window.COURSE_DATA = {"language":"en","lastDownload":"2021-06-23T16:04:19Z","title":"Phase 3","modules":[{"id":16438,"name":"Topic 21: Object-Oriented Programming","status":"completed","unlockDate":null,"prereqs":[],"requirement":"all","sequential":false,"exportId":"g5dd51afde13071767fefa1cc3180c7c2","items":[{"id":153823,"title":"Object-Oriented Programming - Introduction","type":"WikiPage","indent":0,"locked":false,"requirement":"must_mark_done","completed":true,"content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-introduction-to-object-orientation\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-introduction-to-object-orientation\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-introduction-to-object-orientation/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\u003ch2\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eIn this section, you'll be introduced to the concept of Object-oriented programming (OOP) in Python. OOP has become a foundational practice in much of software development and programming, allowing developers to build upon each other's code in a fluent manner.\u003c/p\u003e\n\u003ch2\u003eObject-oriented programming (OOP)\u003c/h2\u003e\n\u003cp\u003eWhen programmers started writing code, originally they took a procedural approach. They'd write a series of sequential steps, using conditional statements and even the dreaded \u003ca href=\"https://en.wikipedia.org/wiki/Goto\"\u003eGOTO\u003c/a\u003e statements to branch their logic and their program flow. There would be a set of global variables (variables with values that anyone could change anywhere within the application) for keeping track of the \"state\" of the application.\u003c/p\u003e\n\u003cp\u003eUnfortunately, as they got bigger such programs were really hard to manage. Firstly, if they ever wanted to repeat the same business logic, they either had to copy it or loop back to where it was in the program, and secondly, it became really hard to keep track of where those variables got changed, making it really hard to reason about the program and to avoid small changes to the program breaking huge pieces of the application.\u003c/p\u003e\n\u003cp\u003eOne solution was to break the common subroutines into separate \"functions\". This was a huge step forward. Now you could write a short script, and by calling it multiple times, passing various parameters you could reuse it safely. You could also write small automated tests (unit tests) to verify the behavior of those functions so you'd know how they would behave with various types of inputs. There is a whole branch of software development devoted just to functional programming and it can be a very effective way to write and reason about complex code bases. Languages such as Haskell and Clojure are primarily \"Functional Programming languages\". They are extremely powerful but have a reputation for being a little harder for developers to learn and use and because of that are less popular than languages that are primarily object-oriented.\u003c/p\u003e\n\u003cp\u003eLanguages like Ruby and Python are often considered to be primarily \"Object-oriented\" (OO) programming languages. In part, OO programming came from the question \"Where do we put our functions and our data?\". One way to organize functions is in libraries. This is still done even in OO programming languages, so you might well have a library like \u003ccode\u003estatsmodels\u003c/code\u003e and use \u003ccode\u003eimport statsmodels.formula.api as smf\u003c/code\u003e to get access to a series of functions related to formulae.\u003c/p\u003e\n\u003cp\u003eHowever, in OO programming languages, you also get the ability to create objects. Objects are a logical bundle of functions (they're called methods if they are associated to an object) and variables (often called properties when they're associated to an object), and for many classes of programming, once you get used to them, they provide a really useful abstraction.\u003c/p\u003e\n\u003cp\u003eSo, you might have a \u003ccode\u003ePerson\u003c/code\u003e class with its code saved in a file called \u003ccode\u003eperson.py\u003c/code\u003e which describes both the properties of a person (height, weight, date of birth) and their behaviors (everything from their \u003ccode\u003efull_name()\u003c/code\u003e to their \u003ccode\u003ecurrent_age()\u003c/code\u003e ).\u003c/p\u003e\n\u003cp\u003ePython comes with a few basic built-in objects to get us started, things like \u003ccode\u003eint\u003c/code\u003e for integer, \u003ccode\u003estr\u003c/code\u003e for string, \u003ccode\u003elist\u003c/code\u003e for list, etc. We call these base types of objects \"Primitives.\" Primitives already have methods we can call on them, for example: \u003ccode\u003e.title()\u003c/code\u003e for a string. But what if we wanted to create a new type of object in our programming universe, a new kind of object for our code? With Object-oriented programming, we can do just that by using the \u003ccode\u003eclass\u003c/code\u003e keyword!\u003c/p\u003e\n\u003ch2\u003eClasses and methods\u003c/h2\u003e\n\u003cp\u003eIn this section, you'll learn a lot about classes. A Python class can be thought of as the blueprint for creating a code object. These objects are known as an instance objects. Since nearly everything in Python is an object, understanding how to work with objects is critical to developing strong Python skills. You'll also learn about instance methods which are analogous to functions but are \"bound\" to instance objects. Don't worry if this sounds confusing right now, you'll have plenty of opportunities to explore classes and methods in this section.\u003c/p\u003e\n\u003ch2\u003eSummary\u003c/h2\u003e\n\u003cp\u003eObject-oriented programming (OOP) is a way of organizing your code that can make many types of applications easier to write by combining related variables/properties and functions/methods into objects containing both behavior and state.\u003c/p\u003e","exportId":"object-oriented-programming-introduction"},{"id":153828,"title":"Classes and Instances","type":"Assignment","indent":0,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_mark_done","completed":true,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-classes-and-instances\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-classes-and-instances/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","exportId":"ge6e90d2504f02461b5762a851c0580c8"},{"id":153834,"title":"Classes and Instances - Lab","type":"Assignment","indent":1,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_mark_done","completed":true,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-classes-and-instances-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-classes-and-instances-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","exportId":"gf587d4088978e189faff23320a25aad4"},{"id":153841,"title":"Instance Methods","type":"Assignment","indent":0,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_mark_done","completed":true,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-instance-methods\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-instance-methods/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","exportId":"g416e8b49941bbf47fd5ac89857046570"},{"id":153845,"title":"Instance Methods - Lab","type":"Assignment","indent":1,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_mark_done","completed":true,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-instance-methods-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-instance-methods-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","exportId":"g411e53067859cfc82a917fe8970317e5"},{"id":153851,"title":"Instance Variables - Lab","type":"Assignment","indent":1,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_mark_done","completed":true,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-instance-variables-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-instance-variables-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","exportId":"g1b96a50ee73471e91e73a34dd33376e9"},{"id":153856,"title":"Object-Oriented Programming - Recap","type":"WikiPage","indent":0,"locked":false,"requirement":"must_mark_done","completed":true,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-oop-recap-v2-1\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-oop-recap-v2-1/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eIn this section, you learned about Object-oriented programming (OOP) as a foundational practice for software development and programming.\u003c/p\u003e\n\u003ch2\u003eOOP overview\u003c/h2\u003e\n\u003cp\u003eYou now know all about OOP and how to define classes. Like functions, using classes in your programming can save you a lot of time by eliminating repetitive tasks. Classes go further than functions by allowing you to persist data. After all, class methods are fairly analogous to functions, while attributes add functionality by acting as data storage containers.\u003c/p\u003e\n\u003ch2\u003eClass structure\u003c/h2\u003e\n\u003cp\u003eAs you saw, the most basic class definition starts off with:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"python\"\u003eclass ClassName:\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eFrom there, you then saw how you can further define class methods:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"python\"\u003eclass ClassName:\n    def method_1(self):\n        pass # Ideally a method does something, but you get the point\n    def method_2(self):\n        print('This is a pretty useless second method.')\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eYou also learned about \u003ccode\u003eself\u003c/code\u003e. Specifically, that \u003ccode\u003eself\u003c/code\u003e is the default parameter used to define methods. This is necessary since instance methods implicitly pass in the object itself as an argument during execution.\u003c/p\u003e\n\u003ch2\u003eCreating instances\u003c/h2\u003e\n\u003cp\u003eRecall that after you define a class, you can then create instances of that class to bring it to life and use it! You're probably wondering what all of this has to do with data science. In turns out you'll use OOP principles when you start working with common data science libraries. For example, you might import the \u003ccode\u003eLinearRegression\u003c/code\u003e class from the scikit-learn library in order to create a regression model!\u003c/p\u003e\n\u003cp\u003eRemember, creating an instance of a class would look like this:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"python\"\u003efrom sklearn.linear_model import LinearRegression() \n\nreg = LinearRegression() \n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eOnce you create an instance object of the class, you can then use all the methods associated with that class!\u003c/p\u003e\n\u003ch2\u003eLevel up\u003c/h2\u003e\n\u003cp\u003eIf you would like to dive deeper into OOP and learn some advanced topics, you can check out the additional OOP lessons and labs in the Appendix.\u003c/p\u003e\n\u003ch2\u003eSummary\u003c/h2\u003e\n\u003cp\u003eCongrats, you now have a solid foundation in OOP! You first learned how to define a class and methods. Then you learned how to create an instance of a class and define instance attributes. These skills will be useful for collaboration and writing concise, modular code!\u003c/p\u003e","exportId":"object-oriented-programming-recap"}]},{"id":16484,"name":"APPENDIX: More OOP","status":"completed","unlockDate":null,"prereqs":[],"requirement":null,"sequential":false,"exportId":"gedc2a09d3e824e79c86c4652780980a0","items":[{"id":154136,"title":" Deeper Dive into \"self\"","type":"Assignment","indent":0,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":null,"completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-understanding-self\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-understanding-self/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","exportId":"gcd9ac0d2c29d37405f643e362e213e27"},{"id":154137,"title":"Object Attributes - Lab","type":"Assignment","indent":1,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":null,"completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-object-attributes-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-object-attributes-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","exportId":"g8388d6bfa6485f82364470c866bc74f9"},{"id":154138,"title":"Object Initialization","type":"Assignment","indent":0,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":null,"completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-object-initialization\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-object-initialization/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","exportId":"gd791ce89a33d32e127ad6502bbda4ffc"},{"id":154139,"title":"Object Initialization - Lab","type":"Assignment","indent":1,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":null,"completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-object-initialization-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-object-initialization-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","exportId":"g73eefc80499c5a2aed9e98957e2895dd"},{"id":154140,"title":"Object Oriented Attributes with Functions","type":"Assignment","indent":0,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":null,"completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-object-oriented-attributes-with-functions\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-object-oriented-attributes-with-functions/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","exportId":"gc2998173ced52b848ddc6f055e66bed8"},{"id":154141,"title":"Object Oriented Attributes With Functions - Lab","type":"Assignment","indent":1,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":null,"completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-object-oriented-attributes-with-functions-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-object-oriented-attributes-with-functions-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","exportId":"g7f35e07eb391ea34a37089b8d1254a6f"},{"id":154142,"title":"Object Oriented Shopping Cart - Lab","type":"Assignment","indent":1,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":null,"completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-object-oriented-shopping-cart-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-object-oriented-shopping-cart-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","exportId":"ge089addce181f8509e2ac7905e8c4b22"},{"id":154143,"title":"Inheritance","type":"Assignment","indent":0,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":null,"completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-inheritance\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-inheritance/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","exportId":"g8177ee3cf3c6fb4202d8eb28958b0c6d"},{"id":154144,"title":"Inheritance - Lab","type":"Assignment","indent":1,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":null,"completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-inheritance-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-inheritance-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","exportId":"g733da2d7d63ad4f2f766abbccc60d5a1"},{"id":154145,"title":"Building an Object-Oriented Simulation","type":"WikiPage","indent":0,"locked":false,"requirement":null,"completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-building-an-object-oriented-simulation\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-building-an-object-oriented-simulation/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eIn this lesson, you'll learn a bit more about best practices for running simulations in the real world using object-oriented programming!\u003c/p\u003e\n\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\n\u003cp\u003eYou will be able to:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eUse inheritance to write nonredundant code\u003c/li\u003e\n\u003cli\u003eCreate methods that calculate statistics of the attributes of an object\u003c/li\u003e\n\u003cli\u003eCreate object-oriented data models that describe the real world with multiple classes and subclasses and interaction between classes\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch2\u003eCreating Stochastic Simulations\u003c/h2\u003e\n\n\u003cp\u003eAs a capstone for everything you've learned about object-oriented programming, you'll be creating a \u003cstrong\u003e\u003cem\u003eHerd Immunity Simulation\u003c/em\u003e\u003c/strong\u003e.\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-building-an-object-oriented-simulation/master/images/herd_immunity.gif\"\u003e\u003c/p\u003e\n\n\u003cp\u003e\u003ca href=\"https://www.reddit.com/r/dataisbeautiful/comments/5v72fw/how_herd_immunity_works_oc/\"\u003egif created by u/theotheredmund\u003c/a\u003e\u003c/p\u003e\n\n\u003cp\u003eThis simulation is meant to model the effects that vaccinations have on the way a communicable disease spreads through a population. The simulation you're building depends on just a few statistics from the CDC (Center for Disease Control):\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003e\n\u003ccode\u003er0\u003c/code\u003e, the average number of people a contagious person infects before they are no longer contagious (because they got better or they died)\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003emortality_rate\u003c/code\u003e, the percentage chance a person infected with a disease will die from it \u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003eThe main workflow of this simulation is as follows:\u003c/p\u003e\n\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003eCreate a \u003ccode\u003ePerson()\u003c/code\u003e class with the following attributes:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003e\u003ccode\u003ealive\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003evaccinated\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eis_infected\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003ehas_been_infected\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003enewly_infected\u003c/code\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eCreate a \u003ccode\u003eSimulation()\u003c/code\u003e class with the following attributes:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003e\u003ccode\u003epopulation\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003evirus_name\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003enum_time_steps\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003er0\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003epercent_pop_vaccinated\u003c/code\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cp\u003eCreate methods for our \u003ccode\u003eSimulation()\u003c/code\u003e class that will cover each step of the simulation. \u003c/p\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\n\u003cp\u003eIn order for our simulation to work, you'll need to define some rules for it:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003e\u003cp\u003eEach infected person will \"interact\" with 100 random people from the \u003ccode\u003epopulation\u003c/code\u003e. If the person the infected individual interacts with is sick, vaccinated, or has had the disease before, nothing happens. However, if the person the infected individual interacts with is healthy, unvaccinated, and has not been infected yet, then that person becomes infected. \u003c/p\u003e\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eAt the end of each round, the following things happen:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eAll currently infected people either get better from the disease or die, with the chance of death corresponding to the mortality rate of the disease \u003c/li\u003e\n\u003cli\u003eAll people that were newly infected during this round become the new infected for the next round\u003cbr\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003eThe simulation continues for the set number of time steps.  Any time someone dies or gets infected, log it in a text file called \u003ccode\u003e'simulation_logs.txt'\u003c/code\u003e.  Once the simulation is over, write some code to quickly parse the text logs into data and visualize the results, so that you can run multiple simulations and answer questions like: \u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eIf vaccination rates for {disease x} dropped by 5%, how many more people become infected in an epidemic? How many more die?\u003c/li\u003e\n\u003cli\u003eWhat does the spread of {disease x} through a population look like?\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003eIf this all seems a bit daunting, don't worry! You'll be provided with much more detail as you build this step-by-step during the lab. \u003c/p\u003e\n\n\u003cp\u003eWith that, go ahead and take a look at this cool simulation lab!\u003c/p\u003e\n\n\u003ch2\u003eSummary\u003c/h2\u003e\n\n\u003cp\u003eIn this lesson, you reviewed some best practices for simulating things in the real world using object-oriented programming!\u003c/p\u003e\n\u003cfooter class=\"fis-footer\" style=\"visibility: hidden;\"\u003e\u003cdiv class=\"fis-feedback\"\u003e\n\u003ch5\u003eHow do you feel about this lesson?\u003c/h5\u003e\n\u003cimg id=\"thumbs-up\" data-repository=\"dsc-building-an-object-oriented-simulation\" title=\"Thumbs up!\" alt=\"thumbs up\"\u003e\u003cimg id=\"thumbs-down\" data-repository=\"dsc-building-an-object-oriented-simulation\" title=\"Thumbs down!\" alt=\"thumbs down\"\u003e\n\u003c/div\u003e\n\u003ch5\u003eHave specific feedback? \u003ca href=\"https://github.com/learn-co-curriculum/dsc-building-an-object-oriented-simulation/issues/new\"\u003eTell us here!\u003c/a\u003e\n\u003c/h5\u003e\u003c/footer\u003e","exportId":"building-an-object-oriented-simulation"},{"id":154146,"title":"Building an Object-Oriented Simulation - Lab","type":"Assignment","indent":1,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":null,"completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-building-an-object-oriented-simulation-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-building-an-object-oriented-simulation-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","exportId":"g1b40d2f2e5c34c44aea75f2dfbf13898"}]},{"id":16442,"name":"Topic 22: Linear Algebra","status":"started","unlockDate":null,"prereqs":[],"requirement":"all","sequential":false,"exportId":"g04749b7a4fa8b8b3be596fb12fffab9b","items":[{"id":153871,"title":"Linear Algebra - Introduction","type":"WikiPage","indent":0,"locked":false,"requirement":"must_mark_done","completed":true,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-linalg-introduction\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-linalg-introduction/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eIn this section, we're going to take a step back to learn some of the basics of linear algebra - the math that powers most machine learning models. You may not need to know linear algebra just to call a method in scikit-learn to do some modeling, but this introduction to linear algebra should give you a much better understanding of how your models are working \"under the hood\".\u003c/p\u003e\n\n\u003ch2\u003eThe importance of linear algebra\u003c/h2\u003e\n\n\u003cp\u003eWe're going to kick this section off by looking at some of the many places that linear algebra is used in machine learning - from deep learning through Natural Language Processing and dimensionality reduction techniques such as Principle Component Analysis.\u003c/p\u003e\n\n\u003ch2\u003eSystems of linear equations\u003c/h2\u003e\n\n\u003cp\u003eWe then start to dig into the math! We look at the idea of linear simultaneous equations - a set of two or more equations each of which is linear (can be plotted on a graph as a straight line). We then see how such equations can be represented as vectors or matrices to represent such systems efficiently.\u003c/p\u003e\n\n\u003ch2\u003eScalars, vectors, matrices, and tensors\u003c/h2\u003e\n\n\u003cp\u003eIn a code along, we'll introduce the concepts and concrete representations (in NumPy) of scalars, vectors, matrices, and tensors - why they are important and how to create them. \u003c/p\u003e\n\n\u003ch2\u003eVector/matrix operations\u003c/h2\u003e\n\n\u003cp\u003eWe then start to build up the basic skills required to perform matrix operations such as addition and multiplication.  You will also cover key techniques used by many machine learning models to perform their calculations covering both the Hadamard product and the (more common) dot product. \u003c/p\u003e\n\n\u003ch2\u003eSolving systems of linear equations using NumPy\u003c/h2\u003e\n\n\u003cp\u003eWe then bring the previous work together to look at how to use NumPy to solve systems of linear equations, introducing the identity and inverse matrices along the way.\u003c/p\u003e\n\n\u003ch2\u003eRegression analysis using linear algebra and NumPy\u003c/h2\u003e\n\n\u003cp\u003eHaving built up a basic mathematical and computational foundation for linear algebra, you will solve a real data problem - looking at how to use NumPy to solve a linear regression using the ordinary least squares (OLS) method.\u003c/p\u003e\n\n\u003ch2\u003eComputational complexity\u003c/h2\u003e\n\n\u003cp\u003eFinally, we look at the idea of computational complexity and Big O notation, showing why OLS is computationally inefficient, and that a gradient descent algorithm can instead be used to solve a linear regression much more efficiently.\u003c/p\u003e\n\n\u003ch2\u003eSummary\u003c/h2\u003e\n\n\u003cp\u003eLinear Algebra is so foundational to machine learning that you're going to see it referenced many times as the course progresses. In this section, the goal is to give you both a theoretical introduction and some computational practice, solving a real-life problem by writing the code required to solve a linear regression using OLS.\u003c/p\u003e\n\u003cfooter class=\"fis-footer\" style=\"visibility: hidden;\"\u003e\u003cdiv class=\"fis-feedback\"\u003e\n\u003ch5\u003eHow do you feel about this lesson?\u003c/h5\u003e\n\u003cimg id=\"thumbs-up\" data-repository=\"dsc-linalg-introduction\" title=\"Thumbs up!\" alt=\"thumbs up\"\u003e\u003cimg id=\"thumbs-down\" data-repository=\"dsc-linalg-introduction\" title=\"Thumbs down!\" alt=\"thumbs down\"\u003e\n\u003c/div\u003e\n\u003ch5\u003eHave specific feedback? \u003ca href=\"https://github.com/learn-co-curriculum/dsc-linalg-introduction/issues/new\"\u003eTell us here!\u003c/a\u003e\n\u003c/h5\u003e\u003c/footer\u003e","exportId":"linear-algebra-introduction"},{"id":153874,"title":"Motivation for Linear Algebra in Data Science","type":"WikiPage","indent":0,"locked":false,"requirement":"must_mark_done","completed":true,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-lingalg-motivation\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-lingalg-motivation/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eIn this section, you'll learn about algebra as a foundational step for data science, and later on statistics. Linear algebra is also very important when moving on to machine learning models, where a solid understanding of linear equations plays a major role. This lesson will attempt to present some motivational examples of how and why a solid foundation of linear algebra is valuable for data scientists.\u003c/p\u003e\n\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\n\u003cp\u003eYou will be able to:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eState the importance of linear algebra in the fields of data science and machine learning \u003c/li\u003e\n\u003cli\u003eDescribe the areas in AI and machine learning where linear algebra might be used for advanced analytics \u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch2\u003eBackground\u003c/h2\u003e\n\n\u003cp\u003eWhile having a deep understanding of linear algebra may not be mandatory, some basic knowledge is undoubtedly extremely helpful in your journey towards becoming a data scientist.\u003c/p\u003e\n\n\u003cp\u003eYou may already know a number of linear algebraic concepts without even knowing it. Examples are: matrix multiplication and dot-products. Later on, you'll learn more complex algebraic concepts like the calculation of matrix determinants, cross-products, and eigenvalues/eigenvectors. As a data scientist, it is important to know some of the theories as well as having a practical understanding of these concepts in a real-world setting.\u003c/p\u003e\n\n\u003ch2\u003eAn analogy\u003c/h2\u003e\n\n\u003cp\u003eThink of a simple example where you first learn about a sine function as an infinite polynomial while learning trigonometry. Students usually practice this function by passing different values to this function and getting the expected results and then manage to relate this to triangles and vertices. When learning advanced physics, students get to learn more applications of sine and other similar functions in the area of sound and light. In the domain of Signal Processing for unidimensional data, these functions pop up again to help you solve filtering, time-series related problems. An introduction to numeric computation around sine functions can not alone help you understand its wider application areas. In fact, sine functions are everywhere in the universe from music to light/sound/radio waves, from pendulum oscillations to alternating current.\u003c/p\u003e\n\n\u003ch2\u003e\u0026nbsp;Why Linear Algebra?\u003c/h2\u003e\n\n\u003cblockquote\u003e\n\u003cp\u003eLinear algebra is the branch of mathematics concerning vector spaces and linear relationships between such spaces. It includes the study of lines, planes, and subspaces, but is also concerned with properties common to all vector spaces.\u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003cp\u003eAnalogous to the example we saw above, it's important that a data scientist understands how data structures are built with vectors and matrices following the geometric intuitions from linear algebra, in addition to the numeric calculations. A data-focused understanding of linear algebra can help machine learning practitioners decide what tools can be applied to a given problem and how to interpret the results of experiments. You'll see that a good understanding of linear algebra is particularly useful in many ML/AI algorithms, especially in deep learning, where a lot of the operations happen under the hood.\u003c/p\u003e\n\n\u003cp\u003eFollowing are some of the areas where linear algebra is commonly practiced in the domain of data science and machine learning:  \u003c/p\u003e\n\n\u003ch3\u003eComputer Vision / Image Processing\u003c/h3\u003e\n\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-lingalg-motivation/master/images/rgb.png\" width=\"600\"\u003e\u003c/p\u003e\n\n\u003cp\u003eComputers are designed to process binary information only (only 0s and 1s). How can an image such as the dog shown here, with multiple attributes like color, be stored in a computer? This is achieved by storing the pixel intensities for red, blue and green colors in a matrix format. Color intensities can be coded into this matrix and can be processed further for analysis and other tasks. Any operation performed on this image would likely use some form of linear algebra with matrices as the back end.\u003c/p\u003e\n\n\u003ch3\u003eDeep Learning - Tensors\u003c/h3\u003e\n\n\u003cp\u003eDeep Learning is a sub-domain of machine learning, concerned with algorithms that can imitate the functions and structure of a biological brain as a computational algorithm. These are called artificial neural networks (ANNs). \u003c/p\u003e\n\n\u003cp\u003eThe algorithms usually store and process data in the form of mathematical entities called tensors. A tensor is often thought of as a generalized matrix. That is, it could be a 1-D matrix (a vector is actually such a tensor), a 2-D matrix (like a data frame), a 3-D matrix (something like a cube of numbers), even a 0-D matrix (a single number), or a higher dimensional structure that is harder to visualize.\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-lingalg-motivation/master/images/tensor.png\" width=\"850\"\u003e\u003c/p\u003e\n\n\u003cp\u003eAs shown in the image above where different input features are being extracted and stored as spatial locations inside a tensor which appears as a cube. A tensor encapsulates the scalar, vector, and the matrix characteristics. For deep learning, creating and processing tensors and operations that are performed on these also require knowledge of linear algebra. Don't worry if you don't fully understand this right now, you'll learn more about tensors later!\u003c/p\u003e\n\n\u003ch3\u003eNatural Language Processing\u003c/h3\u003e\n\n\u003cp\u003eNatural Language Processing (NLP) is another (very popular) area in Machine Learning dealing with text data. The most common techniques employed in NLP include BoW (Bag of Words) representation, Term Document Matrix etc. As shown in the image below, the idea is that words are being encoded as numbers and stored in a matrix format. Here, we just use 3 sentences to illustrate this:\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-lingalg-motivation/master/images/NLPmatrix.png\" width=\"650\"\u003e\u003c/p\u003e\n\n\u003cp\u003eThis is just a short example, but you can store long documents in (giant) matrices like this. Using these counts in a matrix form can help perform tasks like semantic analysis, language translation, language generation etc.\u003c/p\u003e\n\n\u003ch3\u003eDimensionality Reduction\u003c/h3\u003e\n\n\u003cp\u003eDimensionality reduction techniques, which are heavily used when dealing with big datasets, use matrices to process data in order to reduce its dimensions. Principle Component Analysis (PCA) is a widely used dimensionality reduction technique that relies solely on calculating eigenvectors and eigenvalues to identify principal components as a set of highly reduced dimensions. The picture below is an example of a three-dimensional data being mapped into two dimensions using matrix manipulations. \u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-lingalg-motivation/master/images/pca.png\" width=\"900\"\u003e\u003c/p\u003e\n\n\u003cp\u003eGreat, you now know about some key areas where linear algebra is used! In the following lessons, you'll go through an introductory series of lessons and labs that will cover basic ideas of linear algebra: an understanding of vectors and matrices with some basic operations that can be performed on these mathematical entities. We will implement these ideas in Python, in an attempt to give you the foundational knowledge to deal with these algebraic entities and their properties. These skills will be applied in advanced machine learning sections later in the course. \u003c/p\u003e\n\n\u003ch2\u003eFurther Reading\u003c/h2\u003e\n\n\u003cp\u003e\u003ca href=\"https://www.youtube.com/watch?v=_MxCXGF9N-8\"\u003eYoutube: Why Linear Algebra\u003c/a\u003e\u003c/p\u003e\n\n\u003cp\u003e\u003ca href=\"https://towardsdatascience.com/boost-your-data-sciences-skills-learn-linear-algebra-2c30fdd008cf\"\u003eBoost your data science skills. Learn linear algebra.\u003c/a\u003e\u003c/p\u003e\n\n\u003cp\u003e\u003ca href=\"https://www.quora.com/What-are-the-applications-of-linear-algebra-in-machine-learning\"\u003eQuora: Applications of Linear Algebra in Deep Learning\u003c/a\u003e\u003c/p\u003e\n\n\u003ch2\u003eSummary\u003c/h2\u003e\n\n\u003cp\u003eIn this lesson, you learned about some Data Science examples that heavily rely on linear algebra principles. You looked at some use cases in practical machine learning problems where linear algebra and matrix manipulation might come in handy. In the following lessons, you'll take a deeper dive into specific concepts in linear algebra, working your way towards solving a regression problem using linear algebraic operations only. \u003c/p\u003e\n\u003cfooter class=\"fis-footer\" style=\"visibility: hidden;\"\u003e\u003cdiv class=\"fis-feedback\"\u003e\n\u003ch5\u003eHow do you feel about this lesson?\u003c/h5\u003e\n\u003cimg id=\"thumbs-up\" data-repository=\"dsc-lingalg-motivation\" title=\"Thumbs up!\" alt=\"thumbs up\"\u003e\u003cimg id=\"thumbs-down\" data-repository=\"dsc-lingalg-motivation\" title=\"Thumbs down!\" alt=\"thumbs down\"\u003e\n\u003c/div\u003e\n\u003ch5\u003eHave specific feedback? \u003ca href=\"https://github.com/learn-co-curriculum/dsc-lingalg-motivation/issues/new\"\u003eTell us here!\u003c/a\u003e\n\u003c/h5\u003e\u003c/footer\u003e","exportId":"motivation-for-linear-algebra-in-data-science"},{"id":153877,"title":"Systems of Linear Equations","type":"WikiPage","indent":0,"locked":false,"requirement":"must_mark_done","completed":false,"content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-lingalg-linear-equations\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-lingalg-linear-equations\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-lingalg-linear-equations/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eLinear algebra is a sub-field of mathematics concerned with vectors, matrices, and linear transforms between them. The first step towards developing a good understanding of linear algebra is to get a good sense of \u003cem\u003ewhat linear mappings and linear equations\u003c/em\u003e are, \u003cem\u003ehow these relate to vectors and matrices\u003c/em\u003e and \u003cem\u003ewhat this has to do with data analysis\u003c/em\u003e. Let's try to develop a basic intuition around these ideas by first understanding what linear equations are.\u003c/p\u003e\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\u003cp\u003eYou will be able to:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eDescribe a system of linear equations for solving analytical problems\u003c/li\u003e\n\u003cli\u003eDescribe how matrices and vectors can be used to solve linear equations\u003c/li\u003e\n\u003cli\u003eSolve a system of equations using elimination and substitution\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003eWhat are linear equations?\u003c/h2\u003e\n\u003cp\u003eIn mathematics, a system of linear equations (or linear system) is a collection of two or more linear equations involving the same set of variables. For example, look at the following equations:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=3x%20%2b%202y%20-%20z%20=%200\"\u003e\u003c/li\u003e\n\u003cli\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=2x-%202y%20%2b%204z%20=%20-2\"\u003e\u003c/li\u003e\n\u003cli\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=-x%20%2b%200.5y%20-%20z%20=%200\"\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThis is a system of three equations in the three variables \u003cimg src=\"https://render.githubusercontent.com/render/math?math=x\"\u003e , \u003cimg src=\"https://render.githubusercontent.com/render/math?math=y\"\u003e , and \u003cimg src=\"https://render.githubusercontent.com/render/math?math=z\"\u003e . A solution to a linear system is an assignment of values to the variables in a way that \u003cem\u003eall the equations are simultaneously satisfied\u003c/em\u003e. A solution to the system above is given by:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=x%20=%201\"\u003e\u003c/li\u003e\n\u003cli\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=y%20=%20-8/3\"\u003e\u003c/li\u003e\n\u003cli\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=z%20=%20-7/3\"\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThese values make all three equations valid. The word \"system\" indicates that the equations are to be considered collectively, rather than individually.\u003c/p\u003e\n\u003ch2\u003eSolving linear equations\u003c/h2\u003e\n\u003cp\u003eA system of linear equations can always be expressed in a matrix form. Algebraically, both of these express the same thing. Let's work with an example to see how this works:\u003c/p\u003e\n\u003ch3\u003eExample\u003c/h3\u003e\n\u003cp\u003eLet's say you go to a market and buy 2 apples and 1 banana. For this, you end up paying 35 pence. If you denote apples by \u003cimg src=\"https://render.githubusercontent.com/render/math?math=a\"\u003e and bananas by \u003cimg src=\"https://render.githubusercontent.com/render/math?math=b\"\u003e , the relationship between items bought and the price paid can be written down as an equation - let's call it Eq. A:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=2a%20%2b%20b%20=%2035\"\u003e (Eq. A)\u003c/p\u003e\n\u003cp\u003eOn your next trip to the market, you buy 3 apples and 4 bananas, and the cost is 65 pence. Just like above, this can be written as Eq. B:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=3a%20%2b%204b%20=%2065\"\u003e (Eq. B)\u003c/p\u003e\n\u003cp\u003eThese two equations (known as a simultaneous equations) form a system that can be solved by hand for values of \u003cimg src=\"https://render.githubusercontent.com/render/math?math=a\"\u003e and \u003cimg src=\"https://render.githubusercontent.com/render/math?math=b\"\u003e i.e., price of a single apple and banana.\u003c/p\u003e\n\u003cp\u003eLet's solve this system for individual prices using a series of eliminations and substitutions:\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eStep 1:\u003c/strong\u003e Multiply Eq. A by 4\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=8a%20%2b%204b%20=%20140\"\u003e (Eq. C)\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eStep 2 :\u003c/strong\u003e Subtract Eq. B from Eq. C\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=5a%20=%2075\"\u003e which leads to \u003cimg src=\"https://render.githubusercontent.com/render/math?math=a%20=%2015\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eStep 3:\u003c/strong\u003e Substitute the value of \u003cimg src=\"https://render.githubusercontent.com/render/math?math=a\"\u003e in Eq. A\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=30%20%2b%20b%20=%2035\"\u003e which leads to \u003cimg src=\"https://render.githubusercontent.com/render/math?math=b%20=%205\"\u003e\u003c/p\u003e\n\u003cp\u003eSo the price of an apple is 15 pence and the price of the banana is 5 pence.\u003c/p\u003e\n\u003ch2\u003eFrom equations to vectors and matrices\u003c/h2\u003e\n\u003cp\u003eNow, as your number of shopping trips increase along with the number of items you buy at each trip, the system of equations will become more complex and solving a system for individual price may become very expensive in terms of time and effort. In these cases, you can use a computer to find the solution.\u003c/p\u003e\n\u003cp\u003eThe above example is a classic linear algebra problem. The numbers 2 and 1 from Eq. A and 3 and 4 from Eq. B are linear coefficients that relate input variables a and b to the known output 15 and 5.\u003c/p\u003e\n\u003cp\u003eUsing linear algebra, we can write this system of equations as shown below:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-lingalg-linear-equations/master/images/ss.png\" width=\"320\"\u003e\u003c/p\u003e\n\u003cp\u003eYou see that in order for a computational algorithm to solve this (and other similar) problems, we need to first convert the data we have into a set of matrix and vector objects. Machine learning involves building up these objects from the given data, understanding their relationships and how to process them for a particular problem.\u003c/p\u003e\n\u003cp\u003eSolving these equations requires knowledge of defining these vectors and matrices in a computational environment and of operations that can be performed on these entities to solve for unknown variables as we saw above. We'll look into how to do this in upcoming lessons.\u003c/p\u003e\n\u003ch2\u003eSummary\u003c/h2\u003e\n\u003cp\u003eIn this lesson, you learned how a system of linear (simultaneous) equations can be solved using elimination and substitution, and also, how to covert these problems into matrices and vectors to be processed by computational algorithms. In the next couple of lessons, we'll look at how to describe these entities in Python and NumPy and also how to perform arithmetic operations to solve these types of equations.\u003c/p\u003e","exportId":"systems-of-linear-equations"},{"id":153882,"title":"Systems of Linear Equations - Lab","type":"Assignment","indent":1,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_mark_done","completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-lingalg-linear-equations-quiz\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-lingalg-linear-equations-quiz/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","exportId":"gac95296474eef5ec7a7ea3d8dd6059ab"},{"id":153886,"title":"Scalars, Vectors, Matrices, and Tensors - Code Along","type":"Assignment","indent":0,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_mark_done","completed":true,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-scalars-vectors-matrices-tensors-codealong\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-scalars-vectors-matrices-tensors-codealong/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","exportId":"g3547e21885702235e64882937631c05a"},{"id":153890,"title":"Matrix Multiplication - Code Along","type":"Assignment","indent":0,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_mark_done","completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-linalg-mat-multiplication-codealong\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-linalg-mat-multiplication-codealong/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","exportId":"g45d1530438268d58175ba1cafdfc0240"},{"id":153895,"title":"Solving Systems of Linear Equations with NumPy - Code Along","type":"Assignment","indent":0,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_mark_done","completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-lineq-numpy-codealong\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-lineq-numpy-codealong/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","exportId":"gfbb1f52ae29185c60a7943bb30dc61ba"},{"id":153901,"title":"Solving Systems of Linear Equations with NumPy - Lab","type":"Assignment","indent":1,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_mark_done","completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-lineq-numpy-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-lineq-numpy-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","exportId":"g12f688c84032b3b1e7e7230f4d52594f"},{"id":153906,"title":"Regression Analysis using Linear Algebra and NumPy - Code Along","type":"Assignment","indent":0,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_mark_done","completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-linalg-regression-codealong\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-linalg-regression-codealong/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","exportId":"g5fc09481bd2ed62bc9f8f94ffaa91b30"},{"id":153914,"title":"Regression with Linear Algebra - Lab","type":"Assignment","indent":1,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_mark_done","completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-linalg-regression-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-linalg-regression-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","exportId":"gcfcaaf171073f826019e8468eaeee7a8"},{"id":153918,"title":"Computational Complexity: From OLS to Gradient Descent","type":"WikiPage","indent":0,"locked":false,"requirement":"must_mark_done","completed":false,"content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-computational-complexity\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-computational-complexity\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-computational-complexity/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eIn this lesson, youll be introduced to computational complexity. You'll learn about this idea in relationship with OLS regression and see how this may not be the most efficient algorithm to calculate the regression parameters when performing regression with large datasets. You'll set the stage for an optimization algorithm called \"Gradient Descent\" which will be covered in detail later.\u003c/p\u003e\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\u003cp\u003eYou will be able to:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eDescribe computational complexity and how it is related to Big O notation\u003c/li\u003e\n\u003cli\u003eDescribe why OLS with matrix algebra would become problematic for large/complex data\u003c/li\u003e\n\u003cli\u003eExplain how optimizing techniques such as gradient descent can solve complexity issues\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003eComplexities in OLS\u003c/h2\u003e\n\u003cp\u003eRecall the OLS formula for calculating the beta vector:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cbeta%20=(X%5ETX)%5E%7B-1%7DX%5ET%20y\"\u003e\u003c/p\u003e\n\u003cp\u003eThis formula looks very simple, elegant, and intuitive. It works perfectly fine for the case of simple linear regression due to a limited number of computed dimensions, but with datasets that are very large or \u003cstrong\u003ebig data\u003c/strong\u003e sets, it becomes computationally very expensive as it can potentially involve a huge number of complex mathematical operations.\u003c/p\u003e\n\u003cp\u003eFor this formula, we need to find \u003cimg src=\"https://render.githubusercontent.com/render/math?math=(X%5ETX)\"\u003e , and invert it as well, which makes it very expensive. Imagine the matrix \u003cimg src=\"https://render.githubusercontent.com/render/math?math=X_%7B(N%20%5Ctimes%20M%2b1)%7D\"\u003e has \u003cimg src=\"https://render.githubusercontent.com/render/math?math=(M%2b1)\"\u003e columns where \u003cimg src=\"https://render.githubusercontent.com/render/math?math=M\"\u003e is the number of predictors and \u003cimg src=\"https://render.githubusercontent.com/render/math?math=N\"\u003e is the number of rows of observations. In machine learning, you will often find datasets with \u003cimg src=\"https://render.githubusercontent.com/render/math?math=M\u003e1000\"\u003e and \u003cimg src=\"https://render.githubusercontent.com/render/math?math=N\u003e1,000,000\"\u003e . The \u003cimg src=\"https://render.githubusercontent.com/render/math?math=(X%5ETX)\"\u003e matrix itself takes a while to calculate, then you have to invert an \u003cimg src=\"https://render.githubusercontent.com/render/math?math=M%5Ctimes%20M\"\u003e matrix which adds more to the complexity - making it very expensive. You'll also come across situations where the input matrix grows so large that it cannot fit into your computer's memory.\u003c/p\u003e\n\u003ch2\u003eThe Big O Notation\u003c/h2\u003e\n\u003cp\u003eIn computer science, Big O notation is used to describe how \"fast\" an algorithm grows, by comparing the number of operations within the algorithm. Big O notation helps you see the worst-case scenario for an algorithm. Typically, we are most concerned with the Big O time because we are interested in how slowly a given algorithm will possibly run at worst.\u003c/p\u003e\n\u003ch4\u003eExample\u003c/h4\u003e\n\u003cp\u003eImagine you need to find a person you only know the name of. What's the most straightforward way of finding this person? Well, you could go through every single name in the phone book until you find your target. This is known as a simple search. If the phone book is not very long, with say, only 10 names, this is a fairly fast process. But what if there are 10,000 names in the phone book?\u003c/p\u003e\n\u003cp\u003eAt best, your target's name is at the front of the list and you only need to need to check the first item. At worst, your target's name is at the very end of the phone book and you will need to have searched all 10,000 names. As the \"dataset\" (or the phone book) increases in size, the maximum time it takes to run a simple search also linearly increases.\u003c/p\u003e\n\u003cp\u003eBig O notation allows you to describe what the worst case is. The worst case is that you will have to search through all elements ( \u003cimg src=\"https://render.githubusercontent.com/render/math?math=n\"\u003e ) in the phone book. You can describe the run-time as:\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=O(n)\"\u003e where \u003cimg src=\"https://render.githubusercontent.com/render/math?math=n\"\u003e is the number of operations\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eBecause the maximum number of operations is equal to the maximum number of elements in our phone book, we say the Big \u003cimg src=\"https://render.githubusercontent.com/render/math?math=O\"\u003e of a simple search is \u003cimg src=\"https://render.githubusercontent.com/render/math?math=O(n)\"\u003e . \u003cstrong\u003eA simple search will never be slower than \u003cimg src=\"https://render.githubusercontent.com/render/math?math=O(n)\"\u003e time.\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eDifferent algorithms have different run-times. That is, algorithms grow at different rates. The most common Big O run-times, from fastest to slowest, are:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cimg src=\"https://render.githubusercontent.com/render/math?math=O(%5Clog%20n)\"\u003e : aka \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Clog\"\u003e time\u003c/li\u003e\n\u003cli\u003e\n\u003cimg src=\"https://render.githubusercontent.com/render/math?math=O(n)\"\u003e : aka linear time\u003c/li\u003e\n\u003cli\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=O(n%5E2)\"\u003e\u003c/li\u003e\n\u003cli\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=O(n%5E3)\"\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThese rates, as well as some other rates, can be visualized in the following figure:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-computational-complexity/master/images/big_o.png\" width=\"500\"\u003e\u003c/p\u003e\n\u003ch3\u003eOLS and Big O notation\u003c/h3\u003e\n\u003cp\u003eInverting a matrix costs \u003cimg src=\"https://render.githubusercontent.com/render/math?math=O(n%5E3)\"\u003e for computation where n is the number of rows in \u003cimg src=\"https://render.githubusercontent.com/render/math?math=X\"\u003e matrix, i.e., the observations. Here is an explanation of how to calculate Big O for OLS.\u003c/p\u003e\n\u003cp\u003eOLS linear regression is computed as \u003cimg src=\"https://render.githubusercontent.com/render/math?math=(X%5ETX)%5E%7B-1%7DX%5ET%20y\"\u003e .\u003c/p\u003e\n\u003cp\u003eIf \u003cimg src=\"https://render.githubusercontent.com/render/math?math=X\"\u003e is an \u003cimg src=\"https://render.githubusercontent.com/render/math?math=(n%20%5Ctimes%20k)\"\u003e matrix:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cimg src=\"https://render.githubusercontent.com/render/math?math=(X%5ETX)\"\u003e takes \u003cimg src=\"https://render.githubusercontent.com/render/math?math=O(n*k%5E2)\"\u003e time and produces a \u003cimg src=\"https://render.githubusercontent.com/render/math?math=(k%20%5Ctimes%20k)\"\u003e matrix\u003c/li\u003e\n\u003cli\u003eThe matrix inversion of a (k x k) matrix takes \u003cimg src=\"https://render.githubusercontent.com/render/math?math=O(k%5E3)\"\u003e time\u003c/li\u003e\n\u003cli\u003e\n\u003cimg src=\"https://render.githubusercontent.com/render/math?math=(X%5ETY)\"\u003e takes \u003cimg src=\"https://render.githubusercontent.com/render/math?math=O(n*k%5E2)\"\u003e time and produces a \u003cimg src=\"https://render.githubusercontent.com/render/math?math=(k%20%5Ctimes%20k)\"\u003e matrix\u003c/li\u003e\n\u003cli\u003eThe final matrix multiplication of two \u003cimg src=\"https://render.githubusercontent.com/render/math?math=(k%20%5Ctimes%20k)\"\u003e matrices takes \u003cimg src=\"https://render.githubusercontent.com/render/math?math=O(k%5E3)\"\u003e time\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003eSo the Big O running time for OLS is \u003cimg src=\"https://render.githubusercontent.com/render/math?math=O(k%5E%7B2*(n%20%2b%20k)%7D)\"\u003e - which is pretty expensive\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eMoreover, if \u003cimg src=\"https://render.githubusercontent.com/render/math?math=X\"\u003e is ill-conditioned (i.e. it isn't a square matrix), there will be computational errors in the estimation. Another common problem is overfitting and underfitting in estimation of regression coefficients.\u003c/p\u003e\n\u003cp\u003eSo, this leads us to the gradient descent kind of optimization algorithm which can save us from this type of problem. The main reason why gradient descent is used for linear regression is the computational complexity: it's computationally cheaper (faster) to find the solution using the gradient descent in most cases.\u003c/p\u003e\n\u003ch2\u003eGradient Descent\u003c/h2\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-computational-complexity/master/images/gradient_descent.png\" width=\"850\"\u003e\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eGradient Descent is an iterative approach to minimize the model loss (error), used while training a machine learning model like linear regression. It is an optimization algorithm based on a convex function as shown in the figure above, that tweaks its parameters iteratively to minimize a given function to its local minimum.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eIn regression, it is used to find the values of model parameters (coefficients, or the \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cbeta\"\u003e matrix) that minimize a cost function (like RMSE) as far as possible.\u003c/p\u003e\n\u003cp\u003eIn order to fully understand how this works, you need to know what a gradient is and how is it calculated. And for this, you would need some Calculus. It may sound a bit intimidating at this stage, but don't worry. The next few sections will introduce you to the basics of calculus with gradients and derivatives.\u003c/p\u003e\n\u003ch2\u003eFurther Reading\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e\u003ca href=\"https://en.wikipedia.org/wiki/Computational_complexity_of_mathematical_operations\"\u003eWiki: Computational complexity of mathematical operations\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003ca href=\"https://medium.com/karuna-sehgal/a-simplified-explanation-of-the-big-o-notation-82523585e835\"\u003eSimplified Big O notation\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003ca href=\"https://towardsdatascience.com/gradient-descent-in-a-nutshell-eaf8c18212f0\"\u003eGradient descent in a nutshell\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003eSummary\u003c/h2\u003e\n\u003cp\u003eIn this lesson, you learned about the shortcomings and limitations of OLS and matrix inverses. You looked at the Big O notation to explain how calculating inverses and transposes for large matrix might make our analysis unstable and computationally very expensive. This lesson sets a stage for your next section on calculus and gradient descent. You will have a much better understanding of the gradient descent diagram shown above and how it all works by the end of next section.\u003c/p\u003e","exportId":"computational-complexity-from-ols-to-gradient-descent"},{"id":153924,"title":"Linear Algebra - Recap","type":"WikiPage","indent":0,"locked":false,"requirement":"must_mark_done","completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-linalg-section-recap\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-linalg-section-recap/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eIn this section, you learned the fundamentals of linear algebra. An understanding of linear algebra will help you better understand the underlying mathematics behind some machine learning algorithms.\u003c/p\u003e\n\n\u003ch2\u003eKey Takeaways\u003c/h2\u003e\n\n\u003cp\u003eThe goal of this section was to provide both a conceptual and computational introduction to linear algebra - one of the foundational concepts underlying most machine learning models. Some of the key takeaways include: \u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eOne use case for vectors and matrices is for representing and solving systems of linear equations\u003c/li\u003e\n\u003cli\u003eA scalar is a single, real number. A vector is a one-dimensional array of numbers. A matrix is a 2-dimensional array of numbers \u003c/li\u003e\n\u003cli\u003eA tensor is a generalized term for an n-dimensional rectangular grid of numbers. A vector is a one-dimensional (first-order tensor), a matrix is a two-dimensional (second-order tensor), etc.\u003c/li\u003e\n\u003cli\u003eTwo matrices can be added together if they have the same shape\u003c/li\u003e\n\u003cli\u003eScalars can be added to matrices by adding the scalar (number) to each element\u003c/li\u003e\n\u003cli\u003eTo calculate the dot product for matrix multiplication, the first matrix must have the same number of columns as the number of rows in the second matrix \u003c/li\u003e\n\u003cli\u003eOperating on NumPy data types is substantially more computationally efficient than performing the same operations on native Python data types\u003c/li\u003e\n\u003cli\u003eIt is possible to use linear algebra in NumPy to solve for a linear regression using the OLS method\u003c/li\u003e\n\u003cli\u003eOLS is not computationally efficient, so in practice, we usually perform a gradient descent instead to solve a linear regression\u003c/li\u003e\n\u003c/ul\u003e\n\u003cfooter class=\"fis-footer\" style=\"visibility: hidden;\"\u003e\u003cdiv class=\"fis-feedback\"\u003e\n\u003ch5\u003eHow do you feel about this lesson?\u003c/h5\u003e\n\u003cimg id=\"thumbs-up\" data-repository=\"dsc-linalg-section-recap\" title=\"Thumbs up!\" alt=\"thumbs up\"\u003e\u003cimg id=\"thumbs-down\" data-repository=\"dsc-linalg-section-recap\" title=\"Thumbs down!\" alt=\"thumbs down\"\u003e\n\u003c/div\u003e\n\u003ch5\u003eHave specific feedback? \u003ca href=\"https://github.com/learn-co-curriculum/dsc-linalg-section-recap/issues/new\"\u003eTell us here!\u003c/a\u003e\n\u003c/h5\u003e\u003c/footer\u003e","exportId":"linear-algebra-recap"}]},{"id":16486,"name":"APPENDIX: More Linear Algebra","status":"completed","unlockDate":null,"prereqs":[],"requirement":null,"sequential":false,"exportId":"g2690fd56cca30bc8944928a6a3b544a1","items":[{"id":154150,"title":"Vector Addition and Broadcasting in NumPy - Code Along","type":"Assignment","indent":0,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":null,"completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-linalg-vector-addition-codealong\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-linalg-vector-addition-codealong/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","exportId":"gdb4d8a2f135ea18f696e94af952f5c08"},{"id":154151,"title":"Vectors and Matrices in Numpy - Lab","type":"Assignment","indent":1,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":null,"completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-linalg-vector-matrices-numpy-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-linalg-vector-matrices-numpy-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","exportId":"g8426ab3f21931cbe23df251e82e6c068"},{"id":154152,"title":"Properties of Dot Product - Lab","type":"Assignment","indent":1,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":null,"completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-linalg-dot-product-properties-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-linalg-dot-product-properties-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","exportId":"gd719bc423b04acae1066e5cad7066e58"},{"id":154153,"title":"Pure Python vs. Numpy - Lab","type":"Assignment","indent":1,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":null,"completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-linalg-python-vs-numpy-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-linalg-python-vs-numpy-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","exportId":"g3dc50dc7142e35c5b3d7776c252a2bda"}]},{"id":16451,"name":"Topic 23: Calculus, Cost Function and Gradient Descent","status":"unlocked","unlockDate":null,"prereqs":[],"requirement":"all","sequential":false,"exportId":"g09ca015ad4b088456e0e2b8f7752a4ed","items":[{"id":153936,"title":"Introduction","type":"WikiPage","indent":0,"locked":false,"requirement":"must_mark_done","completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-calculus-introduction\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-calculus-introduction/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eIn this section, you'll learn about the mechanism behind many machine learning optimization algorithms: gradient descent!\u003c/p\u003e\n\n\u003ch2\u003eCalculus and Solving a Linear Regression Using Gradient Descent\u003c/h2\u003e\n\n\u003cp\u003eIn this section, we're going to see how you can apply a \"gradient descent\" to solve a linear regression. Along the way, we'll also look at cost functions and will provide a foundation in calculus that will be valuable to you throughout your career as a data scientist.\u003c/p\u003e\n\n\u003ch3\u003eAn Introduction to Derivatives\u003c/h3\u003e\n\n\u003cp\u003eWe're going to start off by introducing derivatives - the \"instantaneous rate of change of a function\" or (more graphically) the \"slope of a curve\". We'll start off by looking at how to calculate the slope of a curve for a straight line, and then we'll explore how to calculate the rate of change for more complex (non-linear) functions.\u003c/p\u003e\n\n\u003ch3\u003eGradient Descent\u003c/h3\u003e\n\n\u003cp\u003eNow that we know how to calculate the slope of a curve - and, by extension, to find a local minima (low point) or maxima (high point) where the curve is flat (the slope of the curve is zero), we'll look at the idea of a gradient descent to step from some random point on a cost curve to find the local optima to solve for a given linear equation. We'll also look at how best to select the step sizes for descending the cost function, and how to use partial derivatives to optimize both slope and offset to more effectively solve a linear regression using gradient descent.\u003c/p\u003e\n\n\u003ch2\u003eSummary\u003c/h2\u003e\n\n\u003cp\u003eJust as we used solving a linear regression using OLS as an excuse to introduce you to linear algebra - one of the foundational elements of mathematics underpinning machine learning, we're now using the idea of gradient descent to introduce enough calculus to both understand and have good intuitions about many of the machine learning models that you're going to learn throughout the rest of the course.\u003c/p\u003e\n\u003cfooter class=\"fis-footer\" style=\"visibility: hidden;\"\u003e\u003cdiv class=\"fis-feedback\"\u003e\n\u003ch5\u003eHow do you feel about this lesson?\u003c/h5\u003e\n\u003cimg id=\"thumbs-up\" data-repository=\"dsc-calculus-introduction\" title=\"Thumbs up!\" alt=\"thumbs up\"\u003e\u003cimg id=\"thumbs-down\" data-repository=\"dsc-calculus-introduction\" title=\"Thumbs down!\" alt=\"thumbs down\"\u003e\n\u003c/div\u003e\n\u003ch5\u003eHave specific feedback? \u003ca href=\"https://github.com/learn-co-curriculum/dsc-calculus-introduction/issues/new\"\u003eTell us here!\u003c/a\u003e\n\u003c/h5\u003e\u003c/footer\u003e","exportId":"introduction"},{"id":153939,"title":"Introduction to Derivatives","type":"Assignment","indent":0,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_mark_done","completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-derivatives-intro\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-derivatives-intro/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","exportId":"geeea38a45e9f7a0543cf69879ca83ca9"},{"id":153943,"title":"Introduction To Derivatives - Lab","type":"Assignment","indent":1,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_mark_done","completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-derivatives-intro-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-derivatives-intro-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","exportId":"g16a3298cfad293e535ed3e9d09e66a57"},{"id":153948,"title":"Derivatives of Non-Linear Functions","type":"Assignment","indent":0,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_mark_done","completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-derivatives-of-non-linear-functions\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-derivatives-of-non-linear-functions/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","exportId":"geb52af323d1b29002c44e7a37a5aa534"},{"id":153954,"title":"Rules for Derivatives","type":"Assignment","indent":0,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_mark_done","completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-rules-for-derivatives\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-rules-for-derivatives/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","exportId":"g37914033df1e7de2e707f964f79dcdbe"},{"id":153959,"title":"Derivatives: Conclusion","type":"Assignment","indent":0,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_mark_done","completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-derivatives-conclusion\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-derivatives-conclusion/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","exportId":"gef19edea98453a15d1e86a5cd962db4b"},{"id":153963,"title":"Introduction to Gradient Descent","type":"Assignment","indent":0,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_mark_done","completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-gradient-descent-intro\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-gradient-descent-intro/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","exportId":"gd4c4efe9ae315b769375d0e0f70ae2d0"},{"id":153969,"title":"Gradient Descent: Step Sizes","type":"Assignment","indent":0,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_mark_done","completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-gradient-descent-step-sizes\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-gradient-descent-step-sizes/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","exportId":"gbdf1eac42db439d31c1d341787937fd4"},{"id":153972,"title":"Gradient Descent: Step Sizes - Lab","type":"Assignment","indent":1,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_mark_done","completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-gradient-descent-step-sizes-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-gradient-descent-step-sizes-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","exportId":"g681a923c77330be3e649a1c400dbc5be"},{"id":153975,"title":"Gradient Descent in 3D","type":"Assignment","indent":0,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_mark_done","completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-gradient-descent-in-3d\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-gradient-descent-in-3d/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","exportId":"g76b9f7bdb0655f18fcc3459eda791388"},{"id":153979,"title":" The Gradient in Gradient Descent","type":"WikiPage","indent":0,"locked":false,"requirement":"must_mark_done","completed":false,"content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-the-gradient-in-gradient-descent\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-the-gradient-in-gradient-descent\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-the-gradient-in-gradient-descent/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eAs you know, we entered our discussion of derivatives to determine the size and direction of a step with which to move along a cost curve. We first used a derivative in a single variable function to see how the output of our cost curve changed with respect to change a change in one of our regression line's variables. Then we learned about partial derivatives to see how a \u003cem\u003ethree-dimensional cost curve\u003c/em\u003e responded to a change in the regression line.\u003c/p\u003e\n\u003cp\u003eHowever, we have not yet explicitly showed how partial derivatives apply to gradient descent.\u003c/p\u003e\n\u003cp\u003eWell, that's what we hope to show in this lesson: explain how we can use partial derivatives to find the path to minimize our cost function, and thus find our \"best fit\" regression line.\u003c/p\u003e\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\u003cp\u003eYou will be able to:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eDefine a gradient in relation to gradient descent\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003eWhat is the gradient?\u003c/h2\u003e\n\u003cp\u003eNow gradient descent literally means that we are taking the shortest path to \u003cem\u003edescend\u003c/em\u003e towards our minimum. However, it is somewhat easier to understand gradient \u003cem\u003eascent\u003c/em\u003e than descent, and the two are quite related, so that's where we'll begin. Gradient ascent, as you could guess, simply means that we want to move in the direction of steepest ascent.\u003c/p\u003e\n\u003cp\u003eNow moving in the direction of greatest ascent for a function \u003cimg src=\"https://render.githubusercontent.com/render/math?math=f(x,y)\"\u003e , means that our next step is a step some distance in the \u003cimg src=\"https://render.githubusercontent.com/render/math?math=x\"\u003e direction and some distance in the \u003cimg src=\"https://render.githubusercontent.com/render/math?math=y\"\u003e direction which is the steepest upward at that point.\u003c/p\u003e\n\u003cp\u003eNote how this is a different task from what we have previously worked on for multivariable functions. So far, we have used partial derivatives to calculate the \u003cstrong\u003egain\u003c/strong\u003e from moving directly in either the \u003cimg src=\"https://render.githubusercontent.com/render/math?math=x\"\u003e direction or the \u003cimg src=\"https://render.githubusercontent.com/render/math?math=y\"\u003e direction.\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eHere, in finding gradient ascent, our task is not to calculate the gain from a move in either the \u003cimg src=\"https://render.githubusercontent.com/render/math?math=x\"\u003e or \u003cimg src=\"https://render.githubusercontent.com/render/math?math=y\"\u003e direction. Instead, our task is to \u003cstrong\u003efind some combination of a change in \u003cimg src=\"https://render.githubusercontent.com/render/math?math=x\"\u003e , \u003cimg src=\"https://render.githubusercontent.com/render/math?math=y\"\u003e that brings the largest change in output\u003c/strong\u003e.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eSo if you look at the path our climbers are taking in the picture above, \u003cem\u003ethat\u003c/em\u003e is the direction of gradient ascent. If they tilt their path to the right or left, they will no longer be moving along the steepest upward path.\u003c/p\u003e\n\u003cp\u003eThe direction of the greatest rate of increase of a function is called the gradient. We denote the gradient with the nabla, which comes from the Greek word for harp, which is kind of what it looks like: \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cnabla\"\u003e. So we can denote the gradient of a function, \u003cimg src=\"https://render.githubusercontent.com/render/math?math=f(x,%20y)\"\u003e , with \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cnabla%20f(x,%20y)\"\u003e .\u003c/p\u003e\n\u003ch2\u003eCalculating the gradient\u003c/h2\u003e\n\u003cp\u003eNow how do we find the direction for the greatest rate of increase? We use partial derivatives. Here's why.\u003c/p\u003e\n\u003cp\u003eAs we know, the partial derivative \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cfrac%7Bdf%7D%7Bdx%7D\"\u003e calculates the change in output from moving a little bit in the \u003cimg src=\"https://render.githubusercontent.com/render/math?math=x\"\u003e direction, and the partial derivative \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cfrac%7Bdf%7D%7Bdy%7D\"\u003e calculates the change in output from moving in the \u003cimg src=\"https://render.githubusercontent.com/render/math?math=y\"\u003e direction. Because with gradient ascent our goal is to make a nudge in \u003cimg src=\"https://render.githubusercontent.com/render/math?math=x,%20y\"\u003e that produces the greatest change in output, if \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cfrac%7Bdf%7D%7Bdy%7D%20\u003e%20%5Cfrac%7Bdf%7D%7Bdx%7D\"\u003e , we should make that move more in the \u003cimg src=\"https://render.githubusercontent.com/render/math?math=y\"\u003e direction than the \u003cimg src=\"https://render.githubusercontent.com/render/math?math=x\"\u003e direction, and vice versa. That is, we want to get the biggest bang for our buck.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-the-gradient-in-gradient-descent/master/images/Denali.jpg\"\u003e\u003c/p\u003e\n\u003cp\u003eLet's relate this again to mountain climbers. Imagine the vertical edge on the left is our y-axis and the horizontal edge is on the bottom is our x-axis. For the climber in the yellow jacket, imagine his step size is three feet. A step straight along the y-axis will move him further upwards than a step along the x-axis. So in taking that step, he should direct himself more towards the y-axis than the x-axis. That will produce a bigger increase per step size.\u003c/p\u003e\n\u003cp\u003eIn fact, the direction of greatest ascent for a function, \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cnabla%20f(x,%20y)\"\u003e , is the direction which is a proportion of \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cfrac%7Bdf%7D%7Bdy%7D\"\u003e steps in the \u003cimg src=\"https://render.githubusercontent.com/render/math?math=y\"\u003e direction and \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cfrac%7Bdf%7D%7Bdx%7D\"\u003e in the \u003cimg src=\"https://render.githubusercontent.com/render/math?math=x\"\u003e direction. So, for example, if \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cfrac%7Bdf%7D%7Bdy%7D\"\u003e = 5 and \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cfrac%7Bdf%7D%7Bdx%7D\"\u003e = 1, the direction of gradient ascent is five times more in the \u003cimg src=\"https://render.githubusercontent.com/render/math?math=y\"\u003e direction than the \u003cimg src=\"https://render.githubusercontent.com/render/math?math=x\"\u003e direction. And this seems to be the path, more or less that our climbers are taking - some combination of \u003cimg src=\"https://render.githubusercontent.com/render/math?math=x\"\u003e and \u003cimg src=\"https://render.githubusercontent.com/render/math?math=y\"\u003e , but tilted more towards the \u003cimg src=\"https://render.githubusercontent.com/render/math?math=y\"\u003e direction.\u003c/p\u003e\n\u003ch2\u003eApplying Gradient Descent\u003c/h2\u003e\n\u003cp\u003eNow that we have a better understanding of a gradient, let's apply our understanding to a multivariable function. Here is a plot of a function:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=f(x,y)%20=%202x%20%2b%203y\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-the-gradient-in-gradient-descent/master/images/new_gradDescinDesc.png\" width=\"400\"\u003e\u003c/p\u003e\n\u003cp\u003eImagine being at the bottom left of the graph at the point \u003cimg src=\"https://render.githubusercontent.com/render/math?math=x%20=%201\"\u003e , \u003cimg src=\"https://render.githubusercontent.com/render/math?math=y%20=%201\"\u003e . What would be the direction of steepest ascent? It seems, just sizing it up visually, that we should move both in the positive \u003cimg src=\"https://render.githubusercontent.com/render/math?math=y\"\u003e direction and the positive \u003cimg src=\"https://render.githubusercontent.com/render/math?math=x\"\u003e direction. Looking more carefully, it seems we should move \u003cstrong\u003emore\u003c/strong\u003e in the \u003cimg src=\"https://render.githubusercontent.com/render/math?math=y\"\u003e direction than the \u003cimg src=\"https://render.githubusercontent.com/render/math?math=x\"\u003e direction. Let's see what our technique of taking the partial derivative indicates.\u003c/p\u003e\n\u003cp\u003eThe gradient of the function \u003cimg src=\"https://render.githubusercontent.com/render/math?math=f(x,y)\"\u003e , that is \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cnabla%20f(x,y)%20=%202x%20%2b%203y\"\u003e is the following:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cfrac%7Bdf%7D%7Bdx%7D(2x%20%2b%203y)%20=%202\"\u003e and \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cfrac%7Bdf%7D%7Bdy%7D(2x%20%2b%203y)%20=%203\"\u003e .\u003c/p\u003e\n\u003cp\u003eSo what this tells us is to move in the direction of greatest ascent for the function \u003cimg src=\"https://render.githubusercontent.com/render/math?math=f(x,y)%20=%202x%20%2b%203y\"\u003e , is to move up three and to the right two. So we would expect our path of greatest ascent to look like the following.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-the-gradient-in-gradient-descent/master/images/gradient-plot.png\" width=\"400\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-the-gradient-in-gradient-descent/master/images/new_gradDescinDesc.png\" width=\"400\"\u003e\u003c/p\u003e\n\u003cp\u003eSo this path maps up well to what we see visually. That is the idea behind gradient descent. The gradient is the partial derivative with respect to each type of variable of a multivariable function, in this case \u003cimg src=\"https://render.githubusercontent.com/render/math?math=x\"\u003e and \u003cimg src=\"https://render.githubusercontent.com/render/math?math=y\"\u003e . And the importance of the gradient is that its direction is the direction of steepest ascent. The negative gradient, that is the negative of each of the partial derivatives, is the direction of steepest descent. So our direction of gradient descent for the graph above is \u003cimg src=\"https://render.githubusercontent.com/render/math?math=x%20=%20-2\"\u003e , \u003cimg src=\"https://render.githubusercontent.com/render/math?math=y%20=%20-3\"\u003e . And looking at the two graphs above, it seems that the steepest downward direction is just the opposite of the steepest upward direction. We get that by mathematically by simply taking the multiplying our partial derivatives by negative one.\u003c/p\u003e\n\u003ch2\u003eSummary\u003c/h2\u003e\n\u003cp\u003eIn this lesson, you saw how to use gradient descent to find the direction of steepest descent. You saw that the direction of steepest descent is generally some combination of a change in your variables to produce the greatest negative rate of change.\u003c/p\u003e\n\u003cp\u003eYou first how saw how to calculate the gradient \u003cstrong\u003eascent\u003c/strong\u003e, or the gradient \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cnabla\"\u003e , by calculating the partial derivative of a function with respect to the variables of the function. So \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cnabla%20f(x,%20y)%20=%20%5Cfrac%7B%5Cdelta%20f%7D%7B%5Cdelta%20y%7D,%20%5Cfrac%7B%5Cdelta%20f%7D%7B%5Cdelta%20x%7D\"\u003e . This means that to take the path of greatest ascent, you should move \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cfrac%7B%5Cdelta%20f%7D%7B%5Cdelta%20y%7D\"\u003e divided by \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cfrac%7B%5Cdelta%20f%7D%7B%5Cdelta%20x%7D\"\u003e . So for example, when \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cfrac%7B%5Cdelta%20f%7D%7B%5Cdelta%20y%7Df(x,%20y)%20%20=%203\"\u003e , and \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cfrac%7B%5Cdelta%20f%7D%7B%5Cdelta%20x%7Df(x,%20y)%20%20=%202\"\u003e , you traveled in line with a slope of 3/2.\u003c/p\u003e\n\u003cp\u003eFor gradient descent, that is to find the direction of greatest decrease, you simply reverse the direction of your partial derivatives and move in \u003cimg src=\"https://render.githubusercontent.com/render/math?math=-%20%5Cfrac%7B%5Cdelta%20f%7D%7B%5Cdelta%20y%7D,%20-%20%5Cfrac%7B%5Cdelta%20f%7D%7B%5Cdelta%20x%7D\"\u003e .\u003c/p\u003e","exportId":"the-gradient-in-gradient-descent"},{"id":153983,"title":"Gradient to Cost Function","type":"Assignment","indent":0,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_mark_done","completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-gradient-to-cost-function-v2-1\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-gradient-to-cost-function-v2-1/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","exportId":"gfa3bd7bfa3ef7de7e8cd69de1c88abae"},{"id":153986,"title":"Applying Gradient Descent - Lab","type":"Assignment","indent":1,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_mark_done","completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-applying-gradient-descent-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-applying-gradient-descent-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","exportId":"ga83ba2495b2eb7cadc1a80606b3fb8e3"},{"id":153990,"title":"Calculus - Section Recap","type":"WikiPage","indent":0,"locked":false,"requirement":"must_mark_done","completed":false,"content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-calculus-section-recap\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-calculus-section-recap\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-calculus-section-recap/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eCongratulations! You have learned about one of the most fundamental concepts at the core machine learning, calculus. In this section, you started with the basics of derivatives and moved all the way to coding out gradient descent with multiple variables.\u003c/p\u003e\n\u003ch2\u003eKey Takeaways\u003c/h2\u003e\n\u003cp\u003eIn this section, we both learned how to traverse a cost function graph to find the local minima to solve a linear regression by using gradient descent and covered some of the foundational calculus that will help you to understand many of the other machine learning models you'll encounter as a professional data scientist.\u003c/p\u003e\n\u003cp\u003eKey takeaways include:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eA derivative is the \"instantaneous rate of change\" of a function - or it can be thought of as the \"slope of the curve\" at a point in time\u003c/li\u003e\n\u003cli\u003eA derivative can also be thought of as a special case of the rate of change over a period of time - as that period of time is zero.\u003c/li\u003e\n\u003cli\u003eIf you calculate the rate of change over a period of time and keep reducing the period of time, it usually tends to a limit - which is the value of that derivative\u003c/li\u003e\n\u003cli\u003eThe power rule, constant factor rule, and addition rule are key tools for calculating derivatives for various kinds of functions\u003c/li\u003e\n\u003cli\u003eThe chain rule can be a useful tool for calculating the derivate of composite functions\u003c/li\u003e\n\u003cli\u003eA derivative can be useful for identifying local maxima or minima as in both cases, the derivative tends to zero\u003c/li\u003e\n\u003cli\u003eA cost curve can be used to plot the values of a cost function (in the case of linear regression) for various values of offset and slope for the best fit line.\u003c/li\u003e\n\u003cli\u003eGradient descent can be used to move towards the local minimum on the cost curve and thus the ideal values for the y-intercept and slope to minimize the selected cost function when performing a linear regression.\u003c/li\u003e\n\u003c/ul\u003e","exportId":"calculus-section-recap"}]},{"id":16485,"name":"APPENDIX: More On Derivatives","status":"completed","unlockDate":null,"prereqs":[],"requirement":null,"sequential":false,"exportId":"gc3ebfa527cc94dc2cb537bd347454809","items":[{"id":154147,"title":"Rules for Derivatives - Lab","type":"Assignment","indent":0,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":null,"completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-rules-for-derivatives-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-rules-for-derivatives-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","exportId":"g838615764c7af308be8585611ad63a78"},{"id":154148,"title":"Derivatives: the Chain Rule","type":"Assignment","indent":0,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":null,"completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-derivatives-chain-rule\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-derivatives-chain-rule/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","exportId":"g3dd0fd184b054c5e13d45821d403d737"},{"id":154149,"title":"Gradient to Cost Function - Appendix","type":"WikiPage","indent":0,"locked":false,"requirement":null,"completed":false,"content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-gradient-to-cost-function-appendix\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-gradient-to-cost-function-appendix\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-gradient-to-cost-function-appendix/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eIn this lesson, you'll find the details on how to compute the partial derivatives in the \"Gradient to cost function\" lesson.\u003c/p\u003e\n\u003ch2\u003eComputing the First Partial Derivative\u003c/h2\u003e\n\u003cp\u003eLet's start with taking the \u003cstrong\u003epartial derivative\u003c/strong\u003e with respect to \u003cimg src=\"https://render.githubusercontent.com/render/math?math=m\"\u003e .\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cfrac%7B%5Cdelta%20J%7D%7B%5Cdelta%20m%7DJ(m,%20b)%20=%20%5Cfrac%7B%5Cdelta%20J%7D%7B%5Cdelta%20m%7D(y%20-%20(mx%20%2b%20b))%5E2\"\u003e\u003c/p\u003e\n\u003cp\u003eNow this is a tricky function to take the derivative of. So we can use functional composition followed by the chain rule to make it easier. Using functional composition, we can rewrite our function \u003cimg src=\"https://render.githubusercontent.com/render/math?math=J\"\u003e as two functions:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=g(m,b)%20=%20y%20-%20(mx%20%2b%20b)%5Cmspace%7B5ex%7D\"\u003e -- set \u003cimg src=\"https://render.githubusercontent.com/render/math?math=g\"\u003e equal to \u003cimg src=\"https://render.githubusercontent.com/render/math?math=y-%5Chat%7By%7D\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=J(g(m,b))%20=%20(g(m,b))%5E2%5Cmspace%7B4ex%7D\"\u003e -- now \u003cimg src=\"https://render.githubusercontent.com/render/math?math=J\"\u003e is a function of \u003cimg src=\"https://render.githubusercontent.com/render/math?math=g\"\u003e and \u003cimg src=\"https://render.githubusercontent.com/render/math?math=J=g%5E2\"\u003e\u003c/p\u003e\n\u003cp\u003eNow using the chain rule to find the partial derivative with respect to a change in the slope, gives us:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5B1%5D%5Cmspace%7B5ex%7D%5Cfrac%7BdJ%7D%7Bdm%7DJ(g)%20=%20%5Cfrac%7BdJ%7D%7Bdg%7DJ(g(m,%20b))*%5Cfrac%7Bdg%7D%7Bdm%7Dg(m,b)\"\u003e\u003c/p\u003e\n\u003cp\u003eBecause \u003cstrong\u003eg\u003c/strong\u003e is a function of \u003cstrong\u003em\u003c/strong\u003e we get \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cboldsymbol%7B%5Cfrac%7Bdg%7D%7Bdm%7D%7D(g)\"\u003e and\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eJ\u003c/strong\u003e is a function of \u003cstrong\u003eg (which is a function of m\u003c/strong\u003e) we get \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cboldsymbol%7B%5Cfrac%7BdJ%7D%7Bdg%7D%7D(J)\"\u003e .\u003c/p\u003e\n\u003cp\u003eOur next step is to solve these derivatives individually.\u003c/p\u003e\n\u003cp\u003eFirst:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cfrac%7BdJ%7D%7Bdg%7DJ(g(m,%20b))%20=%5Cfrac%7BdJ%7D%7Bdg%7Dg(m,b)%5E2\"\u003e\u003c/p\u003e\n\u003cp\u003eSolve \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cboldsymbol%7B%5Cfrac%7BdJ%7D%7Bdg%7D%7D(J)\"\u003e :\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cfrac%7BdJ%7D%7Bdg%7DJ(g(m,%20b))%20=%202*g(m,b)\"\u003e\u003c/p\u003e\n\u003cp\u003eThen:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cfrac%7Bdg%7D%7Bdm%7Dg(m,b)%20=%5Cfrac%7Bdg%7D%7Bdm%7D%20(y%20-%20(mx%20%2bb))\"\u003e\u003c/p\u003e\n\u003cp\u003eSolve \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cboldsymbol%7B%5Cfrac%7Bdg%7D%7Bdm%7D%7D(g)\"\u003e:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cfrac%7Bdg%7D%7Bdm%7Dg(m,b)%20=%5Cfrac%7Bdg%7D%7Bdm%7D%20(y%20-%20mx%20-%20b)\"\u003e \u003cbr\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cmspace%7B9ex%7D=%5Cfrac%7Bdg%7D%7Bdm%7Dy%20-%20%5Cfrac%7Bdg%7D%7Bdm%7Dmx%20-%20%5Cfrac%7Bdg%7D%7Bdm%7Db\"\u003e \u003cbr\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cmspace%7B9ex%7D=%200-x-0\"\u003e \u003cbr\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cmspace%7B9ex%7D=-x\"\u003e\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eEach of the terms are treated as constants, except for the middle term.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eNow plugging these back into our chain rule [1] we have:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Ccolor%7Bblue%7D%7B%5Cfrac%7BdJ%7D%7Bdg%7DJ(g(m,b))%7D%5Ccolor%7Bblack%7D%7B*%7D%5Ccolor%7Bred%7D%7B%5Cfrac%7Bdg%7D%7Bdm%7Dg(m,b)%7D%20%5Ccolor%7Bblack%7D%7B=%7D%5Ccolor%7Bblue%7D%7B(2*g(m,b))%7D%5Ccolor%7Bblack%7D%7B*%7D%5Ccolor%7Bred%7D%7B-x%7D\"\u003e \u003cbr\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cmspace%7B21.75ex%7D=%202*(y%20-%20(mx%20%2b%20b))*-x\"\u003e\u003c/p\u003e\n\u003cp\u003eSo\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5B1%5D%5Cmspace%7B5ex%7D%5Cfrac%7B%5Cdelta%20J%7D%7B%5Cdelta%20m%7DJ(m,%20b)%20=2*(y%20-%20(mx%20%2b%20b))*-x\"\u003e \u003cbr\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cmspace%7B15.75ex%7D=%20-2x*(y%20-%20(mx%20%2b%20b%20))\"\u003e\u003c/p\u003e\n\u003ch2\u003eComputing the Second Partial Derivative\u003c/h2\u003e\n\u003cp\u003eOk, now let's calculate the partial derivative with respect to a change in the y-intercept. We express this mathematically with the following:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cfrac%7B%5Cdelta%20J%7D%7B%5Cdelta%20b%7DJ(m,%20b)%20=%20%5Cfrac%7BdJ%7D%7Bdb%7D(y%20-%20(mx%20%2b%20b))%5E2\"\u003e\u003c/p\u003e\n\u003cp\u003eThen once again, we use functional composition following by the chain rule. So we view our cost function as the same two functions \u003cimg src=\"https://render.githubusercontent.com/render/math?math=g(m,b)\"\u003e and \u003cimg src=\"https://render.githubusercontent.com/render/math?math=J(g(m,b))\"\u003e .\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=g(m,b)%20=%20y%20-%20(mx%20%2b%20b)\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=J(g(m,b))%20=%20(g(m,b))%5E2\"\u003e\u003c/p\u003e\n\u003cp\u003eSo applying the chain rule, to this same function composition, we get:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5B2%5D%5Cmspace%7B5ex%7D%5Cfrac%7BdJ%7D%7Bdb%7DJ(g)%20=%20%5Cfrac%7BdJ%7D%7Bdg%7DJ(g)*%5Cfrac%7Bdg%7D%7Bdb%7Dg(m,b)\"\u003e\u003c/p\u003e\n\u003cp\u003eNow, our next step is to calculate these partial derivatives individually.\u003c/p\u003e\n\u003cp\u003eFrom our earlier calculation of the partial derivative, we know that \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cfrac%7BdJ%7D%7Bdg%7DJ(g(m,b))%20=%20%5Cfrac%7BdJ%7D%7Bdg%7Dg(m,b)%5E2%20=%202*g(m,b)\"\u003e .\u003c/p\u003e\n\u003cp\u003eThe only thing left to calculate is \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cfrac%7Bdg%7D%7Bdb%7Dg(m,b)\"\u003e:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cfrac%7Bdg%7D%7Bdb%7Dg(m,b)%20=%5Cfrac%7Bdg%7D%7Bdb%7D(y%20-%20(mx%20%2b%20b)%20)\"\u003e \u003cbr\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cmspace%7B8.5ex%7D=%5Cfrac%7Bdg%7D%7Bdb%7D(y-mx-b)\"\u003e \u003cbr\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cmspace%7B8.5ex%7D=%5Cfrac%7Bdb%7D%7Bdb%7Dy-%5Cfrac%7Bdb%7D%7Bdb%7Dmx-%5Cfrac%7Bdg%7D%7Bdb%7Db\"\u003e \u003cbr\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cmspace%7B8.5ex%7D=0-0-1\"\u003e \u003cbr\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cmspace%7B8.5ex%7D=%20-1\"\u003e\u003c/p\u003e\n\u003cp\u003eNow we plug our terms into our chain rule [2] and get:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Ccolor%7Bblue%7D%7B%5Cfrac%7BdJ%7D%7Bdg%7DJ(g)%7D%5Ccolor%7Bblack%7D%7B*%7D%5Ccolor%7Bred%7D%7B%5Cfrac%7Bdg%7D%7Bdb%7Dg(m,b)%7D%20%5Ccolor%7Bblack%7D%7B=%7D%20%5Ccolor%7Bblue%7D%7B2*g(m,b)%7D*%5Ccolor%7Bred%7D%7B-1%7D\"\u003e \u003cbr\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cmspace%7B16ex%7D=-2*(y%20-%20(mx%20%2b%20b))\"\u003e\u003c/p\u003e","exportId":"gradient-to-cost-function-appendix"}]},{"id":16463,"name":"Topic 24: Feature Selection, Ridge and Lasso","status":"unlocked","unlockDate":null,"prereqs":[],"requirement":"all","sequential":false,"exportId":"g548fbd0a2b4e4c9cf05da71662e66d15","items":[{"id":153997,"title":"Regularization - Introduction","type":"WikiPage","indent":0,"locked":false,"requirement":"must_mark_done","completed":false,"content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-regularization-intro-v2-1\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-regularization-intro-v2-1\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-regularization-intro-v2-1/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eIn an attempt to fit a good model to data, we often tend to overfit. In this section, you will learn about Regularization, a technique used to avoid overfitting. Regularization discourages overly complex models by penalizing the loss function.\u003c/p\u003e\n\u003ch2\u003eRidge and Lasso\u003c/h2\u003e\n\u003cp\u003eRidge and Lasso regression are two examples of penalized estimation. Penalized estimation makes some or all of the coefficients smaller in magnitude (closer to zero). Some of the penalties have the property of performing both variable selection (setting some coefficients exactly equal to zero) and shrinking the other coefficients.\u003c/p\u003e\n\u003cp\u003eIn Ridge regression, the cost function is changed by adding a penalty term to the square of the magnitude of the coefficients.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Ctext%7BRidge%20cost%20function%7D=%20%5Csum_%7Bi=1%7D%5En(y_i%20-%20%5Chat%7By%7D)%5E2%20=%20%5Csum_%7Bi=1%7D%5En(y_i%20-%20%5Csum_%7Bj=1%7D%5Ek(m_jx_%7Bij%7D)-b)%5E2%20%2b%20%5Clambda%20%5Csum_%7Bj=1%7D%5Ep%20m_j%5E2\"\u003e\u003c/p\u003e\n\u003cp\u003eLasso regression is very similar to Ridge regression, except that the magnitude of the coefficients are not squared in the penalty term.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Ctext%7BLASSO%20cost%20function%7D=%20%5Csum_%7Bi=1%7D%5En(y_i%20-%20%5Chat%7By%7D)%5E2%20=%20%5Csum_%7Bi=1%7D%5En(y_i%20-%20%5Csum_%7Bj=1%7D%5Ek(m_jx_%7Bij%7D)-b)%5E2%20%2b%20%5Clambda%20%5Csum_%7Bj=1%7D%5Ep%20%5Cmid%20m_j%20%5Cmid\"\u003e\u003c/p\u003e\n\u003ch2\u003eAIC and BIC\u003c/h2\u003e\n\u003cp\u003eIn this section you'll also be introduced to two new measures: AIC and BIC, which give you a comprehensive measure of model performace taking into account the additional variables.\u003c/p\u003e\n\u003cp\u003eThe formula for the AIC, invented by Hirotugu Akaike in 1973 and short for \"Akaike's Information Criterion\" is given by:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Ctext%7BAIC(model)%7D%20=%20-%202%20*%20%5Ctext%7Blog-likelihood(model)%7D%20%2b%202%20*%20%5Ctext%7Blength%20of%20the%20parameter%20space%7D\"\u003e\u003c/p\u003e\n\u003cp\u003eThe BIC (Bayesian Information Criterion) is very similar to the AIC and emerged as a Bayesian response to the AIC, but can be used for the exact same purposes.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Ctext%7BBIC(model)%7D%20=%20-2%20*%20%5Ctext%7Blog-likelihood(model)%7D%20%2b%20%5Ctext%7Blog(number%20of%20observations)%7D%20*%20%5Ctext%7B(length%20of%20the%20parameter%20space)%7D\"\u003e\u003c/p\u003e\n\u003cp\u003eLower the values of AIC and BIC, the better your model is performing.\u003c/p\u003e\n\u003ch2\u003eGenerating Data\u003c/h2\u003e\n\u003cp\u003eFinally, you will also see how you can generate practice datasets that allow testing and debugging of algorithms.\u003c/p\u003e\n\u003ch2\u003eSummary\u003c/h2\u003e\n\u003cp\u003eRegularization helps prevent overfitting your models. It is also useful when performing feature selection. On the other hand, if you are comparing multiple models with varying number of features, you can use the AIC and BIC measures. Remember that the lower the AIC (or BIC), the better the model.\u003c/p\u003e","exportId":"regularization-introduction"},{"id":154000,"title":"Ridge and Lasso Regression","type":"Assignment","indent":0,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_mark_done","completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-ridge-and-lasso-regression\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-ridge-and-lasso-regression/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","exportId":"g02cf089874c3fd09715748e0c0550454"},{"id":154003,"title":"Ridge and Lasso Regression - Lab","type":"Assignment","indent":1,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_mark_done","completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-ridge-and-lasso-regression-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-ridge-and-lasso-regression-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","exportId":"g2972e2db8767234f30f672e218a5337f"},{"id":154006,"title":"Feature and Model Selection: AIC and BIC","type":"WikiPage","indent":0,"locked":false,"requirement":"must_mark_done","completed":false,"content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-feature-and-model-selection-aic-and-bic\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-feature-and-model-selection-aic-and-bic\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-feature-and-model-selection-aic-and-bic/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003ePreviously, you've seen how you can select ways of assessing your model fit metrics like MSE, SSE, and \u003cimg src=\"https://render.githubusercontent.com/render/math?math=R%5E2\"\u003e . These values almost always improve when adding more variables, so if you only use these metrics to determine the optimal features of your model, it is highly likely that you will overfit the model to your data. In this lesson you'll be introduced to two new measures: AIC and BIC, which give you a comprehensive measure of model performace taking into account the additional variables.\u003c/p\u003e\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\u003cp\u003eYou will be able to:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eDefine AIC and BIC in the context of assessing model fit\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003eAIC\u003c/h2\u003e\n\u003cp\u003eThe formula for the AIC, invented by Hirotugu Akaike in 1973 and short for \"Akaike's Information Criterion\" is given by:\u003c/p\u003e\n\u003ch4\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Ctext%7BAIC%7D%20=%20-2%5Cln(%5Chat%7BL%7D)%20%2b%202k\"\u003e\u003c/h4\u003e\n\u003cp\u003eWhere:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cimg src=\"https://render.githubusercontent.com/render/math?math=k\"\u003e : length of the parameter space (i.e. the number of features)\u003c/li\u003e\n\u003cli\u003e\n\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Chat%7BL%7D\"\u003e : the maximum value of the likelihood function for the model\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eAnother way to phrase the equation is:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Ctext%7BAIC(model)%7D%20=%20%20-%202%20*%20%5Ctext%7Blog-likelihood(model)%7D%20%2b%202%20*%20%5Ctext%7Blength%20of%20the%20parameter%20space%7D\"\u003e\u003c/p\u003e\n\u003cp\u003eThe AIC is generally used to compare each candidate model. The nice thing about the AIC is that for every model that uses Maximum Likelihood Estimation, the log-likelihood is automatically computed, and as a consequence, the AIC is very easy to calculate.\u003c/p\u003e\n\u003cp\u003eThe AIC acts as a penalized log-likelihood criterion, giving a balance between a good fit (high value of log-likelihood) and complexity (complex models are penalized more than fairly simple ones). The AIC is unbounded so it can take any type of value, but the bottom line is that when comparing models, the model with the \u003cstrong\u003elowest\u003c/strong\u003e AIC should be selected.\u003c/p\u003e\n\u003cp\u003eNote that directly comparing the values of log-likelihood maxima for different models (without including the penalty) is not good enough for model comparison because including more parameters in a model will \u003cem\u003ealways\u003c/em\u003e give rise to an increased value of the maximum likelihood. Due to that reason, searching for the model with maximal log-likelihood would always lead to the model with the most parameters. The AIC balances this by penalizing for the number of parameters, hence searching for models with few parameters but fitting the data well.\u003c/p\u003e\n\u003cp\u003eIn Python, the AIC is built into \u003ccode\u003estatsmodels\u003c/code\u003e and in \u003ccode\u003esklearn\u003c/code\u003e (such as \u003ccode\u003eLassoLarsIC\u003c/code\u003e, which you'll use in the upcoming lab).\u003c/p\u003e\n\u003ch2\u003eBIC\u003c/h2\u003e\n\u003cp\u003eThe BIC (Bayesian Information Criterion) is very similar to the AIC and emerged as a Bayesian response to the AIC, but can be used for the exact same purposes. The idea is to select the candidate model with the highest probability given the data. This idea can be formalized inside a Bayesian framework, involving prior probabilities on candidate models along with prior densities on all parameters in the models. The penalty is slightly changed and depends on the number of rows in the dataset:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Ctext%7BBIC%7D%20=%20-%202%5Cln(%5Chat%7BL%7D)%20%2b%20%5Cln(n)%20*%20k\"\u003e\u003c/p\u003e\n\u003cp\u003eWhere:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Chat%7BL%7D\"\u003e and \u003cimg src=\"https://render.githubusercontent.com/render/math?math=k\"\u003e are the same as in AIC\u003c/li\u003e\n\u003cli\u003e\n\u003cimg src=\"https://render.githubusercontent.com/render/math?math=n\"\u003e : the number of data points (the sample size)\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eAnother way to phrase the equation is:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Ctext%7BBIC(model)%7D%20=%20-2%20*%20%5Ctext%7Blog-likelihood(model)%7D%20%2b%20%5Ctext%7Blog(number%20of%20observations)%7D%20*%20%5Ctext%7B(length%20of%20the%20parameter%20space)%7D\"\u003e\u003c/p\u003e\n\u003cp\u003eLike the AIC, the \u003cstrong\u003elower\u003c/strong\u003e your BIC, the better your model is performing.\u003c/p\u003e\n\u003ch2\u003eUses of AIC and BIC\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003ePerforming feature selection: comparing models with only a few variables and more variables, computing the AIC/BIC and select the features that generated the lowest AIC or BIC\u003c/li\u003e\n\u003cli\u003eSimilarly, selecting or not selecting interactions/polynomial features depending on whether or not the AIC/BIC decreases when adding them in\u003c/li\u003e\n\u003cli\u003eComputing the AIC and BIC for several values of the regularization parameter in Ridge/Lasso models and selecting the best regularization parameter, and many more!\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003eSummary\u003c/h2\u003e\n\u003cp\u003eGreat! In this lesson you learned about AIC and BIC, two measures that are helpful when comparing and evaluating models with varying number of features.\u003c/p\u003e","exportId":"feature-and-model-selection-aic-and-bic"},{"id":154008,"title":"Feature Selection Methods","type":"Assignment","indent":0,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_mark_done","completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-feature-selection-methods\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-feature-selection-methods/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","exportId":"g5d3f046c022e8266135a7dcdb886e34e"},{"id":154012,"title":"Extensions to Linear Models - Lab","type":"Assignment","indent":1,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_mark_done","completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-extensions-to-linear-models-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-extensions-to-linear-models-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","exportId":"gde5a6dd6ef6b31fc0a79a676100531bf"},{"id":154015,"title":"Generating Data","type":"Assignment","indent":0,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_mark_done","completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-generating-data\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-generating-data/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","exportId":"g33d17fe2b6113510b4cff5de22005e96"},{"id":154018,"title":"Generating Data - Lab","type":"Assignment","indent":1,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_mark_done","completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-generating-data-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-generating-data-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","exportId":"ga71184eb3d0d5c34ffb33847b710fa21"},{"id":154020,"title":"Regularization - Recap","type":"WikiPage","indent":0,"locked":false,"requirement":"must_mark_done","completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-regularization-recap-v2-1\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-regularization-recap-v2-1/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\u003ch2\u003eKey Takeaways\u003c/h2\u003e\n\n\u003cul\u003e\n\u003cli\u003eRegularization discourages overly complex models by penalizing the loss function \u003c/li\u003e\n\u003cli\u003eLasso and Ridge are two commonly used so-called regularization techniques \u003c/li\u003e\n\u003cli\u003eIn Ridge regression, the cost function is changed by adding a penalty term to the square of the magnitude of the coefficients \u003c/li\u003e\n\u003cli\u003eRidge regression is often also referred to as L2 Norm Regularization \u003c/li\u003e\n\u003cli\u003eLasso regression is very similar to Ridge regression, except that the magnitude of the coefficients are not squared in the penalty term \u003c/li\u003e\n\u003cli\u003eLasso regression is often also referred to as L1 Norm Regularization \u003c/li\u003e\n\u003cli\u003eAIC and BIC are two measures which give you a comprehensive measure of model performace taking into account the varying number of features\u003cbr\u003e\n\u003c/li\u003e\n\u003cli\u003eThe lower the AIC and/or BIC, the better the model \u003c/li\u003e\n\u003c/ul\u003e\n\u003cfooter class=\"fis-footer\" style=\"visibility: hidden;\"\u003e\u003cdiv class=\"fis-feedback\"\u003e\n\u003ch5\u003eHow do you feel about this lesson?\u003c/h5\u003e\n\u003cimg id=\"thumbs-up\" data-repository=\"dsc-regularization-recap-v2-1\" title=\"Thumbs up!\" alt=\"thumbs up\"\u003e\u003cimg id=\"thumbs-down\" data-repository=\"dsc-regularization-recap-v2-1\" title=\"Thumbs down!\" alt=\"thumbs down\"\u003e\n\u003c/div\u003e\n\u003ch5\u003eHave specific feedback? \u003ca href=\"https://github.com/learn-co-curriculum/dsc-regularization-recap-v2-1/issues/new\"\u003eTell us here!\u003c/a\u003e\n\u003c/h5\u003e\u003c/footer\u003e","exportId":"regularization-recap"}]},{"id":16467,"name":"Topic 25: Introduction to Logistic Regression","status":"started","unlockDate":null,"prereqs":[],"requirement":"all","sequential":false,"exportId":"g6367485b94456be8389e28a407a30f7b","items":[{"id":154027,"title":"Logistic Regression - Introduction","type":"WikiPage","indent":0,"locked":false,"requirement":"must_mark_done","completed":true,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-logistic-regression-intro\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-logistic-regression-intro/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eIn this you'll be introduced to a new type of machine learning technique: classification! You'll learn about an algorithm called logistic regression as well as different ways that data scientists can evaluate the performance of classification models.\u003c/p\u003e\n\n\u003ch2\u003eLogistic Regression\u003c/h2\u003e\n\n\u003cp\u003eYou're familiar with linear regression to predict continuous values. You're now going to return to regression to look at how it can be used as a classifier instead to determine the likelihood of a given data point being associated with one of two categories.\u003c/p\u003e\n\n\u003cp\u003eWe'll start by introducing the sigmoid function and showing how it can be used to fit a curve that matches a binary classifier (e.g. does someone make over or under $40k a year or are they a good or bad credit risk).\u003c/p\u003e\n\n\u003ch2\u003eEvaluating Classifiers\u003c/h2\u003e\n\n\u003cp\u003eWe'll then look at the practicalities of evaluating logistic regression models based on precision, recall, and accuracy to evaluate other classifiers.\u003c/p\u003e\n\n\u003cp\u003eWe also take a little time to look at how to plot a confusion matrix for a logistic regression classifier and introduce a couple of key concepts for determining the optimal precision-recall trade-off for a given classifier - Receiver Operating Characteristic (ROC) curves and AUC (the Area Under the Curve).\u003c/p\u003e\n\n\u003ch2\u003eClass Imbalance Problems\u003c/h2\u003e\n\n\u003cp\u003eWe then introduce the concept of class imbalance. Imagine a classifier for cancer where only 1 screened individual in 1000 is sick. You could obtain over 99 percent accuracy by just saying everyone is fine, but that wouldn't be a very useful approach. We look at the ideas of class weights and over/undersampling and how they can be used to work with highly imbalanced classes.\u003c/p\u003e\n\n\u003ch2\u003eSummary\u003c/h2\u003e\n\n\u003cp\u003eIt's important to be aware of logistic regression as one of the most basic classifiers that you can use, and many of the concepts around model evaluation will be useful whenever you're trying to solve a classification problem.\u003c/p\u003e\n\u003cfooter class=\"fis-footer\" style=\"visibility: hidden;\"\u003e\u003cdiv class=\"fis-feedback\"\u003e\n\u003ch5\u003eHow do you feel about this lesson?\u003c/h5\u003e\n\u003cimg id=\"thumbs-up\" data-repository=\"dsc-logistic-regression-intro\" title=\"Thumbs up!\" alt=\"thumbs up\"\u003e\u003cimg id=\"thumbs-down\" data-repository=\"dsc-logistic-regression-intro\" title=\"Thumbs down!\" alt=\"thumbs down\"\u003e\n\u003c/div\u003e\n\u003ch5\u003eHave specific feedback? \u003ca href=\"https://github.com/learn-co-curriculum/dsc-logistic-regression-intro/issues/new\"\u003eTell us here!\u003c/a\u003e\n\u003c/h5\u003e\u003c/footer\u003e","exportId":"logistic-regression-introduction"},{"id":154030,"title":"Introduction to Supervised Learning","type":"WikiPage","indent":0,"locked":false,"requirement":"must_mark_done","completed":true,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-intro-to-supervised-learning-v2-1\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-intro-to-supervised-learning-v2-1/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eIn this lesson, we'll examine what exactly the term \"Supervised Learning\" means, and where it fits in Data Science. \u003c/p\u003e\n\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\n\u003cul\u003e\n\u003cli\u003eDescribe the components of what makes something a supervised learning task \u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch2\u003eWhat is Supervised Learning?\u003c/h2\u003e\n\n\u003cp\u003eThe term \u003cstrong\u003e\u003cem\u003eSupervised Learning\u003c/em\u003e\u003c/strong\u003e refers to a class of machine learning algorithms that can \"learn\" a task through \u003cstrong\u003e\u003cem\u003elabeled training data\u003c/em\u003e\u003c/strong\u003e. We'll explore this definition more fully in a bit -- but first, it's worth taking some time to understand where supervised learning fits in the overall picture in regards to Data Science. By now, you've probably noticed that many of the things we've learned in Data Science and Computer Science are very hierarchical. This is especially true when it comes to AI and Machine Learning. Let's break down the hierarchy a bit, and see where \u003cstrong\u003e\u003cem\u003eSupervised Learning\u003c/em\u003e\u003c/strong\u003e fits.  \u003c/p\u003e\n\n\u003ch2\u003eArtificial Intelligence\u003c/h2\u003e\n\n\u003cp\u003eAt the top of the hierarchy is \u003cstrong\u003e\u003cem\u003eArtificial Intelligence\u003c/em\u003e\u003c/strong\u003e.  AI is a catch-all term for various kinds of algorithms that can complete tasks that normally require human intelligence to complete. AI is made up of several subcategories, and is also a subcategory itself in the greater hierarchy of Computer Science. When data scientists talk about AI, we're almost focused on a single branch of AI, \u003cstrong\u003e\u003cem\u003eMachine Learning\u003c/em\u003e\u003c/strong\u003e. Machine Learning is responsible for the boom in AI technologies and abilities in the last few decades, but it's worth noting that there are other areas of AI that do not fall under the umbrella of 'Machine Learning'. Other branches of AI include things like \u003cem\u003eGenetic Algorithms\u003c/em\u003e for optimization, or rules-based AI for things like building a bot for players to play against in a video game. While these are still active areas of research, they have little to no application in Data Science, so they're beyond the scope of this lesson. In general, when you see the phrase 'Artificial Intelligence', it's generally safe to assume that that the speaker is probably referring to the subfield of AI known as \u003cstrong\u003e\u003cem\u003eMachine Learning\u003c/em\u003e\u003c/strong\u003e (which is also sometimes referred to by it's older, more traditional name -- \u003cstrong\u003e\u003cem\u003eStatistical Learning\u003c/em\u003e\u003c/strong\u003e).\u003c/p\u003e\n\n\u003cp\u003eThe following graphic shows the breakdown of the 'Machine Learning' branch of AI:\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-intro-to-supervised-learning-v2-1/master/images/new_ml-hierarchy.png\" width=\"600\"\u003e\u003c/p\u003e\n\n\u003ch2\u003eMachine Learning\u003c/h2\u003e\n\n\u003cp\u003eThe field of \u003cem\u003eMachine Learning\u003c/em\u003e can be further divided into two overall categories:\u003c/p\u003e\n\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003e\u003cem\u003eSupervised Learning\u003c/em\u003e\u003c/strong\u003e\u003c/li\u003e\n\u003cli\u003e\u003cem\u003eUnsupervised Learning\u003c/em\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\n\u003cp\u003eThe main difference between these two areas of machine learning is the need for \u003cstrong\u003e\u003cem\u003elabeled training data\u003c/em\u003e\u003c/strong\u003e. In \u003cstrong\u003e\u003cem\u003eSupervised Learning\u003c/em\u003e\u003c/strong\u003e, any data used must have a \u003cstrong\u003e\u003cem\u003elabel\u003c/em\u003e\u003c/strong\u003e. These labels are the \u003cem\u003eground truth\u003c/em\u003e , which allows our supervised learning algorithms to 'check their work'. By comparing its predictions against the actual labels, our algorithm can learn to make less incorrect predictions and improve the overall performance of the task its learning to do. It helps to think of Supervised Learning as close to the type of learning we do as students in grade school. Imagine using practice exams to study for the SAT or ACT test. We can go through all the practice questions we want, but in order to learn from our performance on those practice questions, we need to know what the correct answers are! Without them, we would have no way of knowing which questions we got right and which ones we got wrong, so we wouldn't be able to learn what changes we would need to make to improve our overall performance! \u003c/p\u003e\n\n\u003cblockquote\u003e\n\u003cp\u003e\"A computer program is said to \u003cstrong\u003elearn\u003c/strong\u003e from experience \u003cem\u003eE\u003c/em\u003e with respect to some class of tasks \u003cem\u003eT\u003c/em\u003e and performance measure \u003cem\u003eP\u003c/em\u003e, if its performance at tasks in \u003cem\u003eT\u003c/em\u003e, as measured by \u003cem\u003eP\u003c/em\u003e, improves with experience \u003cem\u003eE\u003c/em\u003e.\"  -- \u003ca href=\"http://www.cs.cmu.edu/%7Etom/\"\u003eTom Mitchell\u003c/a\u003e\u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003cp\u003eLet's pretend we've built and trained a model to detect if a picture contains a cat or not. Using the language from the definition above:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003e\n\u003cstrong\u003eTask (T)\u003c/strong\u003e: predict if a picture contains a cat or not\u003c/li\u003e\n\u003cli\u003e\n\u003cstrong\u003ePerformance Measure (P)\u003c/strong\u003e: The objective function used to score the predictions made by our model for each image\u003c/li\u003e\n\u003cli\u003e\n\u003cstrong\u003eExperience (E)\u003c/strong\u003e: All of our labeled training data. The more training data we provide, the more 'experience' our model gets!\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003eWe'll spend some time learning about \u003cstrong\u003e\u003cem\u003eUnsupervised Learning\u003c/em\u003e\u003c/strong\u003e in the next module, so don't worry about it for now!\u003c/p\u003e\n\n\u003ch2\u003eClassification and Regression\u003c/h2\u003e\n\n\u003cp\u003eThe field of \u003cem\u003eSupervised Learning\u003c/em\u003e can be further broken down into two categories -- \u003cstrong\u003e\u003cem\u003eClassification\u003c/em\u003e\u003c/strong\u003e and \u003cstrong\u003e\u003cem\u003eRegression\u003c/em\u003e\u003c/strong\u003e. At this point in your studies, you already have significant experience with regression -- specifically \u003cstrong\u003eLinear Regression\u003c/strong\u003e , probably the most foundational (and important) machine learning model. Recall that regression allows us to answer questions like \"how much?\" or \"how many?\". If our label is a real-valued number, then the supervised learning problem you're trying to solve is a \u003cem\u003eregression\u003c/em\u003e problem. \u003c/p\u003e\n\n\u003cp\u003eThe other main kind of supervised learning problem is \u003cstrong\u003e\u003cem\u003eClassification\u003c/em\u003e\u003c/strong\u003e. Classification allows us to tell if something belongs to one class or the other. In the case of the \u003ca href=\"https://www.kaggle.com/c/titanic\"\u003etitanic\u003c/a\u003e dataset, this may be something like survival. For example, given various characteristics of a passenger, predict whether they will survive or not. Questions that can be answered in a True/False format (in the titanic example, \"Survived\" or \"Not survived\") are a type of \u003cstrong\u003e\u003cem\u003eBinary Classification\u003c/em\u003e\u003c/strong\u003e. To perform binary classification, you will be introduced to \u003cstrong\u003eLogistic Regression\u003c/strong\u003e. Don't let the name confuse you, although the name contains the word \"regression,\" this important foundational technique is very important in understanding classification problems. There are several other classification techniques you will be learning in this module, but in order to gain a sound understanding of \u003cstrong\u003eClassification\u003c/strong\u003e tasks, this section will be focused exclusively on building and evaluating logistic regression models. \u003c/p\u003e\n\n\u003cp\u003eHowever, we are not limited to only two classes when working with classification algorithms -- we can have as many classes as we see fit. When a supervised learning problem has more than two classes, we refer to it as a \u003cstrong\u003e\u003cem\u003eMulticlass Classification\u003c/em\u003e\u003c/strong\u003e problem. \u003c/p\u003e\n\n\u003ch2\u003eObjective Functions\u003c/h2\u003e\n\n\u003cp\u003eWhenever we're dealing with supervised learning, we have an \u003cstrong\u003e\u003cem\u003eObjective Function\u003c/em\u003e\u003c/strong\u003e (also commonly called a \u003cstrong\u003e\u003cem\u003eLoss Function\u003c/em\u003e\u003c/strong\u003e) that we're trying to optimize against. Regardless of the supervised learning model we're working with, we can be sure that we have some sort of function under the hood that we're using to grade the predictions made by our model against the actual ground-truth labels for each prediction. In the quote from Tom Mitchell listed above, objective functions are \u003cem\u003eP\u003c/em\u003e. While classification and regression models use different kinds of objective functions to evaluate their performance, the concept is the same -- these functions allow the model to evaluate exactly how right or wrong a prediction is, which the algorithm can then \"learn\" from. These objective functions serve an important purpose, because they act as the ground-truth for determining if our model is getting better or not. \u003c/p\u003e\n\n\u003ch3\u003eThe Limitations of Labeled Data\u003c/h3\u003e\n\n\u003cp\u003eBecause supervised learning requires \u003cstrong\u003e\u003cem\u003eLabels\u003c/em\u003e\u003c/strong\u003e for any data used, this severely limits the amount of available data we have for use with supervised learning algorithms.  Of all the data in the world, only a very, very small percentage is labeled. Why? Because labeling data is a purposeful activity that can only be done by humans, and is therefore time-consuming and expensive. In supervised learning, labels are not universal -- they are unique to the problem we're trying to solve. If we're trying to train a model to predict if someone survived the titanic disaster, we need to know the survival results of every passenger in our dataset -- there's no way around it. However, if we're trying to predict how much a person paid for a ticket on the titanic, survival data now no longer works as a label -- instead, we need to know how much each passenger paid for a ticket. In a more generalized sense, this means that for whatever problem we're trying to train a supervised learning model to solve, we need to have a large enough dataset containing examples where humans have already done the things we're trying to get our model to learn how to do.  \u003c/p\u003e\n\n\u003cp\u003eAlthough labeled data is still expensive and time-consuming to get, the internet has made the overall process of getting labeled data a bit easier than it used to be. Nowadays, when companies need to construct a dataset of labeled training data to solve a problem, they typically make use of services like Amazon's \u003ca href=\"https://docs.aws.amazon.com/mturk/index.html\"\u003eAWS Mechanical Turk\u003c/a\u003e, or 'MTurk' for short. Services like this obtain labels by paying people for each label they generate. In this way, a company can crowdsource the work to label the training data needed. The company simply uploads unlabeled training data like an image, and a \"turker\" will then provide a label for that image according to the instructions from the company. Depending on the problem the company is trying to solve, the label for the image might be something as simple as the word \"cat\", or as complex as as boxes drawn around all the cats in the image. \u003c/p\u003e\n\n\u003ch3\u003eNegative Examples\u003c/h3\u003e\n\n\u003cp\u003eWhen creating a labeled dataset for a classification problem, it is worth noting that negative examples are just as important to be included in the dataset as positive examples. If our training data in the titanic dataset only contained data on passengers that all survived, no supervised learning algorithm would be able to learn how to predict if a passenger survived or died with any sort of accuracy. \u003cstrong\u003e\u003cem\u003ePositive Examples\u003c/em\u003e\u003c/strong\u003e are data points that belong to the class we're training our model to recognize. For instance, let's pretend we're building a model to tell if a picture is of a cat or not. All the pictures of cats in our dataset would be positive examples. However, in order to build a good cat classifier, our dataset would also need to contain many different kinds of pictures that don't include cats. Intuitively, this makes sense -- if every picture that our model ever saw had a cat in it, then the only thing that model will learn is that everything is a cat. To truly learn what we need it to learn, this model will also need to learn what a cat \u003cem\u003eisn't\u003c/em\u003e, by looking at pictures that don't include cats -- our \u003cstrong\u003e\u003cem\u003eNegative Examples\u003c/em\u003e\u003c/strong\u003e. In this way, with a complex enough model and enough labeled training data, our classifier will eventually learn that the differentiating factor between images with positive labels and images with negative labels are the shapes and patterns common to cats, but not dogs (or other animals). In this way, supervised learning can be a bit tricky. For instance, if all of the negative examples in our cat classifier dataset are of cars and houses, then the model will almost certainly get a picture of a dog incorrect by predicting that the picture is of a cat. Why does this happen? Because the model hasn't seen a dog before, and therefore has no idea whether this fits. In this particular example, we can guess that any picture of a dog will look more like a cat than it would a house or car, which from the model's perspective means that this is probably a picture of a cat. \u003c/p\u003e\n\n\u003cp\u003eIn summary, this part of supervised learning can often be more art than science -- when creating a dataset, make sure that your dataset contains enough negative examples, and that you are very thoughtful about what those negative examples actually contain! \u003c/p\u003e\n\n\u003ch2\u003eSummary\u003c/h2\u003e\n\n\u003cp\u003eIn this lesson, we learned about \u003cem\u003eSupervised Learning\u003c/em\u003e, and where it fits in relation to Machine Learning and Artificial Intelligence. \u003c/p\u003e\n\u003cfooter class=\"fis-footer\" style=\"visibility: hidden;\"\u003e\u003cdiv class=\"fis-feedback\"\u003e\n\u003ch5\u003eHow do you feel about this lesson?\u003c/h5\u003e\n\u003cimg id=\"thumbs-up\" data-repository=\"dsc-intro-to-supervised-learning-v2-1\" title=\"Thumbs up!\" alt=\"thumbs up\"\u003e\u003cimg id=\"thumbs-down\" data-repository=\"dsc-intro-to-supervised-learning-v2-1\" title=\"Thumbs down!\" alt=\"thumbs down\"\u003e\n\u003c/div\u003e\n\u003ch5\u003eHave specific feedback? \u003ca href=\"https://github.com/learn-co-curriculum/dsc-intro-to-supervised-learning-v2-1/issues/new\"\u003eTell us here!\u003c/a\u003e\n\u003c/h5\u003e\u003c/footer\u003e","exportId":"introduction-to-supervised-learning"},{"id":154033,"title":"Linear to Logistic Regression","type":"Assignment","indent":0,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_mark_done","completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-linear-to-logistic-regression\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-linear-to-logistic-regression/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","exportId":"g63126a9cacec1131b896047af1e9d4e8"},{"id":154036,"title":"Fitting a Logistic Regression Model - Lab","type":"Assignment","indent":1,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_mark_done","completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-fitting-a-logistic-regression-model-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-fitting-a-logistic-regression-model-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","exportId":"gf94b2c5c0d20aa16341b6f6956eadbab"},{"id":154039,"title":"Logistic Regression in scikit-learn","type":"Assignment","indent":0,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_mark_done","completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-logistic-regression-in-scikit-learn\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-logistic-regression-in-scikit-learn/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","exportId":"g24774b938d5ffb2fe4a4d631ff1dbbb6"},{"id":154043,"title":"Logistic Regression in scikit-learn - Lab","type":"Assignment","indent":1,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_mark_done","completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-logistic-regression-in-scikit-learn-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-logistic-regression-in-scikit-learn-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","exportId":"gfdff295f713f47ee6682e21a117d40ff"},{"id":154045,"title":"Confusion Matrices","type":"Assignment","indent":0,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_mark_done","completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-confusion-matrices\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-confusion-matrices/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","exportId":"g377bdea0b6c3635c7c42197af9f2c581"},{"id":154048,"title":"Visualizing Confusion Matrices - Lab","type":"Assignment","indent":1,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_mark_done","completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-visualizing-confusion-matrices-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-visualizing-confusion-matrices-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","exportId":"ge106a5e4ee58121697da7f759dec429f"},{"id":154050,"title":"Evaluation Metrics","type":"WikiPage","indent":0,"locked":false,"requirement":"must_mark_done","completed":false,"content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-evaluation-metrics\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-evaluation-metrics\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-evaluation-metrics/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eIn this lesson, you'll learn about common \u003cstrong\u003e\u003cem\u003eEvaluation Metrics\u003c/em\u003e\u003c/strong\u003e used to quantify the performance of classifiers!\u003c/p\u003e\n\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\n\u003cp\u003eYou will be able to:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eEvaluate classification models using the evaluation metrics appropriate for a specific problem \u003c/li\u003e\n\u003cli\u003eDefine precision and recall \u003c/li\u003e\n\u003cli\u003eDefine accuracy and F1 score \u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch2\u003eEvaluation metrics for classification\u003c/h2\u003e\n\n\u003cp\u003eNow that we've started discussing classification, it's time to examine comparing models to one other and choosing the models that have the best fit. Previously in regression, you were predicting values so it made sense to discuss error as a distance of how far off the estimates were from the actual values. However, in classifying a binary variable you are either correct or incorrect. As a result, we tend to deconstruct this as how many false positives versus false negatives there are in a model. In particular, there are a few different specific measurements when evaluating the performance of a classification algorithm.  \u003c/p\u003e\n\n\u003cp\u003eLet's work through these evaluation metrics to understand what each metric tells us.\u003c/p\u003e\n\n\u003ch2\u003ePrecision and recall\u003c/h2\u003e\n\n\u003cp\u003e\u003cstrong\u003e\u003cem\u003ePrecision\u003c/em\u003e\u003c/strong\u003e and \u003cstrong\u003e\u003cem\u003eRecall\u003c/em\u003e\u003c/strong\u003e are two of the most basic evaluation metrics available to us. \u003cstrong\u003e\u003cem\u003ePrecision\u003c/em\u003e\u003c/strong\u003e measures how precise the predictions are, while \u003cstrong\u003e\u003cem\u003eRecall\u003c/em\u003e\u003c/strong\u003e indicates what percentage of the classes we're interested in were actually captured by the model. \u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-evaluation-metrics/master/./images/new_EvalMatrices.png\" width=\"600\"\u003e\u003c/p\u003e\n\n\u003ch3\u003ePrecision\u003c/h3\u003e\n\n\u003cp\u003eThe following formula shows how to use information found in a confusion matrix to calculate the precision of a model:\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Ctext%7BPrecision%7D%20=%20%5Cfrac%7B%5Ctext%7BNumber%20of%20True%20Positives%7D%7D%7B%5Ctext%7BNumber%20of%20Predicted%20Positives%7D%7D\"\u003e \u003c/p\u003e\n\n\u003cp\u003eTo reuse a previous analogy of a model that predicts whether or not a person has a certain disease, precision allows us to answer the following question:\u003c/p\u003e\n\n\u003cp\u003e\"Out of all the times the model said someone had a disease, how many times did the patient in question actually have the disease?\"\u003c/p\u003e\n\n\u003cp\u003eNote that a high precision score can be a bit misleading.  For instance, let's say we take a model and train it to make predictions on a sample of 10,000 patients. This model predicts that 6000 patients have the disease when in reality, only 5500 have the disease.  This model would have a precision of 91.6%. Now, let's assume we create a second model that only predicts that a person is sick when it's incredibly obvious.  Out of 10,000 patients, this model only predicts that 5 people in the entire population are sick.  However, each of those 5 times, it is correct.  model 2 would have a precision score of 100%, even though it missed 5,495 cases where the patient actually had the disease! In this way, more conservative models can have a high precision score, but this doesn't necessarily mean that they are the \u003cem\u003ebest performing\u003c/em\u003e model!\u003c/p\u003e\n\n\u003ch3\u003eRecall\u003c/h3\u003e\n\n\u003cp\u003eThe following formula shows how we can use the information found in a confusion matrix to calculate the recall of a model:\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Ctext%7BRecall%7D%20=%20%5Cfrac%7B%5Ctext%7BNumber%20of%20True%20Positives%7D%7D%7B%5Ctext%7BNumber%20of%20Actual%20Total%20Positives%7D%7D\"\u003e \u003c/p\u003e\n\n\u003cp\u003eFollowing the same disease analogy, recall allows us to ask:\u003c/p\u003e\n\n\u003cp\u003e\"Out of all the patients we saw that actually had the disease, what percentage of them did our model correctly identify as having the disease?\"\u003c/p\u003e\n\n\u003cp\u003eNote that recall can be a bit of a tricky statistic because improving our recall score doesn't necessarily always mean a better model overall. For example, our model could easily score 100% for recall by just classifying every single patient that walks through the door as having the disease in question. Sure, it would have many False Positives, but it would also correctly identify every single sick person as having the disease!\u003c/p\u003e\n\n\u003ch3\u003eThe relationship between precision and recall\u003c/h3\u003e\n\n\u003cp\u003eAs you may have guessed, precision and recall have an inverse relationship. As our recall goes up, our precision will go down, and vice versa. If this doesn't seem intuitive, let's examine this through the lens of our disease analogy. \u003c/p\u003e\n\n\u003cp\u003eA doctor that is overly obsessed with recall will have a very low threshold for declaring someone as sick because they are most worried about sick patients. Their precision will be quite low, because they classify almost everyone as sick, and don't care when they're wrong -- they only care about making sure that sick people are identified as sick. \u003c/p\u003e\n\n\u003cp\u003eA doctor that is overly obsessed with precision will have a very high threshold for declaring someone as sick, because they only declare someone as sick when they are completely sure that they will be correct if they declare a person as sick. Although their precision will be very high, their recall will be incredibly low, because a lot of people that are sick but don't meet the doctor's threshold will be incorrectly classified as healthy. \u003c/p\u003e\n\n\u003ch3\u003eWhich metric is better?\u003c/h3\u003e\n\n\u003cp\u003eA classic Data Science interview question is to ask \"What is better -- more false positives, or false negatives?\" This is a trick question designed to test your critical thinking on the topics of precision and recall. As you're probably thinking, the answer is \"It depends on the problem!\".  Sometimes, our model may be focused on a problem where False Positives are much worse than False Negatives, or vice versa. For instance, detecting credit card fraud. A False Positive would be when our model flags a transaction as fraudulent, and it isn't.  This results in a slightly annoyed customer. On the other hand, a False Negative might be a fraudulent transaction that the company mistakenly lets through as normal consumer behavior. In this case, the credit card company could be on the hook for reimbursing the customer for thousands of dollars because they missed the signs that the transaction was fraudulent! Although being wrong is never ideal, it makes sense that credit card companies tend to build their models to be a bit too sensitive, because having a high recall saves them more money than having a high precision score.\u003c/p\u003e\n\n\u003cp\u003eTake a few minutes and see if you can think of at least two examples each of situations where a high precision might be preferable to high recall, and two examples where high recall might be preferable to high precision. This is a common interview topic, so it's always handy to have a few examples ready!\u003c/p\u003e\n\n\u003ch2\u003eAccuracy and F1 score\u003c/h2\u003e\n\n\u003cp\u003eThe two most informative metrics that are often cited to describe the performance of a model are \u003cstrong\u003e\u003cem\u003eAccuracy\u003c/em\u003e\u003c/strong\u003e and \u003cstrong\u003e\u003cem\u003eF1 score\u003c/em\u003e\u003c/strong\u003e. Let's take a look at each and see what's so special about them.\u003c/p\u003e\n\n\u003ch3\u003eAccuracy\u003c/h3\u003e\n\n\u003cp\u003e\u003cstrong\u003e\u003cem\u003eAccuracy\u003c/em\u003e\u003c/strong\u003e is probably the most intuitive metric. The formula for accuracy is:\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Ctext%7BAccuracy%7D%20=%20%5Cfrac%7B%5Ctext%7BNumber%20of%20True%20Positives%20%2b%20True%20Negatives%7D%7D%7B%5Ctext%7BTotal%20Observations%7D%7D\"\u003e \u003c/p\u003e\n\n\u003cp\u003eAccuracy is useful because it allows us to measure the total number of predictions a model gets right, including both \u003cstrong\u003e\u003cem\u003eTrue Positives\u003c/em\u003e\u003c/strong\u003e and \u003cstrong\u003e\u003cem\u003eTrue Negatives\u003c/em\u003e\u003c/strong\u003e. \u003c/p\u003e\n\n\u003cp\u003eSticking with our analogy, accuracy allows us to answer:\u003c/p\u003e\n\n\u003cp\u003e\"Out of all the predictions our model made, what percentage were correct?\"\u003c/p\u003e\n\n\u003cp\u003eAccuracy is the most common metric for classification. It provides a solid holistic view of the overall performance of our model. \u003c/p\u003e\n\n\u003ch3\u003eF1 score\u003c/h3\u003e\n\n\u003cp\u003eThe F1 score is a bit more tricky, but also more informative. F1 score represents the \u003cstrong\u003e\u003cem\u003eHarmonic Mean of Precision and Recall\u003c/em\u003e\u003c/strong\u003e.  In short, this means that the F1 score cannot be high without both precision and recall also being high. When a model's F1 score is high, you know that your model is doing well all around. \u003c/p\u003e\n\n\u003cp\u003eThe formula for F1 score is:\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Ctext%7BF1%20score%7D%20=%202%5C%20%5Cfrac%7BPrecision%5C%20x%5C%20Recall%7D%7BPrecision%20%2b%20Recall%7D\"\u003e \u003c/p\u003e\n\n\u003cp\u003eTo demonstrate the effectiveness of F1 score, let's plug in some numbers and compare F1 score with a regular arithmetic average of precision and recall. \u003c/p\u003e\n\n\u003cp\u003eLet's assume that the model has 98% recall and 6% precision.  \u003c/p\u003e\n\n\u003cp\u003eTaking the arithmetic mean of the two, we get:  \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cfrac%7B0.98%20%2b%200.06%7D%7B2%7D%20=%20%5Cfrac%7B1.04%7D%7B2%7D%20=%200.52\"\u003e \u003c/p\u003e\n\n\u003cp\u003eHowever, using these numbers in the F1 score formula results in:\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Ctext%7BF1%20score%7D%20=%202%20%5Cfrac%7B0.98%20*%200.06%7D%7B0.98%20%2b%200.06%7D%20=%202%20%5Cfrac%7B0.0588%7D%7B1.04%7D%20=%202(0.061152)%20=%200.122304\"\u003e or 12.2%!\u003c/p\u003e\n\n\u003cp\u003eAs you can see, F1 score penalizes models heavily if it skews too hard towards either precision or recall. For this reason, F1 score is generally the most used metric for describing the performance of a model. \u003c/p\u003e\n\n\u003ch2\u003eWhich metric to use?\u003c/h2\u003e\n\n\u003cp\u003eThe metrics that are most important to a project will often be dependent on the business use case or goals for that model. This is why it's \u003cstrong\u003e\u003cem\u003every important\u003c/em\u003e\u003c/strong\u003e to understand why you're doing what you're doing, and how your model will be used in the real world! Otherwise, you may optimize your model for the wrong metric! \u003c/p\u003e\n\n\u003cp\u003eIn general, it is worth noting that it's a good idea to calculate all relevant metrics, when in doubt.  In most classification tasks, you don't know which model will perform best when you start. The common workflow is to train each different type of classifier, and select the best by comparing the performance of each. It's common to make tables like the one below, and highlight the best performer for each metric:\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-evaluation-metrics/master/./images/performance-comparisons.png\"\u003e\u003c/p\u003e\n\n\u003ch2\u003eCalculate evaluation metrics with confusion matrices\u003c/h2\u003e\n\n\u003cp\u003eNote that we can only calculate any of the metrics discussed here if we know the \u003cstrong\u003e\u003cem\u003eTrue Positives, True Negatives, False Positives, and False Negatives\u003c/em\u003e\u003c/strong\u003e resulting from the predictions of a model. If we have a confusion matrix, we can easily calculate \u003cstrong\u003e\u003cem\u003ePrecision\u003c/em\u003e\u003c/strong\u003e, \u003cstrong\u003e\u003cem\u003eRecall\u003c/em\u003e\u003c/strong\u003e and \u003cstrong\u003e\u003cem\u003eAccuracy\u003c/em\u003e\u003c/strong\u003e -- and if we know precision and recall, we can easily calculate \u003cstrong\u003e\u003cem\u003eF1 score\u003c/em\u003e\u003c/strong\u003e!\u003c/p\u003e\n\n\u003ch2\u003eClassification reports\u003c/h2\u003e\n\n\u003cp\u003eScikit-learn has a built-in function that will create a \u003cstrong\u003e\u003cem\u003eClassification Report\u003c/em\u003e\u003c/strong\u003e. This classification report even breaks down performance by individual class predictions for your model. You can find the \u003ccode\u003eclassification_report()\u003c/code\u003e function in the \u003ccode\u003esklearn.metrics\u003c/code\u003e module, which takes labels and predictions and returns the precision, recall, F1 score and support (number of occurrences of each label in \u003ccode\u003ey_true\u003c/code\u003e) for the results of a model. \u003c/p\u003e\n\n\u003ch2\u003eSummary\u003c/h2\u003e\n\n\u003cp\u003eIn this lesson you were introduced to several metrics which can be used to evaluate classification models. In the following lab, you'll write functions to calculate each of these manually, as well as explore how you can use existing functions in scikit-learn to quickly calculate and interpret each of these metrics. \u003c/p\u003e","exportId":"evaluation-metrics"},{"id":154052,"title":"Evaluating Logistic Regression Models - Lab","type":"Assignment","indent":1,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_mark_done","completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-evaluating-logistic-regression-models-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-evaluating-logistic-regression-models-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","exportId":"g015f7ecd2ca8f6372c1e2b59c0d13d21"},{"id":154054,"title":"ROC Curves and AUC","type":"Assignment","indent":0,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_mark_done","completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-roc-curves-and-auc\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-roc-curves-and-auc/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","exportId":"g4ce8d159a0531ff71494a2d07f275387"},{"id":154057,"title":"ROC Curves and AUC - Lab","type":"Assignment","indent":1,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_mark_done","completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-roc-curves-and-auc-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-roc-curves-and-auc-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","exportId":"gafbf3bc1cbeaddedc666b5a4ee36b509"},{"id":154060,"title":"Class Imbalance Problems","type":"Assignment","indent":0,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_mark_done","completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-class-imbalance-problems\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-class-imbalance-problems/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","exportId":"g66166d60e0c5f0d6e14ab83e4483c695"},{"id":154063,"title":"Class Imbalance Problems - Lab","type":"Assignment","indent":1,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_mark_done","completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-class-imbalance-problems-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-class-imbalance-problems-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","exportId":"ga286fdb3f1a20bbcc89b41b52975bdde"},{"id":154065,"title":"Logistic Regression - Recap","type":"WikiPage","indent":0,"locked":false,"requirement":"must_mark_done","completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-logistic-regression-section-recap\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-logistic-regression-section-recap/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\u003ch2\u003eKey Takeaways\u003c/h2\u003e\n\n\u003cul\u003e\n\u003cli\u003eIn this section you learned about a different supervised learning technique: classification! Specifically, you practiced building a very basic classification model from scratch - a logistic regression model \u003c/li\u003e\n\u003cli\u003eLogistic regression uses a sigmoid function which helps to plot an \"s\"-like curve that enables a linear function to act as a binary classifier\u003c/li\u003e\n\u003cli\u003eYou can evaluate logistic regression models using some combination of precision, recall, and accuracy\u003c/li\u003e\n\u003cli\u003eA confusion matrix is another common way to visualize the performance of a classification model\u003c/li\u003e\n\u003cli\u003eReceiver Operating Characteristic (ROC) curve and the Area Under the Curve (AUC) can be used to help determine the best precision-recall tradeoff for a given classifier\u003c/li\u003e\n\u003cli\u003eClass weights, under/oversampling, and SMOTE can be used to deal with class imbalance problems\u003c/li\u003e\n\u003c/ul\u003e\n\u003cfooter class=\"fis-footer\" style=\"visibility: hidden;\"\u003e\u003cdiv class=\"fis-feedback\"\u003e\n\u003ch5\u003eHow do you feel about this lesson?\u003c/h5\u003e\n\u003cimg id=\"thumbs-up\" data-repository=\"dsc-logistic-regression-section-recap\" title=\"Thumbs up!\" alt=\"thumbs up\"\u003e\u003cimg id=\"thumbs-down\" data-repository=\"dsc-logistic-regression-section-recap\" title=\"Thumbs down!\" alt=\"thumbs down\"\u003e\n\u003c/div\u003e\n\u003ch5\u003eHave specific feedback? \u003ca href=\"https://github.com/learn-co-curriculum/dsc-logistic-regression-section-recap/issues/new\"\u003eTell us here!\u003c/a\u003e\n\u003c/h5\u003e\u003c/footer\u003e","exportId":"logistic-regression-recap"}]},{"id":16475,"name":"Topic 26: MLE and Logistic Regression","status":"unlocked","unlockDate":null,"prereqs":[],"requirement":"all","sequential":false,"exportId":"g9ac55323099ddbdec167c09d58ac2f43","items":[{"id":154070,"title":"MLE and Logistic Regression - Introduction","type":"WikiPage","indent":0,"locked":false,"requirement":"must_mark_done","completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-mle-logistic-reg-intro\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-mle-logistic-reg-intro/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eIn this section, you'll review maximum likelihood estimation and logistic regression. In fact, you'll see that logistic regression can be seen through a statistical point of view with MLE. This should provide you some additional time to wrangle with statistical concepts and improve your overall coding abilities.\u003c/p\u003e\n\n\u003ch2\u003eMaximum Likelihood Estimation\u003c/h2\u003e\n\n\u003cp\u003eMaximum likelihood estimation is a statistical procedure for determining underlying parameter distributions. As the name implies, the underlying motivation is to find parameters that maximize the theoretical chances of observing the actual observations.\u003c/p\u003e\n\n\u003ch2\u003eLogistic Regression\u003c/h2\u003e\n\n\u003cp\u003eLogistic regression, despite its name, is a classification algorithm. An interesting nuance is that it provides confidence values with its predictions since the raw output is a probability of a class between 0 and 1. The general process for this is similar to linear regression, where coefficients for various feature weights are altered in order to optimize the accuracy of subsequent predictions from the model. \u003c/p\u003e\n\n\u003ch2\u003eSummary\u003c/h2\u003e\n\n\u003cp\u003eIn this lesson, you got a brief overview of the concepts that will covered in this section. With that, dive in and continue to review your knowledge!\u003c/p\u003e\n\u003cfooter class=\"fis-footer\" style=\"visibility: hidden;\"\u003e\u003cdiv class=\"fis-feedback\"\u003e\n\u003ch5\u003eHow do you feel about this lesson?\u003c/h5\u003e\n\u003cimg id=\"thumbs-up\" data-repository=\"dsc-mle-logistic-reg-intro\" title=\"Thumbs up!\" alt=\"thumbs up\"\u003e\u003cimg id=\"thumbs-down\" data-repository=\"dsc-mle-logistic-reg-intro\" title=\"Thumbs down!\" alt=\"thumbs down\"\u003e\n\u003c/div\u003e\n\u003ch5\u003eHave specific feedback? \u003ca href=\"https://github.com/learn-co-curriculum/dsc-mle-logistic-reg-intro/issues/new\"\u003eTell us here!\u003c/a\u003e\n\u003c/h5\u003e\u003c/footer\u003e","exportId":"mle-and-logistic-regression-introduction"},{"id":154073,"title":"MLE Review","type":"WikiPage","indent":0,"locked":false,"requirement":"must_mark_done","completed":false,"content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-mle-review\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-mle-review\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-mle-review/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eYou've seen MLE (Maximum Likelihood Estimation) when discussing Bayesian statistics, but did you know logistic regression can also be seen from this statistical perspective? In this section, you'll gain a deeper understanding of logistic regression by coding it from scratch and analyzing the statistical motivations backing it. But first take some time to review maximum likelihood estimation.\u003c/p\u003e\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\u003cp\u003eYou will be able to:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eDescribe how to take MLE of a binomial variable\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003eMLE\u003c/h2\u003e\n\u003cp\u003eMaximum likelihood estimation can often sound academic, confusing, and cryptic when first introduced. It is often presented and introduced with complex integrals of statistical distributions that scare away many readers. Hopefully, this hasn't been your experience. While the mathematics can quickly become complex, the underlying concepts are actually quite intuitive.\u003c/p\u003e\n\u003cp\u003eTo demonstrate this, imagine a simple coin flipping example. Let's say that you flip a coin 100 times and get 55 heads. Maximum likelihood estimation attempts to uncover the underlying theoretical probability of this coin landing on heads given your observations. In other words, given the observations, what is the chance that the coin was fair and had a 0.5 chance of landing on heads each time? Or what is the chance that the coin actually had a 0.75 probability of lands of heads, given what we observed? It turns out that the answer to these questions is rather intuitive. If you observe 55 out of 100 coin flips, the underlying probability which maximizes the chance of us observing 55 out of 100 coin flips is 0.55. In this simple example, MLE simply returns the current sample mean as the underlying parameter that makes the observations most probable. Slight deviations to this would be almost as probable but slightly less so, and large deviations from our sample mean should be rare. This intuitively makes some sense; as your sample size increases, you expect the sample mean to converge to the true underlying parameter. MLE takes a flipped perspective, asking what underlying parameter is most probable given the observations.\u003c/p\u003e\n\u003ch2\u003eLog-likelihood\u003c/h2\u003e\n\u003cp\u003eWhen calculating maximum likelihood, it is common to use the log-likelihood, as taking the logarithm can simplify calculations. For example, taking the logarithm of a set of products allows you to decompose the problem from products into sums. (You may recall from high school mathematics that \u003cimg src=\"https://render.githubusercontent.com/render/math?math=x%5E%7B(a%2bb)%7D%20=%20x%5Ea%20*%20x%5Eb\"\u003e . Similarly, taking the logarithm of both sides of a function allows you to transform products into sums.\u003c/p\u003e\n\u003ch2\u003eMLE for a binomial variable\u003c/h2\u003e\n\u003cp\u003eLet's take a deeper mathematical investigation into the coin flipping example above.\u003c/p\u003e\n\u003cp\u003eIn general, if you were to observe \u003cimg src=\"https://render.githubusercontent.com/render/math?math=n\"\u003e flips, you would have observations \u003cimg src=\"https://render.githubusercontent.com/render/math?math=y_1,%20y_2,%20...,%20y_n\"\u003e .\u003c/p\u003e\n\u003cp\u003eIn maximum likelihood estimation, you are looking to maximize the likelihood:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=L(p)%20=%20L(y_1,%20y_2,%20...,%20y_n%20%7C%20p)%20=%20p%5Ey%20(1-p)%5E%7Bn-y%7D\"\u003e where \u003cimg src=\"https://render.githubusercontent.com/render/math?math=y%20=%20%5Csum_%7Bi=1%7D%5E%7Bn%7Dy_i\"\u003e\u003c/p\u003e\n\u003cp\u003eTaking the log of both sides:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=ln%5BL(p)%5D%20=%20ln%5Bp%5Ey%20(1-p)%5E%7Bn-y%7D%5D%20=%20y%20ln(p)%2b(n-y)ln(1-p)\"\u003e\u003c/p\u003e\n\u003cp\u003eIf \u003cimg src=\"https://render.githubusercontent.com/render/math?math=y%20=%201,%202,%20...,%20n-1\"\u003e the derivative of \u003cimg src=\"https://render.githubusercontent.com/render/math?math=ln%5BL(p)%5D\"\u003e with respect to \u003cimg src=\"https://render.githubusercontent.com/render/math?math=p\"\u003e is:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cfrac%7Bd%5C,ln%5BL(p)%5D%7D%7Bdp%7D%20=%20y%20(%5Cfrac%7B1%7D%7Bp%7D)%2b(n-y)(%5Cfrac%7B-1%7D%7B1-p%7D)\"\u003e\u003c/p\u003e\n\u003cp\u003eAs you've seen previously, the maximum will then occur when the derivative equals zero:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=0%20=%20y%20(%5Cfrac%7B1%7D%7Bp%7D)%2b(n-y)(%5Cfrac%7B-1%7D%7B1-p%7D)\"\u003e\u003c/p\u003e\n\u003cp\u003eDistributing, you have\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=0%20=%20%5Cfrac%7By%7D%7Bp%7D%20-%20%5Cfrac%7Bn-y%7D%7B1-p%7D\"\u003e\u003c/p\u003e\n\u003cp\u003eAnd solving for p:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cfrac%7Bn-y%7D%7B1-p%7D%20=%20%5Cfrac%7By%7D%7Bp%7D\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=p(n-y)%20=%20%5Cfrac%7By(1-p)%7D%7Bp%7D\"\u003e\u003cbr\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cfrac%7Bn-y%7D%7By%7D%20=%20%5Cfrac%7B1-p%7D%7Bp%7D\"\u003e\u003cbr\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cfrac%7Bn%7D%7By%7D-1%20=%20%5Cfrac%7B1%7D%7Bp%7D-1\"\u003e\u003cbr\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cfrac%7Bn%7D%7By%7D%20=%20%5Cfrac%7B1%7D%7Bp%7D\"\u003e\u003cbr\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=p%20=%20%5Cfrac%7By%7D%7Bn%7D\"\u003e\u003c/p\u003e\n\u003cp\u003eAnd voil, you've verified the intuitive solution discussed above; the maximum likelihood for a binomial sample is the observed frequency!\u003c/p\u003e\n\u003ch2\u003eSummary\u003c/h2\u003e\n\u003cp\u003eIn this lesson, you briefly reviewed maximum likelihood estimation. In the upcoming lesson, you'll see how logistic regression can also be interpreted from this framework, which will help set the stage for you to code a logistic regression function from scratch using NumPy. Continue on to the next lesson to take a look at how this works for logistic regression.\u003c/p\u003e","exportId":"mle-review"},{"id":154076,"title":"MLE and Logistic Regression","type":"WikiPage","indent":0,"locked":false,"requirement":"must_mark_done","completed":false,"content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-mle-logistic-regression\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-mle-logistic-regression\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-mle-logistic-regression/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eIn this lesson, you'll further investigate the connections between maximum likelihood estimation and logistic regression. This is a common perspective for logistic regression and will be the underlying intuition for upcoming lessons where you'll code the algorithm from the ground up using NumPy.\u003c/p\u003e\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\u003cp\u003eYou will be able to:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eDetermine how MLE is tied into logistic regression\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003eMLE formulation\u003c/h2\u003e\n\u003cp\u003eAs discussed, maximum likelihood estimation finds the underlying parameters of an assumed distribution to maximize the likelihood of the observations. Logistic regression expands upon this by investigating the conditional probabilities associated with the various features, treating them as independent probabilities and calculating the respective total probability.\u003c/p\u003e\n\u003cp\u003eFor example, when predicting an individual's risk for heart disease, you might consider various factors such as their family history, weight, diet, exercise routines, blood pressure, and cholesterol. When looked at individually, each of these has an associated conditional probability that the individual has heart disease based on each of these factors. Mathematically, you can write each of these probabilities for each factor \u003cimg src=\"https://render.githubusercontent.com/render/math?math=X\"\u003e as:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cpi_i%20=%20Pr(Y_i%20=%201%7CX_i%20=%20x_i)%20=%20%5Cdfrac%7B%5Ctext%7Bexp%7D(%5Cbeta_0%20%2B%20%5Cbeta_1%20x_i)%7D%7B1%20%2B%20%5Ctext%7Bexp%7D(%5Cbeta_0%20%2B%20%5Cbeta_1%20x_i)%7D\"\u003e\u003c/p\u003e\n\u003cp\u003eThis is the standard linear regression model \u003cimg src=\"https://render.githubusercontent.com/render/math?math=(%5Cbeta_0%2B%5Cbeta_1%20x_i)\"\u003e you have seen previously, modified to have a range of 0 to 1. The range is modified and constrained by applying the sigmoid function since you're predicting probabilities.\u003c/p\u003e\n\u003cp\u003eThen, combining these conditional probabilities from multiple features, you maximize the likelihood function of each of those independent conditional probabilities, giving you:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=L(%5Cbeta_0%2C%5Cbeta_1)%3D%5Cdisplaystyle%5Cprod_%7Bi=1%7D%5EN%5Cpi_i%5E%7By_i%7D(1-%5Cpi_i)%5E%7Bn_i-y_i%7D%3D%5Cdisplaystyle%5Cprod_%7Bi=1%7D%5EN%5Cdfrac%7B%5Ctext%7Bexp%7D%5C%7By_i(%5Cbeta_0%20%2B%20%5Cbeta_1x_i)%5C%7D%7D%7B1%2B%5Ctext%7Bexp%7D(%5Cbeta_0%20%2B%20%5Cbeta_1x_i)%7D\"\u003e\u003c/p\u003e\n\u003ch2\u003eNotes on mathematical symbols\u003c/h2\u003e\n\u003cp\u003eRecall that the \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cprod\"\u003e sign stands for a product of each of these individual probabilities. (Similar to how \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Csum\"\u003e stands for the sum of a series.) Since this is a monotonically increasing function, its maximum will be the same as the logarithm of the function, which is typically used in practice in order to decompose this product of probabilities into a sum of log probabilities for easier calculation of the derivative. In future sections, you'll investigate the derivative of this function and then use that in order to code up our own function for logistic regression.\u003c/p\u003e\n\u003ch2\u003eAlgorithm bias and ethical concerns\u003c/h2\u003e\n\u003cp\u003eIt should also be noted that while this is mathematically sound and a powerful tool, the model will simply reflect the data that is fed in. For example, logistic regression and other algorithms are used to inform a wide range of decisions including whether to provide someone with a loan, the degree of criminal sentencing, or whether to hire an individual for a job. In all of these scenarios, it is again important to remember that the algorithm is simply reflective of the underlying data itself. If an algorithm is trained on a dataset where African Americans have had disproportionate criminal prosecution, the algorithm will continue to perpetuate these racial injustices. Similarly, algorithms trained on data that reflect a gender pay-gap will also continue to promote this bias unless adequately accounted for through careful preprocessing and normalization. With this, substantial thought and analysis regarding problem set up and the resulting model is incredibly important. While future lessons and labs in this section return to underlying mathematical theory and how to implement logistic regression on your own, it is worthwhile to investigate some of the current problems regarding some of these algorithms, and how naive implementations can perpetuate unjust biases.\u003c/p\u003e\n\u003ch2\u003eAdditional resources\u003c/h2\u003e\n\u003cp\u003eBelow are a handful of resources providing further information regarding some of the topics discussed here. Be sure to check out some of the news articles describing how poor safeguards and problem formulation surrounding algorithms such as logistic regression can lead to unjust biases:\u003c/p\u003e\n\u003ch3\u003eAlgorithm bias and ethical concerns\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e\u003ca href=\"https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing\"\u003eMachine Bias\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003ca href=\"https://www.bloomberg.com/opinion/articles/2018-10-16/amazon-s-gender-biased-algorithm-is-not-alone\"\u003eAmazons Gender-Biased Algorithm Is Not Alone\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003ca href=\"https://www.bostonglobe.com/business/2017/12/21/the-software-that-runs-our-lives-can-bigoted-and-unfair-but-can-fix/RK4xG4gYxcVNVTIubeC1JI/story.html\"\u003eThe software that runs our lives can be bigoted and unfair. But we can fix it\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003ca href=\"https://www.bostonglobe.com/ideas/2017/07/07/why-artificial-intelligence-far-too-human/jvG77QR5xPbpwBL2ApAFAN/story.html\"\u003eWhy artificial intelligence is far too human\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003ca href=\"https://www.npr.org/2016/03/14/470427605/can-computers-be-racist-the-human-like-bias-of-algorithms\"\u003eCan Computers Be Racist? The Human-Like Bias Of Algorithms\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003eAdditional mathematical resources\u003c/h3\u003e\n\u003cp\u003eFor a more in-depth discussion of the mathematical ideas, check out Penn State's lecture \u003ca href=\"https://onlinecourses.science.psu.edu/stat504/node/150/\"\u003ehere\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eIf you want to really go down the math rabbit-hole, check out section 4.4 on Logistic Regression from the Elements of Statistical Learning which can be found here: \u003ca href=\"https://web.stanford.edu/%7Ehastie/ElemStatLearn//\"\u003ehttps://web.stanford.edu/~hastie/ElemStatLearn//\u003c/a\u003e.\u003c/p\u003e\n\u003ch2\u003eSummary\u003c/h2\u003e\n\u003cp\u003eIn this lesson, you further analyzed logistic regression from the perspective of maximum likelihood estimation. Additionally, there was a brief pause to consider the setup and interpretation of algorithms such as logistic regression. In particular, remember that issues regarding racial and gender bias that can be perpetuated by these algorithms. Always try to ensure your models are ethically sound. In the proceeding labs and lessons, you will continue to formalize your knowledge of logistic regression, implementing gradient descent and then a full logistic regression algorithm using Python packages in order to give you a deeper understanding of how logistic regression works.\u003c/p\u003e","exportId":"mle-and-logistic-regression"},{"id":154077,"title":"Gradient Descent Review","type":"WikiPage","indent":0,"locked":false,"requirement":"must_mark_done","completed":false,"content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-gradient-descent-review\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-gradient-descent-review\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-gradient-descent-review/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eRecall that gradient descent is a numerical approximation method for finding optimized solutions to problems with no closed form. That is, some mathematical problems are very easy to solve analytically.\u003c/p\u003e\n\u003cp\u003eTrivial examples include basic algebra problems which you undoubtedly saw in grade school: \u003cimg src=\"https://render.githubusercontent.com/render/math?math=x%20%2b%202%20=%2010\"\u003e  subtracting 2 from both sides you get \u003cimg src=\"https://render.githubusercontent.com/render/math?math=x%20=%208\"\u003e . Similarly, some more complex mathematical problems such as ordinary least squares, our preliminary regression approach, also have closed-form solutions where we can follow a rote procedure and be guaranteed a solution.\u003c/p\u003e\n\u003cp\u003eIn other cases, this is not possible and numerical approximation methods are used to find a solution. The first instance that you witnessed of this was adding the L1 and L2 (lasso and ridge, respectively) penalties to OLS regression. In these cases, numerical approximation methods, such as gradient descent, are used in order to find optimal or near-optimal solutions.\u003c/p\u003e\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\u003cp\u003eYou will be able to:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eDescribe the elements of gradient descent in the context of a logistic regression\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003eGradient descent\u003c/h2\u003e\n\u003cp\u003eGradient descent is grounded in basic calculus theory. Whenever you have a minimum or maximum, the derivative at that point is equal to zero. This is displayed visually in the picture below; the slope of the red tangent lines is equal to the derivative of the curve at that point. As you can see, the slope of all of these horizontal tangent lines will be zero.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-gradient-descent-review/master/images/new_dxdy0.png\" width=\"400\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eThe gradient is simply another term for the derivative. Typically, this is the term used when we are dealing with multivariate data. The gradient is the rate of change, which is also the slope of the line tangent.\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eBuilding upon this, gradient descent attempts to find the minimum of a function by taking successive steps in the steepest direction downhill.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-gradient-descent-review/master/images/new_gradient.png\"\u003e\u003c/p\u003e\n\u003cp\u003eWhile this process guarantees a local minimum, the starting point and step size can affect the outcome. For example, for two different runs of gradient descent, one may lead to the global minimum while the other may lead to a local minimum.\u003c/p\u003e\n\u003cp\u003eRecall that the general outline for gradient descent is:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eDefine initial parameters:\n\u003col\u003e\n\u003cli\u003ePick a starting point\u003c/li\u003e\n\u003cli\u003ePick a step size \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Calpha\"\u003e (alpha)\u003c/li\u003e\n\u003cli\u003eChoose a maximum number of iterations; the algorithm will terminate after this many iterations if a minimum has yet to be found\u003c/li\u003e\n\u003cli\u003e(optionally) define a precision parameter; similar to the maximum number of iterations, this will terminate the algorithm early. For example, one might define a precision parameter of 0.00001, in which case if the change in the loss function were less then 0.00001, the algorithm would terminate. The idea is that we are very close to the bottom and further iterations would make a negligible difference\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/li\u003e\n\u003cli\u003eCalculate the gradient at the current point (initially, the starting point)\u003c/li\u003e\n\u003cli\u003eTake a step (of size alpha) in the direction of the gradient\u003c/li\u003e\n\u003cli\u003eRepeat steps 2 and 3 until the maximum number of iterations is met, or the difference between two points is less then your precision parameter\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2\u003eSummary\u003c/h2\u003e\n\u003cp\u003eIn this lesson, you briefly reviewed that a gradient is the derivative of a function, which is the rate of change at a specific point. You then reviewed the intuition behind gradient descent, as well as some of its pitfalls. Finally, you saw a brief outline of the algorithm itself. In the next lab, you'll practice coding gradient descent and applying that to some simple mathematical functions.\u003c/p\u003e","exportId":"gradient-descent-review"},{"id":154079,"title":"Gradient Descent - Lab","type":"Assignment","indent":1,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_mark_done","completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-gradient-descent-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-gradient-descent-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","exportId":"gb06cecd53d5d21b04187b921a72fd028"},{"id":154081,"title":"Coding Logistic Regression from Scratch - Lab","type":"Assignment","indent":1,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_mark_done","completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-coding-logistic-regression-from-scratch\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-coding-logistic-regression-from-scratch/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","exportId":"g94ccb73ca3226687e0b2b7aecf2ba753"},{"id":154084,"title":"Logistic Regression Model Comparisons - Lab","type":"Assignment","indent":1,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_mark_done","completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-logistic-regression-model-comparisons-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-logistic-regression-model-comparisons-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","exportId":"gb11df9755baa2dbf94ca86b32d3c5242"},{"id":154085,"title":"MLE and Logistic Regression - Recap","type":"WikiPage","indent":0,"locked":false,"requirement":"must_mark_done","completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-mle-log-reg-recap\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-mle-log-reg-recap/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eWell done! In this section, you reviewed maximum likelihood estimation and logistic regression. This included writing some challenging code, including gradient descent which pushed you to think critically regarding algorithm implementation. \u003c/p\u003e\n\n\u003ch2\u003eLog-likelihoods in Maximum Likelihood Estimation\u003c/h2\u003e\n\n\u003cp\u003eOne of the nuances you saw in maximum likelihood estimation was that of log-likelihoods. Recall that the purpose of taking log-likelihoods as opposed to likelihoods themselves is that it allows us to decompose the product of probabilities as sums of log probabilities. Analytically, this is essential to calculating subsequent gradients in order to find the next steps for our optimization algorithm.\u003c/p\u003e\n\n\u003ch2\u003eLocal minima in Gradient Descent\u003c/h2\u003e\n\n\u003cp\u003eOne of the most important notes from this section is that \u003cstrong\u003egradient descent does not guarantee an optimal solution\u003c/strong\u003e. Gradient descent is meant to find optimal solutions, but it only guarantees a local minimum. For this reason, gradient descent is frequently run multiple times, and the parameters with the lowest loss function then being selected for the final model.\u003c/p\u003e\n\n\u003ch2\u003eLogistic Regression\u003c/h2\u003e\n\n\u003cp\u003eAfter coding logistic regression on your own, you then further investigated tuning such models using regularization. Recall that while precision, recall, and accuracy are useful metrics for evaluating classifiers, determining an appropriate balance between false positives and false negatives will depend on the particular problem application and the relative costs of each. For example, in the context of medical screening, a false negative could be devastating, eliminating the possibility for early intervention of the given disease. On the other hand, in another context, such as finding spam email, the cost of false positives might be much higher than false negatives -- after all, having a spam email sneak its way into your inbox is probably preferable then missing an important time-sensitive email because it was marked as spam. Due to these contextual considerations, you as the practitioner are responsible for selecting appropriate tradeoffs.\u003c/p\u003e\n\n\u003ch2\u003eSummary\u003c/h2\u003e\n\n\u003cp\u003eThis section was designed to give you additional practice coding algorithms in Python, and a deeper understanding of how iterative algorithms such as logistic regression converge to produce underlying model parameters.\u003c/p\u003e\n\u003cfooter class=\"fis-footer\" style=\"visibility: hidden;\"\u003e\u003cdiv class=\"fis-feedback\"\u003e\n\u003ch5\u003eHow do you feel about this lesson?\u003c/h5\u003e\n\u003cimg id=\"thumbs-up\" data-repository=\"dsc-mle-log-reg-recap\" title=\"Thumbs up!\" alt=\"thumbs up\"\u003e\u003cimg id=\"thumbs-down\" data-repository=\"dsc-mle-log-reg-recap\" title=\"Thumbs down!\" alt=\"thumbs down\"\u003e\n\u003c/div\u003e\n\u003ch5\u003eHave specific feedback? \u003ca href=\"https://github.com/learn-co-curriculum/dsc-mle-log-reg-recap/issues/new\"\u003eTell us here!\u003c/a\u003e\n\u003c/h5\u003e\u003c/footer\u003e","exportId":"mle-and-logistic-regression-recap"}]},{"id":16478,"name":"Topic 27: K Nearest Neighbors","status":"unlocked","unlockDate":null,"prereqs":[],"requirement":"all","sequential":false,"exportId":"gee2622a5e283cec85544cc9d93c79532","items":[{"id":154086,"title":"K-Nearest Neighbors - Introduction","type":"WikiPage","indent":0,"locked":false,"requirement":"must_mark_done","completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-knn-intro\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-knn-intro/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eIn this section you'll look at an intuitive algorithm known as K-Nearest Neighbors (KNN). KNN is an effective classification and regression algorithm that uses nearby points in order to generate a prediction. \u003c/p\u003e\n\n\u003ch2\u003eKNN\u003c/h2\u003e\n\n\u003cp\u003eThe K-Nearest Neighbors algorithm works as follows: \u003c/p\u003e\n\n\u003col\u003e\n\u003cli\u003eChoose a point \u003c/li\u003e\n\u003cli\u003eFind the K-nearest points\n\n\u003col\u003e\n\u003cli\u003eK is a predefined user constant such as 1, 3, 5, or 11 \u003c/li\u003e\n\u003c/ol\u003e\n\u003c/li\u003e\n\u003cli\u003ePredict a label for the current point:\n\n\u003col\u003e\n\u003cli\u003eClassification - Take the most common class of the k neighbors\u003c/li\u003e\n\u003cli\u003eRegression - Take the average target metric of the k neighbors\u003c/li\u003e\n\u003cli\u003eBoth classification or regression can also be modified to use weighted averages based on the distance of the neighbors \u003c/li\u003e\n\u003c/ol\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\n\u003ch2\u003eDistance metrics\u003c/h2\u003e\n\n\u003cp\u003eAn incredibly important decision when using the KNN algorithm is determining an appropriate distance metric. This makes a monumental impact to the output of the algorithm. While there are additional distance metrics, such as cosine distance which we will not cover, you'll get a solid introduction to distance metrics by looking at the standard Euclidean distance and its more generic counterpart, Minkowski distance.\u003c/p\u003e\n\n\u003ch2\u003eK-means\u003c/h2\u003e\n\n\u003cp\u003eWhile outside the scope of this section, it is worth mentioning the related K-means algorithm which uses similar principles as KNN but serves as an unsupervised learning clustering algorithm. In the K-means algorithm, K represents the number of clusters rather then the number of neighbors. Unlike KNN, K-means is an iterative algorithm which repeats until convergence. Nonetheless, its underlying principle is the same, in that it groups data points together using a distance metric in order to create homogeneous groupings.\u003c/p\u003e\n\n\u003ch2\u003eSummary\u003c/h2\u003e\n\n\u003cp\u003eIn this brief lesson, you were introduced to the KNN algorithm. From here, you'll jump straight to the details of KNN, practice coding your own implementation and then get an introduction to use pre-built tools within scikit-learn for KNN.\u003c/p\u003e\n\u003cfooter class=\"fis-footer\" style=\"visibility: hidden;\"\u003e\u003cdiv class=\"fis-feedback\"\u003e\n\u003ch5\u003eHow do you feel about this lesson?\u003c/h5\u003e\n\u003cimg id=\"thumbs-up\" data-repository=\"dsc-knn-intro\" title=\"Thumbs up!\" alt=\"thumbs up\"\u003e\u003cimg id=\"thumbs-down\" data-repository=\"dsc-knn-intro\" title=\"Thumbs down!\" alt=\"thumbs down\"\u003e\n\u003c/div\u003e\n\u003ch5\u003eHave specific feedback? \u003ca href=\"https://github.com/learn-co-curriculum/dsc-knn-intro/issues/new\"\u003eTell us here!\u003c/a\u003e\n\u003c/h5\u003e\u003c/footer\u003e","exportId":"k-nearest-neighbors-introduction"},{"id":154087,"title":"Distance Metrics","type":"Assignment","indent":0,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_mark_done","completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-distance-metrics\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-distance-metrics/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","exportId":"gff21df4a7b65d3895a0c59921e08c5a6"},{"id":154088,"title":"Distance Metrics - Lab","type":"Assignment","indent":1,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_mark_done","completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-distance-metrics-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-distance-metrics-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","exportId":"g43db40e3744b4e42bbeb39ea8342dada"},{"id":154089,"title":"K-Nearest Neighbors","type":"WikiPage","indent":0,"locked":false,"requirement":"must_mark_done","completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-k-nearest-neighbors\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-k-nearest-neighbors/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eIn this lesson, you'll learn about a supervised learning algorithm, \u003cstrong\u003e\u003cem\u003eK-Nearest Neighbors\u003c/em\u003e\u003c/strong\u003e; and how you can use it to make predictions for classification and regression tasks!\u003c/p\u003e\n\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\n\u003cp\u003eYou will be able to:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eDescribe how KNN makes classifications\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch2\u003eWhat is K-Nearest Neighbors?\u003c/h2\u003e\n\n\u003cp\u003e\u003cstrong\u003e\u003cem\u003eK-Nearest Neighbors\u003c/em\u003e\u003c/strong\u003e (or KNN, for short) is a supervised learning algorithm that can be used for both \u003cstrong\u003e\u003cem\u003eClassification\u003c/em\u003e\u003c/strong\u003e and \u003cstrong\u003e\u003cem\u003eRegression\u003c/em\u003e\u003c/strong\u003e tasks. However, in this section, we will cover KNN only in the context of classification. KNN is a distance-based classifier, meaning that it implicitly assumes that the smaller the distance between two points, the more similar they are. In KNN, each column acts as a dimension. In a dataset with two columns, we can easily visualize this by treating values for one column as X coordinates and and the other as Y coordinates. Since this is a \u003cstrong\u003e\u003cem\u003eSupervised learning algorithm\u003c/em\u003e\u003c/strong\u003e, you must also have the labels for each point in the dataset, or else you wouldn't know what to predict!\u003c/p\u003e\n\n\u003ch2\u003eFitting the model\u003c/h2\u003e\n\n\u003cp\u003eKNN is unique compared to other classifiers in that it does almost nothing during the \"fit\" step, and all the work during the \"predict\" step. During the \"fit\" step, KNN just stores all the training data and corresponding labels. No distances are calculated at this point. \u003c/p\u003e\n\n\u003ch2\u003eMaking predictions with K\u003c/h2\u003e\n\n\u003cp\u003eAll the magic happens during the \"predict\" step. During this step, KNN takes a point that you want a class prediction for, and calculates the distances between that point and every single point in the training set. It then finds the \u003ccode\u003eK\u003c/code\u003e closest points, or \u003cstrong\u003e\u003cem\u003eNeighbors\u003c/em\u003e\u003c/strong\u003e, and examines the labels of each. You can think of each of the K-closest points getting to 'vote' about the predicted class. Naturally, they all vote for the same class that they belong to. The majority wins, and the algorithm predicts the point in question as whichever class has the highest count among all of the k-nearest neighbors.\u003c/p\u003e\n\n\u003cp\u003eIn the following animation, K=3: \u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-k-nearest-neighbors/master/images/knn.gif\"\u003e\u003c/p\u003e\n\n\u003cp\u003e\u003ca href=\"https://gfycat.com/wildsorrowfulchevrotain\"\u003egif source\u003c/a\u003e\u003c/p\u003e\n\n\u003ch2\u003eDistance metrics\u003c/h2\u003e\n\n\u003cp\u003eWhen using KNN, you can use \u003cstrong\u003e\u003cem\u003eManhattan\u003c/em\u003e\u003c/strong\u003e, \u003cstrong\u003e\u003cem\u003eEuclidean\u003c/em\u003e\u003c/strong\u003e, \u003cstrong\u003e\u003cem\u003eMinkowski distance\u003c/em\u003e\u003c/strong\u003e, or any other distance metric. Choosing an appropriate distance metric is essential and will depend on the context of the problem at hand.\u003c/p\u003e\n\n\u003ch2\u003eEvaluating model performance\u003c/h2\u003e\n\n\u003cp\u003eHow to evaluate the model performance depends on whether you're using the model for a classification or regression task. KNN can be used for regression (by averaging the target scores from each of the K-nearest neighbors), as well as for both binary and multicategorical classification tasks. \u003c/p\u003e\n\n\u003cp\u003eEvaluating classification performance for KNN works the same as evaluating performance for any other classification algorithm -- you need a set of predictions, and the corresponding ground-truth labels for each of the points you made a prediction on. You can then compute evaluation metrics such as \u003cstrong\u003e\u003cem\u003ePrecision, Recall, Accuracy, F1-Score\u003c/em\u003e\u003c/strong\u003e etc. \u003c/p\u003e\n\n\u003ch2\u003eSummary\u003c/h2\u003e\n\n\u003cp\u003eGreat! Now that you know how the KNN classifier works, you'll implement KNN using Python from scratch in the next lab.\u003c/p\u003e\n\u003cfooter class=\"fis-footer\" style=\"visibility: hidden;\"\u003e\u003cdiv class=\"fis-feedback\"\u003e\n\u003ch5\u003eHow do you feel about this lesson?\u003c/h5\u003e\n\u003cimg id=\"thumbs-up\" data-repository=\"dsc-k-nearest-neighbors\" title=\"Thumbs up!\" alt=\"thumbs up\"\u003e\u003cimg id=\"thumbs-down\" data-repository=\"dsc-k-nearest-neighbors\" title=\"Thumbs down!\" alt=\"thumbs down\"\u003e\n\u003c/div\u003e\n\u003ch5\u003eHave specific feedback? \u003ca href=\"https://github.com/learn-co-curriculum/dsc-k-nearest-neighbors/issues/new\"\u003eTell us here!\u003c/a\u003e\n\u003c/h5\u003e\u003c/footer\u003e","exportId":"k-nearest-neighbors"},{"id":154090,"title":"K-Nearest Neighbors - Lab","type":"Assignment","indent":1,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_mark_done","completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-k-nearest-neighbors-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-k-nearest-neighbors-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","exportId":"g30eef00ddca4c05b9dc1e562aa33a729"},{"id":154091,"title":"Finding the Best Value for K","type":"WikiPage","indent":0,"locked":false,"requirement":"must_mark_done","completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-finding-the-best-value-for-k\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-finding-the-best-value-for-k/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eIn this lesson, you'll investigate how changing the value for K can affect the performance of the model, and how to use this to find the best value for K.\u003c/p\u003e\n\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\n\u003cul\u003e\n\u003cli\u003eConduct a parameter search to find the optimal value for K \u003c/li\u003e\n\u003cli\u003eExplain how KNN is related to the curse of dimensionality \u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch2\u003eFinding the optimal number of neighbors\u003c/h2\u003e\n\n\u003cp\u003eBy now, you've got a strong understanding of how the K-Nearest Neighbors algorithm works, but you likely have at least one lingering question\u003cstrong\u003e\u003cem\u003ewhat is the best value to use for K\u003c/em\u003e\u003c/strong\u003e? There's no set number that works best. If there was, it wouldn't be called \u003cstrong\u003e\u003cem\u003eK\u003c/em\u003e\u003c/strong\u003e-nearest neighbors. While the best value for K is not immediately obvious for any problem, there are some strategies that you can use to select a good or near optimal value. \u003c/p\u003e\n\n\u003ch2\u003eK, overfitting, and underfitting\u003c/h2\u003e\n\n\u003cp\u003eIn general, the smaller K is, the tighter the \"fit\" of the model. Remember that with supervised learning, you want to fit a model to the data as closely as possible without \u003cstrong\u003e\u003cem\u003eoverfitting\u003c/em\u003e\u003c/strong\u003e to patterns in the training set that don't generalize.  This can happen if your model pays too much attention to every little detail and makes a very complex decision boundary. Conversely, if your model is overly simplistic, then you may have \u003cstrong\u003e\u003cem\u003eunderfit\u003c/em\u003e\u003c/strong\u003e the model, limiting its potential. A visual explanation helps demonstrate this concept in practice:\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-finding-the-best-value-for-k/master/images/fit_fs.png\" width=\"700\"\u003e\u003c/p\u003e\n\n\u003cp\u003eWhen K is small, any given prediction only takes into account a very small number of points around it to make the prediction. If K is too small, this can end up with a decision boundary that looks like the overfit picture on the right. \u003c/p\u003e\n\n\u003cp\u003eConversely, as K grows larger, it takes into account more and more points, that are farther and farther away from the point in question, increasing the overall size of the region taken into account. If K grows too large, then the model begins to underfit the data. \u003c/p\u003e\n\n\u003cp\u003eIt's important to try to find the best value for K by iterating over a multiple values and comparing performance at each step. \u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-finding-the-best-value-for-k/master/images/best_k_fs.png\" width=\"550\"\u003e\u003c/p\u003e\n\n\u003cp\u003eAs you can see from the image above, \u003ccode\u003ek=1\u003c/code\u003e and \u003ccode\u003ek=3\u003c/code\u003e will provide different results! \u003c/p\u003e\n\n\u003ch2\u003eIterating over values of K\u003c/h2\u003e\n\n\u003cp\u003eSince the model arrives at a prediction by voting, it makes sense that you should only use odd values for k, to avoid ties and subsequent arbitrary guesswork. By adding this constraint (an odd value for k) the model will never be able to evenly split between two classes. From here, finding an optimal value of K requires some iterative investigation.\u003c/p\u003e\n\n\u003cp\u003eThe best way to find an optimal value for K is to choose a minimum and maximum boundary and try them all! In practice, this means:\u003c/p\u003e\n\n\u003col\u003e\n\u003cli\u003eFit a KNN classifier for each value of K \u003c/li\u003e\n\u003cli\u003eGenerate predictions with that model\u003cbr\u003e\n\u003c/li\u003e\n\u003cli\u003eCalculate and evaluate a performance metric using the predictions the model made \u003c/li\u003e\n\u003cli\u003eCompare the results for every model and find the one with the lowest overall error, or highest overall score!\u003c/li\u003e\n\u003c/ol\u003e\n\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-finding-the-best-value-for-k/master/images/plot_fs.png\" width=\"550\"\u003e\u003c/p\u003e\n\n\u003cp\u003eA common way to find the best value for K at a glance is to plot the error for each value of K. Find the value for K where the error is lowest. If this graph continued into higher values of K, we would likely see the error numbers go back up as K increased. \u003c/p\u003e\n\n\u003ch2\u003eKNN and the curse of dimensionality\u003c/h2\u003e\n\n\u003cp\u003eNote that KNN isn't the best choice for extremely large datasets, and/or models with high dimensionality. This is because the time complexity (what computer scientists call \"Big O\", which you saw briefly earlier) of this algorithm is exponential. As you add more data points to the dataset, the number of operations needed to complete all the steps of the algorithm grows exponentially! That said, for smaller datasets, KNN often works surprisingly well, given the simplicity of the overall algorithm. However, if your dataset contains millions of rows and thousands of columns, you may want to choose another algorithm, as the algorithm may not run in any reasonable amount of time;in some cases, it could quite literally take years to complete! \u003c/p\u003e\n\n\u003ch2\u003eSummary\u003c/h2\u003e\n\n\u003cp\u003eIn this lesson you learned how to determine the best value for K and that the KNN algorithm may not necessarily be the best choice for large datasets due to the large amount of time it can take for the algorithm to run. \u003c/p\u003e\n\u003cfooter class=\"fis-footer\" style=\"visibility: hidden;\"\u003e\u003cdiv class=\"fis-feedback\"\u003e\n\u003ch5\u003eHow do you feel about this lesson?\u003c/h5\u003e\n\u003cimg id=\"thumbs-up\" data-repository=\"dsc-finding-the-best-value-for-k\" title=\"Thumbs up!\" alt=\"thumbs up\"\u003e\u003cimg id=\"thumbs-down\" data-repository=\"dsc-finding-the-best-value-for-k\" title=\"Thumbs down!\" alt=\"thumbs down\"\u003e\n\u003c/div\u003e\n\u003ch5\u003eHave specific feedback? \u003ca href=\"https://github.com/learn-co-curriculum/dsc-finding-the-best-value-for-k/issues/new\"\u003eTell us here!\u003c/a\u003e\n\u003c/h5\u003e\u003c/footer\u003e","exportId":"finding-the-best-value-for-k"},{"id":154092,"title":"KNN with scikit-learn","type":"WikiPage","indent":0,"locked":false,"requirement":"must_mark_done","completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-knn-with-scikit-learn\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-knn-with-scikit-learn/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eIn this lesson, you'll explore how to use scikit-learn's implementation of the K-Nearest Neighbors algorithm. In addition, you'll also learn about best practices for using the algorithm. \u003c/p\u003e\n\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\n\u003cp\u003eYou will be able to:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eList the considerations when fitting a KNN model using scikit-learn\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch2\u003eWhy use scikit-learn?\u003c/h2\u003e\n\n\u003cp\u003eWhile you've written your own implementation of the KNN algorithm, scikit-learn adds many backend optimizations which can make the algorithm perform faster and more efficiently. Building your own implementation of any machine learning algorithm is a valuable experience, providing great insight into how said algorithm works. However, in general, you should always use professional toolsets such as scikit-learn whenever possible; since their implementations will always be best-in-class, in a way a single developer or data scientist simply can't hope to rival on their own. In the case of KNN, you'll find scikit-learn's implementation to be much more robust and fast, because of optimizations such as caching distances in clever ways under the hood. \u003c/p\u003e\n\n\u003ch2\u003eRead the \u003ccode\u003esklearn\u003c/code\u003e docs\u003c/h2\u003e\n\n\u003cp\u003eAs a rule of thumb, you should familiarize yourself with any documentation available for any libraries or frameworks you use. scikit-learn provides high-quality documentation. For every algorithm, you'll find a general \u003ca href=\"https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html\"\u003edocumentation page\u003c/a\u003e which tells you inputs, parameters, outputs, and caveats of any algorithm. In addition, you'll also find very informative \u003ca href=\"https://scikit-learn.org/stable/modules/neighbors.html#classification\"\u003eUser Guides\u003c/a\u003e that explain both how the algorithm works, and how to best use it, complete with sample code! \u003c/p\u003e\n\n\u003cp\u003eFor example, the following image can be found in the scikit-learn user guide for K-Nearest Neighbors, along with an explanation of how different parameters can affect the overall performance of the model. \u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-knn-with-scikit-learn/master/images/knn_docs.png\"\u003e\u003c/p\u003e\n\n\u003ch2\u003eBest practices\u003c/h2\u003e\n\n\u003cp\u003eYou'll also find that scikit-learn provides robust implementations for additional components of the algorithm implementation process such as evaluation metrics. With that, you can easily evaluate models using precision, accuracy, or recall scores on the fly using built-in functions!\u003c/p\u003e\n\n\u003cp\u003eWith that, it's important to focus on practical questions when completing the upcoming lab. In particular, try to focus on the following questions:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003e\u003cp\u003eWhat decisions do I need to make regarding my data? How might these decisions affect overall performance?\u003c/p\u003e\u003c/li\u003e\n\u003cli\u003e\u003cp\u003eWhich predictors do I need? How can I confirm that I have the right predictors?\u003c/p\u003e\u003c/li\u003e\n\u003cli\u003e\u003cp\u003eWhat parameter values (if any) should I choose for my model? How can I find the optimal value for a given parameter?\u003c/p\u003e\u003c/li\u003e\n\u003cli\u003e\u003cp\u003eWhat metrics will I use to evaluate the performance of my model? Why?\u003c/p\u003e\u003c/li\u003e\n\u003cli\u003e\u003cp\u003eHow do I know if there's room left for improvement with my model? Are the potential performance gains worth the time needed to reach them?\u003c/p\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch2\u003eA final note\u003c/h2\u003e\n\n\u003cp\u003eAfter cleaning, preprocessing, and modeling the data in the next lab, you'll be given the opportunity to iterate on your model. \u003c/p\u003e\n\n\u003ch2\u003eSummary\u003c/h2\u003e\n\n\u003cp\u003eIn this section, you got a brief overview of some of the advantages of using scikit-learn's built-in KNN implementation. While you haven't seen specific code examples, you now have an indispensable resource: the official documentation to guide you. Since it's an incredibly important skill to know where to seek out information and how to digest that into actionable processes, it'll be up to you to piece through the necessary documentation to complete the upcoming lab. Good luck!\u003c/p\u003e\n\u003cfooter class=\"fis-footer\" style=\"visibility: hidden;\"\u003e\u003cdiv class=\"fis-feedback\"\u003e\n\u003ch5\u003eHow do you feel about this lesson?\u003c/h5\u003e\n\u003cimg id=\"thumbs-up\" data-repository=\"dsc-knn-with-scikit-learn\" title=\"Thumbs up!\" alt=\"thumbs up\"\u003e\u003cimg id=\"thumbs-down\" data-repository=\"dsc-knn-with-scikit-learn\" title=\"Thumbs down!\" alt=\"thumbs down\"\u003e\n\u003c/div\u003e\n\u003ch5\u003eHave specific feedback? \u003ca href=\"https://github.com/learn-co-curriculum/dsc-knn-with-scikit-learn/issues/new\"\u003eTell us here!\u003c/a\u003e\n\u003c/h5\u003e\u003c/footer\u003e","exportId":"knn-with-scikit-learn"},{"id":154093,"title":"KNN with scikit-learn - Lab","type":"Assignment","indent":1,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_mark_done","completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-knn-with-scikit-learn-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-knn-with-scikit-learn-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","exportId":"ge848c6abcaec82f3f37cd659ee8878de"},{"id":154094,"title":"K-Nearest Neighbors - Recap","type":"WikiPage","indent":0,"locked":false,"requirement":"must_mark_done","completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-knn-recap\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-knn-recap/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eIn this lesson you'll briefly review some of the key concepts covered in this section including KNN's computational complexity and how to properly tune a model using scikit-learn. \u003c/p\u003e\n\n\u003ch2\u003eK-Nearest Neighbors\u003c/h2\u003e\n\n\u003cp\u003eAs you saw, KNN is an intuitive algorithm: to generate a prediction for a given data point, it finds the k-nearest data points and then predicts the majority class of these k points.\u003c/p\u003e\n\n\u003ch3\u003eComputational complexity\u003c/h3\u003e\n\n\u003cp\u003eAlso of note is the computational complexity of the KNN algorithm. As the number of data points and features increase, the required calculations increases exponentially! As such, KNN is extremely resource intensive for large datasets.\u003c/p\u003e\n\n\u003ch2\u003eDistance metrics\u003c/h2\u003e\n\n\u003cp\u003eYou learned about Minkowski distance and two cases of Minkowski distance: Euclidean and Manhattan distance. Other distance metrics such as Hamming distance can even be used to compare strings! (Hamming distance can be used to offer typo correction-suggestions for instance by comparing similar words generated by changing only one or two letters from the mistyped word). \u003c/p\u003e\n\n\u003ch2\u003eModel tuning in scikit-learn\u003c/h2\u003e\n\n\u003cp\u003eRemember that model tuning encapsulates the entire gamut of the data science process from problem formulation and preprocessing through hyperparameter tuning. Furthermore, you also need to choose a validation method to determine the model's ability to generalize to new cases such as train-test split or cross-validation. Good models require careful thought, ample preprocessing, and exploration followed by hyperparameter tuning.\u003c/p\u003e\n\n\u003ch2\u003eSummary\u003c/h2\u003e\n\n\u003cp\u003eWell done! You have added another algorithm in your toolset. Even though KNN doesn't scale well to larger datasets, it has many useful applications from recommendations to classification. \u003c/p\u003e\n\u003cfooter class=\"fis-footer\" style=\"visibility: hidden;\"\u003e\u003cdiv class=\"fis-feedback\"\u003e\n\u003ch5\u003eHow do you feel about this lesson?\u003c/h5\u003e\n\u003cimg id=\"thumbs-up\" data-repository=\"dsc-knn-recap\" title=\"Thumbs up!\" alt=\"thumbs up\"\u003e\u003cimg id=\"thumbs-down\" data-repository=\"dsc-knn-recap\" title=\"Thumbs down!\" alt=\"thumbs down\"\u003e\n\u003c/div\u003e\n\u003ch5\u003eHave specific feedback? \u003ca href=\"https://github.com/learn-co-curriculum/dsc-knn-recap/issues/new\"\u003eTell us here!\u003c/a\u003e\n\u003c/h5\u003e\u003c/footer\u003e","exportId":"k-nearest-neighbors-recap"}]},{"id":16479,"name":"Topic 28: Bayes Classification","status":"unlocked","unlockDate":null,"prereqs":[],"requirement":"all","sequential":false,"exportId":"g0f1fe16bdeb559751edd7ee7992b5034","items":[{"id":154095,"title":"Bayesian Classification - Introduction","type":"WikiPage","indent":0,"locked":false,"requirement":"must_mark_done","completed":false,"content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-bayesian-classification-intro-v2-1\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-bayesian-classification-intro-v2-1\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-bayesian-classification-intro-v2-1/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eIn an earlier section, you learned about Bayesian statistics with plenty of theory and application of Bayes theorem. You'll now take a look at using Bayes theorem to perform some classification tasks. Here, you'll see that the Bayes theorem can be applied to multiple variables simultaneously.\u003c/p\u003e\n\u003ch2\u003eBayes Classification\u003c/h2\u003e\n\u003cp\u003eNaive Bayes algorithms extend Bayes' formula to multiple variables by assuming that these features are independent of one another, which may not be met, (hence its naivety) it can nonetheless provide strong results in scenarios with clean and well normalized datasets. This then allows you to estimate an overall probability by multiplying the conditional probabilities for each of the independent features.\u003c/p\u003e\n\u003cp\u003eBayes' formula extended to multiple features is:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5CLarge%20P(y%7Cx_1,%20x_2,%20...,%20x_n)%20=%20%5Cfrac%7BP(y)%5Cdisplaystyle%5Cprod_%7Bi%7D%5E%7Bn%7DP(x_i%7Cy)%7D%7BP(x_1,%20x_2,%20...,%20x_n)%7D\"\u003e\u003c/p\u003e\n\u003ch2\u003eDocument Classification\u003c/h2\u003e\n\u003cp\u003eAn interesting application of Bayes' theorem is to use \u003cem\u003ebag of words\u003c/em\u003e for document classification. A bag of words representation takes a text document and converts it into a word frequency representation. In this section, you'll use bag of words and Naive Bayes to classify YouTube videos into appropriate topics.\u003c/p\u003e\n\u003ch2\u003eSummary\u003c/h2\u003e\n\u003cp\u003eOver the next few lessons you will learn about another fundamental classification algorithm which has many practical applications. It's time to jump into the wonderful Bayesian world again! This section will help you solidify your understanding of Bayesian stats.\u003c/p\u003e","exportId":"bayesian-classification-introduction"},{"id":154096,"title":"Classifiers with Bayes","type":"WikiPage","indent":0,"locked":false,"requirement":"must_mark_done","completed":false,"content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-classifiers-with-bayes\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-classifiers-with-bayes\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-classifiers-with-bayes/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eNow that you're familiar with Bayes' theorem and foundational concepts of Bayesian statistics, you'll take a look at how to implement some of these ideas for machine learning. Classification tasks can be a natural application of Bayes' theorem since you are looking to predict some label given other information, which can be conceptualized through conditional probability.\u003c/p\u003e\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\u003cp\u003eYou will be able to:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eExplain the assumption that leads to Naive Bayes being \"naive\"\u003c/li\u003e\n\u003cli\u003eExplain how to use the probabilities generated by Naive Bayes to make a classification\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003eNaive Bayes\u003c/h2\u003e\n\u003cp\u003eNaive Bayes algorithms extend Bayes' formula to multiple variables by assuming that these features are independent of one another. This then allows you to estimate an overall probability by multiplying the conditional probabilities for each of the independent features.\u003c/p\u003e\n\u003cp\u003eFor example, extending the previous medical examples of Bayes' theorem, a researcher might examine multiple patient measurements to better predict whether or not an individual has a given disease. Provided that these measurements are independent (and uncorrelated from one another), one can then examine the conditional probability of each of these metrics and apply Bayes' theorem to determine a relative probability of having the disease or not. Combining these probabilities can then give an overall confidence of a patient having the disease given all the information. From this, one can then make a prediction for whether or not you believe an individual has the disease or not based on which probability is higher.\u003c/p\u003e\n\u003cp\u003eMathematically, if \u003cimg src=\"https://render.githubusercontent.com/render/math?math=Y\"\u003e is a class you wish to predict (such as having a disease) and \u003cimg src=\"https://render.githubusercontent.com/render/math?math=X_1,%20X_2,%20...,%20X_n\"\u003e are the various measurements for the given individual or case, then the probability of class \u003cimg src=\"https://render.githubusercontent.com/render/math?math=Y\"\u003e can be written as:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Clarge%20P(Y%7CX_1,%20X_2,%20...,%20X_n)%20=%20%5Cdfrac%7BP(X_1%7CY)%20%5Ccdot%20P(X_2%7CY)%20%5Ccdot%20...%20%5Ccdot%20P(X_n%7CY)%7D%7BP(X_1,%20X_2,%20...,%20X_n)%7DP(Y)\"\u003e\u003c/p\u003e\n\u003cp\u003eAgain, note that multiplying the conditional probabilities is based on the assumption that these probabilities (and their underlying features) are independent -- and it is this assumption that the Naive Bayes algorithm is considered naive, or simple, because this is almost never true. However, Naives Bayes can prove to be quite efficient given the right circumstances, as you will see in the upcoming lessons.\u003c/p\u003e\n\u003cp\u003eIn practice, calculating the denominator, \u003cimg src=\"https://render.githubusercontent.com/render/math?math=P(X_1,%20X_2,%20...,%20X_n)\"\u003e is often impractical or impossible as this exact combination of features may not have been previously observed. However, doing so is often not required. This is because when implementing a classifier, the exact probabilities themselves are not required to generate a prediction. Instead, you must simply answer which option is the most probable. To do this, you would calculate \u003cimg src=\"https://render.githubusercontent.com/render/math?math=P(Y_0)\"\u003e , the probability of not having the disease as well as \u003cimg src=\"https://render.githubusercontent.com/render/math?math=P(Y_1)\"\u003e , the probability of having the disease. Furthermore, since the denominator, \u003cimg src=\"https://render.githubusercontent.com/render/math?math=P(X_1,%20X_2,%20...,%20X_n)\"\u003e , is equal for both \u003cimg src=\"https://render.githubusercontent.com/render/math?math=P(Y_0)\"\u003e and \u003cimg src=\"https://render.githubusercontent.com/render/math?math=P(Y_1)\"\u003e , you can compare the numerators, as these will be proportional to the overall probability. You'll investigate this further as you code some Naive Bayes classification algorithms yourself in the upcoming lessons.\u003c/p\u003e\n\u003ch2\u003eSummary\u003c/h2\u003e\n\u003cp\u003eIn this lesson, you briefly explored how Bayes' theorem can be used to build classification algorithms. In the upcoming lessons and labs you'll investigate particular implementations of Naive Bayes classifiers which differ in how the individual conditional probabilities themselves are constructed. As you will see, Naive Bayes can be extremely effective or trivially useful depending on the context and implementation.\u003c/p\u003e","exportId":"classifiers-with-bayes"},{"id":154097,"title":"Gaussian Naive Bayes","type":"Assignment","indent":0,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_mark_done","completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-gaussian-naive-bayes\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-gaussian-naive-bayes/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","exportId":"ge43739e7a344d7fc8d0e9c3e93358249"},{"id":154098,"title":"Gaussian Naive Bayes - Lab","type":"Assignment","indent":1,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_mark_done","completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-gaussian-naive-bayes-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-gaussian-naive-bayes-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","exportId":"gf3a619c5dd8f7ed6ca56d3a886b961f1"},{"id":154099,"title":"Document Classification with Naive Bayes","type":"Assignment","indent":0,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_mark_done","completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-document-classification-with-naive-bayes\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-document-classification-with-naive-bayes/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","exportId":"g0aa558e0a5cd6c4eb73c11beeb1d394f"},{"id":154100,"title":"Document Classification with Naive Bayes - Lab","type":"Assignment","indent":1,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_mark_done","completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-document-classification-with-naive-bayes-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-document-classification-with-naive-bayes-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","exportId":"g490b04613e6dbbafe98301e1aee554ee"},{"id":154101,"title":"Bayesian Classification - Recap","type":"WikiPage","indent":0,"locked":false,"requirement":"must_mark_done","completed":false,"content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-bayesian-classification-recap-v2-1\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-bayesian-classification-recap-v2-1\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-bayesian-classification-recap-v2-1/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\u003ch2\u003eKey Takeaways\u003c/h2\u003e\n\u003cp\u003eThe key takeaways from this section include:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eNaive Bayes algorithms extend Bayes' formula to multiple variables by assuming that features are independent of one another. This then allows you to estimate an overall probability by multiplying the conditional probabilities for each of the independent features\u003c/li\u003e\n\u003cli\u003eThis assumption (that the underlying features are independent) is why Naive Bayes algorithm is considered naive -- because this is almost never true. However, Naives Bayes can prove to be quite efficient\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eExpanding to multiple features, the multinomial Bayes' formula is:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5CLarge%20P(y%7Cx_1,%20x_2,%20...,%20x_n)%20=%20%5Cfrac%7BP(y)%5Cdisplaystyle%5Cprod_%7Bi%7D%5E%7Bn%7DP(x_i%7Cy)%7D%7BP(x_1,%20x_2,%20...,%20x_n)%7D\"\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eFinally, you saw how Naive Bayes algorithm can be used for document classification by classifying YouTube videos into the appropriate topic, and classifying documents as \"spam\" or \"no spam\"\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eDue to insufficient text preprocessing (which you will learn how to do in a later module), the performance of this algorithm was trivial\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e","exportId":"bayesian-classification-recap"}]},{"id":16480,"name":"Topic 29: Decision Trees","status":"unlocked","unlockDate":null,"prereqs":[],"requirement":"all","sequential":false,"exportId":"g0ebc4761db432d19a0083623978d718d","items":[{"id":154102,"title":"Decision Trees - Introduction","type":"WikiPage","indent":0,"locked":false,"requirement":"must_mark_done","completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-decision-trees-section-intro\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-decision-trees-section-intro/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eIn this section, we're going to introduce another kind of model for predicting values that can be used for both continuous and categorical predictions - decision trees. Decision trees are used to classify (or estimate continuous values) by partitioning the sample space as efficiently as possible into sets with similar data points until you get to (or close to) a homogenous set and can reasonably predict the value for new data points. \u003c/p\u003e\n\n\u003cp\u003eDespite the fact that they've been around for decades, they are still (in conjunction with ensemble methods that we'll learn about in the next section) one of the most powerful modeling tools available in the field of machine learning. They are also highly interpretable when compared to more complex models (they're simple to explain and it's easy to understand how they make their decisions).\u003c/p\u003e\n\n\u003ch3\u003eEntropy and Information Gain\u003c/h3\u003e\n\n\u003cp\u003eDue to the nature of decision trees, you can get very different predictions depending on what questions you ask and in what order. The question then is how to come up with the right questions to ask in the right order. In this section, we also introduce the idea of entropy and information gain as mechanisms for selecting the most promising questions to ask in a decision tree.\u003c/p\u003e\n\n\u003ch3\u003eID3 Classification Trees\u003c/h3\u003e\n\n\u003cp\u003eWe also talk about Ross Quinlan's ID3 (Iterative Dichotomiser 3) algorithm for generating a decision tree from a dataset. \u003c/p\u003e\n\n\u003ch3\u003eBuilding Trees using Scikit-learn\u003c/h3\u003e\n\n\u003cp\u003eNext up, we look at how to build a decision tree using the built-in functions available in scikit-learn, and how to test the accuracy of the predictions using a simple accuracy measure, AUC, and a confusion matrix. We also show how to use the \u003ccode\u003egraph_viz\u003c/code\u003e library to generate a visualization of the resulting decision tree.\u003c/p\u003e\n\n\u003ch3\u003eHyperparameter Tuning and Pruning\u003c/h3\u003e\n\n\u003cp\u003eWe then look at some of the hyperparameters available when optimizing a decision tree. For example, if you're not careful, generated decision trees can lead to overfitting of data (wherein a model is a perfect match for training data, but horrible for test data). There are a number of hyperparameters you can use when generating a tree to minimize overfitting such as maximum depth or minimum leaf sample size. We look at these various \"pruning\" strategies to avoid overfitting of the data and to create a better model. \u003c/p\u003e\n\n\u003ch3\u003eRegression with CART Trees\u003c/h3\u003e\n\n\u003cp\u003eIn addition to building decision tree classifiers, you will also build decision trees for regression problems. \u003c/p\u003e\n\n\u003ch2\u003eSummary\u003c/h2\u003e\n\n\u003cp\u003eDecision trees are highly effective and interpretable. This section will provide you with the skills to create both classifiers and to perform regression using decision trees and to use hyperparameter tuning to optimize your model.\u003c/p\u003e\n\u003cfooter class=\"fis-footer\" style=\"visibility: hidden;\"\u003e\u003cdiv class=\"fis-feedback\"\u003e\n\u003ch5\u003eHow do you feel about this lesson?\u003c/h5\u003e\n\u003cimg id=\"thumbs-up\" data-repository=\"dsc-decision-trees-section-intro\" title=\"Thumbs up!\" alt=\"thumbs up\"\u003e\u003cimg id=\"thumbs-down\" data-repository=\"dsc-decision-trees-section-intro\" title=\"Thumbs down!\" alt=\"thumbs down\"\u003e\n\u003c/div\u003e\n\u003ch5\u003eHave specific feedback? \u003ca href=\"https://github.com/learn-co-curriculum/dsc-decision-trees-section-intro/issues/new\"\u003eTell us here!\u003c/a\u003e\n\u003c/h5\u003e\u003c/footer\u003e","exportId":"decision-trees-introduction"},{"id":154103,"title":"Introduction to Decision Trees","type":"WikiPage","indent":0,"locked":false,"requirement":"must_mark_done","completed":false,"content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-introduction-to-decision-trees\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-introduction-to-decision-trees\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-introduction-to-decision-trees/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eIn this lesson, we'll take a look at \u003cstrong\u003e\u003cem\u003edecision tree classifiers\u003c/em\u003e\u003c/strong\u003e. These are rule-based classifiers and belong to the first generation of modern AI. Despite the fact that this algorithm has been used in practice for decades, its simplicity and effectiveness for routine classification tasks is still on par with more sophisticated approaches. They are quite common in the business world because they have decent effectiveness without sacrificing explainability. Let's get started!\u003c/p\u003e\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\u003cp\u003eYou will be able to:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eDescribe a decision tree algorithm in terms of graph architecture\u003c/li\u003e\n\u003cli\u003eDescribe how decision trees are used to create partitions in a sample space\u003c/li\u003e\n\u003cli\u003eDescribe the training and prediction process of a decision tree\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003eFrom graphs to decision trees\u003c/h2\u003e\n\u003cp\u003eWe have seen basic classification algorithms (a.k.a classifiers), including Naive Bayes and logistic regression, in earlier sections. A decision tree is a different type of classifier that performs a \u003cstrong\u003erecursive partition of the sample space\u003c/strong\u003e. In this lesson, you will get a conceptual understanding of how this is achieved.\u003c/p\u003e\n\u003cp\u003eA decision tree comprises of decisions that originate from a chosen point in sample space. If you are familiar with Graph theory, a tree is a \u003cstrong\u003edirected acyclic graph with a root called \"root node\" that has no incoming edges\u003c/strong\u003e. All other nodes have one (and only one) incoming edge. Nodes having outgoing edges are known as \u003cstrong\u003einternal\u003c/strong\u003e nodes. All other nodes are called \u003cstrong\u003eleaves\u003c/strong\u003e. Nodes with an incoming edge, but no outgoing edges, are called \u003cstrong\u003eterminal nodes\u003c/strong\u003e.\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003eDirected Acyclic Graphs\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eIn computer science and mathematics, a directed graph is a collection of nodes and edges such that edges can be traversed only in a specified direction (eg, from node A to node B, but not from node B to node A). An acyclic graph is a graph such that it is impossible for a node to be visited twice along any path from one node to another. So, a directed acyclic graph (or, a DAG) is a directed graph with no cycles. A DAG has a \u003cstrong\u003etopological ordering\u003c/strong\u003e, or, a sequence of the nodes such that every edge is directed from earlier to later in the sequence.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch2\u003ePartitioning the sample space\u003c/h2\u003e\n\u003cp\u003eSo, a decision tree is effectively a DAG, such as the one seen below where \u003cstrong\u003eeach internal node partitions the sample space into two (or more) sub-spaces\u003c/strong\u003e. These nodes are partitioned according to some discrete function that takes the attributes of the sample space as input.\u003c/p\u003e\n\u003cp\u003eIn the simplest and most frequent case, each internal node considers a single attribute so that space is partitioned according to the attributes value. In the case of numeric attributes, the condition refers to a range.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-introduction-to-decision-trees/master/images/dt1.png\" width=\"600\"\u003e\u003c/p\u003e\n\u003cp\u003eThis is the basic idea behind decision trees: every internal node checks for a condition and performs a decision, and every terminal node (AKA leaf node) represents a discrete class. Decision tree induction is closely related to \u003cstrong\u003erule induction\u003c/strong\u003e. In essence, a decision tree is a just series of IF-ELSE statements (rules). Each path from the root of a decision tree to one of its leaves can be transformed into a rule simply by combining the decisions along the path to form the antecedent, and taking the leafs class prediction as the consequence (IF-ELSE statements follow the form: IF \u003cem\u003eantecedent\u003c/em\u003e THEN \u003cem\u003econsequence\u003c/em\u003e ).\u003c/p\u003e\n\u003ch2\u003eDefinition\u003c/h2\u003e\n\u003cblockquote\u003e\n\u003cp\u003eA decision tree is a DAG type of classifier where each internal node represents a choice between a number of alternatives and each leaf node represents a classification. An unknown (or test) instance is routed down the tree according to the values of the attributes in the successive nodes. When the instance reaches a leaf, it is classified according to the label assigned to the corresponded leaf.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-introduction-to-decision-trees/master/images/dt2.png\" width=\"850\"\u003e\u003c/p\u003e\n\u003cp\u003eA real dataset would usually have a lot more features than the example above and will create much bigger trees, but the idea will remain exactly the same. The idea of feature importance is crucial to decision trees, since selecting the correct feature to make a split on will affect the complexity and efficacy of the classification process. Regression trees are represented in the same manner, but instead they predict continuous values like the price of a house.\u003c/p\u003e\n\u003ch2\u003eTraining process\u003c/h2\u003e\n\u003cp\u003eThe process of training a decision tree and predicting the target features of a dataset is as follows:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003ePresent a dataset of training examples containing features/predictors and a target (similar to classifiers we have seen earlier).\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eTrain the tree model by making splits for the target using the values of predictors. Which features to use as predictors gets selected following the idea of feature selection and uses measures like \"\u003cstrong\u003einformation gain\u003c/strong\u003e\" and \"\u003cstrong\u003eGini Index\u003c/strong\u003e\". We shall cover these shortly.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eThe tree is grown until some \u003cstrong\u003estopping criteria\u003c/strong\u003e is achieved. This could be a set depth of the tree or any other similar measure.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eShow a new set of features to the tree, with an unknown class and let the example propagate through a trained tree. The resulting leaf node represents the class prediction for this example datum.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-introduction-to-decision-trees/master/images/dt3.png\" width=\"650\"\u003e\u003c/p\u003e\n\u003ch2\u003eSplitting criteria\u003c/h2\u003e\n\u003cp\u003eThe training process of a decision tree can be generalized as \"\u003cstrong\u003erecursive binary splitting\u003c/strong\u003e\".\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eIn this procedure, all the features are considered and different split points are tried and tested using some \u003cstrong\u003ecost function\u003c/strong\u003e. The split with the lowest cost is selected.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eThere are a couple of algorithms used to build a decision tree:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cstrong\u003eCART (Classification and Regression Trees)\u003c/strong\u003e uses the Gini Index as a metric\u003c/li\u003e\n\u003cli\u003e\n\u003cstrong\u003eID3 (Iterative Dichotomiser 3)\u003c/strong\u003e uses the entropy function and information gain as metrics\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003eGreedy search\u003c/h2\u003e\n\u003cp\u003eWe need to determine the attribute that \u003cstrong\u003ebest\u003c/strong\u003e classifies the training data, and use this attribute at the root of the tree. At each node, we repeat this process creating further splits, until a leaf node is achieved, i.e., all data gets classified.\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eThis means we are performing a top-down, greedy search through the space of possible decision trees.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eIn order to identify the best attribute for ID3 classification trees, we use the \"information gain\" criteria. Information gain (IG) measures how much \"information\" a feature gives us about the class. Decision trees always try to maximize information gain. So, the attribute with the highest information gain will be split on first.\u003c/p\u003e\n\u003cp\u003eLet's move on to the next lesson where we shall look into these criteria with simple examples.\u003c/p\u003e\n\u003ch2\u003eAdditional resources\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ca href=\"http://www.r2d3.us/visual-intro-to-machine-learning-part-1/\"\u003eR2D3\u003c/a\u003e: This is highly recommended for getting a visual introduction to decision trees. Excellent animations explaining the training and prediction stages shown above.\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003ca href=\"http://www.dataversity.net/introduction-machine-learning-decision-trees/\"\u003eDataversity: Decision Trees Intro\u003c/a\u003e: A quick and visual introduction to DTs.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003ca href=\"https://cran.r-project.org/web/packages/ggdag/vignettes/intro-to-dags.html\"\u003eDirected Acyclic Graphs\u003c/a\u003e: This would help relate early understanding of graph computation to decision tree architectures.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003eSummary\u003c/h2\u003e\n\u003cp\u003eIn this lesson, we saw an introduction to decision trees as simple yet effective classifiers. We looked at how decision trees partition the sample space based on learning rules from a given dataset. We also looked at how feature selection for splitting the tree is of such high importance. Next, we shall look at information gain criteria used for feature selection.\u003c/p\u003e","exportId":"introduction-to-decision-trees"},{"id":154104,"title":"Entropy and Information Gain","type":"WikiPage","indent":0,"locked":false,"requirement":"must_mark_done","completed":false,"content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-entropy-and-information-gain\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-entropy-and-information-gain\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-entropy-and-information-gain/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003e\u003cstrong\u003e\u003cem\u003eInformation gain\u003c/em\u003e\u003c/strong\u003e is calculated using a statistical measure called \u003cstrong\u003e\u003cem\u003eEntropy\u003c/em\u003e\u003c/strong\u003e. Entropy is a widely used concept in the fields of Physics, Mathematics, Computer Science (information theory), and more. You may have come across the idea of entropy in thermodynamics, societal dynamics, and a number of other domains. In electronics and computer science, the idea of entropy is usually derived from \u003cstrong\u003eShannon's\u003c/strong\u003e description of entropy to measure the information gain against some cost incurred in the process. In this lesson, we shall look at how this works with the simple example we introduced in the previous lesson.\u003c/p\u003e\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\u003cp\u003eYou will be able to:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eExplain the process for selecting the best attribute for a split\u003c/li\u003e\n\u003cli\u003eCalculate entropy and information gain by hand for a simple dataset\u003c/li\u003e\n\u003cli\u003eCompare and contrast entropy and information gain\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003eShannon's Entropy\u003c/h2\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003eEntropy is a measure of disorder or uncertainty.\u003c/strong\u003e\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eThe measure is named after \u003cem\u003eClaude Shannon\u003c/em\u003e, who is known as the \"father of information theory\". Information theory provides measures of uncertainty associated with random variables. These measures help calculate the average information content one is missing when one does not know the value of the random variable. This uncertainty is measured in bits, i.e., the amount of information (in bits) contained per average instance in a stream of instances.\u003c/p\u003e\n\u003cp\u003eConceptually, information can be thought of as being stored or transmitted as variables that can take on different values. A variable can be thought of as a unit of storage that can take on, at different times, one of several different specified values, following some process for taking on those values. Informally, we get information from a variable by looking at its value, just as we get information from an email by reading its contents. In the case of the variable, the information is about the process behind the variable.\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eThe entropy of a variable is the \"amount of information\" contained in the variable.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eThis amount is not only determined by the number of different values the variable can take, just as the information in an email is not quantified just by the number of words in the email or the different possible words in the language of the email. Informally, the amount of information in an email is proportional to the amount of surprise its reading causes.\u003c/p\u003e\n\u003cp\u003eFor example, if an email is simply a repeat of an earlier email, then it is not informative at all. On the other hand, if, for example, the email reveals the outcome of an election, then it is highly informative. Similarly, the information in a variable is tied to the amount of surprise the value of that variable causes when revealed.\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eShannons entropy quantifies the amount of information in a variable, thus providing the foundation for a theory around the notion of information.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eIn terms of data, we can informally describe entropy as an indicator of how messy your data is. A high degree of entropy always reflects \"messed-up\" data with low/no information content. The uncertainty about the content of the data, before viewing the data remains the same (or almost the same) as that before the data was available.\u003c/p\u003e\n\u003cp\u003eIn a nutshell, higher entropy means less predictive power when it comes to doing data science with that data.\u003c/p\u003e\n\u003ch2\u003eEntropy and decision trees\u003c/h2\u003e\n\u003cblockquote\u003e\n\u003cp\u003eDecision trees aim to tidy the data by separating the samples and re-grouping them in the classes they belong to.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eBecause decision trees use a supervised learning approach, we know the target variable of our data. So, we maximize the \u003cstrong\u003epurity\u003c/strong\u003e of the classes \u003cstrong\u003eas much as possible\u003c/strong\u003e while making splits, aiming to have \u003cstrong\u003eclarity\u003c/strong\u003e in the leaf nodes. Remember, it may not be possible to remove the uncertainty totally, i.e., to fully clean up the data. Have a look at the image below:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-entropy-and-information-gain/master/images/split_fs.png\" width=\"300\"\u003e\u003c/p\u003e\n\u003cp\u003eWe can see that the split has not \u003cstrong\u003eFULLY\u003c/strong\u003e classified the data above, but the resulting data is \u003cstrong\u003etidier\u003c/strong\u003e than it was before the split. By using a series of such splits that focus on different features, we try to clean up the data as much as possible in the leaf nodes. At each step, we want to decrease the entropy, so \u003cstrong\u003eentropy is computed before and after the split\u003c/strong\u003e. If it decreases, the split is retained and we can proceed to the next step, otherwise, we must try to split with another feature or stop this branch (or quit, in which case we claim that the current tree is the best solution).\u003c/p\u003e\n\u003ch3\u003eCalculating entropy\u003c/h3\u003e\n\u003cp\u003eLet's pretend we have a sample, \u003cimg src=\"https://render.githubusercontent.com/render/math?math=S\"\u003e . This sample contains \u003cimg src=\"https://render.githubusercontent.com/render/math?math=N\"\u003e total items falling into two different categories, \u003ccode\u003eTrue\u003c/code\u003e and \u003ccode\u003eFalse\u003c/code\u003e. Of the \u003cimg src=\"https://render.githubusercontent.com/render/math?math=N\"\u003e total items we have, \u003cimg src=\"https://render.githubusercontent.com/render/math?math=n\"\u003e observations have a target value equal to \u003ccode\u003eTrue\u003c/code\u003e , and \u003cimg src=\"https://render.githubusercontent.com/render/math?math=m\"\u003e observations have a target value equal to \u003ccode\u003eFalse\u003c/code\u003e. Note that if we know \u003cimg src=\"https://render.githubusercontent.com/render/math?math=N\"\u003e and \u003cimg src=\"https://render.githubusercontent.com/render/math?math=n\"\u003e , we can easily calculate \u003cimg src=\"https://render.githubusercontent.com/render/math?math=m\"\u003e to be \u003cimg src=\"https://render.githubusercontent.com/render/math?math=m%20=%20N%20-%20n\"\u003e .\u003c/p\u003e\n\u003cp\u003eLet's assume our boss brings us the dataset \u003cimg src=\"https://render.githubusercontent.com/render/math?math=S\"\u003e , and asks us to group each observation in \u003cimg src=\"https://render.githubusercontent.com/render/math?math=N\"\u003e according to whether their target value is \u003ccode\u003eTrue\u003c/code\u003e or \u003ccode\u003eFalse\u003c/code\u003e. They also want to know the ratio of \u003ccode\u003eTrue\u003c/code\u003es to \u003ccode\u003eFalse\u003c/code\u003es in our dataset. We can calculate this as follows:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=p%20=%20n/N%20-%20(class%201)\"\u003e\u003cbr\u003e\u003cbr\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=q%20=%20m/N%20=%201-p%20-%20(class%202)\"\u003e\u003c/p\u003e\n\u003cp\u003eIf we know these ratios, we can calculate the \u003cem\u003eentropy\u003c/em\u003e of the dataset \u003cimg src=\"https://render.githubusercontent.com/render/math?math=S\"\u003e . This will provide us with an easy way to see how organized or disorganized our dataset is. For instance, let's assume that our boss believes that the dataset should mostly be full of \u003ccode\u003eTrue\u003c/code\u003es, with some occasional \u003ccode\u003eFalse\u003c/code\u003es slipping through. The more \u003ccode\u003eFalse\u003c/code\u003es in with the \u003ccode\u003eTrue\u003c/code\u003es (or \u003ccode\u003eTrue\u003c/code\u003es in with the \u003ccode\u003eFalse\u003c/code\u003es!), the more disorganized our dataset is.\u003c/p\u003e\n\u003cp\u003eWe can calculate entropy using the following equation:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=E%20=%20-p%20.%20log_2(p)%20-%20q%20.%20log_2(q)\"\u003e\u003c/p\u003e\n\u003cp\u003eDon't worry too much about this equation yet -- we'll dig deeper into what it means in a minute.\u003c/p\u003e\n\u003cp\u003eThe equation above tells us that a dataset is considered tidy if it only contains one class (i.e. no uncertainty or confusion). If the dataset contains a mix of classes for our target variable, the entropy increases. This is easier to understand when we visualize it. Consider the following graph of entropy in a dataset that has two classes for our target variable:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-entropy-and-information-gain/master/images/new_entropy_fs.png\" width=\"500\"\u003e\u003c/p\u003e\n\u003cp\u003eAs you can see, when the classes are split equally, \u003cimg src=\"https://render.githubusercontent.com/render/math?math=p%20=%200.5\"\u003e and \u003cimg src=\"https://render.githubusercontent.com/render/math?math=q%20=%201%20-%20p%20=%200.5\"\u003e , the entropy value is at its maximum, 1. Conversely, when the proportion of the split is at 0 (all of one target class) or at 1 (all of the other target class), the entropy value is 0! This means that we can easily think of entropy as follows: the more one-sided the proportion of target classes, the less entropy. Think of a sock drawer that may or may not have some underwear mixed in. If the sock drawer contains only socks (or only underwear), then entropy is 0. If you reach in and pull out an article of clothing, you know exactly what you're going to get. However, if 10% of the items in that sock drawer are actually underwear, you are less certain what that random draw will give you. That uncertainty increases as more and more underwear gets mixed into that sock drawer, right up until there is the exact same amount of socks and underwear in the drawer. When the proportion is exactly equal, you have no way of knowing item of clothing a random draw might give you -- maximum entropy, and perfect chaos!\u003c/p\u003e\n\u003cp\u003eThis is where the logic behind decision trees comes in -- what if we could split the contents of our sock drawer into different subsets, which might divide the drawer into more organized subsets? For instance, let's assume that we've built a laundry robot that can separate items of clothing by color. If a majority of our socks are white, and a majority of our underwear is some other color, then we can safely assume that the two subsets will have a better separation between socks and underwear, even if the original chaotic drawer had a 50/50 mix of the two!\u003c/p\u003e\n\u003ch3\u003eGeneralization of entropy\u003c/h3\u003e\n\u003cp\u003eNow that we have a good real-world example to cling to, let's get back to thinking about the mathematical definition of entropy.\u003c/p\u003e\n\u003cp\u003eEntropy \u003cimg src=\"https://render.githubusercontent.com/render/math?math=H(S)\"\u003e is a measure of the amount of uncertainty in the dataset \u003cimg src=\"https://render.githubusercontent.com/render/math?math=S\"\u003e . We can see this is a measurement or characterization of the amount of information contained within the dataset \u003cimg src=\"https://render.githubusercontent.com/render/math?math=S\"\u003e .\u003c/p\u003e\n\u003cp\u003eWe saw how to calculate entropy for a two-class variable before. However, in the real world we deal with multiclass problems very often, so it would be a good idea to see a general representation of the formula we saw before. The general representation is:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Clarge%20H(S)%20=%20-%5Csum%20(P_i%20.%20log_2(P_i))\"\u003e\u003c/p\u003e\n\u003cp\u003eWhen \u003cimg src=\"https://render.githubusercontent.com/render/math?math=H(S)%20=%200\"\u003e , this means that the set \u003cimg src=\"https://render.githubusercontent.com/render/math?math=S\"\u003e is perfectly classified, meaning that there is no disorganization in our data because all of our data in S is the exact same class. If we know how much entropy exists in a subset (and remember, we can subset our data by just splitting it into 2 or more groups according to whatever metric we choose), then we can easily calculate how much \u003cstrong\u003e\u003cem\u003einformation gain\u003c/em\u003e\u003c/strong\u003e each potential split would give us!\u003c/p\u003e\n\u003ch2\u003eInformation gain\u003c/h2\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003eInformation gain is an impurity/uncertainty based criterion that uses the entropy as the measure of impurity.\u003c/strong\u003e\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eThere are several different algorithms out there for creating decision trees. Of those, the ID3 algorithm is one of the most popular. Information gain is the key criterion that is used by the ID3 classification tree algorithm to construct a decision tree. The decision tree algorithm will always try to \u003cstrong\u003emaximize information gain\u003c/strong\u003e. The entropy of the dataset is calculated using each attribute, and the attribute showing highest information gain is used to create the split at each node. A simple understanding of information gain can be written as:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Ctext%7BInformation%20Gain%7D%20=%20%5Ctext%7BEntropy%7D_%7B%5Ctext%7Bparent%7D%7D%20-%20%5Ctext%7BEntropy%7D_%7B%5Ctext%7Bchild%7D%7D.%5B%5Ctext%7Bchild%20weighted%20average%7D%5D\"\u003e\u003c/p\u003e\n\u003cp\u003eA weighted average based on the number of samples in each class is multiplied by the child's entropy, since most datasets have class imbalance. Thus the information gain calculation for each attribute is calculated and compared, and the attribute showing the highest information gain will be selected for the split.\u003c/p\u003e\n\u003cp\u003eWhen we measure information gain, we're really measuring the difference in entropy from before the split (an untidy sock drawer) to after the split (a group of white socks and underwear, and a group of non-white socks and underwear). Information gain allows us to put a number to exactly how much we've reduced our \u003cem\u003euncertainty\u003c/em\u003e after splitting a dataset \u003cimg src=\"https://render.githubusercontent.com/render/math?math=S\"\u003e on some attribute, \u003cimg src=\"https://render.githubusercontent.com/render/math?math=A\"\u003e.\u003c/p\u003e\n\u003cp\u003eThe equation for information gain is:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Clarge%20IG(A,%20S)%20=%20H(S)%20-%20%5Csum%7B%7D%7Bp(t)H(t)%7D\"\u003e\u003c/p\u003e\n\u003cp\u003eWhere:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cimg src=\"https://render.githubusercontent.com/render/math?math=H(S)\"\u003e is the entropy of set \u003cimg src=\"https://render.githubusercontent.com/render/math?math=S\"\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cimg src=\"https://render.githubusercontent.com/render/math?math=t\"\u003e is a subset of the attributes contained in \u003cimg src=\"https://render.githubusercontent.com/render/math?math=A\"\u003e (we represent all subsets \u003cimg src=\"https://render.githubusercontent.com/render/math?math=t\"\u003e as \u003cimg src=\"https://render.githubusercontent.com/render/math?math=T\"\u003e )\u003c/li\u003e\n\u003cli\u003e\n\u003cimg src=\"https://render.githubusercontent.com/render/math?math=p(t)\"\u003e is the proportion of the number of elements in \u003cimg src=\"https://render.githubusercontent.com/render/math?math=t\"\u003e to the number of elements in \u003cimg src=\"https://render.githubusercontent.com/render/math?math=S\"\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cimg src=\"https://render.githubusercontent.com/render/math?math=H(t)\"\u003e is the entropy of a given subset \u003cimg src=\"https://render.githubusercontent.com/render/math?math=t\"\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eIn the ID3 algorithm, we use entropy to calculate information gain, and then pick the attribute with the largest possible information gain to split our data on at each iteration.\u003c/p\u003e\n\u003ch2\u003eEntropy and information gain example\u003c/h2\u003e\n\u003cp\u003eSo far, we've focused heavily on the math behind entropy and information gain. This usually makes the calculations look scarier than they actually are. To show that calculating entropy/information gain is actually pretty simple, let's take a look at an example problem -- predicting if we want to play tennis or not, based on the weather, temperature, humidity, and windiness of a given day!\u003c/p\u003e\n\u003cp\u003eOur dataset is as follows:\u003c/p\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth style=\"text-align: center;\"\u003eoutlook\u003c/th\u003e\n\u003cth style=\"text-align: center;\"\u003etemp\u003c/th\u003e\n\u003cth style=\"text-align: center;\"\u003ehumidity\u003c/th\u003e\n\u003cth style=\"text-align: center;\"\u003ewindy\u003c/th\u003e\n\u003cth style=\"text-align: center;\"\u003eplay\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd style=\"text-align: center;\"\u003eovercast\u003c/td\u003e\n\u003ctd style=\"text-align: center;\"\u003ecool\u003c/td\u003e\n\u003ctd style=\"text-align: center;\"\u003ehigh\u003c/td\u003e\n\u003ctd style=\"text-align: center;\"\u003eY\u003c/td\u003e\n\u003ctd style=\"text-align: center;\"\u003eyes\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd style=\"text-align: center;\"\u003eovercast\u003c/td\u003e\n\u003ctd style=\"text-align: center;\"\u003emild\u003c/td\u003e\n\u003ctd style=\"text-align: center;\"\u003enormal\u003c/td\u003e\n\u003ctd style=\"text-align: center;\"\u003eN\u003c/td\u003e\n\u003ctd style=\"text-align: center;\"\u003eyes\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd style=\"text-align: center;\"\u003esunny\u003c/td\u003e\n\u003ctd style=\"text-align: center;\"\u003ecool\u003c/td\u003e\n\u003ctd style=\"text-align: center;\"\u003enormal\u003c/td\u003e\n\u003ctd style=\"text-align: center;\"\u003eN\u003c/td\u003e\n\u003ctd style=\"text-align: center;\"\u003eyes\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd style=\"text-align: center;\"\u003eovercast\u003c/td\u003e\n\u003ctd style=\"text-align: center;\"\u003ehot\u003c/td\u003e\n\u003ctd style=\"text-align: center;\"\u003ehigh\u003c/td\u003e\n\u003ctd style=\"text-align: center;\"\u003eY\u003c/td\u003e\n\u003ctd style=\"text-align: center;\"\u003eno\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd style=\"text-align: center;\"\u003esunny\u003c/td\u003e\n\u003ctd style=\"text-align: center;\"\u003ehot\u003c/td\u003e\n\u003ctd style=\"text-align: center;\"\u003enormal\u003c/td\u003e\n\u003ctd style=\"text-align: center;\"\u003eY\u003c/td\u003e\n\u003ctd style=\"text-align: center;\"\u003eyes\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd style=\"text-align: center;\"\u003erain\u003c/td\u003e\n\u003ctd style=\"text-align: center;\"\u003emild\u003c/td\u003e\n\u003ctd style=\"text-align: center;\"\u003ehigh\u003c/td\u003e\n\u003ctd style=\"text-align: center;\"\u003eN\u003c/td\u003e\n\u003ctd style=\"text-align: center;\"\u003eno\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd style=\"text-align: center;\"\u003erain\u003c/td\u003e\n\u003ctd style=\"text-align: center;\"\u003ecool\u003c/td\u003e\n\u003ctd style=\"text-align: center;\"\u003enormal\u003c/td\u003e\n\u003ctd style=\"text-align: center;\"\u003eN\u003c/td\u003e\n\u003ctd style=\"text-align: center;\"\u003eno\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd style=\"text-align: center;\"\u003esunny\u003c/td\u003e\n\u003ctd style=\"text-align: center;\"\u003emild\u003c/td\u003e\n\u003ctd style=\"text-align: center;\"\u003ehigh\u003c/td\u003e\n\u003ctd style=\"text-align: center;\"\u003eN\u003c/td\u003e\n\u003ctd style=\"text-align: center;\"\u003eyes\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd style=\"text-align: center;\"\u003esunny\u003c/td\u003e\n\u003ctd style=\"text-align: center;\"\u003ecool\u003c/td\u003e\n\u003ctd style=\"text-align: center;\"\u003enormal\u003c/td\u003e\n\u003ctd style=\"text-align: center;\"\u003eY\u003c/td\u003e\n\u003ctd style=\"text-align: center;\"\u003eyes\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd style=\"text-align: center;\"\u003esunny\u003c/td\u003e\n\u003ctd style=\"text-align: center;\"\u003emild\u003c/td\u003e\n\u003ctd style=\"text-align: center;\"\u003enormal\u003c/td\u003e\n\u003ctd style=\"text-align: center;\"\u003eY\u003c/td\u003e\n\u003ctd style=\"text-align: center;\"\u003eyes\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd style=\"text-align: center;\"\u003eovercast\u003c/td\u003e\n\u003ctd style=\"text-align: center;\"\u003ecool\u003c/td\u003e\n\u003ctd style=\"text-align: center;\"\u003ehigh\u003c/td\u003e\n\u003ctd style=\"text-align: center;\"\u003eN\u003c/td\u003e\n\u003ctd style=\"text-align: center;\"\u003eyes\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd style=\"text-align: center;\"\u003erain\u003c/td\u003e\n\u003ctd style=\"text-align: center;\"\u003ecool\u003c/td\u003e\n\u003ctd style=\"text-align: center;\"\u003ehigh\u003c/td\u003e\n\u003ctd style=\"text-align: center;\"\u003eY\u003c/td\u003e\n\u003ctd style=\"text-align: center;\"\u003eno\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd style=\"text-align: center;\"\u003esunny\u003c/td\u003e\n\u003ctd style=\"text-align: center;\"\u003ehot\u003c/td\u003e\n\u003ctd style=\"text-align: center;\"\u003enormal\u003c/td\u003e\n\u003ctd style=\"text-align: center;\"\u003eY\u003c/td\u003e\n\u003ctd style=\"text-align: center;\"\u003eno\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd style=\"text-align: center;\"\u003esunny\u003c/td\u003e\n\u003ctd style=\"text-align: center;\"\u003emild\u003c/td\u003e\n\u003ctd style=\"text-align: center;\"\u003ehigh\u003c/td\u003e\n\u003ctd style=\"text-align: center;\"\u003eN\u003c/td\u003e\n\u003ctd style=\"text-align: center;\"\u003eyes\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003cp\u003eLet's apply the formulas we saw earlier to this problem:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=H(S)%20=%20%5Csum%7B%7D%7B-p(C)%20log_2%20p(C)%7D\"\u003e \u003cbr\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=C=%7B%5C%7Byes,%20no%5C%7D%7D\"\u003e\u003c/p\u003e\n\u003cp\u003eOut of 14 instances, 9 are classified as yes, and 5 as no. So:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=p(yes)%20=%20-(9/14)log_2(9/14)%20=%200.28\"\u003e \u003cbr\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=p(no)%20=%20-(5/14)log_2(5/14)%20=%200.37\"\u003e \u003cbr\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=H(S)%20=%20p(yes)%20%2b%20p(no)%20=%200.65\"\u003e\u003c/p\u003e\n\u003cp\u003eThe current entropy of our dataset is 0.65. In the next lesson, we'll see how we can improve this by subsetting our dataset into different groups by calculating the entropy/information gain of each possible split, and then picking the one that performs best until we have a fully fleshed-out decision tree!\u003c/p\u003e\n\u003ch2\u003eSummary\u003c/h2\u003e\n\u003cp\u003eIn this lesson, we looked at calculating entropy and information gain measures for building decision trees. We looked at a simple example and saw how to use these measures to select the best split at each node. Next, we calculate these measures in Python, before digging deeper into decision trees.\u003c/p\u003e","exportId":"entropy-and-information-gain"},{"id":154105,"title":"ID3 Classification Trees: Perfect Split with Information Gain - Lab","type":"Assignment","indent":1,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_mark_done","completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-ID3-trees-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-ID3-trees-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","exportId":"g92a79bdb58c268e3896971638328cfda"},{"id":154106,"title":"Building Trees using scikit-learn","type":"Assignment","indent":0,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_mark_done","completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-decision-trees-with-sklearn-codealong\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-decision-trees-with-sklearn-codealong/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","exportId":"g6d29959318dd25b71c0d335a961f5a90"},{"id":154107,"title":"Building Trees using scikit-learn - Lab","type":"Assignment","indent":1,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_mark_done","completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-decision-trees-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-decision-trees-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","exportId":"g3c2aa2f625d8be27c9fbab09bb2c9221"},{"id":154108,"title":"Hyperparameter Tuning and Pruning in Decision Trees","type":"WikiPage","indent":0,"locked":false,"requirement":"must_mark_done","completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-tuning-decision-trees\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-tuning-decision-trees/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eHyperparameter tuning relates to how we sample candidate model architectures from the space of all possible hyperparameter values. This is often referred to as \u003cstrong\u003esearching the hyperparameter space for the optimum values\u003c/strong\u003e. In this lesson, we'll look at some of the key hyperparameters for decision trees and how they affect the learning and prediction processes. \u003c/p\u003e\n\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\n\u003cul\u003e\n\u003cli\u003eIdentify the role of pruning while training decision trees\u003cbr\u003e\n\u003c/li\u003e\n\u003cli\u003eList the different hyperparameters for tuning decision trees \u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch2\u003eHyperparameter Optimization\u003c/h2\u003e\n\n\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003eIn machine learning, a hyperparameter is a parameter whose value is set before the learning process begins.\u003c/strong\u003e\u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003cp\u003eBy contrast, the values of model parameters are derived via training as we have seen previously.\nDifferent model training algorithms require different hyperparameters, some simple algorithms (such as ordinary least squares regression) require none. Given these hyperparameters, the training algorithm learns the parameters from the data. For instance, Lasso is an algorithm that adds a regularization hyperparameter to ordinary least squares regression, which has to be set before estimating the parameters through the training algorithm. \u003c/p\u003e\n\n\u003cp\u003eIn this lesson, we'll look at these sorts of optimizations in the context of decision trees and see how these can affect the predictive performance as well as the computational complexity of the tree. \u003c/p\u003e\n\n\u003ch2\u003eTree pruning\u003c/h2\u003e\n\n\u003cp\u003eNow that we know how to grow a decision tree using Python and scikit-learn, let's move on and practice \u003cstrong\u003eoptimizing\u003c/strong\u003e a classifier. We can tweak a few parameters in the decision tree algorithm before the actual learning takes place. \u003c/p\u003e\n\n\u003cp\u003eA decision tree, grown beyond a certain level of complexity leads to overfitting. If we grow our tree and carry on using poor predictors that don't have any impact on the accuracy, we will eventually a) slow down the learning, and b) cause overfitting.  Different tree pruning parameters can adjust the amount of overfitting or underfitting in order to optimize for increased accuracy, precision, and/or recall.\u003c/p\u003e\n\n\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003eThis process of trimming decision trees to optimize the learning process is called \"tree pruning\".\u003c/strong\u003e\u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003cp\u003eWe can prune our trees using:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003e\u003cp\u003eMaximum depth: Reduce the depth of the tree to build a generalized tree. Set the depth of the tree to 3, 5, 10 depending after verification on test data\u003c/p\u003e\u003c/li\u003e\n\u003cli\u003e\u003cp\u003eMinimum samples leaf with split: Restrict the size of sample leaf\u003c/p\u003e\u003c/li\u003e\n\u003cli\u003e\u003cp\u003eMinimum leaf sample size: Size in terminal nodes can be fixed to 30, 100, 300 or 5% of total\u003c/p\u003e\u003c/li\u003e\n\u003cli\u003e\u003cp\u003eMaximum leaf nodes: Reduce the number of leaf nodes\u003c/p\u003e\u003c/li\u003e\n\u003cli\u003e\u003cp\u003eMaximum features: Maximum number of features to consider when splitting a node\u003c/p\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003eLet's look at a few hyperparameters and learn about their impact on classifier performance:  \u003c/p\u003e\n\n\u003ch2\u003e\u003ccode\u003emax_depth\u003c/code\u003e\u003c/h2\u003e\n\n\u003cp\u003eThe parameter for decision trees that we normally tune first is \u003ccode\u003emax_depth\u003c/code\u003e. This parameter indicates how deep we want our tree to be. If the tree is too deep, it means we are creating a large number of splits in the parameter space and capturing more information about underlying data. This may result in \u003cstrong\u003eoverfitting\u003c/strong\u003e as it will lead to learning granular information from given data, which makes it difficult for our model to generalize on unseen data. \nGenerally speaking, a low training error but a large testing error is a strong indication of this. \u003c/p\u003e\n\n\u003cp\u003eIf, on the other hand, the tree is too shallow, we may run into \u003cstrong\u003eunderfitting\u003c/strong\u003e, i.e., we are not learning enough information about the data and the accuracy of the model stays low for both the test and training samples. The following example shows the training and test AUC scores for a decision tree with depths ranging from 1 to 32.\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-tuning-decision-trees/master/images/depth.png\" width=\"400\"\u003e\u003c/p\u003e\n\n\u003cp\u003eIn the above example, we see that as the tree depth increases, our validation/test accuracy starts to go down after a depth of around 4. But with even greater depths, the training accuracy keeps on rising, as the classifier learns more information from the data. However this information can not be mapped onto unseen examples, hence the validation accuracy falls down constantly. Finding the sweet spot (e.g. depth = 4) in this case would be the first hyperparameter that we need to tune. \u003c/p\u003e\n\n\u003ch2\u003e\u003ccode\u003emin_samples_split\u003c/code\u003e\u003c/h2\u003e\n\n\u003cp\u003eThe hyperparameter \u003ccode\u003emin_samples_split\u003c/code\u003e is used to set the \u003cstrong\u003eminimum number of samples required to split an internal node\u003c/strong\u003e. This can vary between two extremes, i.e., considering only one sample at each node vs. considering all of the samples at each node - for a given attribute. \u003c/p\u003e\n\n\u003cp\u003eWhen we increase this parameter value, the tree becomes more constrained as it has to consider more samples at each node. Here we will vary the parameter from 10% to 100% of the samples.\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-tuning-decision-trees/master/images/split.png\" width=\"500\"\u003e\u003c/p\u003e\n\n\u003cp\u003eIn the above plot, we see that the training and test accuracy stabilize at a certain minimum sample split size, and stays the same even if we carry on increasing the size of the split. This means that we will have a complex model, with similar accuracy than a much simpler model could potentially exhibit. Therefore, it is imperative that we try to identify the optimal sample size during the training phase. \u003c/p\u003e\n\n\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003eNote\u003c/strong\u003e: \u003ccode\u003emax_depth\u003c/code\u003e and \u003ccode\u003emin_samples_split\u003c/code\u003e are also both related to the computational cost involved with growing the tree. Large values for these parameters can create complex, dense, and long trees. For large datasets, it may become extremely time-consuming to use default values.  \u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003ch2\u003e\u003ccode\u003emin_samples_leaf\u003c/code\u003e\u003c/h2\u003e\n\n\u003cp\u003eThis hyperparameter is used to identify the minimum number of samples that we want a leaf node to contain. When this minimum size is achieved at a node, it does not get split any further.  This parameter is similar to \u003ccode\u003emin_samples_splits\u003c/code\u003e, however, this describes the minimum number of samples at the leaves, the base of the tree.\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-tuning-decision-trees/master/images/leaf.png\" width=\"400\"\u003e\u003c/p\u003e\n\n\u003cp\u003eThe above plot shows the impact of this parameter on the accuracy of the classifier. We see that increasing this parameter value after an optimal point reduces accuracy. That is due to underfitting again, as keeping too many samples in our leaf nodes means that there is still a high level of uncertainty in the data. \u003c/p\u003e\n\n\u003cp\u003eThe main difference between the two is that \u003ccode\u003emin_samples_leaf\u003c/code\u003e guarantees a minimum number of samples in a leaf, while \u003ccode\u003emin_samples_split\u003c/code\u003e can create arbitrary small leaves, though \u003ccode\u003emin_samples_split\u003c/code\u003e is more common in practice. These two hyperparameters make the distinction between a leaf (terminal/external node) and an internal node. An internal node will have further splits (also called children), while a leaf is by definition a node without any children (without any further splits).\u003c/p\u003e\n\n\u003cp\u003eFor instance, if \u003ccode\u003emin_samples_split = 5\u003c/code\u003e, and there are 7 samples at an internal node, then the split is allowed. But let's say the split results in two leaves, one with 1 sample, and another with 6 samples. If \u003ccode\u003emin_samples_leaf = 2\u003c/code\u003e, then the split won't be allowed (even if the internal node has 7 samples) because one of the leaves resulted will have less than the minimum number of samples required to be at a leaf node.\u003c/p\u003e\n\n\u003ch3\u003eAre there more hyperparameters?\u003c/h3\u003e\n\n\u003cp\u003eYes, there are! Scikit-learn offers a number of other hyperparameters for further fine-tuning the learning process. \u003ca href=\"https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html\"\u003eConsult the official doc\u003c/a\u003e to look at them in detail. The hyperparameters mentioned here are directly related to the complexity which may arise in decision trees and are normally tuned when growing trees. We'll shortly see this in action with a real dataset. \u003c/p\u003e\n\n\u003ch2\u003eAdditional Resources\u003c/h2\u003e\n\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://cloud.google.com/ml-engine/docs/tensorflow/hyperparameter-tuning-overview\"\u003eOverview of hyperparameter tuning\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://towardsdatascience.com/demystifying-hyper-parameter-tuning-acb83af0258f\"\u003eDemystifying hyperparameter tuning\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://www.displayr.com/machine-learning-pruning-decision-trees/\"\u003ePruning decision trees\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch2\u003eSummary\u003c/h2\u003e\n\n\u003cp\u003eIn this lesson, we looked at the idea of optimizing hyperparameters and how pruning plays an important role in restricting the growth of a decision tree. We looked at a few hyperparameters which directly impact the potential overfitting/underfitting in trees. Next, we'll see these in practice using scikit-learn.   \u003c/p\u003e\n\u003cfooter class=\"fis-footer\" style=\"visibility: hidden;\"\u003e\u003cdiv class=\"fis-feedback\"\u003e\n\u003ch5\u003eHow do you feel about this lesson?\u003c/h5\u003e\n\u003cimg id=\"thumbs-up\" data-repository=\"dsc-tuning-decision-trees\" title=\"Thumbs up!\" alt=\"thumbs up\"\u003e\u003cimg id=\"thumbs-down\" data-repository=\"dsc-tuning-decision-trees\" title=\"Thumbs down!\" alt=\"thumbs down\"\u003e\n\u003c/div\u003e\n\u003ch5\u003eHave specific feedback? \u003ca href=\"https://github.com/learn-co-curriculum/dsc-tuning-decision-trees/issues/new\"\u003eTell us here!\u003c/a\u003e\n\u003c/h5\u003e\u003c/footer\u003e","exportId":"hyperparameter-tuning-and-pruning-in-decision-trees"},{"id":154109,"title":"Hyperparameter Tuning and Pruning in Decision Trees - Lab","type":"Assignment","indent":1,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_mark_done","completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-tuning-decision-trees-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-tuning-decision-trees-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","exportId":"gb53019a69149a0bca741d35cf2fc001a"},{"id":154110,"title":"Regression with CART Trees","type":"Assignment","indent":0,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_mark_done","completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-regression-cart-trees-codealong\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-regression-cart-trees-codealong/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","exportId":"g788207398f92235119a984d3e4791ebd"},{"id":154111,"title":"Regression with CART Trees - Lab","type":"Assignment","indent":1,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_mark_done","completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-regression-cart-trees-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-regression-cart-trees-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","exportId":"g7df89b6489cd90836a98f6c229d019a8"},{"id":154112,"title":"Regression Trees and Model Optimization - Lab","type":"Assignment","indent":1,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_mark_done","completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-tuning-regression-trees-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-tuning-regression-trees-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","exportId":"g67e1252db6cff2589c8f39e5a6841e8c"},{"id":154113,"title":"Decision Trees - Recap","type":"WikiPage","indent":0,"locked":false,"requirement":"must_mark_done","completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-decision-trees-section-recap\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-decision-trees-section-recap/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\u003ch2\u003eKey Takeaways\u003c/h2\u003e\n\n\u003cp\u003eThe key takeaways from this section include:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eDecision trees can be used for both categorization and regression tasks\u003c/li\u003e\n\u003cli\u003eThey are a powerful and interpretable technique for many machine learning problems (especially when combined with ensemble methods)\u003c/li\u003e\n\u003cli\u003eDecision trees are a form of Directed Acyclic Graphs (DAGs) - you traverse them in a specified direction, and there are no \"loops\" in the graphs to go backward\u003c/li\u003e\n\u003cli\u003eAlgorithms for generating decision trees are designed to maximize the information gain from each split\u003c/li\u003e\n\u003cli\u003eA popular algorithm for generating decision trees is ID3 - the Iterative Dichotomiser 3 algorithm\u003c/li\u003e\n\u003cli\u003eThere are several hyperparameters for decision trees to reduce overfitting - including maximum depth, minimum samples to split a node that is currently a leaf, minimum leaf sample size, maximum leaf nodes, and maximum features \u003c/li\u003e\n\u003c/ul\u003e\n\u003cfooter class=\"fis-footer\" style=\"visibility: hidden;\"\u003e\u003cdiv class=\"fis-feedback\"\u003e\n\u003ch5\u003eHow do you feel about this lesson?\u003c/h5\u003e\n\u003cimg id=\"thumbs-up\" data-repository=\"dsc-decision-trees-section-recap\" title=\"Thumbs up!\" alt=\"thumbs up\"\u003e\u003cimg id=\"thumbs-down\" data-repository=\"dsc-decision-trees-section-recap\" title=\"Thumbs down!\" alt=\"thumbs down\"\u003e\n\u003c/div\u003e\n\u003ch5\u003eHave specific feedback? \u003ca href=\"https://github.com/learn-co-curriculum/dsc-decision-trees-section-recap/issues/new\"\u003eTell us here!\u003c/a\u003e\n\u003c/h5\u003e\u003c/footer\u003e","exportId":"decision-trees-recap"}]},{"id":16481,"name":"Topic 30: Ensemble Methods","status":"started","unlockDate":null,"prereqs":[],"requirement":"all","sequential":false,"exportId":"g5f0a37b51c6187bf03adf0a228573d67","items":[{"id":154114,"title":"Ensemble Methods - Introduction","type":"WikiPage","indent":0,"locked":false,"requirement":"must_mark_done","completed":true,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-ensemble-methods-section-intro\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-ensemble-methods-section-intro/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eIn this section, you'll learn about some of the most powerful machine learning algorithms: ensemble models! This lesson summarizes the topics we'll be covering in this section.\u003c/p\u003e\n\u003ch2\u003eEnsembles\u003c/h2\u003e\n\u003cp\u003eThe idea of ensembles is to bring together multiple models to use them to improve the quality of your predictions when compared to just using a single model. In many real-world problems and Kaggle competitions, ensemble methods tend to outperform any single model.\u003c/p\u003e\n\u003ch3\u003eEnsemble Methods\u003c/h3\u003e\n\u003cp\u003eWe start the section by providing an introduction to the concept of ensemble methods, explaining how they take advantage of the delphic technique (or \"wisdom of crowds\") where the average of multiple independent estimates is usually more consistently accurate than the individual estimates.\u003c/p\u003e\n\u003cp\u003eWe also provide an introduction to the idea of bagging (Bootstrap Aggregation).\u003c/p\u003e\n\u003ch3\u003eRandom Forests\u003c/h3\u003e\n\u003cp\u003eWe then look at random forests - an ensemble method for decision trees that takes advantage of bagging and the subspace sampling method to create a \"forest\" of decision trees that provides consistently better predictions than any single decision tree.\u003c/p\u003e\n\u003ch3\u003eGridsearchCV\u003c/h3\u003e\n\u003cp\u003eWe will also introduce some of the common hyperparameters for tuning decision trees. In this lesson, we look at how you can use GridSearchCV to perform an exhaustive search across multiple hyperparameters and multiple possible values to come up with a better performing model.\u003c/p\u003e\n\u003ch3\u003eGradient Boosting and Weak Learners\u003c/h3\u003e\n\u003cp\u003eNext up, we introduce the concept of boosting which is at the heart of some of the most powerful ensemble methods such as Adaboost and Gradient Boosted Trees.\u003c/p\u003e\n\u003ch3\u003eXGBoost\u003c/h3\u003e\n\u003cp\u003eFinally, we end this section by introducing XGBoost (eXtreme Gradient Boosting) - the top gradient boosting algorithm currently in use.\u003c/p\u003e\n\u003ch2\u003eSummary\u003c/h2\u003e\n\u003cp\u003eYou will often find yourself using a range of ensemble techniques to improve the performance of your models, so this section will introduce you to the techniques that will help you to improve the quality of your models.\u003c/p\u003e\n\u003cfooter class=\"fis-footer\" style=\"visibility: hidden;\"\u003e\n\u003cdiv class=\"fis-feedback\"\u003e\n\u003ch5\u003eHow do you feel about this lesson?\u003c/h5\u003e\n\u003cimg id=\"thumbs-up\" title=\"Thumbs up!\" alt=\"thumbs up\" data-repository=\"dsc-ensemble-methods-section-intro\"\u003e\u003cimg id=\"thumbs-down\" title=\"Thumbs down!\" alt=\"thumbs down\" data-repository=\"dsc-ensemble-methods-section-intro\"\u003e\n\u003c/div\u003e\n\u003ch5\u003eHave specific feedback? \u003ca href=\"https://github.com/learn-co-curriculum/dsc-ensemble-methods-section-intro/issues/new\"\u003eTell us here!\u003c/a\u003e\n\u003c/h5\u003e\n\u003c/footer\u003e","exportId":"ensemble-methods-introduction"},{"id":154115,"title":"Ensemble Methods","type":"WikiPage","indent":0,"locked":false,"requirement":"must_mark_done","completed":false,"content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-ensemble-methods\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-ensemble-methods\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-ensemble-methods/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eIn this lesson, we'll learn about \u003cstrong\u003e\u003cem\u003eensembles\u003c/em\u003e\u003c/strong\u003e and why they're such an effective technique for supervised learning.\u003c/p\u003e\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\u003cp\u003eYou will be able to:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eExplain what is meant by \"ensemble methods\"\u003c/li\u003e\n\u003cli\u003eExplain the concept of bagging as it applies to ensemble methods\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003eWhat are ensembles?\u003c/h2\u003e\n\u003cp\u003eIn Data Science, the term \u003cstrong\u003e\u003cem\u003eensemble\u003c/em\u003e\u003c/strong\u003e refers to an algorithm that makes use of more than one model to make a prediction. Typically, when people talk about ensembles, they are referring to Supervised Learning, although there has been some ongoing research on using ensembles for unsupervised learning tasks. Ensemble methods are typically more effective when compared with single-model results for supervised learning tasks. Most Kaggle competitions are won using ensemble methods, and \u003ca href=\"https://blogs.sas.com/content/subconsciousmusings/2017/05/18/stacked-ensemble-models-win-data-science-competitions/\"\u003emuch has been written\u003c/a\u003e about why they tend to be so successful for these tasks.\u003c/p\u003e\n\u003ch3\u003eExample\u003c/h3\u003e\n\u003cp\u003eConsider the following scenario -- you are looking to invest in a company, and you want to know if that company's stock will go up or down in the next year. Instead of just asking a single person, you have the following experts available to you:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cstrong\u003e\u003cem\u003eStock Broker\u003c/em\u003e\u003c/strong\u003e: This person makes correct predictions 80% of the time\u003c/li\u003e\n\u003cli\u003e\n\u003cstrong\u003e\u003cem\u003eFinance Professor\u003c/em\u003e\u003c/strong\u003e: This person is correct 65% of the time\u003c/li\u003e\n\u003cli\u003e\n\u003cstrong\u003e\u003cem\u003eInvestment Expert\u003c/em\u003e\u003c/strong\u003e: This person is correct 85% of the time\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eIf we could only take advice from one person, we would pick the Investment Expert, and we can only be 85% sure that they are right.\u003c/p\u003e\n\u003cp\u003eHowever, if we can use all three, we can combine their knowledge to increase our overall accuracy. If they all agree that the stock is a good investment, what is the overall accuracy of the combined prediction?\u003c/p\u003e\n\u003cp\u003eWe can calculate this by multiplying the chances that each of them are wrong together, which is \u003cimg src=\"https://render.githubusercontent.com/render/math?math=0.2%20*%200.35%20*%200.15%20=%200.0105%5C%20error%5C%20rate\"\u003e , which means that our combined accuracy is \u003cimg src=\"https://render.githubusercontent.com/render/math?math=1%20-%200.0105%20=%200.9895\"\u003e , or \u003cstrong\u003e\u003cem\u003e98.95%\u003c/em\u003e\u003c/strong\u003e!\u003c/p\u003e\n\u003cp\u003eObviously, this analogy is a bit of an oversimplification -- we're assuming that each prediction is independent, which is unlikely in the real world since there's likely some overlap between the things each person is using to make their prediction. We also haven't calculated the accuracy percentages for the cases where they disagree. However, the main point of this example is that when we combine predictions, we get better overall results.\u003c/p\u003e\n\u003ch2\u003eResiliency to variance\u003c/h2\u003e\n\u003cp\u003eEnsemble methods are analogous to \"Wisdom of the crowd\". This phrase refers to the phenomenon that the average estimate of all predictions typically outperforms any single prediction by a statistically significant margin -- often, quite a large one. A Finance Professor named Jack Treynor once demonstrated this with the classic jar full of jellybeans. Professor Treynor asked all 850 of his students to guess the number of jellybeans in the jar. When he averaged the guesses, he found that of all the guesses in the class, only one student had guessed a better estimate than the group average.\u003c/p\u003e\n\u003cp\u003eThink back to what you've learned about sampling, inferential statistics, and the Central Limit theorem. The same magic is at work here. Estimators are rarely perfect. When Professor Treynor asked each student to provide an estimate of the number of jellybeans in the jar, he found that the estimates were normally distributed. This is where \"Wisdom of the crowd\" kicks in because we can expect the number of people who underestimate the number of jellybeans in the jar to be roughly equal to the number of people who overestimate the number of jellybeans. So we can safely assume the extra variance above and below the average essentially cancel each other out, leaving our average close to the ground truth value!\u003c/p\u003e\n\u003cp\u003eConsider the top-right example in this graphic that visually demonstrates high variance:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-ensemble-methods/master/images/new_bias-and-variance.png\" width=\"600\"\u003e\u003c/p\u003e\n\u003cp\u003eMost points miss the bullseye, but they are just as likely to miss in any direction. If we averaged all of these points, we would be extremely close to the bullseye! This is a great analogy for how ensemble methods work so well -- we know that no model is likely to make perfect estimates, so we have many of them make predictions, and average them, knowing that the overestimates and the underestimates will likely cancel out to be very close to the ground truth. The idea that the overestimates and underestimates will (at least partially) cancel each other out is sometimes referred to as \u003cstrong\u003e\u003cem\u003esmoothing\u003c/em\u003e\u003c/strong\u003e.\u003c/p\u003e\n\u003ch3\u003eWhich models are used in ensembles?\u003c/h3\u003e\n\u003cp\u003eFor this section, we'll be focusing exclusively on tree-based ensemble methods, such as \u003cstrong\u003e\u003cem\u003eRandom forests\u003c/em\u003e\u003c/strong\u003e and \u003cstrong\u003e\u003cem\u003eGradient boosted trees\u003c/em\u003e\u003c/strong\u003e. However, we can technically use any models in an ensemble! It's not uncommon to see \u003cstrong\u003e\u003cem\u003eModel stacking\u003c/em\u003e\u003c/strong\u003e, also called \u003cstrong\u003e\u003cem\u003eMeta-ensembling\u003c/em\u003e\u003c/strong\u003e, where multiple different models are stacked, and their predictions are aggregated. In this case, the more different the models are, the better! This is because the more different the models are, the more likely they have the potential to pick up on different characteristics of the data. It's not uncommon to see ensembles consisting of multiple logistic regressions, Naive Bayes classifiers, Tree-based models (including ensembles such as random forests), and even deep neural networks!\u003c/p\u003e\n\u003cp\u003eFor a much more in-depth explanation of what model stacking looks like and why it is effective, take a look at this great \u003ca href=\"http://blog.kaggle.com/2016/12/27/a-kagglers-guide-to-model-stacking-in-practice/\"\u003earticle from Kaggle's blog, No Free Hunch!\u003c/a\u003e\u003c/p\u003e\n\u003ch2\u003eBootstrap aggregation\u003c/h2\u003e\n\u003cp\u003eThe main concept that makes ensembling possible is \u003cstrong\u003e\u003cem\u003eBagging\u003c/em\u003e\u003c/strong\u003e, which is short for \u003cstrong\u003e\u003cem\u003eBootstrap Aggregation\u003c/em\u003e\u003c/strong\u003e. Bootstrap aggregation is itself a combination of two ideas -- bootstrap resampling and aggregation. You're already familiar with bootstrap resampling from our section on the Central Limit theorem. It refers to the subsets of your dataset by sampling with replacement, much as we did to calculate our sample means when working with the Central Limit theorem. Aggregation is exactly as it sounds -- the practice of combining all the different estimates to arrive at a single estimate -- although the specifics for how we combine them are up to us. A common approach is to treat each classifier in the ensemble's prediction as a \"vote\" and let our overall prediction be the majority vote. It's also common to see ensembles that take the arithmetic mean of all predictions, or compute a weighted average.\u003c/p\u003e\n\u003cp\u003eThe process for training an ensemble through bootstrap aggregation is as follows:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eGrab a sizable sample from your dataset, with replacement\u003c/li\u003e\n\u003cli\u003eTrain a classifier on this sample\u003c/li\u003e\n\u003cli\u003eRepeat until all classifiers have been trained on their own sample from the dataset\u003c/li\u003e\n\u003cli\u003eWhen making a prediction, have each classifier in the ensemble make a prediction\u003c/li\u003e\n\u003cli\u003eAggregate all predictions from all classifiers into a single prediction, using the method of your choice\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-ensemble-methods/master/images/new_bagging.png\"\u003e\u003c/p\u003e\n\u003cp\u003eDecision trees are often used because they are very sensitive to variance. On their own, this is a weakness. However, when aggregated together into an ensemble, this actually becomes a good thing!\u003c/p\u003e\n\u003ch2\u003eSummary\u003c/h2\u003e\n\u003cp\u003eIn this lesson, we learned about what constitutes an \u003cstrong\u003eEnsemble\u003c/strong\u003e, and how \u003cstrong\u003eBagging\u003c/strong\u003e plays a central role in this. In the next lesson, we'll see how bagging is combined with another important technique to create one of the most effective ensemble algorithms available today -- \u003cstrong\u003e\u003cem\u003eRandom forests\u003c/em\u003e\u003c/strong\u003e!\u003c/p\u003e","exportId":"ensemble-methods"},{"id":154116,"title":"Random Forests","type":"WikiPage","indent":0,"locked":false,"requirement":"must_mark_done","completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-random-forests\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-random-forests/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eIn this lesson, we'll learn about a powerful and popular ensemble method that makes use of decision trees -- a random forest!\u003c/p\u003e\n\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\n\u003cp\u003eYou will be able to:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eDescribe how the random forest algorithm works \u003c/li\u003e\n\u003cli\u003eDescribe the subspace sampling method that makes random forests \"random\" \u003c/li\u003e\n\u003cli\u003eExplain the benefits and drawbacks of random forest models \u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch2\u003eUnderstanding the Random forest algorithm\u003c/h2\u003e\n\n\u003cp\u003eThe \u003cstrong\u003e\u003cem\u003eRandom Forest\u003c/em\u003e\u003c/strong\u003e algorithm is a supervised learning algorithm that can be used both for classification and regression tasks. Decision trees are the cornerstone of random forests -- if you don't remember much about decision trees, now may be a good time to go back and review that section until you feel comfortable with the topic. \u003c/p\u003e\n\n\u003cp\u003ePut simply, the random forest algorithm is an ensemble of decision trees. However, you may recall that decision trees use a \u003cstrong\u003e\u003cem\u003egreedy algorithm\u003c/em\u003e\u003c/strong\u003e, meaning that given the same data, the algorithm will make a choice that maximizes information gain at every step. By itself, this presents a problem -- it doesn't matter how many trees we add to our forest if they're all the same tree! Trees trained on the same dataset will come out the exact same way every time -- there is no randomness to this algorithm. It doesn't matter if our forest has a million decision trees; if they are all exactly the same, then our performance will be no better than if we just had a single tree.\u003c/p\u003e\n\n\u003cp\u003eThink about this from a business perspective -- would you rather have a team at your disposal where everyone has exactly the same training and skills, or a team where each member has their own individual strengths and weaknesses? The second team will almost always do much better!\u003c/p\u003e\n\n\u003cp\u003eAs we learned when reading up on ensemble methods, variance is a good thing in any ensemble. So how do we create high variance among all the trees in our random forest? The answer lies in two clever techniques that the algorithm uses to make sure that each tree focuses on different things -- \u003cstrong\u003e\u003cem\u003eBagging\u003c/em\u003e\u003c/strong\u003e and the \u003cstrong\u003e\u003cem\u003eSubspace Sampling Method\u003c/em\u003e\u003c/strong\u003e.\u003c/p\u003e\n\n\u003ch2\u003eBagging\u003c/h2\u003e\n\n\u003cp\u003eThe first way to encourage differences among the trees in our forest is to train them on different samples of data. Although more data is generally better, if we gave every tree the entire dataset, we would end up with each tree being exactly the same. Because of this, we instead use \u003cstrong\u003e\u003cem\u003eBootstrap Aggregation\u003c/em\u003e\u003c/strong\u003e (AKA \u003cstrong\u003e\u003cem\u003eBagging\u003c/em\u003e\u003c/strong\u003e) to obtain a portion of our data by sampling with replacement. For each tree, we sample two-thirds of our training data with replacement -- this is the data that will be used to build our tree. The remaining data is used as an internal test set to test each tree -- this remaining one-third is referred to as \u003cstrong\u003e\u003cem\u003eOut-Of-Bag Data\u003c/em\u003e\u003c/strong\u003e, or \u003cstrong\u003e\u003cem\u003eOOB\u003c/em\u003e\u003c/strong\u003e. For each new tree created, the algorithm then uses the remaining one-third of data that wasn't sampled to calculate the \u003cstrong\u003e\u003cem\u003eOut-Of-Bag Error\u003c/em\u003e\u003c/strong\u003e, in order to get a running, unbiased estimate of overall tree performance for each tree in the forest. \u003c/p\u003e\n\n\u003cp\u003eTraining each tree on its own individual \"bag\" of data is a great start for getting us some variability between the decision trees in our forest. However, with just bagging, all the trees are still focusing on all the same predictors. This allows for a potential weakness to affect all the trees at once -- if a predictor that usually provides strong signal provides bad information for a given observation, then it's likely that all the trees will fall for this false signal and make the wrong prediction. This is where the second major part of the Random forest algorithm comes in!\u003c/p\u003e\n\n\u003ch2\u003eSubspace sampling method\u003c/h2\u003e\n\n\u003cp\u003eAfter bagging the data, the random forest uses the \u003cstrong\u003e\u003cem\u003eSubspace sampling method\u003c/em\u003e\u003c/strong\u003e to further increase variability between the trees. Although it has a fancy mathematical-sounding name, all this method does is randomly select a subset of features to use as predictors for each node when training a decision tree, instead of using all predictors available at each node. \u003c/p\u003e\n\n\u003cp\u003eLet's pretend we're training our random forest on a dataset with 3000 rows and 10 columns. For each given tree, we would randomly \"bag\" 2000 rows with replacement. Next, we perform a subspace sample by randomly selecting a number of predictors at each node of a decision tree. Exactly how many predictors are used is a tunable parameter for this algorithm -- for simplicity's sake, let's assume we pick 6 predictors in this example. \u003c/p\u003e\n\n\u003cp\u003eThis brings us to the following pseudocode so far:\u003c/p\u003e\n\n\u003cp\u003eFor each tree in the dataset:\u003c/p\u003e\n\n\u003col\u003e\n\u003cli\u003eBag 2/3 of the overall data -- in our example, 2000 rows \u003c/li\u003e\n\u003cli\u003eRandomly select a set number of features to use for training each node within this -- in this example, 6 features\u003cbr\u003e\n\u003c/li\u003e\n\u003cli\u003eTrain the tree on the modified dataset, which is now a DataFrame consisting of 2000 rows and 6 columns\u003cbr\u003e\n\u003c/li\u003e\n\u003cli\u003eDrop the unused columns from step 3 from the out-of-bag rows that weren't bagged in step 1, and then use this as an internal testing set to calculate the out-of-bag error for this particular tree \u003c/li\u003e\n\u003c/ol\u003e\n\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-random-forests/master/images/new_rf-diagram.png\" width=\"750\"\u003e\u003c/p\u003e\n\n\u003ch3\u003eResiliency to overfitting\u003c/h3\u003e\n\n\u003cp\u003eOnce we've created our target number of trees, we'll be left with a random forest filled with a diverse set of decision trees that are trained on different sets of data, and also look at different subsets of features to make predictions. This amount of diversity among the trees in our forest will make for a model that is extremely resilient to noisy data, thus reducing the chance of overfitting.\u003c/p\u003e\n\n\u003cp\u003eTo understand why this is the case, let's put it in practical terms. Let's assume that of the 10 columns that we mentioned in our hypothetical dataset, column 2 correlates heavily with our target. However, there is still some noise in this dataset, and this column doesn't correlate \u003cem\u003eperfectly\u003c/em\u003e with our target -- there will be times where it suggests one class or another, but this isn't actually the case -- let's call these rows \"false signals\". In the case of a single decision tree, or even a forest where all trees focus on all the same predictors, we can expect to get the model to almost always get these false signal examples wrong. Why? Because the model will have learned to treat column 2 as a \"star player\" of sorts. When column 2 provides a false signal, our model will fall for it and get the prediction wrong. \u003c/p\u003e\n\n\u003cp\u003eNow, let's assume that we have a random forest complete with subspace sampling. If we randomly use 6 out of 10 predictors when creating each node of each tree, then this means that ~40% of the nodes of the trees in our forest won't even know column 2 exists! In the cases where column 2 provides a \"false signal\", the nodes of trees that use column 2 will likely make an incorrect prediction -- but that only matters to the ~60% that look at column 2. Our forest still contains another 40% of nodes within trees that are essentially \"immune\" to the false signal in column 2, because they don't use that predictor. In this way, the \"wisdom of the crowd\" buffers the performance of every constituent in that crowd. Although for any given example, some trees may draw the wrong conclusion from a particular predictor, the odds that \u003cem\u003eevery tree\u003c/em\u003e makes the same mistake because they looked at the same predictor is infinitesimally small!\u003c/p\u003e\n\n\u003ch3\u003eMaking predictions with random forests\u003c/h3\u003e\n\n\u003cp\u003eOnce we have trained all the trees in our random forest, we can effectively use it to make predictions! When given data to make predictions on, the algorithm provides only the appropriate features to each tree in the forest, gets that tree's individual prediction, and then aggregates all predictions together to determine the overall prediction that the algorithm will make for said data. In essence, each tree \"votes\" for the prediction that the forest will make, with the majority winning. \u003c/p\u003e\n\n\u003ch2\u003eBenefits and drawbacks\u003c/h2\u003e\n\n\u003cp\u003eLike any algorithm, the random forest comes with its own benefits and drawbacks. \u003c/p\u003e\n\n\u003ch3\u003eBenefits\u003c/h3\u003e\n\n\u003cul\u003e\n\u003cli\u003e\u003cp\u003e\u003cstrong\u003e\u003cem\u003eStrong performance\u003c/em\u003e\u003c/strong\u003e: The random forest algorithm usually has very strong performance on most problems, when compared with other classification algorithms. Because this is an ensemble algorithm, the model is naturally resistant to noise and variance in the data, and generally tends to perform quite well. \u003c/p\u003e\u003c/li\u003e\n\u003cli\u003e\u003cp\u003e\u003cstrong\u003e\u003cem\u003eInterpretability\u003c/em\u003e\u003c/strong\u003e:  Conveniently, since each tree in the random forest is a \u003cstrong\u003e\u003cem\u003eGlass-Box Model\u003c/em\u003e\u003c/strong\u003e (meaning that the model is interpretable, allowing us to see how it arrived at a certain decision), the overall random forest is, as well! You'll demonstrate this yourself in the upcoming lab, by inspecting feature importances for both individual trees and the entire random forest. \u003c/p\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch3\u003eDrawbacks\u003c/h3\u003e\n\n\u003cul\u003e\n\u003cli\u003e\u003cp\u003e\u003cstrong\u003e\u003cem\u003eComputational complexity\u003c/em\u003e\u003c/strong\u003e: Like any ensemble method, training multiple models means paying the computational cost of training each model. On large datasets, the runtime can be quite slow compared to other algorithms.\u003c/p\u003e\u003c/li\u003e\n\u003cli\u003e\u003cp\u003e\u003cstrong\u003e\u003cem\u003eMemory usage\u003c/em\u003e\u003c/strong\u003e: Another side effect of the ensembled nature of this algorithm, having multiple models means storing each in memory. Random forests tend to have a larger memory footprint than other models. Whereas a parametric model like a logistic regression just needs to store each of the coefficients, a random forest has to remember every aspect of every tree! It's not uncommon to see random forests that were trained on large datasets have memory footprints in the tens or even hundreds of MB. For data scientists working on modern computers, this isn't typically a problem -- however, there are special cases where the memory footprint can make this an untenable choice -- for instance, an app on a smartphone that uses machine learning may not be able to afford to spend that much disk space on a random forest model!\u003c/p\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch2\u003e(Optional) Random forests White paper\u003c/h2\u003e\n\n\u003cp\u003eThis algorithm was not invented all at once -- there were several iterations by different researchers that built upon each previous idea. However, the version used today is the one created by Leo Breiman and Adele Cutler, who also own the trademark for the name \"random forest\". \u003c/p\u003e\n\n\u003cp\u003eAlthough not strictly necessary for understanding how to use random forests, we highly recommend taking a look at the following resources from Breiman and Cutler if you're interested in really digging into how random forests work:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003e\u003cp\u003e\u003ca href=\"https://www.stat.berkeley.edu/%7Ebreiman/randomforest2001.pdf\"\u003eRandom forests paper\u003c/a\u003e\u003c/p\u003e\u003c/li\u003e\n\u003cli\u003e\u003cp\u003e\u003ca href=\"https://www.stat.berkeley.edu/%7Ebreiman/RandomForests/cc_home.htm\"\u003eRandom forests website\u003c/a\u003e\u003c/p\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch2\u003eSummary\u003c/h2\u003e\n\n\u003cp\u003eIn this lesson, you learned about a random forest, which is a powerful and popular ensemble method that uses decision trees!\u003c/p\u003e\n\u003cfooter class=\"fis-footer\" style=\"visibility: hidden;\"\u003e\u003cdiv class=\"fis-feedback\"\u003e\n\u003ch5\u003eHow do you feel about this lesson?\u003c/h5\u003e\n\u003cimg id=\"thumbs-up\" data-repository=\"dsc-random-forests\" title=\"Thumbs up!\" alt=\"thumbs up\"\u003e\u003cimg id=\"thumbs-down\" data-repository=\"dsc-random-forests\" title=\"Thumbs down!\" alt=\"thumbs down\"\u003e\n\u003c/div\u003e\n\u003ch5\u003eHave specific feedback? \u003ca href=\"https://github.com/learn-co-curriculum/dsc-random-forests/issues/new\"\u003eTell us here!\u003c/a\u003e\n\u003c/h5\u003e\u003c/footer\u003e","exportId":"random-forests"},{"id":154117,"title":"Tree Ensembles and Random Forests - Lab","type":"Assignment","indent":1,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_mark_done","completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-tree-ensembles-random-forests-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-tree-ensembles-random-forests-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","exportId":"g386828ef7d8aebe42e3aa5618b551fe5"},{"id":154118,"title":"GridSearchCV","type":"WikiPage","indent":0,"locked":false,"requirement":"must_mark_done","completed":true,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-gridsearchcv\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-gridsearchcv/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eIn this lesson, we'll explore the concept of parameter tuning to maximize our model performance using a combinatorial grid search!\u003c/p\u003e\n\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\n\u003cp\u003eYou will be able to:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eDesign a parameter grid for use with scikit-learn's \u003ccode\u003eGridSearchCV\u003c/code\u003e\u003cbr\u003e\n\u003c/li\u003e\n\u003cli\u003eUse \u003ccode\u003eGridSearchCV\u003c/code\u003e to increase model performance through parameter tuning\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch2\u003eParameter tuning\u003c/h2\u003e\n\n\u003cp\u003eBy now, you've seen that the process of building and training a supervised learning model is an iterative one. Your first model rarely performs the best! There are multiple ways we can potentially improve model performance. Thus far, most of the techniques we've used have been focused on our data. We can get better data, or more data, or both. We can engineer certain features, or clean up the data by removing rows/variables that hurt model performance, like multicollinearity. \u003c/p\u003e\n\n\u003cp\u003eThe other major way to potentially improve model performance is to find good parameters to set when creating the model. For example, if we allow a decision tree to have too many leaves, the model will almost certainly overfit the data. Too few, and the model will underfit. However, each modeling problem is unique -- the same parameters could cause either of those situations, depending on the data, the task at hand, and the complexity of the model needed to best fit the data. \u003c/p\u003e\n\n\u003cp\u003eIn this lesson, we'll learn how we can use a \u003cstrong\u003e\u003cem\u003ecombinatorial grid search\u003c/em\u003e\u003c/strong\u003e to find the best combination of parameters for a given model. \u003c/p\u003e\n\n\u003ch2\u003eGrid search\u003c/h2\u003e\n\n\u003cp\u003eWhen we set parameters in a model, the parameters are not independent of one another -- the value set for one parameter can have significant effects on other parameters, thereby affecting overall model performance. Consider the following grid.\u003c/p\u003e\n\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth style=\"text-align: center;\"\u003eParameter\u003c/th\u003e\n\u003cth style=\"text-align: center;\"\u003e1\u003c/th\u003e\n\u003cth style=\"text-align: center;\"\u003e2\u003c/th\u003e\n\u003cth style=\"text-align: center;\"\u003e3\u003c/th\u003e\n\u003cth style=\"text-align: center;\"\u003e4\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd style=\"text-align: center;\"\u003ecriterion\u003c/td\u003e\n\u003ctd style=\"text-align: center;\"\u003e\"gini\"\u003c/td\u003e\n\u003ctd style=\"text-align: center;\"\u003e\"entropy\"\u003c/td\u003e\n\u003ctd style=\"text-align: center;\"\u003e\u003c/td\u003e\n\u003ctd style=\"text-align: center;\"\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd style=\"text-align: center;\"\u003emax_depth\u003c/td\u003e\n\u003ctd style=\"text-align: center;\"\u003e1\u003c/td\u003e\n\u003ctd style=\"text-align: center;\"\u003e2\u003c/td\u003e\n\u003ctd style=\"text-align: center;\"\u003e5\u003c/td\u003e\n\u003ctd style=\"text-align: center;\"\u003e10\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd style=\"text-align: center;\"\u003emin\u003cem\u003esamples\u003c/em\u003esplit\u003c/td\u003e\n\u003ctd style=\"text-align: center;\"\u003e1\u003c/td\u003e\n\u003ctd style=\"text-align: center;\"\u003e5\u003c/td\u003e\n\u003ctd style=\"text-align: center;\"\u003e10\u003c/td\u003e\n\u003ctd style=\"text-align: center;\"\u003e20\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\n\u003cp\u003eAll the parameters above work together to create the framework of the decision tree that will be trained. For a given problem, it may be the case that increasing the value of the parameter for \u003ccode\u003emin_samples_split\u003c/code\u003e generally improves model performance up to a certain point, by reducing overfitting. However, if the value for \u003ccode\u003emax_depth\u003c/code\u003e is too low or too high, this may doom the model to overfitting or underfitting, by having a tree with too many arbitrary levels and splits that overfit on noise, or limiting the model to nothing more than a \"stump\" by only allowing it to grow to one or two levels. \u003c/p\u003e\n\n\u003cp\u003eSo how do we know which combination of parameters is best? The only way we can really know for sure is to try \u003cstrong\u003e\u003cem\u003eevery single combination!\u003c/em\u003e\u003c/strong\u003e For this reason, grid search is sometimes referred to as an \u003cstrong\u003e\u003cem\u003eexhaustive search\u003c/em\u003e\u003c/strong\u003e. \u003c/p\u003e\n\n\u003ch2\u003eUse \u003ccode\u003eGridSearchCV\u003c/code\u003e\n\u003c/h2\u003e\n\n\u003cp\u003eThe \u003ccode\u003esklearn\u003c/code\u003e library provides an easy way to tune model parameters through an exhaustive search by using its \u003ca href=\"https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html\"\u003e\u003ccode\u003eGridSearchCV\u003c/code\u003e\u003c/a\u003e class, which can be found inside the \u003ccode\u003emodel_selection\u003c/code\u003e module. \u003ccode\u003eGridsearchCV\u003c/code\u003e combines \u003cstrong\u003e\u003cem\u003eK-Fold Cross-Validation\u003c/em\u003e\u003c/strong\u003e with a grid search of parameters. In order to do this, we must first create a \u003cstrong\u003e\u003cem\u003eparameter grid\u003c/em\u003e\u003c/strong\u003e that tells \u003ccode\u003esklearn\u003c/code\u003e which parameters to tune, and which values to try for each of those parameters. \u003c/p\u003e\n\n\u003cp\u003eThe following code snippet demonstrates how to use \u003ccode\u003eGridSearchCV\u003c/code\u003e to perform a parameter grid search using a sample parameter grid, \u003ccode\u003eparam_grid\u003c/code\u003e. Our parameter grid should be a dictionary, where the keys are the parameter names, and the values are the different parameter values we want to use in our grid search for each given key. After creating the dictionary, all you need to do is pass it to \u003ccode\u003eGridSearchCV()\u003c/code\u003e along with the classifier. YOu can also use K-fold cross-validation during this process, by specifying the \u003ccode\u003ecv\u003c/code\u003e parameter. In this case, we choose to use 3-fold cross-validation for each model created inside our grid search. \u003c/p\u003e\n\n\u003cpre\u003e\u003ccode class=\"python\"\u003e\nclf = DecisionTreeClassifier()\n\nparam_grid = {\n    'criterion': ['gini', 'entropy'],\n    'max_depth': [1, 2, 5, 10],\n    'min_samples_split': [1, 5, 10, 20]\n}\n\ngs_tree = GridSearchCV(clf, param_grid, cv=3)\ngs_tree.fit(train_data, train_labels)\n\ngs_tree.best_params_\n\u003c/code\u003e\u003c/pre\u003e\n\n\u003cp\u003eThis code will run all combinations of the parameters above. The first model to be trained would be \u003ccode\u003eDecisionTreeClassifier(criterion='gini', max_depth=1, min_samples_split=1)\u003c/code\u003e using a 3-fold cross-validation, and recording the average score. Then, it will change one parameter, and repeat the process (e.g., \u003ccode\u003eDecisionTreeClassifier(criterion='gini', max_depth=1, min_samples_split=5)\u003c/code\u003e, and so on), keeping track of the overall performance of each model. Once it has tried every combination, the \u003ccode\u003eGridSearchCV\u003c/code\u003e object we created will automatically default the model that had the best score. We can even access the best combination of parameters by checking the \u003ccode\u003ebest_params_\u003c/code\u003e attribute! \u003c/p\u003e\n\n\u003ch2\u003eDrawbacks of \u003ccode\u003eGridSearchCV\u003c/code\u003e\n\u003c/h2\u003e\n\n\u003cp\u003eGridSearchCV is a great tool for finding the best combination of parameters. However, it is only as good as the parameters we put in our parameter grid -- so we need to be very thoughtful during this step! \u003c/p\u003e\n\n\u003cp\u003eThe main drawback of an exhaustive search such as \u003ccode\u003eGridsearchCV\u003c/code\u003e is that there is no way of telling what's best until we've exhausted all possibilities! This means training many versions of the same machine learning model, which can be very time consuming and computationally expensive. Consider the example code above -- we have three different parameters, with 2, 4, and 4 variations to try, respectively. We also set the model to use cross-validation with a value of 3, meaning that each model will be built 3 times, and their performances averaged together. If we do some simple math, we can see that this simple grid search we see above actually results in \u003ccode\u003e2 * 4 * 4 * 3 =\u003c/code\u003e \u003cstrong\u003e\u003cem\u003e96 different models trained!\u003c/em\u003e\u003c/strong\u003e For projects that involve complex models and/or very large datasets, the time needed to run a grid search can often be prohibitive. For this reason, be very thoughtful about the parameters you set -- sometimes the extra runtime isn't worth it -- especially when there's no guarantee that the model performance will improve!\u003c/p\u003e\n\n\u003ch2\u003eSummary\u003c/h2\u003e\n\n\u003cp\u003eIn this lesson, you learned about grid search, how to perform grid search, and the drawbacks associated with the method!\u003c/p\u003e\n\u003cfooter class=\"fis-footer\" style=\"visibility: hidden;\"\u003e\u003cdiv class=\"fis-feedback\"\u003e\n\u003ch5\u003eHow do you feel about this lesson?\u003c/h5\u003e\n\u003cimg id=\"thumbs-up\" data-repository=\"dsc-gridsearchcv\" title=\"Thumbs up!\" alt=\"thumbs up\"\u003e\u003cimg id=\"thumbs-down\" data-repository=\"dsc-gridsearchcv\" title=\"Thumbs down!\" alt=\"thumbs down\"\u003e\n\u003c/div\u003e\n\u003ch5\u003eHave specific feedback? \u003ca href=\"https://github.com/learn-co-curriculum/dsc-gridsearchcv/issues/new\"\u003eTell us here!\u003c/a\u003e\n\u003c/h5\u003e\u003c/footer\u003e","exportId":"gridsearchcv"},{"id":154119,"title":"GridSearchCV - Lab","type":"Assignment","indent":1,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_mark_done","completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-gridsearchcv-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-gridsearchcv-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","exportId":"g99518519ccfe706ed4e9a8718be7ff1a"},{"id":154120,"title":"Gradient Boosting and Weak Learners","type":"WikiPage","indent":0,"locked":false,"requirement":"must_mark_done","completed":false,"content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-gradient-boosting-and-weak-learners\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-gradient-boosting-and-weak-learners\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-gradient-boosting-and-weak-learners/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eIn this lesson, we'll explore one of the most powerful ensemble methods around -- gradient boosting!\u003c/p\u003e\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\u003cp\u003eYou will be able to:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eCompare and contrast weak and strong learners and explain the role of weak learners in boosting algorithms\u003c/li\u003e\n\u003cli\u003eDescribe the process of boosting in Adaboost and Gradient Boosting\u003c/li\u003e\n\u003cli\u003eExplain the concept of a learning rate and the role it plays in gradient boosting algorithms\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003eWeak learners and boosting\u003c/h2\u003e\n\u003cp\u003eThe first ensemble technique we learned about was \u003cstrong\u003e\u003cem\u003eBagging\u003c/em\u003e\u003c/strong\u003e, which refers to training different models independently on different subsets of data by sampling with replacement. The goal of bagging is to create variability in the ensemble of models. The next ensemble technique we'll learn about is \u003cstrong\u003e\u003cem\u003eBoosting\u003c/em\u003e\u003c/strong\u003e. This technique is at the heart of some very powerful, top-of-class ensemble methods currently used in machine learning, such as \u003cstrong\u003e\u003cem\u003eAdaboost\u003c/em\u003e\u003c/strong\u003e and \u003cstrong\u003e\u003cem\u003eGradient Boosted Trees\u003c/em\u003e\u003c/strong\u003e.\u003c/p\u003e\n\u003cp\u003eIn order to understand boosting, let's first examine the cornerstone of boosting algorithms -- \u003cstrong\u003e\u003cem\u003eWeak Learners\u003c/em\u003e\u003c/strong\u003e.\u003c/p\u003e\n\u003ch3\u003eWeak learners\u003c/h3\u003e\n\u003cp\u003eAll the models we've learned so far are \u003cstrong\u003e\u003cem\u003eStrong Learners\u003c/em\u003e\u003c/strong\u003e -- models with the goal of doing as well as possible on the classification or regression task they are given. The term \u003cstrong\u003e\u003cem\u003eWeak Learner\u003c/em\u003e\u003c/strong\u003e refers to simple models that do only slightly better than random chance. Boosting algorithms start with a single weak learner (tree methods are overwhelmingly used here), but technically, any model will do. Boosting works as follows:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eTrain a single weak learner\u003c/li\u003e\n\u003cli\u003eFigure out which examples the weak learner got wrong\u003c/li\u003e\n\u003cli\u003eBuild another weak learner that focuses on the areas the first weak learner got wrong\u003c/li\u003e\n\u003cli\u003eContinue this process until a predetermined stopping condition is met, such as until a set number of weak learners have been created, or the model's performance has plateaued\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eIn this way, each new weak learner is specifically tuned to focus on the weak points of the previous weak learner(s). The more often an example is missed, the more likely it is that the next weak learner will be the one that can classify that example correctly. In this way, all the weak learners work together to make up a single strong learner.\u003c/p\u003e\n\u003ch2\u003eBoosting and random forests\u003c/h2\u003e\n\u003ch3\u003eSimilarities\u003c/h3\u003e\n\u003cp\u003eBoosting algorithms share some similarities with random forests, as well as some notable differences. Like random forests, boosting algorithms are an ensemble of many different models with high inter-group diversity. Boosting algorithms also aggregate the predictions of each constituent model into an overall prediction. Both algorithms also make use of tree models (although this isn't strictly required, in the case of boosting).\u003c/p\u003e\n\u003ch3\u003eDifferences\u003c/h3\u003e\n\u003ch4\u003e1: Independent vs. iterative\u003c/h4\u003e\n\u003cp\u003eThe difference is in the approach to training the trees. Whereas a random forest trains each tree independently and at the same time, boosting trains each tree iteratively. In a random forest model, how well or poorly a given tree does has no effect on any of the other trees since they are all trained at the same time. Boosting, on the other hand, trains trees one at a time, identifies the weak points for those trees, and then purposefully creates the next round of trees in such a way as to specialize in those weak points.\u003c/p\u003e\n\u003ch4\u003e2: Weak vs. strong\u003c/h4\u003e\n\u003cp\u003eAnother major difference between random forests and boosting algorithms is the overall size of the trees. In a random forest, each tree is a strong learner -- they would do just fine as a decision tree on their own. In boosting algorithms, trees are artificially limited to a very shallow depth (usually only 1 split), to ensure that each model is only slightly better than random chance. For this reason, boosting algorithms are also highly resilient against noisy data and overfitting. Since the individual weak learners are too simple to overfit, it is very hard to combine them in such a way as to overfit the training data as a whole -- especially when they focus on different things, due to the iterative nature of the algorithm.\u003c/p\u003e\n\u003ch4\u003e3: Aggregate predictions\u003c/h4\u003e\n\u003cp\u003eThe final major difference we'll talk about between the two is the way predictions are aggregated. Whereas in a random forest, each tree simply votes for the final result, boosting algorithms usually employ a system of weights to determine how important the input for each tree is. Since we know how well each weak learner performs on the dataset by calculating its performance at each step, we can see which weak learners do better on hard tasks. Think of it like this -- harder problems deserve more weight. If there are many learners in the overall ensemble that can get the same questions right, then that tree isn't super important -- other trees already provide the same value that it does. This tree will have its overall weight reduced. As more and more trees get a hard problem wrong, the \"reward\" for a tree getting that hard problem correct goes higher and higher. This \"reward\" is actually just a higher weight when calculating the overall vote. Intuitively, this makes sense -- trees that can do what few other trees can do are the ones that we should probably listen to more than others, as they are the most likely to get hard examples correct. Since other trees tend to get this wrong, we can expect to see a general split of about 50/50 among the trees that do not \"specialize\" in the hard problems. Since our \"specialized\" tree has more weight, its correct vote will carry more weight than the combined votes of the half of the \"unspecialized\" trees that get it wrong. It is worth noting that the \"specialized\" trees will often do quite poorly on the examples that are easy to predict. However, since these examples are easier, we can expect a strong majority of the trees in our ensemble to get it right, meaning that the combined, collective weight of their agreement will be enough to overrule the trees with higher weights that get it wrong.\u003c/p\u003e\n\u003ch2\u003eUnderstanding Adaboost and Gradient boosting\u003c/h2\u003e\n\u003cp\u003eThere are two main algorithms that come to mind when Data Scientists talk about boosting: \u003cstrong\u003e\u003cem\u003eAdaboost\u003c/em\u003e\u003c/strong\u003e (short for Adaptive Boosting), and \u003cstrong\u003e\u003cem\u003eGradient Boosted Trees\u003c/em\u003e\u003c/strong\u003e. Both are generally very effective, but they use different methods to achieve their results.\u003c/p\u003e\n\u003ch3\u003eAdaboost\u003c/h3\u003e\n\u003cp\u003eAdaboost was the first boosting algorithm invented. Although there have been marked improvements made to this algorithm, Adaboost still tends to be quite an effective algorithm! More importantly, it's a good starting place for understanding how boosting algorithms actually work.\u003c/p\u003e\n\u003cp\u003eIn Adaboost, each learner is trained on a subsample of the dataset, much like we saw with \u003cstrong\u003e\u003cem\u003eBagging\u003c/em\u003e\u003c/strong\u003e. Initially, the bag is randomly sampled with replacement. However, each data point in the dataset has a weight assigned. As learners correctly classify an example, that example's weight is reduced. Conversely, when learners get an example wrong, the weight for that sample increases. In each iteration, these weights act as the probability that an item will be sampled into the \"bag\" which will be used to train the next weak learner. As the number of learners grows, you can imagine that the examples that are easy to get correct will become less and less prevalent in the samples used to train each new learner. This is a good thing -- if our ensemble already contains multiple learners that can correctly classify that example, then we don't need more that can do this. Instead, the \"bags\" of data will contain multiple instances of the hard examples, thereby increasing the likelihood that the learner will create a split that focuses on getting the hard example correct.\u003c/p\u003e\n\u003cp\u003eThe following diagram demonstrates how the weights change for each example as classifiers get them right and wrong.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-gradient-boosting-and-weak-learners/master/images/new_adaboost.png\" width=\"600\"\u003e\u003c/p\u003e\n\u003cp\u003ePay attention to the colors of the pluses and minuses -- pluses are meant to be in the blue section, and minuses are meant to be in the red. The decision boundary of the tree can be interpreted as the line drawn between the red and blue sections. As you can see above, examples that were misclassified are larger in the next iteration, while examples that were classified correctly are smaller. As we combine the decision boundaries of each new classifier, we end up with a classifier that correctly classifies all of the examples!\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e\u003cem\u003eKey Takeaway:\u003c/em\u003e\u003c/strong\u003e Adaboost creates new classifiers by continually influencing the distribution of the data sampled to train each successive learner.\u003c/p\u003e\n\u003ch3\u003eGradient boosting\u003c/h3\u003e\n\u003cp\u003e\u003cstrong\u003e\u003cem\u003eGradient Boosted Trees\u003c/em\u003e\u003c/strong\u003e are a more advanced boosting algorithm that makes use of \u003cstrong\u003e\u003cem\u003eGradient Descent.\u003c/em\u003e\u003c/strong\u003e Much like Adaboost, gradient boosting starts with a weak learner that makes predictions on the dataset. The algorithm then checks this learner's performance, identifying examples that it got right and wrong. However, this is where the gradient boosting algorithm diverges from Adaboost's methodology. The model then calculates the \u003cstrong\u003e\u003cem\u003eResiduals\u003c/em\u003e\u003c/strong\u003e for each data point, to determine how far off the mark each prediction was. The model then combines these residuals with a \u003cstrong\u003e\u003cem\u003eLoss Function\u003c/em\u003e\u003c/strong\u003e to calculate the overall loss. There are many loss functions that are used -- the thing that matters most is that the loss function is \u003cstrong\u003e\u003cem\u003edifferentiable\u003c/em\u003e\u003c/strong\u003e so that we can use calculus to compute the gradient for the loss, given the inputs of the model. We then use the gradients and the loss as predictors to train the next tree against! In this way, we can use \u003cstrong\u003e\u003cem\u003eGradient Descent\u003c/em\u003e\u003c/strong\u003e to minimize the overall loss.\u003c/p\u003e\n\u003cp\u003eSince the loss is most heavily inflated by examples where the model was wrong, gradient descent will push the algorithm towards creating a new learner that will focus on these harder examples. If the next tree gets these right, then the loss goes down! In this way, gradient descent allows us to continually train and improve on the loss for each model to improve the overall performance of the ensemble as a whole by focusing on the \"hard\" examples that cause the loss to be high.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-gradient-boosting-and-weak-learners/master/images/new_gradient-boosting.png\"\u003e\u003c/p\u003e\n\u003ch3\u003eLearning rates\u003c/h3\u003e\n\u003cp\u003eOften, we want to artificially limit the \"step size\" we take in gradient descent. Small, controlled changes in the parameters we're optimizing with gradient descent will mean that the overall process is slower, but the parameters are more likely to converge to their optimal values. The learning rate for your model is a small scalar meant to artificially reduce the step size in gradient descent. Learning rate is a tunable parameter for your model that you can set -- large learning rates get closer to the optimal values more quickly, but have trouble landing exactly at the optimal values because the step size is too big for the small distances it needs to travel when it gets close. Conversely, small learning rates means the model will take a longer time to get to the optimal parameters, but when it does get there, it will be extremely close to the optimal values, thereby providing the best overall performance for the model.\u003c/p\u003e\n\u003cp\u003eYou'll often see learning rates denoted by the symbol, \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cgamma\"\u003e -- this is the greek letter, \u003cstrong\u003e\u003cem\u003egamma\u003c/em\u003e\u003c/strong\u003e. Don't worry if you're still hazy on the concept of gradient descent -- we'll explore it in much more detail when we start studying deep learning!\u003c/p\u003e\n\u003cp\u003eThe \u003ccode\u003esklearn\u003c/code\u003e library contains some excellent implementations of Adaboost, as well as several different types of gradient boosting classifiers. These classifiers can be found in the \u003ccode\u003eensemble\u003c/code\u003e module, which you will make use of in the upcoming lesson.\u003c/p\u003e\n\u003ch2\u003eSummary\u003c/h2\u003e\n\u003cp\u003eIn this lesson, we learned about \u003cstrong\u003e\u003cem\u003eWeak Learners\u003c/em\u003e\u003c/strong\u003e, and how they are used in various \u003cstrong\u003e\u003cem\u003eGradient Boosting\u003c/em\u003e\u003c/strong\u003e algorithms. We also learned about two specific algorithms -- \u003cstrong\u003e\u003cem\u003eAdaBoost\u003c/em\u003e\u003c/strong\u003e and \u003cstrong\u003e\u003cem\u003eGradient Boosted Trees\u003c/em\u003e\u003c/strong\u003e, and we compared how they are similar and how they are different!\u003c/p\u003e","exportId":"gradient-boosting-and-weak-learners"},{"id":154121,"title":"Gradient Boosting - Lab","type":"Assignment","indent":1,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_mark_done","completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-gradient-boosting-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-gradient-boosting-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","exportId":"g402c8fa1483e3cb383bc414c7ed728b1"},{"id":154122,"title":"XGBoost","type":"WikiPage","indent":0,"locked":false,"requirement":"must_mark_done","completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-xgboost\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-xgboost/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eNow that you are familiar with gradient boosting, you'll learn about the top gradient boosting algorithm currently in use -- XGBoost!\u003c/p\u003e\n\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\n\u003cp\u003eYou will be able to:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eCompare XGBoost to other boosting algorithms \u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch2\u003eWhat is XGBoost?\u003c/h2\u003e\n\n\u003cp\u003eGradient boosting is one of the most powerful concepts in machine learning right now. As you've seen, the term \u003cem\u003egradient boosting\u003c/em\u003e refers to a class of algorithms, rather than any single one. The version with the highest performance right now is \u003cstrong\u003e\u003cem\u003eXGBoost\u003c/em\u003e\u003c/strong\u003e, which is short for \u003cstrong\u003e\u003cem\u003eeXtreme Gradient Boosting\u003c/em\u003e\u003c/strong\u003e. \u003c/p\u003e\n\n\u003cp\u003e\u003ccode\u003eXGBoost\u003c/code\u003e is a stand-alone library that implements popular gradient boosting algorithms in the fastest, most performant way possible. There are many under-the-hood optimizations that allow XGBoost to train more quickly than any other library implementations of gradient boosting algorithms. For instance, XGBoost is configured in such a way that it parallelizes the construction of trees across all your computer's CPU cores during the training phase. It also allows for more advanced use cases, such as distributing training across a cluster of computers, which is often a technique used to speed up computation. The algorithm even automatically handles missing values!\u003c/p\u003e\n\n\u003ch2\u003eInstalling \u003ccode\u003eXGBoost\u003c/code\u003e\n\u003c/h2\u003e\n\n\u003cp\u003e\u003ccode\u003eXGBoost\u003c/code\u003e is an independent library that provides implementations in C++, Python, and other languages. Luckily, the open-source community has had the good sense to make the Python API for \u003ccode\u003eXGBoost\u003c/code\u003e mirror that of \u003ccode\u003esklearn\u003c/code\u003e, so using \u003ccode\u003eXGBoost\u003c/code\u003e feels no different than using any other supervised learning algorithm from \u003ccode\u003esklearn\u003c/code\u003e. The only downside is that it does not come packaged with \u003ccode\u003esklearn\u003c/code\u003e, so we must install it ourselves. \u003cstrong\u003econda\u003c/strong\u003e makes this quite easy. \u003c/p\u003e\n\n\u003cp\u003eAll you need to do is run the command \u003ccode\u003econda install py-xgboost\u003c/code\u003e in your terminal, and \u003ccode\u003econda\u003c/code\u003e will take care of the rest!\u003c/p\u003e\n\n\u003ch2\u003eUse cases\u003c/h2\u003e\n\n\u003cp\u003eXGBoost has risen to prominence by being the go-to algorithm for winning competitions on \u003ca href=\"https://www.kaggle.com/\"\u003eKaggle\u003c/a\u003e, a competitive data science platform. It is so common to see XGBoost cited as an algorithm used by the winners of Kaggle competitions that it has become a bit of a running joke in the community. \u003ca href=\"https://github.com/dmlc/xgboost/tree/master/demo#machine-learning-challenge-winning-solutions\"\u003eThis page\u003c/a\u003e contains an (incomplete) list of all the recent competitions with place winners that used XGBoost for their solution!\u003c/p\u003e\n\n\u003cp\u003eXGBoost is a great choice for classification tasks. It provides best-in-class performance compared to other classification algorithms (with the exception of Deep Learning, which we'll learn more about soon).\u003c/p\u003e\n\n\u003ch2\u003eTakeaways\u003c/h2\u003e\n\n\u003cp\u003eWhen approaching a supervised learning problem, you should always use multiple algorithms, and compare the performances of the various models. There will always be use cases where some classes of models tend to outperform others. However, there are some models that generally outperform all the others -- XGBoost is at the top of this list! Make sure that this is an algorithm you're familiar with, as there are many situations where you'll find it quite useful!\u003c/p\u003e\n\n\u003cp\u003eYou can find the full documentation for XGBoost \u003ca href=\"https://xgboost.readthedocs.io/en/latest/\"\u003ehere\u003c/a\u003e. \u003c/p\u003e\n\n\u003ch2\u003eSummary\u003c/h2\u003e\n\n\u003cp\u003eIn this lesson, we learned about what XGBoost is, and why it is so powerful and useful to Data Scientists!\u003c/p\u003e\n\u003cfooter class=\"fis-footer\" style=\"visibility: hidden;\"\u003e\u003cdiv class=\"fis-feedback\"\u003e\n\u003ch5\u003eHow do you feel about this lesson?\u003c/h5\u003e\n\u003cimg id=\"thumbs-up\" data-repository=\"dsc-xgboost\" title=\"Thumbs up!\" alt=\"thumbs up\"\u003e\u003cimg id=\"thumbs-down\" data-repository=\"dsc-xgboost\" title=\"Thumbs down!\" alt=\"thumbs down\"\u003e\n\u003c/div\u003e\n\u003ch5\u003eHave specific feedback? \u003ca href=\"https://github.com/learn-co-curriculum/dsc-xgboost/issues/new\"\u003eTell us here!\u003c/a\u003e\n\u003c/h5\u003e\u003c/footer\u003e","exportId":"xgboost"},{"id":154123,"title":"XGBoost - Lab","type":"Assignment","indent":1,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_mark_done","completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-xgboost-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-xgboost-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","exportId":"gdbf5e9577a46c704d4f83f451f46a921"},{"id":154124,"title":"Ensembles - Recap","type":"WikiPage","indent":0,"locked":false,"requirement":"must_mark_done","completed":false,"content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-ensemble-methods-section-recap\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-ensemble-methods-section-recap\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-ensemble-methods-section-recap/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\u003ch2\u003eKey Takeaways\u003c/h2\u003e\n\u003cp\u003eThe key takeaways from this section include:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eMultiple independent estimates are consistently more accurate than any single estimate, so ensemble techniques are a powerful way for improving the quality of your models\u003c/li\u003e\n\u003cli\u003eSometimes you'll use model stacking or meta-ensembles where you use a combination of different types of models for your ensemble\u003c/li\u003e\n\u003cli\u003eIt's also common to have multiple similar models in an ensemble - e.g. a bunch of decision trees\u003c/li\u003e\n\u003cli\u003eBagging (Bootstrap AGGregation) is a technique that leverages Bootstrap Resampling and Aggregation\u003c/li\u003e\n\u003cli\u003eBootstrap resampling uses multiple smaller samples from the test dataset to create independent estimates, and aggregate these estimates to make predictions\u003c/li\u003e\n\u003cli\u003eA random forest is an ensemble method for decision trees using Bagging and the Subspace Sampling method to create variance among the trees\u003c/li\u003e\n\u003cli\u003eWith a random forest, for each tree, we sample two-thirds of the training data and the remaining third is used to calculate the out-of-bag error\u003c/li\u003e\n\u003cli\u003eIn addition, the Subspace Sampling method is used to further increase variability by randomly selecting the subset of features to use as predictors for training any given tree\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003eGridsearchCV\u003c/code\u003e is an exhaustive search technique for finding optimal combinations of hyperparameters\u003c/li\u003e\n\u003cli\u003eBoosting leverages an ensemble of weak learners (weak models) to create a strong combined model\u003c/li\u003e\n\u003cli\u003eBoosting (when compared to random forests) is an iterative rather than independent process, using each iteration to strengthen the weaknesses of the previous iterations\u003c/li\u003e\n\u003cli\u003eTwo of the most common algorithms for Boosting are Adaboost (Adaptive Boosting) and Gradient Boosted Trees\u003c/li\u003e\n\u003cli\u003eAdaboost creates new classifiers by continually influencing the distribution of the data sampled to train each successive tree\u003c/li\u003e\n\u003cli\u003eGradient Boosting is a more advanced boosting algorithm that makes use of Gradient Descent\u003c/li\u003e\n\u003cli\u003eXGBoost (eXtreme Gradient Boosting) is one of the top gradient boosting algorithms currently in use\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003eXGBoost\u003c/code\u003e is a stand-alone library that implements popular gradient boosting algorithms in the fastest, most performant way possible\u003c/li\u003e\n\u003c/ul\u003e","exportId":"ensembles-recap"}]},{"id":16482,"name":"Topic 31: Support Vector Machines","status":"started","unlockDate":null,"prereqs":[],"requirement":"all","sequential":false,"exportId":"g518a55fe5599d7dff4047b35234b114b","items":[{"id":154125,"title":"Support Vector Machines - Introduction","type":"WikiPage","indent":0,"locked":false,"requirement":"must_mark_done","completed":true,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-svm-intro\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-svm-intro/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eA Support Vector Machine (SVM) is a type of classifier which modifies the loss function for optimization to not only take into account overall accuracy metrics of the resulting predictions, but also to maximize the decision boundary between the data points. In essence, this further helps tune the classifier as a good balance between underfitting and overfitting.\u003c/p\u003e\n\n\u003ch2\u003eSupport Vector Machines\u003c/h2\u003e\n\n\u003cp\u003eIn addition to optimizing for accuracy, support vector machines add a slack component, trading in accuracy to increase the distance between data points and the decision boundary. This provides an interesting perspective that can help formalize the intuitive visual choices a human would make in balancing precision and generalization to strike a balance between overfitting and underfitting.\u003c/p\u003e\n\n\u003ch2\u003eKernel Functions\u003c/h2\u003e\n\n\u003cp\u003eInitially, you'll explore linear support vector machines that divide data points into their respective groups by drawing hyperplanes using the dimensions from the feature space. In practice, these have limitations and the dataset may not be cleanly separable. As a result, kernel functions are an additional tool that can be used. Essentially, kernels reproject data onto a new parameter space using combinations of existing features. From there, the same process of applying SVMs to this transformed space can then be employed.\u003c/p\u003e\n\n\u003ch2\u003eSummary\u003c/h2\u003e\n\n\u003cp\u003eSupport Vector Machines are a powerful algorithm and may have the top performance among the out of the box classifiers from scikit-learn. Moreover, learning to properly tune SVMs is critical. In the upcoming labs and lessons, you'll investigate and apply these concepts.\u003c/p\u003e\n\u003cfooter class=\"fis-footer\" style=\"visibility: hidden;\"\u003e\u003cdiv class=\"fis-feedback\"\u003e\n\u003ch5\u003eHow do you feel about this lesson?\u003c/h5\u003e\n\u003cimg id=\"thumbs-up\" data-repository=\"dsc-svm-intro\" title=\"Thumbs up!\" alt=\"thumbs up\"\u003e\u003cimg id=\"thumbs-down\" data-repository=\"dsc-svm-intro\" title=\"Thumbs down!\" alt=\"thumbs down\"\u003e\n\u003c/div\u003e\n\u003ch5\u003eHave specific feedback? \u003ca href=\"https://github.com/learn-co-curriculum/dsc-svm-intro/issues/new\"\u003eTell us here!\u003c/a\u003e\n\u003c/h5\u003e\u003c/footer\u003e","exportId":"support-vector-machines-introduction"},{"id":154126,"title":"Introduction to Support Vector Machines","type":"WikiPage","indent":0,"locked":false,"requirement":"must_mark_done","completed":false,"content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-introduction-to-support-vector-machines\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-introduction-to-support-vector-machines\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-introduction-to-support-vector-machines/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eBy now you've learned a few techniques for classification. You touched upon it when talking about Naive Bayes, and again when you saw some supervised learning techniques such as logistic regression and decision trees. Now it's time for another popular classification technique -- Support Vector Machines.\u003c/p\u003e\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\u003cp\u003eYou will be able to:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eDescribe what is meant by margin classifiers\u003c/li\u003e\n\u003cli\u003eDescribe the mathematical components underlying soft and max-margin classifiers\u003c/li\u003e\n\u003cli\u003eCompare and contrast max-margin classifiers and soft-margin classifiers\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003eThe idea\u003c/h2\u003e\n\u003cp\u003eThe idea behind Support Vector Machines (also referred to as SVMs) is that you perform classification by finding the separation line or (in higher dimensions) \"hyperplane\" that maximizes the distance between two classes. Taking a look at the concept visually helps make sense of the process.\u003c/p\u003e\n\u003cp\u003eImagine you have a dataset containing two classes:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-introduction-to-support-vector-machines/master/images/new_SVM_1.png\" width=\"400\"\u003e\u003c/p\u003e\n\u003cp\u003eIn SVM, you want to find a hyperplane or \"decision boundary\" that divides one class from the other. Which one works best?\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-introduction-to-support-vector-machines/master/images/new_SVM_3.png\" width=\"400\"\u003e\u003c/p\u003e\n\u003cp\u003eThis would be a good line:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-introduction-to-support-vector-machines/master/images/new_SVM_2.png\" width=\"400\"\u003e\u003c/p\u003e\n\u003cp\u003eWhile this seems intuitive, there are other decision boundaries which also separate the classes. Which one is best? Rather than solely focus on the final accuracy of the model, Support Vector Machines aim to \u003cstrong\u003emaximize the margin\u003c/strong\u003e between the decision boundary and the various data points.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-introduction-to-support-vector-machines/master/images/new_SVM_4.png\" width=\"400\"\u003e\u003c/p\u003e\n\u003cp\u003eThe margin is defined as the distance between the separating line (hyperplane) and the training set cases that are closest to this hyperplane. These cases define \"support vectors\". The support vectors in this particular case are highlighted in the image below. As you can see, the max-margin hyperplane is the midpoint between the two lines defined by the support vectors.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-introduction-to-support-vector-machines/master/images/new_SVM_fin.png\" width=\"400\"\u003e\u003c/p\u003e\n\u003ch2\u003eThe Max Margin classifier\u003c/h2\u003e\n\u003cp\u003eWhy would you bother maximizing the margins? Don't these other hyperplanes discriminate just as well? Remember that you are fitting the hyperplane on your training data. Imagine you start looking at your test data, which will slightly differ from your training data.\u003c/p\u003e\n\u003cp\u003eAssuming your test set is big enough and randomly drawn from your entire dataset, you might end up with a test case as shown in the image below. This test case diverts a little bit from the training set cases observed earlier. While the max-margin classifier would classify this test set case correctly, the hyperplane closer to the right would have been classified this case incorrectly. Of course, this is just one example and other test cases will end up in different spots. Nonetheless, the purpose of choosing the max-margin classifier is to minimize the generalization error when applying the model to future unseen data points.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-introduction-to-support-vector-machines/master/images/new_SVM_test2.png\" width=\"400\"\u003e\u003c/p\u003e\n\u003cp\u003eBefore diving into the underlying mathematics, take a look at the image again:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-introduction-to-support-vector-machines/master/images/new_SVM_fin.png\" width=\"400\"\u003e\u003c/p\u003e\n\u003cp\u003eNow you can start exploring the mathematics behind the image. First, define some numeric labels for the two classes. Set the circles to be -1 and the diamonds to be 1. Normally, 0 and 1 are used for class labels but in this particular case using -1 and 1 simplifies the mathematics.\u003c/p\u003e\n\u003cp\u003eNow some terminology: The lines defined by the support vectors are the negative (to the left) and the positive (to the right) hyperplanes, respectively. These hyperplanes are defined by two terms: \u003cimg src=\"https://render.githubusercontent.com/render/math?math=w_T\"\u003e and \u003cimg src=\"https://render.githubusercontent.com/render/math?math=b\"\u003e .\u003c/p\u003e\n\u003cp\u003eThe \u003cimg src=\"https://render.githubusercontent.com/render/math?math=w_T\"\u003e term is called the \u003cstrong\u003eweight vector\u003c/strong\u003e and contains the weights that are used in the classification.\u003c/p\u003e\n\u003cp\u003eThe \u003cimg src=\"https://render.githubusercontent.com/render/math?math=b\"\u003e term is called the \u003cstrong\u003ebias\u003c/strong\u003e and functions as an offset term. If there were no bias term, the hyperplane would always go through the origin which would not be very generalizable!\u003c/p\u003e\n\u003cp\u003eThe equation describing the positive hyperplane is: \u003cimg src=\"https://render.githubusercontent.com/render/math?math=b%20%2b%20w_Tx_%7Bpos%7D%20=1\"\u003e\u003c/p\u003e\n\u003cp\u003eand the equation describing the negative hyperplane is: \u003cimg src=\"https://render.githubusercontent.com/render/math?math=b%20%2b%20w_Tx_%7Bneg%7D%20=-1\"\u003e\u003c/p\u003e\n\u003cp\u003eRemember, your goal is to maximize the separation between the two hyperplanes. To do this, first subtract the negative hyperplane's equation from the positive hyperplane's equation:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=w_T(x_%7Bpos%7D-x_%7Bneg%7D)%20=%202\"\u003e\u003c/p\u003e\n\u003cp\u003eNext, normalize \u003cimg src=\"https://render.githubusercontent.com/render/math?math=w_T\"\u003e by dividing both sides of the equation by its norm, \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%7C%7Cw%7C%7C\"\u003e :\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%7C%7C%20w%20%7C%7C=%20%5Csqrt%7B%5Csum%5Em_%7Bj-1%7Dw_j%5E2%7D\"\u003e\u003c/p\u003e\n\u003cp\u003eDividing the former expression by \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%7C%7Cw%7C%7C\"\u003e yields the equation below. The left side of the resulting equation can be interpreted as the distance between the positive and negative hyperplanes. This is the \u003cstrong\u003emargin\u003c/strong\u003e you're trying to maximize.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cdfrac%7Bw_T(x_%7Bpos%7D-x_%7Bneg%7D)%7D%7B%5ClVert%20w%20%5CrVert%7D%20=%20%5Cdfrac%7B2%7D%7B%5ClVert%20w%20%5CrVert%7D\"\u003e\u003c/p\u003e\n\u003cp\u003eThe objective of the SVM is then maximizing \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cdfrac%7B2%7D%7B%5ClVert%20w%20%5CrVert%7D\"\u003e under the constraint that the samples are classified correctly. Mathematically,\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=b%20%2b%20w_Tx%5E%7B(i)%7D%20%5Cgeq%201\"\u003e if \u003cimg src=\"https://render.githubusercontent.com/render/math?math=y%20%5E%7B(i)%7D%20=%201\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=b%20%2b%20w_Tx%5E%7B(i)%7D%20%5Cleq%20-1\"\u003e if \u003cimg src=\"https://render.githubusercontent.com/render/math?math=y%20%5E%7B(i)%7D%20=%20-1\"\u003e\u003c/p\u003e\n\u003cp\u003eFor \u003cimg src=\"https://render.githubusercontent.com/render/math?math=i=%201,%5Cldots%20,N\"\u003e\u003c/p\u003e\n\u003cp\u003eThese equations basically say that all negative samples should fall on the left side of the negative hyperplane, whereas all the positive samples should fall on the right of the positive hyperplane. This can also be written in one line as follows:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=y%20%5E%7B(i)%7D%20(b%20%2b%20w_Tx%5E%7B(i)%7D%20)%5Cgeq%201\"\u003e for each \u003cimg src=\"https://render.githubusercontent.com/render/math?math=i\"\u003e\u003c/p\u003e\n\u003cp\u003eNote that maximizing \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cdfrac%7B2%7D%7B%5ClVert%20w%20%5CrVert%7D\"\u003e means we're minimizing \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5ClVert%20w%20%5CrVert\"\u003e , or, as is done in practice because it seems to be easier to be minimized, \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cdfrac%7B1%7D%7B2%7D%5ClVert%20w%20%5CrVert%5E2\"\u003e .\u003c/p\u003e\n\u003ch2\u003eThe Soft Margin classifier\u003c/h2\u003e\n\u003cp\u003eIntroducing slack variables \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cxi\"\u003e . The idea for introducing slack variables is that the linear constraints need to be relaxed for data that are not linearly separable, as not relaxing the constraints might lead to the algorithm that doesn't converge.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=b%20%2b%20w_Tx%5E%7B(i)%7D%20%5Cgeq%201-%5Cxi%5E%7B(i)%7D\"\u003e if \u003cimg src=\"https://render.githubusercontent.com/render/math?math=y%20%5E%7B(i)%7D%20=%201\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=b%20%2b%20w_Tx%5E%7B(i)%7D%20%5Cleq%20-1%2b%5Cxi%5E%7B(i)%7D\"\u003e if \u003cimg src=\"https://render.githubusercontent.com/render/math?math=y%20%5E%7B(i)%7D%20=%20-1\"\u003e\u003c/p\u003e\n\u003cp\u003eFor \u003cimg src=\"https://render.githubusercontent.com/render/math?math=i=%201,%5Cldots%20,N\"\u003e\u003c/p\u003e\n\u003cp\u003eThe objective function (AKA the function you want to minimize) is\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cdfrac%7B1%7D%7B2%7D%5ClVert%20w%20%5CrVert%5E2%2b%20C(%5Csum_i%20%5Cxi%5E%7B(i)%7D)\"\u003e\u003c/p\u003e\n\u003cp\u003eYou're basically adding these slack variables in your objective function, making clear that you want to minimize the amount of slack you allow for. You can tune this with \u003cimg src=\"https://render.githubusercontent.com/render/math?math=C\"\u003e as shown in the above equation. \u003cimg src=\"https://render.githubusercontent.com/render/math?math=C\"\u003e will define how much slack we're allowing.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eA big value for \u003cimg src=\"https://render.githubusercontent.com/render/math?math=C\"\u003e will lead to the picture on the left: misclassifications are heavily punished, so the optimization prioritizes classifying correctly over having a big margin.\u003c/li\u003e\n\u003cli\u003eA small value for \u003cimg src=\"https://render.githubusercontent.com/render/math?math=C\"\u003e will lead to the picture on the right: it is OK to have some misclassifications, in order to gain a bigger margin overall. (This can help avoid overfitting to the training data.)\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-introduction-to-support-vector-machines/master/images/new_SVM_C.png\"\u003e\u003c/p\u003e\n\u003ch2\u003eSummary\u003c/h2\u003e\n\u003cp\u003eGreat! You now understand both max-margin classifiers as well as soft-margin classifiers. In the next lab, you'll try to code these fairly straightforward linear classifiers from scratch!\u003c/p\u003e","exportId":"introduction-to-support-vector-machines"},{"id":154127,"title":"Building an SVM from Scratch - Lab","type":"Assignment","indent":1,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_mark_done","completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-building-an-svm-from-scratch-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-building-an-svm-from-scratch-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","exportId":"g20e461cd69734561fae01407a52fb4ba"},{"id":154128,"title":"Building an SVM using scikit-learn - Lab","type":"Assignment","indent":1,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_mark_done","completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-building-an-svm-using-scikit-learn-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-building-an-svm-using-scikit-learn-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","exportId":"g1c2d3b6e28d3da996a0e47a884010444"},{"id":154129,"title":"The Kernel Trick","type":"WikiPage","indent":0,"locked":false,"requirement":"must_mark_done","completed":false,"content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-the-kernel-trick\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-the-kernel-trick\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-the-kernel-trick/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eIn this lesson, you'll learn how to create SVMs with non-linear decision boundaries data using kernels!\u003c/p\u003e\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\u003cp\u003eYou will be able to:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eDefine the kernel trick and explain why it is important in an SVM model\u003c/li\u003e\n\u003cli\u003eDescribe a radial basis function kernel\u003c/li\u003e\n\u003cli\u003eDescribe a sigmoid kernel\u003c/li\u003e\n\u003cli\u003eDescribe a polynomial kernel\u003c/li\u003e\n\u003cli\u003eDetermine when it is best to use specific kernels within SVM\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003eNon-linear problems: The Kernel trick\u003c/h2\u003e\n\u003cp\u003eIn the previous lab, you looked at a plot where a linear boundary was clearly not sufficient to separate the two classes cleanly. Another example where a linear boundary would not work well is shown below. How would you draw a max margin classifier here? The intuitive solution is to draw an arc around the circles, separating them from the surrounding diamonds. To generate non-linear boundaries such as this, you use what is known as a kernel.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-the-kernel-trick/master/images/new_SVM_nonlin.png\" width=\"500\"\u003e\u003c/p\u003e\n\u003cp\u003eThe idea behind kernel methods is to create (nonlinear) combinations of the original features and project them onto a higher-dimensional space. For example, take a look at how this dataset could be transformed with an appropriate kernel from a two-dimensional dataset onto a new three-dimensional feature space.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-the-kernel-trick/master/images/new_SVM_kernel.png\" width=\"500\"\u003e\u003c/p\u003e\n\u003ch2\u003eTypes of kernels\u003c/h2\u003e\n\u003cp\u003eThere are several kernels, and an overview can be found in this lesson, as well as in the scikit-learn documentation \u003ca href=\"https://scikit-learn.org/stable/modules/svm.html#kernel-functions\"\u003ehere\u003c/a\u003e. The idea is that kernels are inner products in a transformed space.\u003c/p\u003e\n\u003ch3\u003eThe Linear kernel\u003c/h3\u003e\n\u003cp\u003eThe linear kernel is, as you've seen, the default kernel and simply creates linear decision boundaries. The linear kernel is represented by the inner product of the \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Clangle%20x,%20x'%20%5Crangle\"\u003e . It is important to note that some kernels have additional parameters that can be specified and knowing how these parameters work is critical to tuning SVMs.\u003c/p\u003e\n\u003ch3\u003eThe RBF kernel\u003c/h3\u003e\n\u003cp\u003eThere are two parameters when training an SVM with the \u003cem\u003eR\u003c/em\u003eadial \u003cem\u003eB\u003c/em\u003easis \u003cem\u003eF\u003c/em\u003eunction: \u003cimg src=\"https://render.githubusercontent.com/render/math?math=C\"\u003e and \u003cimg src=\"https://render.githubusercontent.com/render/math?math=gamma\"\u003e .\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eThe parameter \u003cimg src=\"https://render.githubusercontent.com/render/math?math=C\"\u003e is common to all SVM kernels. Again, by tuning the \u003cimg src=\"https://render.githubusercontent.com/render/math?math=C\"\u003e parameter when using kernels, you can provide a trade-off between misclassification of the training set and simplicity of the decision function. A high \u003cimg src=\"https://render.githubusercontent.com/render/math?math=C\"\u003e will classify as many samples correctly as possible (and might potentially lead to overfitting)\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=gamma\"\u003e defines how much influence a single training example has. The larger \u003cimg src=\"https://render.githubusercontent.com/render/math?math=gamma\"\u003e is, the closer other examples must be to be affected\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThe RBF kernel is specified as:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cexp%7B(-%5Cgamma%20%5ClVert%20%20x%20-%20%20x'%20%5CrVert%5E2)%7D\"\u003e\u003c/p\u003e\n\u003cp\u003eGamma has a strong effect on the results: a \u003cimg src=\"https://render.githubusercontent.com/render/math?math=gamma\"\u003e that is too large will lead to overfitting, while a \u003cimg src=\"https://render.githubusercontent.com/render/math?math=gamma\"\u003e which is too small will lead to underfitting (kind of like a simple linear boundary for a complex problem).\u003c/p\u003e\n\u003cp\u003eIn scikit-learn, you can specify a value for \u003cimg src=\"https://render.githubusercontent.com/render/math?math=gamma\"\u003e using the parameter \u003ccode\u003egamma\u003c/code\u003e. The default \u003ccode\u003egamma\u003c/code\u003e value is \"auto\", if no other gamma is specified, gamma is set to \u003cimg src=\"https://render.githubusercontent.com/render/math?math=1/%5Ctext%7Bnumber_of_features%7D\"\u003e . You can find more on parameters in the RBF kernel \u003ca href=\"https://scikit-learn.org/stable/auto_examples/svm/plot_rbf_parameters.html\"\u003ehere\u003c/a\u003e.\u003c/p\u003e\n\u003ch3\u003eThe Polynomial kernel\u003c/h3\u003e\n\u003cp\u003eThe Polynomial kernel is specified as\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=(%5Cgamma%20%5Clangle%20%20x%20-%20%20x'%20%5Crangle%2br)%5Ed\"\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cimg src=\"https://render.githubusercontent.com/render/math?math=d\"\u003e can be specified by the parameter \u003ccode\u003edegree\u003c/code\u003e. The default degree is 3.\u003c/li\u003e\n\u003cli\u003e\n\u003cimg src=\"https://render.githubusercontent.com/render/math?math=r\"\u003e can be specified by the parameter \u003ccode\u003ecoef0\u003c/code\u003e. The default is 0.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003eThe Sigmoid kernel\u003c/h3\u003e\n\u003cp\u003eThe sigmoid kernel is specified as:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Ctanh%20(%20%5Cgamma%5Clangle%20%20x%20-%20%20x'%20%5Crangle%2br)\"\u003e\u003c/p\u003e\n\u003cp\u003eThis kernel is similar to the signoid function in logistic regression.\u003c/p\u003e\n\u003ch2\u003eSome more notes on SVC, NuSVC, and LinearSVC\u003c/h2\u003e\n\u003ch3\u003eNuSVC\u003c/h3\u003e\n\u003cp\u003eNuSVC is similar to SVC, but adds an additional parameter, \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cnu\"\u003e , which controls the number of support vectors and training errors. \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cnu\"\u003e jointly creates an upper bound on training errors and a lower bound on support vectors.\u003c/p\u003e\n\u003cp\u003eJust like SVC, NuSVC implements the \"one-against-one\" approach when there are more than 2 classes. This means that when there are n classes, \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cdfrac%7Bn*(n-1)%7D%7B2%7D\"\u003e classifiers are created, and each one classifies samples in 2 classes.\u003c/p\u003e\n\u003ch3\u003eLinearSVC\u003c/h3\u003e\n\u003cp\u003eLinearSVC is similar to SVC, but instead of the \"one-versus-one\" method, a \"one-vs-rest\" method is used. So in this case, when there are \u003cimg src=\"https://render.githubusercontent.com/render/math?math=n\"\u003e classes, just \u003cimg src=\"https://render.githubusercontent.com/render/math?math=n\"\u003e classifiers are created, and each one classifies samples in 2 classes, the one of interest, and all the other classes. This means that SVC generates more classifiers, so in cases with many classes, LinearSVC actually tends to scale better.\u003c/p\u003e\n\u003ch2\u003eProbabilities and predictions\u003c/h2\u003e\n\u003cp\u003eYou can make predictions using support vector machines. The SVC decision function gives a probability score per class. However, this is not done by default. You'll need to set the \u003ccode\u003eprobability\u003c/code\u003e argument equal to \u003ccode\u003eTrue\u003c/code\u003e. Scikit-learn internally performs cross-validation to compute the probabilities, so you can expect that setting \u003ccode\u003eprobability\u003c/code\u003e to \u003ccode\u003eTrue\u003c/code\u003e makes the calculations longer. For large datasets, computation can take considerable time to execute.\u003c/p\u003e\n\u003ch2\u003eSummary\u003c/h2\u003e\n\u003cp\u003eGreat! You now have a basic understanding of how to use kernel functions in Support Vector Machines. You'll do just that in the upcoming lab!\u003c/p\u003e","exportId":"the-kernel-trick"},{"id":154130,"title":"Kernels in scikit-learn - Lab","type":"Assignment","indent":1,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_mark_done","completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-kernels-in-scikit-learn-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-kernels-in-scikit-learn-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","exportId":"g9899dfdc4a8c9f30195910663bdc2a62"},{"id":154131,"title":"Support Vector Machines - Recap","type":"WikiPage","indent":0,"locked":false,"requirement":"must_mark_done","completed":false,"content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-svm-recap\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-svm-recap\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-svm-recap/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eAs you saw, support vector machines are another classification tool to add to your repertoire. While computationally expensive, they can be powerful tools providing substantial performance gains in many instances.\u003c/p\u003e\n\u003ch2\u003eKernel Functions\u003c/h2\u003e\n\u003cp\u003eProbably the most important information worth reviewing is some of the various kernel functions that you can apply:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eRadial Basis Function (RBF)\n\u003col\u003e\n\u003cli\u003e\u003ccode\u003ec\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e\n\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cgamma\"\u003e , which can be specified using \u003ccode\u003egamma\u003c/code\u003e in scikit-learn\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/li\u003e\n\u003cli\u003ePolynomial Kernel\n\u003col\u003e\n\u003cli\u003e\n\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cgamma\"\u003e , which can be specified using \u003ccode\u003egamma\u003c/code\u003e in scikit-learn\u003c/li\u003e\n\u003cli\u003e\n\u003cimg src=\"https://render.githubusercontent.com/render/math?math=r\"\u003e , which can be specified using \u003ccode\u003ecoef0\u003c/code\u003e in scikit-learn\u003c/li\u003e\n\u003cli\u003e\n\u003cimg src=\"https://render.githubusercontent.com/render/math?math=d\"\u003e , which can be specified using \u003ccode\u003edegree\u003c/code\u003e in scikit-learn\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/li\u003e\n\u003cli\u003eSigmoid Kernel\n\u003col\u003e\n\u003cli\u003e\n\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cgamma\"\u003e , which can be specified using \u003ccode\u003egamma\u003c/code\u003e in scikit-learn\u003c/li\u003e\n\u003cli\u003e\n\u003cimg src=\"https://render.githubusercontent.com/render/math?math=r\"\u003e , which can be specified using \u003ccode\u003ecoef0\u003c/code\u003e in scikit-learn\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eAlso recall that in general, \u003ccode\u003ec\u003c/code\u003e is the parameter for balancing standard accuracy metrics for tuning classifiers with the decision boundary distance.\u003c/p\u003e\n\u003ch2\u003eSummary\u003c/h2\u003e\n\u003cp\u003eWhile it may appear that this section was a bit brief, Support Vector Machines are a powerful algorithm that deserve attention, so make sure you investigate them properly. Moreover, learning to properly tune SVMs using kernels and an appropriate \u003ccode\u003ec\u003c/code\u003e value is critical.\u003c/p\u003e","exportId":"support-vector-machines-recap"}]},{"id":16483,"name":"Topic 32: Building a Machine Learning Pipeline","status":"unlocked","unlockDate":null,"prereqs":[],"requirement":"all","sequential":false,"exportId":"gc603a77934c356afb5d391648b85846b","items":[{"id":154132,"title":"Pipelines - Introduction","type":"WikiPage","indent":0,"locked":false,"requirement":"must_mark_done","completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-pipelines-intro\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-pipelines-intro/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eIn this section, you'll learn about machine learning pipelines. Pipelines are extremely useful for allowing data scientists to quickly and consistently transform data, train, and use machine learning models. This lesson will summarize the key topics you'll be covering.\u003c/p\u003e\n\n\u003ch2\u003eBuilding a Machine Learning Pipeline\u003c/h2\u003e\n\n\u003cp\u003eBy now, you know that the data science process is a flow of activities, from inspecting the data to cleaning it, transforming it, running a model, and discussing the results. Wouldn't it be nice if there was a streamlined process to create nice machine learning workflows? Enter machine learning pipelines in scikit-learn!\u003c/p\u003e\n\n\u003cp\u003eIn this section, you'll learn how you can use a pipeline to integrate several steps of the machine learning workflow. Additionally, you'll compare several classification techniques with each other, and integrate grid search in your pipeline so you can tune several hyperparameters in each of the machine learning models.\u003c/p\u003e\n\n\u003ch2\u003eSummary\u003c/h2\u003e\n\n\u003cp\u003eThis section will quickly introduce how to create pipelines in scikit-learn, but it will then be up to you to explore the magical world of pipelines and practice all your machine learning knowledge gained in this module!\u003c/p\u003e\n\u003cfooter class=\"fis-footer\" style=\"visibility: hidden;\"\u003e\u003cdiv class=\"fis-feedback\"\u003e\n\u003ch5\u003eHow do you feel about this lesson?\u003c/h5\u003e\n\u003cimg id=\"thumbs-up\" data-repository=\"dsc-pipelines-intro\" title=\"Thumbs up!\" alt=\"thumbs up\"\u003e\u003cimg id=\"thumbs-down\" data-repository=\"dsc-pipelines-intro\" title=\"Thumbs down!\" alt=\"thumbs down\"\u003e\n\u003c/div\u003e\n\u003ch5\u003eHave specific feedback? \u003ca href=\"https://github.com/learn-co-curriculum/dsc-pipelines-intro/issues/new\"\u003eTell us here!\u003c/a\u003e\n\u003c/h5\u003e\u003c/footer\u003e","exportId":"pipelines-introduction"},{"id":154133,"title":"Introduction to Pipelines","type":"WikiPage","indent":0,"locked":false,"requirement":"must_mark_done","completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-pipelines-v2-1\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-pipelines-v2-1/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eYou've learned a substantial number of different supervised learning algorithms. Now, it's time to learn about a handy tool used to integrate these algorithms into a single manageable pipeline.\u003c/p\u003e\n\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\n\u003cp\u003eYou will be able to:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eExplain how pipelines can be used to combine various parts of a machine learning workflow\u003cbr\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch2\u003eWhy Use Pipelines?\u003c/h2\u003e\n\n\u003cp\u003ePipelines are extremely useful tools to write clean and manageable code for machine learning. Recall how we start preparing our dataset: we want to clean our data, transform it, potentially use feature selection, and then run a machine learning algorithm. Using pipelines, you can do all these steps in one go!\u003c/p\u003e\n\n\u003cp\u003ePipeline functionality can be found in scikit-learn's \u003ccode\u003ePipeline\u003c/code\u003e module. Pipelines can be coded in a very simple way:\u003c/p\u003e\n\n\u003cpre\u003e\u003ccode class=\"python\"\u003efrom sklearn.pipeline import Pipeline\n\npipe = Pipeline([('mms', MinMaxScaler()),\n                 ('tree', DecisionTreeClassifier(random_state=123))])\n\u003c/code\u003e\u003c/pre\u003e\n\n\u003cp\u003eThis pipeline will ensure that first we'll apply a Min-Max scaler on our data before fitting a decision tree. However, the \u003ccode\u003ePipeline()\u003c/code\u003e function above is only defining the sequence of actions to perform. In order to actually fit the model, you need to call the \u003ccode\u003e.fit()\u003c/code\u003e method like so: \u003c/p\u003e\n\n\u003cpre\u003e\u003ccode class=\"python\"\u003epipe.fit(X_train, y_train)\n\u003c/code\u003e\u003c/pre\u003e\n\n\u003cp\u003eThen, to score the model on test data, you can call the \u003ccode\u003e.score()\u003c/code\u003e method like so: \u003c/p\u003e\n\n\u003cpre\u003e\u003ccode class=\"python\"\u003epipe.score(X_test, y_test)\n\u003c/code\u003e\u003c/pre\u003e\n\n\u003cp\u003eA really good blog post on the basic ideas of pipelines can be found \u003ca href=\"https://www.kdnuggets.com/2017/12/managing-machine-learning-workflows-scikit-learn-pipelines-part-1.html\"\u003ehere\u003c/a\u003e.\u003c/p\u003e\n\n\u003ch2\u003eIntegrating Grid Search in Pipelines\u003c/h2\u003e\n\n\u003cp\u003eNote that the above pipeline simply creates one pipeline for a training set, and evaluates on a test set. Is it possible to create a pipeline that performs grid search? And cross-validation? Yes, it is!\u003c/p\u003e\n\n\u003cp\u003eFirst, you define the pipeline in the same way as above. Next, you create a parameter grid. When this is all done, you use the function \u003ccode\u003eGridSearchCV()\u003c/code\u003e, which you've seen before, and specify the pipeline as the estimator and the parameter grid. You also have to define how many folds you'll use in your cross-validation. \u003c/p\u003e\n\n\u003cpre\u003e\u003ccode class=\"python\"\u003epipe = Pipeline([('mms', MinMaxScaler()),\n                 ('tree', DecisionTreeClassifier(random_state=123))])\n\ngrid = [{'tree__max_depth': [None, 2, 6, 10], \n         'tree__min_samples_split': [5, 10]}]\n\n\ngridsearch = GridSearchCV(estimator=pipe, \n                          param_grid=grid, \n                          scoring='accuracy', \n                          cv=5)\n\ngridsearch.fit(X_train, y_train)\n\ngridsearch.score(X_test, y_test)\n\u003c/code\u003e\u003c/pre\u003e\n\n\u003cp\u003eAn article with a detailed workflow can be found \u003ca href=\"https://www.kdnuggets.com/2018/01/managing-machine-learning-workflows-scikit-learn-pipelines-part-2.html\"\u003ehere\u003c/a\u003e.\u003c/p\u003e\n\n\u003ch2\u003eSummary\u003c/h2\u003e\n\n\u003cp\u003eGreat, this wasn't too difficult! The proof of all this is in the pudding. In the next lab, you'll use this workflow to build pipelines applying classification algorithms you have learned so far in this module. \u003c/p\u003e\n\u003cfooter class=\"fis-footer\" style=\"visibility: hidden;\"\u003e\u003cdiv class=\"fis-feedback\"\u003e\n\u003ch5\u003eHow do you feel about this lesson?\u003c/h5\u003e\n\u003cimg id=\"thumbs-up\" data-repository=\"dsc-pipelines-v2-1\" title=\"Thumbs up!\" alt=\"thumbs up\"\u003e\u003cimg id=\"thumbs-down\" data-repository=\"dsc-pipelines-v2-1\" title=\"Thumbs down!\" alt=\"thumbs down\"\u003e\n\u003c/div\u003e\n\u003ch5\u003eHave specific feedback? \u003ca href=\"https://github.com/learn-co-curriculum/dsc-pipelines-v2-1/issues/new\"\u003eTell us here!\u003c/a\u003e\n\u003c/h5\u003e\u003c/footer\u003e","exportId":"introduction-to-pipelines"},{"id":154134,"title":"Pipelines in scikit-learn - Lab","type":"Assignment","indent":1,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_mark_done","completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-pipelines-lab-v2-1\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-pipelines-lab-v2-1/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","exportId":"g5cebdd8af99a364284986be844159457"},{"id":154135,"title":"Pipelines - Recap","type":"WikiPage","indent":0,"locked":false,"requirement":"must_mark_done","completed":false,"content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-pipelines-recap\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-pipelines-recap\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-pipelines-recap/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\u003ch2\u003eKey Takeaways\u003c/h2\u003e\n\u003cp\u003eThe key takeaways from this section include:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eMachine Learning Pipelines create a nice workflow to combine data manipulations, preprocessing, and modeling\u003c/li\u003e\n\u003cli\u003eMachine Learning Pipelines can be used along with grid search to evaluate several parameter settings\u003c/li\u003e\n\u003cli\u003eGrid search can considerably blow up computation time when computing for several parameters along with cross-validation\u003c/li\u003e\n\u003cli\u003eSome models are very sensitive to hyperparameter changes, so they should be chosen with care, and even with big grids a good outcome isn't always guaranteed\u003c/li\u003e\n\u003c/ul\u003e","exportId":"pipelines-recap"}]},{"id":16489,"name":" Milestones","status":"completed","unlockDate":null,"prereqs":[],"requirement":null,"sequential":false,"exportId":"g5bd83e0ffdfb9cce5f9cbc0a5225ccbb","items":[{"id":154158,"title":"Phase 3 Project","type":"Assignment","indent":0,"locked":false,"submissionTypes":"a file upload","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":null,"completed":false,"content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-phase-3-project\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-phase-3-project\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-phase-3-project/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\u003cp\u003eCongratulations! You've made it through another \u003cem\u003eintense\u003c/em\u003e module, and now you're ready to show off your newfound Machine Learning skills!\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-phase-3-project/main/images/smart.gif\" alt=\"awesome\"\u003e\u003c/p\u003e\n\u003cp\u003eAll that remains in Phase 3 is to put your new skills to use with another large project! This project should take 20 to 30 hours to complete.\u003c/p\u003e\n\u003ch2\u003eProject Overview\u003c/h2\u003e\n\u003cp\u003eFor this project, you will engage in the full data science process from start to finish, solving a classification problem using a dataset of your choice.\u003c/p\u003e\n\u003ch3\u003eThe Data\u003c/h3\u003e\n\u003cp\u003eYou have the option to either \u003cstrong\u003echoose a dataset from a curated list\u003c/strong\u003e or \u003cstrong\u003echoose your own dataset \u003cem\u003enot on the list\u003c/em\u003e\u003c/strong\u003e. The goal is to choose a dataset appropriate to the type of business problem and/or classification methods that most interests you. It is up to you to define a stakeholder and business problem appropriate to the dataset you choose. If you are feeling overwhelmed or behind, we recommend you choose dataset #2 or #3 from the curated list.\u003c/p\u003e\n\u003cp\u003eIf you choose a dataset from the curated list, \u003cstrong\u003einform your instructor which dataset you chose\u003c/strong\u003e and jump right into the project. If you choose your own dataset, \u003cstrong\u003erun the dataset and business problem by your instructor for approval\u003c/strong\u003e before starting your project.\u003c/p\u003e\n\u003ch3\u003eCurated List of Datasets\u003c/h3\u003e\n\u003cp\u003eYou may select any of the four datasets below - we provide brief descriptions of each. Follow the links to learn more about the dataset and business problems before making a final decision.\u003c/p\u003e\n\u003ch4\u003e1) \u003ca href=\"https://data.cityofchicago.org/Transportation/Traffic-Crashes-Crashes/85ca-t3if\"\u003eChicago Car Crashes\u003c/a\u003e\u003c/h4\u003e\n\u003cp\u003eNote this links also to \u003ca href=\"https://data.cityofchicago.org/Transportation/Traffic-Crashes-Vehicles/68nd-jvt3\"\u003eVehicle Data\u003c/a\u003e and to \u003ca href=\"https://data.cityofchicago.org/Transportation/Traffic-Crashes-People/u6pd-qa9d\"\u003eDriver/Passenger Data\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eBuild a classifier to predict the primary contributory cause of a car accident, given information about the car, the people in the car, the road conditions etc. You might imagine your audience as a Vehicle Safety Board who's interested in reducing traffic accidents, or as the City of Chicago who's interested in becoming aware of any interesting patterns. Note that there is a \u003cstrong\u003emulti-class\u003c/strong\u003e classification problem. You will almost certainly want to bin or trim or otherwise limit the number of target categories on which you ultimately predict. Note e.g. that some primary contributory causes have very few samples.\u003c/p\u003e\n\u003ch4\u003e2) \u003ca href=\"https://catalog.data.gov/dataset/terry-stops\"\u003eTerry Traffic Stops\u003c/a\u003e\u003c/h4\u003e\n\u003cp\u003eIn \u003ca href=\"https://www.oyez.org/cases/1967/67\"\u003e\u003cem\u003eTerry v. Ohio\u003c/em\u003e\u003c/a\u003e, a landmark Supreme Court case in 1967-8, the court found that a police officer was not in violation of the \"unreasonable search and seizure\" clause of the Fourth Amendment, even though he stopped and frisked a couple of suspects only because their behavior was suspicious. Thus was born the notion of \"reasonable suspicion\", according to which an agent of the police may e.g. temporarily detain a person, even in the absence of clearer evidence that would be required for full-blown arrests etc. Terry Stops are stops made of suspicious drivers.\u003c/p\u003e\n\u003cp\u003eBuild a classifier to predict whether an arrest was made after a Terry Stop, given information about the presence of weapons, the time of day of the call, etc. Note that this is a \u003cstrong\u003ebinary\u003c/strong\u003e classification problem.\u003c/p\u003e\n\u003cp\u003eNote that this dataset also includes information about gender and race. You \u003cstrong\u003emay\u003c/strong\u003e use this data as well. You may, e.g. pitch your project as an inquiry into whether race (of officer or of subject) plays a role in whether or not an arrest is made.\u003c/p\u003e\n\u003cp\u003eIf you \u003cstrong\u003edo\u003c/strong\u003e elect to make use of race or gender data, be aware that this can make your project a highly sensitive one; your discretion will be important, as well as your transparency about how you use the data and the ethical issues surrounding it.\u003c/p\u003e\n\u003ch4\u003e3) \u003ca href=\"https://www.kaggle.com/becksddf/churn-in-telecoms-dataset\"\u003eSyriaTel Customer Churn\u003c/a\u003e\u003c/h4\u003e\n\u003cp\u003eBuild a classifier to predict whether a customer will (\"soon\") stop doing business with SyriaTel, a telecommunications company. Note that this is a \u003cstrong\u003ebinary\u003c/strong\u003e classification problem.\u003c/p\u003e\n\u003cp\u003eMost naturally, your audience here would be the telecom business itself, interested in losing money on customers who don't stick around very long. Are there any predictable patterns here?\u003c/p\u003e\n\u003ch4\u003e4) \u003ca href=\"https://www.drivendata.org/competitions/7/pump-it-up-data-mining-the-water-table/page/23/\"\u003eTanzanian Water Well Data\u003c/a\u003e\u003c/h4\u003e\n\u003cp\u003eThis dataset is part of an \u003cem\u003eactive competition\u003c/em\u003e until April 31, 2021!\u003c/p\u003e\n\u003cp\u003eTanzania, as a developing country, struggles with providing clean water to its population of over 57,000,000. There are many waterpoints already established in the country, but some are in need of repair while others have failed altogether.\u003c/p\u003e\n\u003cp\u003eBuild a classifier to predict the condition of a water well, using information about the sort of pump, when it was installed, etc. Note that this is a \u003cstrong\u003eternary\u003c/strong\u003e classification problem.\u003c/p\u003e\n\u003ch3\u003eSourcing Your Own Data\u003c/h3\u003e\n\u003cp\u003eSourcing new data is a valuable skill for data scientists, but it requires a great deal of care. An inappropriate dataset or an unclear business problem can lead you spend a lot of time on a project that delivers underwhelming results. The guidelines below will help you complete a project that demonstrates your ability to engage in the full data science process.\u003c/p\u003e\n\u003cp\u003eYour dataset must be...\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eAppropriate for classification.\u003c/strong\u003e It should have a categorical outcome or the data needed to engineer one.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eUsable to solve a specific business problem.\u003c/strong\u003e This solution must rely on your classification model.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eSomewhat complex.\u003c/strong\u003e It should contain a minimum of 1000 rows and 10 features.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eUnfamiliar.\u003c/strong\u003e It can't be one we've already worked with during the course or that is commonly used for demonstration purposes (e.g. MNIST).\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eManageable.\u003c/strong\u003e Stick to datasets that you can model using the techniques introduced in Phase 3.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eOnce you've sourced your own dataset and identified the business problem you want to solve with it, you must to \u003cstrong\u003erun them by your instructor for approval\u003c/strong\u003e.\u003c/p\u003e\n\u003ch4\u003eProblem First, or Data First?\u003c/h4\u003e\n\u003cp\u003eThere are two ways that you can source your own dataset: \u003cstrong\u003e\u003cem\u003eProblem First\u003c/em\u003e\u003c/strong\u003e or \u003cstrong\u003e\u003cem\u003eData First\u003c/em\u003e\u003c/strong\u003e. The less time you have to complete the project, the more strongly we recommend a Data First approach to this project.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e\u003cem\u003eProblem First\u003c/em\u003e\u003c/strong\u003e: Start with a problem that you are interested in that you could potentially solve with a classification model. Then look for data that you could use to solve that problem. This approach is high-risk, high-reward: Very rewarding if you are able to solve a problem you are invested in, but frustrating if you end up sinking lots of time in without finding appropriate data. To mitigate the risk, set a firm limit for the amount of time you will allow yourself to look for data before moving on to the Data First approach.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e\u003cem\u003eData First\u003c/em\u003e\u003c/strong\u003e: Take a look at some of the most popular internet repositories of cool data sets we've listed below. If you find a data set that's particularly interesting for you, then it's totally okay to build your problem around that data set.\u003c/p\u003e\n\u003cp\u003eThere are plenty of amazing places that you can get your data from. We recommend you start looking at data sets in some of these resources first:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://archive.ics.uci.edu/ml/datasets.html\"\u003eUCI Machine Learning Datasets Repository\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://www.kaggle.com/datasets\"\u003eKaggle Datasets\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/awesomedata/awesome-public-datasets\"\u003eAwesome Datasets Repo on Github\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://opendata.cityofnewyork.us/\"\u003eNew York City Open Data Portal\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"http://insideairbnb.com/\"\u003eInside AirBNB\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003eThe Deliverables\u003c/h2\u003e\n\u003cp\u003eThere are three deliverables for this project:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eA \u003cstrong\u003eGitHub repository\u003c/strong\u003e\u003c/li\u003e\n\u003cli\u003eA \u003cstrong\u003eJupyter Notebook\u003c/strong\u003e\u003c/li\u003e\n\u003cli\u003eA \u003cstrong\u003enon-technical presentation\u003c/strong\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eReview the \"Project Submission \u0026amp; Review\" page in the \"Milestones Instructions\" topic for instructions on creating and submitting your deliverables. Refer to the rubric associated with this assignment for specifications describing high-quality deliverables.\u003c/p\u003e\n\u003ch3\u003eKey Points\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eYour deliverables should explicitly address each step of the data science process.\u003c/strong\u003e Refer to \u003ca href=\"https://github.com/learn-co-curriculum/dsc-data-science-processes\"\u003ethe Data Science Process lesson\u003c/a\u003e from Topic 19 for more information about process models you can use.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eYour Jupyter Notebook should demonstrate an iterative approach to modeling.\u003c/strong\u003e This means that you begin with a basic model, evaluate it, and then provide justification for and proceed to a new model. We encourage you to try a bunch of different models: logistic regression, decision trees, or anything else you think would be appropriate.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eYou must choose appropriate classification metrics and use them to evaluate your models.\u003c/strong\u003e Choosing the right classification metrics is a key data science skill, and should be informed by data exploration and the business problem itself. You must then use this metric to evaluate your model performance using both training and testing data.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003eGetting Started\u003c/h2\u003e\n\u003cp\u003eCreate a new repository for your project to get started. We recommend structuring your project repository similar to the structure in \u003ca href=\"https://github.com/learn-co-curriculum/dsc-project-template\"\u003ethe Phase 1 Project Template\u003c/a\u003e. You can do this either by creating a new fork of that repository to work in or by building a new repository from scratch that mimics that structure.\u003c/p\u003e\n\u003ch2\u003eProject Submission and Review\u003c/h2\u003e\n\u003cp\u003eReview the \"Project Submission \u0026amp; Review\" page in the \"Milestones Instructions\" topic to learn how to submit your project and how it will be reviewed. Your project must pass review for you to progress to the next Phase.\u003c/p\u003e\n\u003ch2\u003eSummary\u003c/h2\u003e\n\u003cp\u003eThis project is an opportunity to expand your data science toolkit by evaluating, choosing, and working with new datasets. Spending time up front making sure you have a good dataset for a solvable problem will help avoid the major problems that can sometimes derail data science projects. You've got this!\u003c/p\u003e","exportId":"gc6ad6eb095b74767a3900ea52caf102b"},{"id":154159,"title":"Phase 3 Project - GitHub Repository URL","type":"Assignment","indent":1,"locked":false,"submissionTypes":"a website url","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":null,"completed":false,"content":"\u003cp\u003e\u003cspan\u003ePlease put the URL to your Phase 3 Project GitHub Repository here.\u0026nbsp;\u003c/span\u003e\u003c/p\u003e","exportId":"gc8125a8313e71955700ce06b0b50f54c"},{"id":154161,"title":"Phase 3 Blog Post","type":"Assignment","indent":0,"locked":false,"submissionTypes":"a website url","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":null,"completed":false,"content":"\u003cp\u003e\u003cspan\u003ePlease put the URL to your Phase 3 Blog Post here. \u003c/span\u003e\u003cspan\u003eRefer to the \u003c/span\u003e\u003ca title=\"Blogging Overview\" href=\"pages/blogging-overview\"\u003eBlogging Overview\u003c/a\u003e\u003cspan\u003e to learn about how to write good blog posts that\u003c/span\u003e\u003cspan style=\"font-family: inherit; font-size: 1rem;\"\u003e meet Flatiron Schools requirements.\u003c/span\u003e\u003c/p\u003e","exportId":"g17c854c55744c3b81c57c0c6229504cf"}]}],"pages":[{"exportId":"motivation-for-linear-algebra-in-data-science","title":"Motivation for Linear Algebra in Data Science","type":"WikiPage","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-lingalg-motivation\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-lingalg-motivation/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eIn this section, you'll learn about algebra as a foundational step for data science, and later on statistics. Linear algebra is also very important when moving on to machine learning models, where a solid understanding of linear equations plays a major role. This lesson will attempt to present some motivational examples of how and why a solid foundation of linear algebra is valuable for data scientists.\u003c/p\u003e\n\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\n\u003cp\u003eYou will be able to:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eState the importance of linear algebra in the fields of data science and machine learning \u003c/li\u003e\n\u003cli\u003eDescribe the areas in AI and machine learning where linear algebra might be used for advanced analytics \u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch2\u003eBackground\u003c/h2\u003e\n\n\u003cp\u003eWhile having a deep understanding of linear algebra may not be mandatory, some basic knowledge is undoubtedly extremely helpful in your journey towards becoming a data scientist.\u003c/p\u003e\n\n\u003cp\u003eYou may already know a number of linear algebraic concepts without even knowing it. Examples are: matrix multiplication and dot-products. Later on, you'll learn more complex algebraic concepts like the calculation of matrix determinants, cross-products, and eigenvalues/eigenvectors. As a data scientist, it is important to know some of the theories as well as having a practical understanding of these concepts in a real-world setting.\u003c/p\u003e\n\n\u003ch2\u003eAn analogy\u003c/h2\u003e\n\n\u003cp\u003eThink of a simple example where you first learn about a sine function as an infinite polynomial while learning trigonometry. Students usually practice this function by passing different values to this function and getting the expected results and then manage to relate this to triangles and vertices. When learning advanced physics, students get to learn more applications of sine and other similar functions in the area of sound and light. In the domain of Signal Processing for unidimensional data, these functions pop up again to help you solve filtering, time-series related problems. An introduction to numeric computation around sine functions can not alone help you understand its wider application areas. In fact, sine functions are everywhere in the universe from music to light/sound/radio waves, from pendulum oscillations to alternating current.\u003c/p\u003e\n\n\u003ch2\u003e\u0026nbsp;Why Linear Algebra?\u003c/h2\u003e\n\n\u003cblockquote\u003e\n\u003cp\u003eLinear algebra is the branch of mathematics concerning vector spaces and linear relationships between such spaces. It includes the study of lines, planes, and subspaces, but is also concerned with properties common to all vector spaces.\u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003cp\u003eAnalogous to the example we saw above, it's important that a data scientist understands how data structures are built with vectors and matrices following the geometric intuitions from linear algebra, in addition to the numeric calculations. A data-focused understanding of linear algebra can help machine learning practitioners decide what tools can be applied to a given problem and how to interpret the results of experiments. You'll see that a good understanding of linear algebra is particularly useful in many ML/AI algorithms, especially in deep learning, where a lot of the operations happen under the hood.\u003c/p\u003e\n\n\u003cp\u003eFollowing are some of the areas where linear algebra is commonly practiced in the domain of data science and machine learning:  \u003c/p\u003e\n\n\u003ch3\u003eComputer Vision / Image Processing\u003c/h3\u003e\n\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-lingalg-motivation/master/images/rgb.png\" width=\"600\"\u003e\u003c/p\u003e\n\n\u003cp\u003eComputers are designed to process binary information only (only 0s and 1s). How can an image such as the dog shown here, with multiple attributes like color, be stored in a computer? This is achieved by storing the pixel intensities for red, blue and green colors in a matrix format. Color intensities can be coded into this matrix and can be processed further for analysis and other tasks. Any operation performed on this image would likely use some form of linear algebra with matrices as the back end.\u003c/p\u003e\n\n\u003ch3\u003eDeep Learning - Tensors\u003c/h3\u003e\n\n\u003cp\u003eDeep Learning is a sub-domain of machine learning, concerned with algorithms that can imitate the functions and structure of a biological brain as a computational algorithm. These are called artificial neural networks (ANNs). \u003c/p\u003e\n\n\u003cp\u003eThe algorithms usually store and process data in the form of mathematical entities called tensors. A tensor is often thought of as a generalized matrix. That is, it could be a 1-D matrix (a vector is actually such a tensor), a 2-D matrix (like a data frame), a 3-D matrix (something like a cube of numbers), even a 0-D matrix (a single number), or a higher dimensional structure that is harder to visualize.\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-lingalg-motivation/master/images/tensor.png\" width=\"850\"\u003e\u003c/p\u003e\n\n\u003cp\u003eAs shown in the image above where different input features are being extracted and stored as spatial locations inside a tensor which appears as a cube. A tensor encapsulates the scalar, vector, and the matrix characteristics. For deep learning, creating and processing tensors and operations that are performed on these also require knowledge of linear algebra. Don't worry if you don't fully understand this right now, you'll learn more about tensors later!\u003c/p\u003e\n\n\u003ch3\u003eNatural Language Processing\u003c/h3\u003e\n\n\u003cp\u003eNatural Language Processing (NLP) is another (very popular) area in Machine Learning dealing with text data. The most common techniques employed in NLP include BoW (Bag of Words) representation, Term Document Matrix etc. As shown in the image below, the idea is that words are being encoded as numbers and stored in a matrix format. Here, we just use 3 sentences to illustrate this:\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-lingalg-motivation/master/images/NLPmatrix.png\" width=\"650\"\u003e\u003c/p\u003e\n\n\u003cp\u003eThis is just a short example, but you can store long documents in (giant) matrices like this. Using these counts in a matrix form can help perform tasks like semantic analysis, language translation, language generation etc.\u003c/p\u003e\n\n\u003ch3\u003eDimensionality Reduction\u003c/h3\u003e\n\n\u003cp\u003eDimensionality reduction techniques, which are heavily used when dealing with big datasets, use matrices to process data in order to reduce its dimensions. Principle Component Analysis (PCA) is a widely used dimensionality reduction technique that relies solely on calculating eigenvectors and eigenvalues to identify principal components as a set of highly reduced dimensions. The picture below is an example of a three-dimensional data being mapped into two dimensions using matrix manipulations. \u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-lingalg-motivation/master/images/pca.png\" width=\"900\"\u003e\u003c/p\u003e\n\n\u003cp\u003eGreat, you now know about some key areas where linear algebra is used! In the following lessons, you'll go through an introductory series of lessons and labs that will cover basic ideas of linear algebra: an understanding of vectors and matrices with some basic operations that can be performed on these mathematical entities. We will implement these ideas in Python, in an attempt to give you the foundational knowledge to deal with these algebraic entities and their properties. These skills will be applied in advanced machine learning sections later in the course. \u003c/p\u003e\n\n\u003ch2\u003eFurther Reading\u003c/h2\u003e\n\n\u003cp\u003e\u003ca href=\"https://www.youtube.com/watch?v=_MxCXGF9N-8\"\u003eYoutube: Why Linear Algebra\u003c/a\u003e\u003c/p\u003e\n\n\u003cp\u003e\u003ca href=\"https://towardsdatascience.com/boost-your-data-sciences-skills-learn-linear-algebra-2c30fdd008cf\"\u003eBoost your data science skills. Learn linear algebra.\u003c/a\u003e\u003c/p\u003e\n\n\u003cp\u003e\u003ca href=\"https://www.quora.com/What-are-the-applications-of-linear-algebra-in-machine-learning\"\u003eQuora: Applications of Linear Algebra in Deep Learning\u003c/a\u003e\u003c/p\u003e\n\n\u003ch2\u003eSummary\u003c/h2\u003e\n\n\u003cp\u003eIn this lesson, you learned about some Data Science examples that heavily rely on linear algebra principles. You looked at some use cases in practical machine learning problems where linear algebra and matrix manipulation might come in handy. In the following lessons, you'll take a deeper dive into specific concepts in linear algebra, working your way towards solving a regression problem using linear algebraic operations only. \u003c/p\u003e\n\u003cfooter class=\"fis-footer\" style=\"visibility: hidden;\"\u003e\u003cdiv class=\"fis-feedback\"\u003e\n\u003ch5\u003eHow do you feel about this lesson?\u003c/h5\u003e\n\u003cimg id=\"thumbs-up\" data-repository=\"dsc-lingalg-motivation\" title=\"Thumbs up!\" alt=\"thumbs up\"\u003e\u003cimg id=\"thumbs-down\" data-repository=\"dsc-lingalg-motivation\" title=\"Thumbs down!\" alt=\"thumbs down\"\u003e\n\u003c/div\u003e\n\u003ch5\u003eHave specific feedback? \u003ca href=\"https://github.com/learn-co-curriculum/dsc-lingalg-motivation/issues/new\"\u003eTell us here!\u003c/a\u003e\n\u003c/h5\u003e\u003c/footer\u003e","frontPage":false},{"exportId":"ensembles-recap","title":"Ensembles - Recap","type":"WikiPage","content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-ensemble-methods-section-recap\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-ensemble-methods-section-recap\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-ensemble-methods-section-recap/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\u003ch2\u003eKey Takeaways\u003c/h2\u003e\n\u003cp\u003eThe key takeaways from this section include:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eMultiple independent estimates are consistently more accurate than any single estimate, so ensemble techniques are a powerful way for improving the quality of your models\u003c/li\u003e\n\u003cli\u003eSometimes you'll use model stacking or meta-ensembles where you use a combination of different types of models for your ensemble\u003c/li\u003e\n\u003cli\u003eIt's also common to have multiple similar models in an ensemble - e.g. a bunch of decision trees\u003c/li\u003e\n\u003cli\u003eBagging (Bootstrap AGGregation) is a technique that leverages Bootstrap Resampling and Aggregation\u003c/li\u003e\n\u003cli\u003eBootstrap resampling uses multiple smaller samples from the test dataset to create independent estimates, and aggregate these estimates to make predictions\u003c/li\u003e\n\u003cli\u003eA random forest is an ensemble method for decision trees using Bagging and the Subspace Sampling method to create variance among the trees\u003c/li\u003e\n\u003cli\u003eWith a random forest, for each tree, we sample two-thirds of the training data and the remaining third is used to calculate the out-of-bag error\u003c/li\u003e\n\u003cli\u003eIn addition, the Subspace Sampling method is used to further increase variability by randomly selecting the subset of features to use as predictors for training any given tree\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003eGridsearchCV\u003c/code\u003e is an exhaustive search technique for finding optimal combinations of hyperparameters\u003c/li\u003e\n\u003cli\u003eBoosting leverages an ensemble of weak learners (weak models) to create a strong combined model\u003c/li\u003e\n\u003cli\u003eBoosting (when compared to random forests) is an iterative rather than independent process, using each iteration to strengthen the weaknesses of the previous iterations\u003c/li\u003e\n\u003cli\u003eTwo of the most common algorithms for Boosting are Adaboost (Adaptive Boosting) and Gradient Boosted Trees\u003c/li\u003e\n\u003cli\u003eAdaboost creates new classifiers by continually influencing the distribution of the data sampled to train each successive tree\u003c/li\u003e\n\u003cli\u003eGradient Boosting is a more advanced boosting algorithm that makes use of Gradient Descent\u003c/li\u003e\n\u003cli\u003eXGBoost (eXtreme Gradient Boosting) is one of the top gradient boosting algorithms currently in use\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003eXGBoost\u003c/code\u003e is a stand-alone library that implements popular gradient boosting algorithms in the fastest, most performant way possible\u003c/li\u003e\n\u003c/ul\u003e","frontPage":false},{"exportId":"calculus-section-recap","title":"Calculus - Section Recap","type":"WikiPage","content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-calculus-section-recap\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-calculus-section-recap\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-calculus-section-recap/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eCongratulations! You have learned about one of the most fundamental concepts at the core machine learning, calculus. In this section, you started with the basics of derivatives and moved all the way to coding out gradient descent with multiple variables.\u003c/p\u003e\n\u003ch2\u003eKey Takeaways\u003c/h2\u003e\n\u003cp\u003eIn this section, we both learned how to traverse a cost function graph to find the local minima to solve a linear regression by using gradient descent and covered some of the foundational calculus that will help you to understand many of the other machine learning models you'll encounter as a professional data scientist.\u003c/p\u003e\n\u003cp\u003eKey takeaways include:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eA derivative is the \"instantaneous rate of change\" of a function - or it can be thought of as the \"slope of the curve\" at a point in time\u003c/li\u003e\n\u003cli\u003eA derivative can also be thought of as a special case of the rate of change over a period of time - as that period of time is zero.\u003c/li\u003e\n\u003cli\u003eIf you calculate the rate of change over a period of time and keep reducing the period of time, it usually tends to a limit - which is the value of that derivative\u003c/li\u003e\n\u003cli\u003eThe power rule, constant factor rule, and addition rule are key tools for calculating derivatives for various kinds of functions\u003c/li\u003e\n\u003cli\u003eThe chain rule can be a useful tool for calculating the derivate of composite functions\u003c/li\u003e\n\u003cli\u003eA derivative can be useful for identifying local maxima or minima as in both cases, the derivative tends to zero\u003c/li\u003e\n\u003cli\u003eA cost curve can be used to plot the values of a cost function (in the case of linear regression) for various values of offset and slope for the best fit line.\u003c/li\u003e\n\u003cli\u003eGradient descent can be used to move towards the local minimum on the cost curve and thus the ideal values for the y-intercept and slope to minimize the selected cost function when performing a linear regression.\u003c/li\u003e\n\u003c/ul\u003e","frontPage":false},{"exportId":"support-vector-machines-introduction","title":"Support Vector Machines - Introduction","type":"WikiPage","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-svm-intro\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-svm-intro/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eA Support Vector Machine (SVM) is a type of classifier which modifies the loss function for optimization to not only take into account overall accuracy metrics of the resulting predictions, but also to maximize the decision boundary between the data points. In essence, this further helps tune the classifier as a good balance between underfitting and overfitting.\u003c/p\u003e\n\n\u003ch2\u003eSupport Vector Machines\u003c/h2\u003e\n\n\u003cp\u003eIn addition to optimizing for accuracy, support vector machines add a slack component, trading in accuracy to increase the distance between data points and the decision boundary. This provides an interesting perspective that can help formalize the intuitive visual choices a human would make in balancing precision and generalization to strike a balance between overfitting and underfitting.\u003c/p\u003e\n\n\u003ch2\u003eKernel Functions\u003c/h2\u003e\n\n\u003cp\u003eInitially, you'll explore linear support vector machines that divide data points into their respective groups by drawing hyperplanes using the dimensions from the feature space. In practice, these have limitations and the dataset may not be cleanly separable. As a result, kernel functions are an additional tool that can be used. Essentially, kernels reproject data onto a new parameter space using combinations of existing features. From there, the same process of applying SVMs to this transformed space can then be employed.\u003c/p\u003e\n\n\u003ch2\u003eSummary\u003c/h2\u003e\n\n\u003cp\u003eSupport Vector Machines are a powerful algorithm and may have the top performance among the out of the box classifiers from scikit-learn. Moreover, learning to properly tune SVMs is critical. In the upcoming labs and lessons, you'll investigate and apply these concepts.\u003c/p\u003e\n\u003cfooter class=\"fis-footer\" style=\"visibility: hidden;\"\u003e\u003cdiv class=\"fis-feedback\"\u003e\n\u003ch5\u003eHow do you feel about this lesson?\u003c/h5\u003e\n\u003cimg id=\"thumbs-up\" data-repository=\"dsc-svm-intro\" title=\"Thumbs up!\" alt=\"thumbs up\"\u003e\u003cimg id=\"thumbs-down\" data-repository=\"dsc-svm-intro\" title=\"Thumbs down!\" alt=\"thumbs down\"\u003e\n\u003c/div\u003e\n\u003ch5\u003eHave specific feedback? \u003ca href=\"https://github.com/learn-co-curriculum/dsc-svm-intro/issues/new\"\u003eTell us here!\u003c/a\u003e\n\u003c/h5\u003e\u003c/footer\u003e","frontPage":false},{"exportId":"random-forests","title":"Random Forests","type":"WikiPage","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-random-forests\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-random-forests/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eIn this lesson, we'll learn about a powerful and popular ensemble method that makes use of decision trees -- a random forest!\u003c/p\u003e\n\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\n\u003cp\u003eYou will be able to:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eDescribe how the random forest algorithm works \u003c/li\u003e\n\u003cli\u003eDescribe the subspace sampling method that makes random forests \"random\" \u003c/li\u003e\n\u003cli\u003eExplain the benefits and drawbacks of random forest models \u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch2\u003eUnderstanding the Random forest algorithm\u003c/h2\u003e\n\n\u003cp\u003eThe \u003cstrong\u003e\u003cem\u003eRandom Forest\u003c/em\u003e\u003c/strong\u003e algorithm is a supervised learning algorithm that can be used both for classification and regression tasks. Decision trees are the cornerstone of random forests -- if you don't remember much about decision trees, now may be a good time to go back and review that section until you feel comfortable with the topic. \u003c/p\u003e\n\n\u003cp\u003ePut simply, the random forest algorithm is an ensemble of decision trees. However, you may recall that decision trees use a \u003cstrong\u003e\u003cem\u003egreedy algorithm\u003c/em\u003e\u003c/strong\u003e, meaning that given the same data, the algorithm will make a choice that maximizes information gain at every step. By itself, this presents a problem -- it doesn't matter how many trees we add to our forest if they're all the same tree! Trees trained on the same dataset will come out the exact same way every time -- there is no randomness to this algorithm. It doesn't matter if our forest has a million decision trees; if they are all exactly the same, then our performance will be no better than if we just had a single tree.\u003c/p\u003e\n\n\u003cp\u003eThink about this from a business perspective -- would you rather have a team at your disposal where everyone has exactly the same training and skills, or a team where each member has their own individual strengths and weaknesses? The second team will almost always do much better!\u003c/p\u003e\n\n\u003cp\u003eAs we learned when reading up on ensemble methods, variance is a good thing in any ensemble. So how do we create high variance among all the trees in our random forest? The answer lies in two clever techniques that the algorithm uses to make sure that each tree focuses on different things -- \u003cstrong\u003e\u003cem\u003eBagging\u003c/em\u003e\u003c/strong\u003e and the \u003cstrong\u003e\u003cem\u003eSubspace Sampling Method\u003c/em\u003e\u003c/strong\u003e.\u003c/p\u003e\n\n\u003ch2\u003eBagging\u003c/h2\u003e\n\n\u003cp\u003eThe first way to encourage differences among the trees in our forest is to train them on different samples of data. Although more data is generally better, if we gave every tree the entire dataset, we would end up with each tree being exactly the same. Because of this, we instead use \u003cstrong\u003e\u003cem\u003eBootstrap Aggregation\u003c/em\u003e\u003c/strong\u003e (AKA \u003cstrong\u003e\u003cem\u003eBagging\u003c/em\u003e\u003c/strong\u003e) to obtain a portion of our data by sampling with replacement. For each tree, we sample two-thirds of our training data with replacement -- this is the data that will be used to build our tree. The remaining data is used as an internal test set to test each tree -- this remaining one-third is referred to as \u003cstrong\u003e\u003cem\u003eOut-Of-Bag Data\u003c/em\u003e\u003c/strong\u003e, or \u003cstrong\u003e\u003cem\u003eOOB\u003c/em\u003e\u003c/strong\u003e. For each new tree created, the algorithm then uses the remaining one-third of data that wasn't sampled to calculate the \u003cstrong\u003e\u003cem\u003eOut-Of-Bag Error\u003c/em\u003e\u003c/strong\u003e, in order to get a running, unbiased estimate of overall tree performance for each tree in the forest. \u003c/p\u003e\n\n\u003cp\u003eTraining each tree on its own individual \"bag\" of data is a great start for getting us some variability between the decision trees in our forest. However, with just bagging, all the trees are still focusing on all the same predictors. This allows for a potential weakness to affect all the trees at once -- if a predictor that usually provides strong signal provides bad information for a given observation, then it's likely that all the trees will fall for this false signal and make the wrong prediction. This is where the second major part of the Random forest algorithm comes in!\u003c/p\u003e\n\n\u003ch2\u003eSubspace sampling method\u003c/h2\u003e\n\n\u003cp\u003eAfter bagging the data, the random forest uses the \u003cstrong\u003e\u003cem\u003eSubspace sampling method\u003c/em\u003e\u003c/strong\u003e to further increase variability between the trees. Although it has a fancy mathematical-sounding name, all this method does is randomly select a subset of features to use as predictors for each node when training a decision tree, instead of using all predictors available at each node. \u003c/p\u003e\n\n\u003cp\u003eLet's pretend we're training our random forest on a dataset with 3000 rows and 10 columns. For each given tree, we would randomly \"bag\" 2000 rows with replacement. Next, we perform a subspace sample by randomly selecting a number of predictors at each node of a decision tree. Exactly how many predictors are used is a tunable parameter for this algorithm -- for simplicity's sake, let's assume we pick 6 predictors in this example. \u003c/p\u003e\n\n\u003cp\u003eThis brings us to the following pseudocode so far:\u003c/p\u003e\n\n\u003cp\u003eFor each tree in the dataset:\u003c/p\u003e\n\n\u003col\u003e\n\u003cli\u003eBag 2/3 of the overall data -- in our example, 2000 rows \u003c/li\u003e\n\u003cli\u003eRandomly select a set number of features to use for training each node within this -- in this example, 6 features\u003cbr\u003e\n\u003c/li\u003e\n\u003cli\u003eTrain the tree on the modified dataset, which is now a DataFrame consisting of 2000 rows and 6 columns\u003cbr\u003e\n\u003c/li\u003e\n\u003cli\u003eDrop the unused columns from step 3 from the out-of-bag rows that weren't bagged in step 1, and then use this as an internal testing set to calculate the out-of-bag error for this particular tree \u003c/li\u003e\n\u003c/ol\u003e\n\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-random-forests/master/images/new_rf-diagram.png\" width=\"750\"\u003e\u003c/p\u003e\n\n\u003ch3\u003eResiliency to overfitting\u003c/h3\u003e\n\n\u003cp\u003eOnce we've created our target number of trees, we'll be left with a random forest filled with a diverse set of decision trees that are trained on different sets of data, and also look at different subsets of features to make predictions. This amount of diversity among the trees in our forest will make for a model that is extremely resilient to noisy data, thus reducing the chance of overfitting.\u003c/p\u003e\n\n\u003cp\u003eTo understand why this is the case, let's put it in practical terms. Let's assume that of the 10 columns that we mentioned in our hypothetical dataset, column 2 correlates heavily with our target. However, there is still some noise in this dataset, and this column doesn't correlate \u003cem\u003eperfectly\u003c/em\u003e with our target -- there will be times where it suggests one class or another, but this isn't actually the case -- let's call these rows \"false signals\". In the case of a single decision tree, or even a forest where all trees focus on all the same predictors, we can expect to get the model to almost always get these false signal examples wrong. Why? Because the model will have learned to treat column 2 as a \"star player\" of sorts. When column 2 provides a false signal, our model will fall for it and get the prediction wrong. \u003c/p\u003e\n\n\u003cp\u003eNow, let's assume that we have a random forest complete with subspace sampling. If we randomly use 6 out of 10 predictors when creating each node of each tree, then this means that ~40% of the nodes of the trees in our forest won't even know column 2 exists! In the cases where column 2 provides a \"false signal\", the nodes of trees that use column 2 will likely make an incorrect prediction -- but that only matters to the ~60% that look at column 2. Our forest still contains another 40% of nodes within trees that are essentially \"immune\" to the false signal in column 2, because they don't use that predictor. In this way, the \"wisdom of the crowd\" buffers the performance of every constituent in that crowd. Although for any given example, some trees may draw the wrong conclusion from a particular predictor, the odds that \u003cem\u003eevery tree\u003c/em\u003e makes the same mistake because they looked at the same predictor is infinitesimally small!\u003c/p\u003e\n\n\u003ch3\u003eMaking predictions with random forests\u003c/h3\u003e\n\n\u003cp\u003eOnce we have trained all the trees in our random forest, we can effectively use it to make predictions! When given data to make predictions on, the algorithm provides only the appropriate features to each tree in the forest, gets that tree's individual prediction, and then aggregates all predictions together to determine the overall prediction that the algorithm will make for said data. In essence, each tree \"votes\" for the prediction that the forest will make, with the majority winning. \u003c/p\u003e\n\n\u003ch2\u003eBenefits and drawbacks\u003c/h2\u003e\n\n\u003cp\u003eLike any algorithm, the random forest comes with its own benefits and drawbacks. \u003c/p\u003e\n\n\u003ch3\u003eBenefits\u003c/h3\u003e\n\n\u003cul\u003e\n\u003cli\u003e\u003cp\u003e\u003cstrong\u003e\u003cem\u003eStrong performance\u003c/em\u003e\u003c/strong\u003e: The random forest algorithm usually has very strong performance on most problems, when compared with other classification algorithms. Because this is an ensemble algorithm, the model is naturally resistant to noise and variance in the data, and generally tends to perform quite well. \u003c/p\u003e\u003c/li\u003e\n\u003cli\u003e\u003cp\u003e\u003cstrong\u003e\u003cem\u003eInterpretability\u003c/em\u003e\u003c/strong\u003e:  Conveniently, since each tree in the random forest is a \u003cstrong\u003e\u003cem\u003eGlass-Box Model\u003c/em\u003e\u003c/strong\u003e (meaning that the model is interpretable, allowing us to see how it arrived at a certain decision), the overall random forest is, as well! You'll demonstrate this yourself in the upcoming lab, by inspecting feature importances for both individual trees and the entire random forest. \u003c/p\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch3\u003eDrawbacks\u003c/h3\u003e\n\n\u003cul\u003e\n\u003cli\u003e\u003cp\u003e\u003cstrong\u003e\u003cem\u003eComputational complexity\u003c/em\u003e\u003c/strong\u003e: Like any ensemble method, training multiple models means paying the computational cost of training each model. On large datasets, the runtime can be quite slow compared to other algorithms.\u003c/p\u003e\u003c/li\u003e\n\u003cli\u003e\u003cp\u003e\u003cstrong\u003e\u003cem\u003eMemory usage\u003c/em\u003e\u003c/strong\u003e: Another side effect of the ensembled nature of this algorithm, having multiple models means storing each in memory. Random forests tend to have a larger memory footprint than other models. Whereas a parametric model like a logistic regression just needs to store each of the coefficients, a random forest has to remember every aspect of every tree! It's not uncommon to see random forests that were trained on large datasets have memory footprints in the tens or even hundreds of MB. For data scientists working on modern computers, this isn't typically a problem -- however, there are special cases where the memory footprint can make this an untenable choice -- for instance, an app on a smartphone that uses machine learning may not be able to afford to spend that much disk space on a random forest model!\u003c/p\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch2\u003e(Optional) Random forests White paper\u003c/h2\u003e\n\n\u003cp\u003eThis algorithm was not invented all at once -- there were several iterations by different researchers that built upon each previous idea. However, the version used today is the one created by Leo Breiman and Adele Cutler, who also own the trademark for the name \"random forest\". \u003c/p\u003e\n\n\u003cp\u003eAlthough not strictly necessary for understanding how to use random forests, we highly recommend taking a look at the following resources from Breiman and Cutler if you're interested in really digging into how random forests work:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003e\u003cp\u003e\u003ca href=\"https://www.stat.berkeley.edu/%7Ebreiman/randomforest2001.pdf\"\u003eRandom forests paper\u003c/a\u003e\u003c/p\u003e\u003c/li\u003e\n\u003cli\u003e\u003cp\u003e\u003ca href=\"https://www.stat.berkeley.edu/%7Ebreiman/RandomForests/cc_home.htm\"\u003eRandom forests website\u003c/a\u003e\u003c/p\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch2\u003eSummary\u003c/h2\u003e\n\n\u003cp\u003eIn this lesson, you learned about a random forest, which is a powerful and popular ensemble method that uses decision trees!\u003c/p\u003e\n\u003cfooter class=\"fis-footer\" style=\"visibility: hidden;\"\u003e\u003cdiv class=\"fis-feedback\"\u003e\n\u003ch5\u003eHow do you feel about this lesson?\u003c/h5\u003e\n\u003cimg id=\"thumbs-up\" data-repository=\"dsc-random-forests\" title=\"Thumbs up!\" alt=\"thumbs up\"\u003e\u003cimg id=\"thumbs-down\" data-repository=\"dsc-random-forests\" title=\"Thumbs down!\" alt=\"thumbs down\"\u003e\n\u003c/div\u003e\n\u003ch5\u003eHave specific feedback? \u003ca href=\"https://github.com/learn-co-curriculum/dsc-random-forests/issues/new\"\u003eTell us here!\u003c/a\u003e\n\u003c/h5\u003e\u003c/footer\u003e","frontPage":false},{"exportId":"classifiers-with-bayes","title":"Classifiers with Bayes","type":"WikiPage","content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-classifiers-with-bayes\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-classifiers-with-bayes\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-classifiers-with-bayes/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eNow that you're familiar with Bayes' theorem and foundational concepts of Bayesian statistics, you'll take a look at how to implement some of these ideas for machine learning. Classification tasks can be a natural application of Bayes' theorem since you are looking to predict some label given other information, which can be conceptualized through conditional probability.\u003c/p\u003e\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\u003cp\u003eYou will be able to:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eExplain the assumption that leads to Naive Bayes being \"naive\"\u003c/li\u003e\n\u003cli\u003eExplain how to use the probabilities generated by Naive Bayes to make a classification\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003eNaive Bayes\u003c/h2\u003e\n\u003cp\u003eNaive Bayes algorithms extend Bayes' formula to multiple variables by assuming that these features are independent of one another. This then allows you to estimate an overall probability by multiplying the conditional probabilities for each of the independent features.\u003c/p\u003e\n\u003cp\u003eFor example, extending the previous medical examples of Bayes' theorem, a researcher might examine multiple patient measurements to better predict whether or not an individual has a given disease. Provided that these measurements are independent (and uncorrelated from one another), one can then examine the conditional probability of each of these metrics and apply Bayes' theorem to determine a relative probability of having the disease or not. Combining these probabilities can then give an overall confidence of a patient having the disease given all the information. From this, one can then make a prediction for whether or not you believe an individual has the disease or not based on which probability is higher.\u003c/p\u003e\n\u003cp\u003eMathematically, if \u003cimg src=\"https://render.githubusercontent.com/render/math?math=Y\"\u003e is a class you wish to predict (such as having a disease) and \u003cimg src=\"https://render.githubusercontent.com/render/math?math=X_1,%20X_2,%20...,%20X_n\"\u003e are the various measurements for the given individual or case, then the probability of class \u003cimg src=\"https://render.githubusercontent.com/render/math?math=Y\"\u003e can be written as:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Clarge%20P(Y%7CX_1,%20X_2,%20...,%20X_n)%20=%20%5Cdfrac%7BP(X_1%7CY)%20%5Ccdot%20P(X_2%7CY)%20%5Ccdot%20...%20%5Ccdot%20P(X_n%7CY)%7D%7BP(X_1,%20X_2,%20...,%20X_n)%7DP(Y)\"\u003e\u003c/p\u003e\n\u003cp\u003eAgain, note that multiplying the conditional probabilities is based on the assumption that these probabilities (and their underlying features) are independent -- and it is this assumption that the Naive Bayes algorithm is considered naive, or simple, because this is almost never true. However, Naives Bayes can prove to be quite efficient given the right circumstances, as you will see in the upcoming lessons.\u003c/p\u003e\n\u003cp\u003eIn practice, calculating the denominator, \u003cimg src=\"https://render.githubusercontent.com/render/math?math=P(X_1,%20X_2,%20...,%20X_n)\"\u003e is often impractical or impossible as this exact combination of features may not have been previously observed. However, doing so is often not required. This is because when implementing a classifier, the exact probabilities themselves are not required to generate a prediction. Instead, you must simply answer which option is the most probable. To do this, you would calculate \u003cimg src=\"https://render.githubusercontent.com/render/math?math=P(Y_0)\"\u003e , the probability of not having the disease as well as \u003cimg src=\"https://render.githubusercontent.com/render/math?math=P(Y_1)\"\u003e , the probability of having the disease. Furthermore, since the denominator, \u003cimg src=\"https://render.githubusercontent.com/render/math?math=P(X_1,%20X_2,%20...,%20X_n)\"\u003e , is equal for both \u003cimg src=\"https://render.githubusercontent.com/render/math?math=P(Y_0)\"\u003e and \u003cimg src=\"https://render.githubusercontent.com/render/math?math=P(Y_1)\"\u003e , you can compare the numerators, as these will be proportional to the overall probability. You'll investigate this further as you code some Naive Bayes classification algorithms yourself in the upcoming lessons.\u003c/p\u003e\n\u003ch2\u003eSummary\u003c/h2\u003e\n\u003cp\u003eIn this lesson, you briefly explored how Bayes' theorem can be used to build classification algorithms. In the upcoming lessons and labs you'll investigate particular implementations of Naive Bayes classifiers which differ in how the individual conditional probabilities themselves are constructed. As you will see, Naive Bayes can be extremely effective or trivially useful depending on the context and implementation.\u003c/p\u003e","frontPage":false},{"exportId":"logistic-regression-introduction","title":"Logistic Regression - Introduction","type":"WikiPage","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-logistic-regression-intro\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-logistic-regression-intro/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eIn this you'll be introduced to a new type of machine learning technique: classification! You'll learn about an algorithm called logistic regression as well as different ways that data scientists can evaluate the performance of classification models.\u003c/p\u003e\n\n\u003ch2\u003eLogistic Regression\u003c/h2\u003e\n\n\u003cp\u003eYou're familiar with linear regression to predict continuous values. You're now going to return to regression to look at how it can be used as a classifier instead to determine the likelihood of a given data point being associated with one of two categories.\u003c/p\u003e\n\n\u003cp\u003eWe'll start by introducing the sigmoid function and showing how it can be used to fit a curve that matches a binary classifier (e.g. does someone make over or under $40k a year or are they a good or bad credit risk).\u003c/p\u003e\n\n\u003ch2\u003eEvaluating Classifiers\u003c/h2\u003e\n\n\u003cp\u003eWe'll then look at the practicalities of evaluating logistic regression models based on precision, recall, and accuracy to evaluate other classifiers.\u003c/p\u003e\n\n\u003cp\u003eWe also take a little time to look at how to plot a confusion matrix for a logistic regression classifier and introduce a couple of key concepts for determining the optimal precision-recall trade-off for a given classifier - Receiver Operating Characteristic (ROC) curves and AUC (the Area Under the Curve).\u003c/p\u003e\n\n\u003ch2\u003eClass Imbalance Problems\u003c/h2\u003e\n\n\u003cp\u003eWe then introduce the concept of class imbalance. Imagine a classifier for cancer where only 1 screened individual in 1000 is sick. You could obtain over 99 percent accuracy by just saying everyone is fine, but that wouldn't be a very useful approach. We look at the ideas of class weights and over/undersampling and how they can be used to work with highly imbalanced classes.\u003c/p\u003e\n\n\u003ch2\u003eSummary\u003c/h2\u003e\n\n\u003cp\u003eIt's important to be aware of logistic regression as one of the most basic classifiers that you can use, and many of the concepts around model evaluation will be useful whenever you're trying to solve a classification problem.\u003c/p\u003e\n\u003cfooter class=\"fis-footer\" style=\"visibility: hidden;\"\u003e\u003cdiv class=\"fis-feedback\"\u003e\n\u003ch5\u003eHow do you feel about this lesson?\u003c/h5\u003e\n\u003cimg id=\"thumbs-up\" data-repository=\"dsc-logistic-regression-intro\" title=\"Thumbs up!\" alt=\"thumbs up\"\u003e\u003cimg id=\"thumbs-down\" data-repository=\"dsc-logistic-regression-intro\" title=\"Thumbs down!\" alt=\"thumbs down\"\u003e\n\u003c/div\u003e\n\u003ch5\u003eHave specific feedback? \u003ca href=\"https://github.com/learn-co-curriculum/dsc-logistic-regression-intro/issues/new\"\u003eTell us here!\u003c/a\u003e\n\u003c/h5\u003e\u003c/footer\u003e","frontPage":false},{"exportId":"ensemble-methods","title":"Ensemble Methods","type":"WikiPage","content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-ensemble-methods\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-ensemble-methods\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-ensemble-methods/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eIn this lesson, we'll learn about \u003cstrong\u003e\u003cem\u003eensembles\u003c/em\u003e\u003c/strong\u003e and why they're such an effective technique for supervised learning.\u003c/p\u003e\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\u003cp\u003eYou will be able to:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eExplain what is meant by \"ensemble methods\"\u003c/li\u003e\n\u003cli\u003eExplain the concept of bagging as it applies to ensemble methods\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003eWhat are ensembles?\u003c/h2\u003e\n\u003cp\u003eIn Data Science, the term \u003cstrong\u003e\u003cem\u003eensemble\u003c/em\u003e\u003c/strong\u003e refers to an algorithm that makes use of more than one model to make a prediction. Typically, when people talk about ensembles, they are referring to Supervised Learning, although there has been some ongoing research on using ensembles for unsupervised learning tasks. Ensemble methods are typically more effective when compared with single-model results for supervised learning tasks. Most Kaggle competitions are won using ensemble methods, and \u003ca href=\"https://blogs.sas.com/content/subconsciousmusings/2017/05/18/stacked-ensemble-models-win-data-science-competitions/\"\u003emuch has been written\u003c/a\u003e about why they tend to be so successful for these tasks.\u003c/p\u003e\n\u003ch3\u003eExample\u003c/h3\u003e\n\u003cp\u003eConsider the following scenario -- you are looking to invest in a company, and you want to know if that company's stock will go up or down in the next year. Instead of just asking a single person, you have the following experts available to you:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cstrong\u003e\u003cem\u003eStock Broker\u003c/em\u003e\u003c/strong\u003e: This person makes correct predictions 80% of the time\u003c/li\u003e\n\u003cli\u003e\n\u003cstrong\u003e\u003cem\u003eFinance Professor\u003c/em\u003e\u003c/strong\u003e: This person is correct 65% of the time\u003c/li\u003e\n\u003cli\u003e\n\u003cstrong\u003e\u003cem\u003eInvestment Expert\u003c/em\u003e\u003c/strong\u003e: This person is correct 85% of the time\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eIf we could only take advice from one person, we would pick the Investment Expert, and we can only be 85% sure that they are right.\u003c/p\u003e\n\u003cp\u003eHowever, if we can use all three, we can combine their knowledge to increase our overall accuracy. If they all agree that the stock is a good investment, what is the overall accuracy of the combined prediction?\u003c/p\u003e\n\u003cp\u003eWe can calculate this by multiplying the chances that each of them are wrong together, which is \u003cimg src=\"https://render.githubusercontent.com/render/math?math=0.2%20*%200.35%20*%200.15%20=%200.0105%5C%20error%5C%20rate\"\u003e , which means that our combined accuracy is \u003cimg src=\"https://render.githubusercontent.com/render/math?math=1%20-%200.0105%20=%200.9895\"\u003e , or \u003cstrong\u003e\u003cem\u003e98.95%\u003c/em\u003e\u003c/strong\u003e!\u003c/p\u003e\n\u003cp\u003eObviously, this analogy is a bit of an oversimplification -- we're assuming that each prediction is independent, which is unlikely in the real world since there's likely some overlap between the things each person is using to make their prediction. We also haven't calculated the accuracy percentages for the cases where they disagree. However, the main point of this example is that when we combine predictions, we get better overall results.\u003c/p\u003e\n\u003ch2\u003eResiliency to variance\u003c/h2\u003e\n\u003cp\u003eEnsemble methods are analogous to \"Wisdom of the crowd\". This phrase refers to the phenomenon that the average estimate of all predictions typically outperforms any single prediction by a statistically significant margin -- often, quite a large one. A Finance Professor named Jack Treynor once demonstrated this with the classic jar full of jellybeans. Professor Treynor asked all 850 of his students to guess the number of jellybeans in the jar. When he averaged the guesses, he found that of all the guesses in the class, only one student had guessed a better estimate than the group average.\u003c/p\u003e\n\u003cp\u003eThink back to what you've learned about sampling, inferential statistics, and the Central Limit theorem. The same magic is at work here. Estimators are rarely perfect. When Professor Treynor asked each student to provide an estimate of the number of jellybeans in the jar, he found that the estimates were normally distributed. This is where \"Wisdom of the crowd\" kicks in because we can expect the number of people who underestimate the number of jellybeans in the jar to be roughly equal to the number of people who overestimate the number of jellybeans. So we can safely assume the extra variance above and below the average essentially cancel each other out, leaving our average close to the ground truth value!\u003c/p\u003e\n\u003cp\u003eConsider the top-right example in this graphic that visually demonstrates high variance:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-ensemble-methods/master/images/new_bias-and-variance.png\" width=\"600\"\u003e\u003c/p\u003e\n\u003cp\u003eMost points miss the bullseye, but they are just as likely to miss in any direction. If we averaged all of these points, we would be extremely close to the bullseye! This is a great analogy for how ensemble methods work so well -- we know that no model is likely to make perfect estimates, so we have many of them make predictions, and average them, knowing that the overestimates and the underestimates will likely cancel out to be very close to the ground truth. The idea that the overestimates and underestimates will (at least partially) cancel each other out is sometimes referred to as \u003cstrong\u003e\u003cem\u003esmoothing\u003c/em\u003e\u003c/strong\u003e.\u003c/p\u003e\n\u003ch3\u003eWhich models are used in ensembles?\u003c/h3\u003e\n\u003cp\u003eFor this section, we'll be focusing exclusively on tree-based ensemble methods, such as \u003cstrong\u003e\u003cem\u003eRandom forests\u003c/em\u003e\u003c/strong\u003e and \u003cstrong\u003e\u003cem\u003eGradient boosted trees\u003c/em\u003e\u003c/strong\u003e. However, we can technically use any models in an ensemble! It's not uncommon to see \u003cstrong\u003e\u003cem\u003eModel stacking\u003c/em\u003e\u003c/strong\u003e, also called \u003cstrong\u003e\u003cem\u003eMeta-ensembling\u003c/em\u003e\u003c/strong\u003e, where multiple different models are stacked, and their predictions are aggregated. In this case, the more different the models are, the better! This is because the more different the models are, the more likely they have the potential to pick up on different characteristics of the data. It's not uncommon to see ensembles consisting of multiple logistic regressions, Naive Bayes classifiers, Tree-based models (including ensembles such as random forests), and even deep neural networks!\u003c/p\u003e\n\u003cp\u003eFor a much more in-depth explanation of what model stacking looks like and why it is effective, take a look at this great \u003ca href=\"http://blog.kaggle.com/2016/12/27/a-kagglers-guide-to-model-stacking-in-practice/\"\u003earticle from Kaggle's blog, No Free Hunch!\u003c/a\u003e\u003c/p\u003e\n\u003ch2\u003eBootstrap aggregation\u003c/h2\u003e\n\u003cp\u003eThe main concept that makes ensembling possible is \u003cstrong\u003e\u003cem\u003eBagging\u003c/em\u003e\u003c/strong\u003e, which is short for \u003cstrong\u003e\u003cem\u003eBootstrap Aggregation\u003c/em\u003e\u003c/strong\u003e. Bootstrap aggregation is itself a combination of two ideas -- bootstrap resampling and aggregation. You're already familiar with bootstrap resampling from our section on the Central Limit theorem. It refers to the subsets of your dataset by sampling with replacement, much as we did to calculate our sample means when working with the Central Limit theorem. Aggregation is exactly as it sounds -- the practice of combining all the different estimates to arrive at a single estimate -- although the specifics for how we combine them are up to us. A common approach is to treat each classifier in the ensemble's prediction as a \"vote\" and let our overall prediction be the majority vote. It's also common to see ensembles that take the arithmetic mean of all predictions, or compute a weighted average.\u003c/p\u003e\n\u003cp\u003eThe process for training an ensemble through bootstrap aggregation is as follows:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eGrab a sizable sample from your dataset, with replacement\u003c/li\u003e\n\u003cli\u003eTrain a classifier on this sample\u003c/li\u003e\n\u003cli\u003eRepeat until all classifiers have been trained on their own sample from the dataset\u003c/li\u003e\n\u003cli\u003eWhen making a prediction, have each classifier in the ensemble make a prediction\u003c/li\u003e\n\u003cli\u003eAggregate all predictions from all classifiers into a single prediction, using the method of your choice\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-ensemble-methods/master/images/new_bagging.png\"\u003e\u003c/p\u003e\n\u003cp\u003eDecision trees are often used because they are very sensitive to variance. On their own, this is a weakness. However, when aggregated together into an ensemble, this actually becomes a good thing!\u003c/p\u003e\n\u003ch2\u003eSummary\u003c/h2\u003e\n\u003cp\u003eIn this lesson, we learned about what constitutes an \u003cstrong\u003eEnsemble\u003c/strong\u003e, and how \u003cstrong\u003eBagging\u003c/strong\u003e plays a central role in this. In the next lesson, we'll see how bagging is combined with another important technique to create one of the most effective ensemble algorithms available today -- \u003cstrong\u003e\u003cem\u003eRandom forests\u003c/em\u003e\u003c/strong\u003e!\u003c/p\u003e","frontPage":false},{"exportId":"k-nearest-neighbors-introduction","title":"K-Nearest Neighbors - Introduction","type":"WikiPage","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-knn-intro\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-knn-intro/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eIn this section you'll look at an intuitive algorithm known as K-Nearest Neighbors (KNN). KNN is an effective classification and regression algorithm that uses nearby points in order to generate a prediction. \u003c/p\u003e\n\n\u003ch2\u003eKNN\u003c/h2\u003e\n\n\u003cp\u003eThe K-Nearest Neighbors algorithm works as follows: \u003c/p\u003e\n\n\u003col\u003e\n\u003cli\u003eChoose a point \u003c/li\u003e\n\u003cli\u003eFind the K-nearest points\n\n\u003col\u003e\n\u003cli\u003eK is a predefined user constant such as 1, 3, 5, or 11 \u003c/li\u003e\n\u003c/ol\u003e\n\u003c/li\u003e\n\u003cli\u003ePredict a label for the current point:\n\n\u003col\u003e\n\u003cli\u003eClassification - Take the most common class of the k neighbors\u003c/li\u003e\n\u003cli\u003eRegression - Take the average target metric of the k neighbors\u003c/li\u003e\n\u003cli\u003eBoth classification or regression can also be modified to use weighted averages based on the distance of the neighbors \u003c/li\u003e\n\u003c/ol\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\n\u003ch2\u003eDistance metrics\u003c/h2\u003e\n\n\u003cp\u003eAn incredibly important decision when using the KNN algorithm is determining an appropriate distance metric. This makes a monumental impact to the output of the algorithm. While there are additional distance metrics, such as cosine distance which we will not cover, you'll get a solid introduction to distance metrics by looking at the standard Euclidean distance and its more generic counterpart, Minkowski distance.\u003c/p\u003e\n\n\u003ch2\u003eK-means\u003c/h2\u003e\n\n\u003cp\u003eWhile outside the scope of this section, it is worth mentioning the related K-means algorithm which uses similar principles as KNN but serves as an unsupervised learning clustering algorithm. In the K-means algorithm, K represents the number of clusters rather then the number of neighbors. Unlike KNN, K-means is an iterative algorithm which repeats until convergence. Nonetheless, its underlying principle is the same, in that it groups data points together using a distance metric in order to create homogeneous groupings.\u003c/p\u003e\n\n\u003ch2\u003eSummary\u003c/h2\u003e\n\n\u003cp\u003eIn this brief lesson, you were introduced to the KNN algorithm. From here, you'll jump straight to the details of KNN, practice coding your own implementation and then get an introduction to use pre-built tools within scikit-learn for KNN.\u003c/p\u003e\n\u003cfooter class=\"fis-footer\" style=\"visibility: hidden;\"\u003e\u003cdiv class=\"fis-feedback\"\u003e\n\u003ch5\u003eHow do you feel about this lesson?\u003c/h5\u003e\n\u003cimg id=\"thumbs-up\" data-repository=\"dsc-knn-intro\" title=\"Thumbs up!\" alt=\"thumbs up\"\u003e\u003cimg id=\"thumbs-down\" data-repository=\"dsc-knn-intro\" title=\"Thumbs down!\" alt=\"thumbs down\"\u003e\n\u003c/div\u003e\n\u003ch5\u003eHave specific feedback? \u003ca href=\"https://github.com/learn-co-curriculum/dsc-knn-intro/issues/new\"\u003eTell us here!\u003c/a\u003e\n\u003c/h5\u003e\u003c/footer\u003e","frontPage":false},{"exportId":"introduction-to-support-vector-machines","title":"Introduction to Support Vector Machines","type":"WikiPage","content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-introduction-to-support-vector-machines\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-introduction-to-support-vector-machines\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-introduction-to-support-vector-machines/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eBy now you've learned a few techniques for classification. You touched upon it when talking about Naive Bayes, and again when you saw some supervised learning techniques such as logistic regression and decision trees. Now it's time for another popular classification technique -- Support Vector Machines.\u003c/p\u003e\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\u003cp\u003eYou will be able to:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eDescribe what is meant by margin classifiers\u003c/li\u003e\n\u003cli\u003eDescribe the mathematical components underlying soft and max-margin classifiers\u003c/li\u003e\n\u003cli\u003eCompare and contrast max-margin classifiers and soft-margin classifiers\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003eThe idea\u003c/h2\u003e\n\u003cp\u003eThe idea behind Support Vector Machines (also referred to as SVMs) is that you perform classification by finding the separation line or (in higher dimensions) \"hyperplane\" that maximizes the distance between two classes. Taking a look at the concept visually helps make sense of the process.\u003c/p\u003e\n\u003cp\u003eImagine you have a dataset containing two classes:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-introduction-to-support-vector-machines/master/images/new_SVM_1.png\" width=\"400\"\u003e\u003c/p\u003e\n\u003cp\u003eIn SVM, you want to find a hyperplane or \"decision boundary\" that divides one class from the other. Which one works best?\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-introduction-to-support-vector-machines/master/images/new_SVM_3.png\" width=\"400\"\u003e\u003c/p\u003e\n\u003cp\u003eThis would be a good line:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-introduction-to-support-vector-machines/master/images/new_SVM_2.png\" width=\"400\"\u003e\u003c/p\u003e\n\u003cp\u003eWhile this seems intuitive, there are other decision boundaries which also separate the classes. Which one is best? Rather than solely focus on the final accuracy of the model, Support Vector Machines aim to \u003cstrong\u003emaximize the margin\u003c/strong\u003e between the decision boundary and the various data points.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-introduction-to-support-vector-machines/master/images/new_SVM_4.png\" width=\"400\"\u003e\u003c/p\u003e\n\u003cp\u003eThe margin is defined as the distance between the separating line (hyperplane) and the training set cases that are closest to this hyperplane. These cases define \"support vectors\". The support vectors in this particular case are highlighted in the image below. As you can see, the max-margin hyperplane is the midpoint between the two lines defined by the support vectors.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-introduction-to-support-vector-machines/master/images/new_SVM_fin.png\" width=\"400\"\u003e\u003c/p\u003e\n\u003ch2\u003eThe Max Margin classifier\u003c/h2\u003e\n\u003cp\u003eWhy would you bother maximizing the margins? Don't these other hyperplanes discriminate just as well? Remember that you are fitting the hyperplane on your training data. Imagine you start looking at your test data, which will slightly differ from your training data.\u003c/p\u003e\n\u003cp\u003eAssuming your test set is big enough and randomly drawn from your entire dataset, you might end up with a test case as shown in the image below. This test case diverts a little bit from the training set cases observed earlier. While the max-margin classifier would classify this test set case correctly, the hyperplane closer to the right would have been classified this case incorrectly. Of course, this is just one example and other test cases will end up in different spots. Nonetheless, the purpose of choosing the max-margin classifier is to minimize the generalization error when applying the model to future unseen data points.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-introduction-to-support-vector-machines/master/images/new_SVM_test2.png\" width=\"400\"\u003e\u003c/p\u003e\n\u003cp\u003eBefore diving into the underlying mathematics, take a look at the image again:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-introduction-to-support-vector-machines/master/images/new_SVM_fin.png\" width=\"400\"\u003e\u003c/p\u003e\n\u003cp\u003eNow you can start exploring the mathematics behind the image. First, define some numeric labels for the two classes. Set the circles to be -1 and the diamonds to be 1. Normally, 0 and 1 are used for class labels but in this particular case using -1 and 1 simplifies the mathematics.\u003c/p\u003e\n\u003cp\u003eNow some terminology: The lines defined by the support vectors are the negative (to the left) and the positive (to the right) hyperplanes, respectively. These hyperplanes are defined by two terms: \u003cimg src=\"https://render.githubusercontent.com/render/math?math=w_T\"\u003e and \u003cimg src=\"https://render.githubusercontent.com/render/math?math=b\"\u003e .\u003c/p\u003e\n\u003cp\u003eThe \u003cimg src=\"https://render.githubusercontent.com/render/math?math=w_T\"\u003e term is called the \u003cstrong\u003eweight vector\u003c/strong\u003e and contains the weights that are used in the classification.\u003c/p\u003e\n\u003cp\u003eThe \u003cimg src=\"https://render.githubusercontent.com/render/math?math=b\"\u003e term is called the \u003cstrong\u003ebias\u003c/strong\u003e and functions as an offset term. If there were no bias term, the hyperplane would always go through the origin which would not be very generalizable!\u003c/p\u003e\n\u003cp\u003eThe equation describing the positive hyperplane is: \u003cimg src=\"https://render.githubusercontent.com/render/math?math=b%20%2b%20w_Tx_%7Bpos%7D%20=1\"\u003e\u003c/p\u003e\n\u003cp\u003eand the equation describing the negative hyperplane is: \u003cimg src=\"https://render.githubusercontent.com/render/math?math=b%20%2b%20w_Tx_%7Bneg%7D%20=-1\"\u003e\u003c/p\u003e\n\u003cp\u003eRemember, your goal is to maximize the separation between the two hyperplanes. To do this, first subtract the negative hyperplane's equation from the positive hyperplane's equation:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=w_T(x_%7Bpos%7D-x_%7Bneg%7D)%20=%202\"\u003e\u003c/p\u003e\n\u003cp\u003eNext, normalize \u003cimg src=\"https://render.githubusercontent.com/render/math?math=w_T\"\u003e by dividing both sides of the equation by its norm, \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%7C%7Cw%7C%7C\"\u003e :\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%7C%7C%20w%20%7C%7C=%20%5Csqrt%7B%5Csum%5Em_%7Bj-1%7Dw_j%5E2%7D\"\u003e\u003c/p\u003e\n\u003cp\u003eDividing the former expression by \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%7C%7Cw%7C%7C\"\u003e yields the equation below. The left side of the resulting equation can be interpreted as the distance between the positive and negative hyperplanes. This is the \u003cstrong\u003emargin\u003c/strong\u003e you're trying to maximize.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cdfrac%7Bw_T(x_%7Bpos%7D-x_%7Bneg%7D)%7D%7B%5ClVert%20w%20%5CrVert%7D%20=%20%5Cdfrac%7B2%7D%7B%5ClVert%20w%20%5CrVert%7D\"\u003e\u003c/p\u003e\n\u003cp\u003eThe objective of the SVM is then maximizing \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cdfrac%7B2%7D%7B%5ClVert%20w%20%5CrVert%7D\"\u003e under the constraint that the samples are classified correctly. Mathematically,\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=b%20%2b%20w_Tx%5E%7B(i)%7D%20%5Cgeq%201\"\u003e if \u003cimg src=\"https://render.githubusercontent.com/render/math?math=y%20%5E%7B(i)%7D%20=%201\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=b%20%2b%20w_Tx%5E%7B(i)%7D%20%5Cleq%20-1\"\u003e if \u003cimg src=\"https://render.githubusercontent.com/render/math?math=y%20%5E%7B(i)%7D%20=%20-1\"\u003e\u003c/p\u003e\n\u003cp\u003eFor \u003cimg src=\"https://render.githubusercontent.com/render/math?math=i=%201,%5Cldots%20,N\"\u003e\u003c/p\u003e\n\u003cp\u003eThese equations basically say that all negative samples should fall on the left side of the negative hyperplane, whereas all the positive samples should fall on the right of the positive hyperplane. This can also be written in one line as follows:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=y%20%5E%7B(i)%7D%20(b%20%2b%20w_Tx%5E%7B(i)%7D%20)%5Cgeq%201\"\u003e for each \u003cimg src=\"https://render.githubusercontent.com/render/math?math=i\"\u003e\u003c/p\u003e\n\u003cp\u003eNote that maximizing \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cdfrac%7B2%7D%7B%5ClVert%20w%20%5CrVert%7D\"\u003e means we're minimizing \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5ClVert%20w%20%5CrVert\"\u003e , or, as is done in practice because it seems to be easier to be minimized, \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cdfrac%7B1%7D%7B2%7D%5ClVert%20w%20%5CrVert%5E2\"\u003e .\u003c/p\u003e\n\u003ch2\u003eThe Soft Margin classifier\u003c/h2\u003e\n\u003cp\u003eIntroducing slack variables \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cxi\"\u003e . The idea for introducing slack variables is that the linear constraints need to be relaxed for data that are not linearly separable, as not relaxing the constraints might lead to the algorithm that doesn't converge.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=b%20%2b%20w_Tx%5E%7B(i)%7D%20%5Cgeq%201-%5Cxi%5E%7B(i)%7D\"\u003e if \u003cimg src=\"https://render.githubusercontent.com/render/math?math=y%20%5E%7B(i)%7D%20=%201\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=b%20%2b%20w_Tx%5E%7B(i)%7D%20%5Cleq%20-1%2b%5Cxi%5E%7B(i)%7D\"\u003e if \u003cimg src=\"https://render.githubusercontent.com/render/math?math=y%20%5E%7B(i)%7D%20=%20-1\"\u003e\u003c/p\u003e\n\u003cp\u003eFor \u003cimg src=\"https://render.githubusercontent.com/render/math?math=i=%201,%5Cldots%20,N\"\u003e\u003c/p\u003e\n\u003cp\u003eThe objective function (AKA the function you want to minimize) is\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cdfrac%7B1%7D%7B2%7D%5ClVert%20w%20%5CrVert%5E2%2b%20C(%5Csum_i%20%5Cxi%5E%7B(i)%7D)\"\u003e\u003c/p\u003e\n\u003cp\u003eYou're basically adding these slack variables in your objective function, making clear that you want to minimize the amount of slack you allow for. You can tune this with \u003cimg src=\"https://render.githubusercontent.com/render/math?math=C\"\u003e as shown in the above equation. \u003cimg src=\"https://render.githubusercontent.com/render/math?math=C\"\u003e will define how much slack we're allowing.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eA big value for \u003cimg src=\"https://render.githubusercontent.com/render/math?math=C\"\u003e will lead to the picture on the left: misclassifications are heavily punished, so the optimization prioritizes classifying correctly over having a big margin.\u003c/li\u003e\n\u003cli\u003eA small value for \u003cimg src=\"https://render.githubusercontent.com/render/math?math=C\"\u003e will lead to the picture on the right: it is OK to have some misclassifications, in order to gain a bigger margin overall. (This can help avoid overfitting to the training data.)\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-introduction-to-support-vector-machines/master/images/new_SVM_C.png\"\u003e\u003c/p\u003e\n\u003ch2\u003eSummary\u003c/h2\u003e\n\u003cp\u003eGreat! You now understand both max-margin classifiers as well as soft-margin classifiers. In the next lab, you'll try to code these fairly straightforward linear classifiers from scratch!\u003c/p\u003e","frontPage":false},{"exportId":"knn-with-scikit-learn","title":"KNN with scikit-learn","type":"WikiPage","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-knn-with-scikit-learn\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-knn-with-scikit-learn/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eIn this lesson, you'll explore how to use scikit-learn's implementation of the K-Nearest Neighbors algorithm. In addition, you'll also learn about best practices for using the algorithm. \u003c/p\u003e\n\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\n\u003cp\u003eYou will be able to:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eList the considerations when fitting a KNN model using scikit-learn\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch2\u003eWhy use scikit-learn?\u003c/h2\u003e\n\n\u003cp\u003eWhile you've written your own implementation of the KNN algorithm, scikit-learn adds many backend optimizations which can make the algorithm perform faster and more efficiently. Building your own implementation of any machine learning algorithm is a valuable experience, providing great insight into how said algorithm works. However, in general, you should always use professional toolsets such as scikit-learn whenever possible; since their implementations will always be best-in-class, in a way a single developer or data scientist simply can't hope to rival on their own. In the case of KNN, you'll find scikit-learn's implementation to be much more robust and fast, because of optimizations such as caching distances in clever ways under the hood. \u003c/p\u003e\n\n\u003ch2\u003eRead the \u003ccode\u003esklearn\u003c/code\u003e docs\u003c/h2\u003e\n\n\u003cp\u003eAs a rule of thumb, you should familiarize yourself with any documentation available for any libraries or frameworks you use. scikit-learn provides high-quality documentation. For every algorithm, you'll find a general \u003ca href=\"https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html\"\u003edocumentation page\u003c/a\u003e which tells you inputs, parameters, outputs, and caveats of any algorithm. In addition, you'll also find very informative \u003ca href=\"https://scikit-learn.org/stable/modules/neighbors.html#classification\"\u003eUser Guides\u003c/a\u003e that explain both how the algorithm works, and how to best use it, complete with sample code! \u003c/p\u003e\n\n\u003cp\u003eFor example, the following image can be found in the scikit-learn user guide for K-Nearest Neighbors, along with an explanation of how different parameters can affect the overall performance of the model. \u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-knn-with-scikit-learn/master/images/knn_docs.png\"\u003e\u003c/p\u003e\n\n\u003ch2\u003eBest practices\u003c/h2\u003e\n\n\u003cp\u003eYou'll also find that scikit-learn provides robust implementations for additional components of the algorithm implementation process such as evaluation metrics. With that, you can easily evaluate models using precision, accuracy, or recall scores on the fly using built-in functions!\u003c/p\u003e\n\n\u003cp\u003eWith that, it's important to focus on practical questions when completing the upcoming lab. In particular, try to focus on the following questions:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003e\u003cp\u003eWhat decisions do I need to make regarding my data? How might these decisions affect overall performance?\u003c/p\u003e\u003c/li\u003e\n\u003cli\u003e\u003cp\u003eWhich predictors do I need? How can I confirm that I have the right predictors?\u003c/p\u003e\u003c/li\u003e\n\u003cli\u003e\u003cp\u003eWhat parameter values (if any) should I choose for my model? How can I find the optimal value for a given parameter?\u003c/p\u003e\u003c/li\u003e\n\u003cli\u003e\u003cp\u003eWhat metrics will I use to evaluate the performance of my model? Why?\u003c/p\u003e\u003c/li\u003e\n\u003cli\u003e\u003cp\u003eHow do I know if there's room left for improvement with my model? Are the potential performance gains worth the time needed to reach them?\u003c/p\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch2\u003eA final note\u003c/h2\u003e\n\n\u003cp\u003eAfter cleaning, preprocessing, and modeling the data in the next lab, you'll be given the opportunity to iterate on your model. \u003c/p\u003e\n\n\u003ch2\u003eSummary\u003c/h2\u003e\n\n\u003cp\u003eIn this section, you got a brief overview of some of the advantages of using scikit-learn's built-in KNN implementation. While you haven't seen specific code examples, you now have an indispensable resource: the official documentation to guide you. Since it's an incredibly important skill to know where to seek out information and how to digest that into actionable processes, it'll be up to you to piece through the necessary documentation to complete the upcoming lab. Good luck!\u003c/p\u003e\n\u003cfooter class=\"fis-footer\" style=\"visibility: hidden;\"\u003e\u003cdiv class=\"fis-feedback\"\u003e\n\u003ch5\u003eHow do you feel about this lesson?\u003c/h5\u003e\n\u003cimg id=\"thumbs-up\" data-repository=\"dsc-knn-with-scikit-learn\" title=\"Thumbs up!\" alt=\"thumbs up\"\u003e\u003cimg id=\"thumbs-down\" data-repository=\"dsc-knn-with-scikit-learn\" title=\"Thumbs down!\" alt=\"thumbs down\"\u003e\n\u003c/div\u003e\n\u003ch5\u003eHave specific feedback? \u003ca href=\"https://github.com/learn-co-curriculum/dsc-knn-with-scikit-learn/issues/new\"\u003eTell us here!\u003c/a\u003e\n\u003c/h5\u003e\u003c/footer\u003e","frontPage":false},{"exportId":"gradient-descent-review","title":"Gradient Descent Review","type":"WikiPage","content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-gradient-descent-review\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-gradient-descent-review\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-gradient-descent-review/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eRecall that gradient descent is a numerical approximation method for finding optimized solutions to problems with no closed form. That is, some mathematical problems are very easy to solve analytically.\u003c/p\u003e\n\u003cp\u003eTrivial examples include basic algebra problems which you undoubtedly saw in grade school: \u003cimg src=\"https://render.githubusercontent.com/render/math?math=x%20%2b%202%20=%2010\"\u003e  subtracting 2 from both sides you get \u003cimg src=\"https://render.githubusercontent.com/render/math?math=x%20=%208\"\u003e . Similarly, some more complex mathematical problems such as ordinary least squares, our preliminary regression approach, also have closed-form solutions where we can follow a rote procedure and be guaranteed a solution.\u003c/p\u003e\n\u003cp\u003eIn other cases, this is not possible and numerical approximation methods are used to find a solution. The first instance that you witnessed of this was adding the L1 and L2 (lasso and ridge, respectively) penalties to OLS regression. In these cases, numerical approximation methods, such as gradient descent, are used in order to find optimal or near-optimal solutions.\u003c/p\u003e\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\u003cp\u003eYou will be able to:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eDescribe the elements of gradient descent in the context of a logistic regression\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003eGradient descent\u003c/h2\u003e\n\u003cp\u003eGradient descent is grounded in basic calculus theory. Whenever you have a minimum or maximum, the derivative at that point is equal to zero. This is displayed visually in the picture below; the slope of the red tangent lines is equal to the derivative of the curve at that point. As you can see, the slope of all of these horizontal tangent lines will be zero.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-gradient-descent-review/master/images/new_dxdy0.png\" width=\"400\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eThe gradient is simply another term for the derivative. Typically, this is the term used when we are dealing with multivariate data. The gradient is the rate of change, which is also the slope of the line tangent.\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eBuilding upon this, gradient descent attempts to find the minimum of a function by taking successive steps in the steepest direction downhill.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-gradient-descent-review/master/images/new_gradient.png\"\u003e\u003c/p\u003e\n\u003cp\u003eWhile this process guarantees a local minimum, the starting point and step size can affect the outcome. For example, for two different runs of gradient descent, one may lead to the global minimum while the other may lead to a local minimum.\u003c/p\u003e\n\u003cp\u003eRecall that the general outline for gradient descent is:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eDefine initial parameters:\n\u003col\u003e\n\u003cli\u003ePick a starting point\u003c/li\u003e\n\u003cli\u003ePick a step size \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Calpha\"\u003e (alpha)\u003c/li\u003e\n\u003cli\u003eChoose a maximum number of iterations; the algorithm will terminate after this many iterations if a minimum has yet to be found\u003c/li\u003e\n\u003cli\u003e(optionally) define a precision parameter; similar to the maximum number of iterations, this will terminate the algorithm early. For example, one might define a precision parameter of 0.00001, in which case if the change in the loss function were less then 0.00001, the algorithm would terminate. The idea is that we are very close to the bottom and further iterations would make a negligible difference\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/li\u003e\n\u003cli\u003eCalculate the gradient at the current point (initially, the starting point)\u003c/li\u003e\n\u003cli\u003eTake a step (of size alpha) in the direction of the gradient\u003c/li\u003e\n\u003cli\u003eRepeat steps 2 and 3 until the maximum number of iterations is met, or the difference between two points is less then your precision parameter\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2\u003eSummary\u003c/h2\u003e\n\u003cp\u003eIn this lesson, you briefly reviewed that a gradient is the derivative of a function, which is the rate of change at a specific point. You then reviewed the intuition behind gradient descent, as well as some of its pitfalls. Finally, you saw a brief outline of the algorithm itself. In the next lab, you'll practice coding gradient descent and applying that to some simple mathematical functions.\u003c/p\u003e","frontPage":false},{"exportId":"evaluation-metrics","title":"Evaluation Metrics","type":"WikiPage","content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-evaluation-metrics\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-evaluation-metrics\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-evaluation-metrics/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eIn this lesson, you'll learn about common \u003cstrong\u003e\u003cem\u003eEvaluation Metrics\u003c/em\u003e\u003c/strong\u003e used to quantify the performance of classifiers!\u003c/p\u003e\n\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\n\u003cp\u003eYou will be able to:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eEvaluate classification models using the evaluation metrics appropriate for a specific problem \u003c/li\u003e\n\u003cli\u003eDefine precision and recall \u003c/li\u003e\n\u003cli\u003eDefine accuracy and F1 score \u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch2\u003eEvaluation metrics for classification\u003c/h2\u003e\n\n\u003cp\u003eNow that we've started discussing classification, it's time to examine comparing models to one other and choosing the models that have the best fit. Previously in regression, you were predicting values so it made sense to discuss error as a distance of how far off the estimates were from the actual values. However, in classifying a binary variable you are either correct or incorrect. As a result, we tend to deconstruct this as how many false positives versus false negatives there are in a model. In particular, there are a few different specific measurements when evaluating the performance of a classification algorithm.  \u003c/p\u003e\n\n\u003cp\u003eLet's work through these evaluation metrics to understand what each metric tells us.\u003c/p\u003e\n\n\u003ch2\u003ePrecision and recall\u003c/h2\u003e\n\n\u003cp\u003e\u003cstrong\u003e\u003cem\u003ePrecision\u003c/em\u003e\u003c/strong\u003e and \u003cstrong\u003e\u003cem\u003eRecall\u003c/em\u003e\u003c/strong\u003e are two of the most basic evaluation metrics available to us. \u003cstrong\u003e\u003cem\u003ePrecision\u003c/em\u003e\u003c/strong\u003e measures how precise the predictions are, while \u003cstrong\u003e\u003cem\u003eRecall\u003c/em\u003e\u003c/strong\u003e indicates what percentage of the classes we're interested in were actually captured by the model. \u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-evaluation-metrics/master/./images/new_EvalMatrices.png\" width=\"600\"\u003e\u003c/p\u003e\n\n\u003ch3\u003ePrecision\u003c/h3\u003e\n\n\u003cp\u003eThe following formula shows how to use information found in a confusion matrix to calculate the precision of a model:\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Ctext%7BPrecision%7D%20=%20%5Cfrac%7B%5Ctext%7BNumber%20of%20True%20Positives%7D%7D%7B%5Ctext%7BNumber%20of%20Predicted%20Positives%7D%7D\"\u003e \u003c/p\u003e\n\n\u003cp\u003eTo reuse a previous analogy of a model that predicts whether or not a person has a certain disease, precision allows us to answer the following question:\u003c/p\u003e\n\n\u003cp\u003e\"Out of all the times the model said someone had a disease, how many times did the patient in question actually have the disease?\"\u003c/p\u003e\n\n\u003cp\u003eNote that a high precision score can be a bit misleading.  For instance, let's say we take a model and train it to make predictions on a sample of 10,000 patients. This model predicts that 6000 patients have the disease when in reality, only 5500 have the disease.  This model would have a precision of 91.6%. Now, let's assume we create a second model that only predicts that a person is sick when it's incredibly obvious.  Out of 10,000 patients, this model only predicts that 5 people in the entire population are sick.  However, each of those 5 times, it is correct.  model 2 would have a precision score of 100%, even though it missed 5,495 cases where the patient actually had the disease! In this way, more conservative models can have a high precision score, but this doesn't necessarily mean that they are the \u003cem\u003ebest performing\u003c/em\u003e model!\u003c/p\u003e\n\n\u003ch3\u003eRecall\u003c/h3\u003e\n\n\u003cp\u003eThe following formula shows how we can use the information found in a confusion matrix to calculate the recall of a model:\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Ctext%7BRecall%7D%20=%20%5Cfrac%7B%5Ctext%7BNumber%20of%20True%20Positives%7D%7D%7B%5Ctext%7BNumber%20of%20Actual%20Total%20Positives%7D%7D\"\u003e \u003c/p\u003e\n\n\u003cp\u003eFollowing the same disease analogy, recall allows us to ask:\u003c/p\u003e\n\n\u003cp\u003e\"Out of all the patients we saw that actually had the disease, what percentage of them did our model correctly identify as having the disease?\"\u003c/p\u003e\n\n\u003cp\u003eNote that recall can be a bit of a tricky statistic because improving our recall score doesn't necessarily always mean a better model overall. For example, our model could easily score 100% for recall by just classifying every single patient that walks through the door as having the disease in question. Sure, it would have many False Positives, but it would also correctly identify every single sick person as having the disease!\u003c/p\u003e\n\n\u003ch3\u003eThe relationship between precision and recall\u003c/h3\u003e\n\n\u003cp\u003eAs you may have guessed, precision and recall have an inverse relationship. As our recall goes up, our precision will go down, and vice versa. If this doesn't seem intuitive, let's examine this through the lens of our disease analogy. \u003c/p\u003e\n\n\u003cp\u003eA doctor that is overly obsessed with recall will have a very low threshold for declaring someone as sick because they are most worried about sick patients. Their precision will be quite low, because they classify almost everyone as sick, and don't care when they're wrong -- they only care about making sure that sick people are identified as sick. \u003c/p\u003e\n\n\u003cp\u003eA doctor that is overly obsessed with precision will have a very high threshold for declaring someone as sick, because they only declare someone as sick when they are completely sure that they will be correct if they declare a person as sick. Although their precision will be very high, their recall will be incredibly low, because a lot of people that are sick but don't meet the doctor's threshold will be incorrectly classified as healthy. \u003c/p\u003e\n\n\u003ch3\u003eWhich metric is better?\u003c/h3\u003e\n\n\u003cp\u003eA classic Data Science interview question is to ask \"What is better -- more false positives, or false negatives?\" This is a trick question designed to test your critical thinking on the topics of precision and recall. As you're probably thinking, the answer is \"It depends on the problem!\".  Sometimes, our model may be focused on a problem where False Positives are much worse than False Negatives, or vice versa. For instance, detecting credit card fraud. A False Positive would be when our model flags a transaction as fraudulent, and it isn't.  This results in a slightly annoyed customer. On the other hand, a False Negative might be a fraudulent transaction that the company mistakenly lets through as normal consumer behavior. In this case, the credit card company could be on the hook for reimbursing the customer for thousands of dollars because they missed the signs that the transaction was fraudulent! Although being wrong is never ideal, it makes sense that credit card companies tend to build their models to be a bit too sensitive, because having a high recall saves them more money than having a high precision score.\u003c/p\u003e\n\n\u003cp\u003eTake a few minutes and see if you can think of at least two examples each of situations where a high precision might be preferable to high recall, and two examples where high recall might be preferable to high precision. This is a common interview topic, so it's always handy to have a few examples ready!\u003c/p\u003e\n\n\u003ch2\u003eAccuracy and F1 score\u003c/h2\u003e\n\n\u003cp\u003eThe two most informative metrics that are often cited to describe the performance of a model are \u003cstrong\u003e\u003cem\u003eAccuracy\u003c/em\u003e\u003c/strong\u003e and \u003cstrong\u003e\u003cem\u003eF1 score\u003c/em\u003e\u003c/strong\u003e. Let's take a look at each and see what's so special about them.\u003c/p\u003e\n\n\u003ch3\u003eAccuracy\u003c/h3\u003e\n\n\u003cp\u003e\u003cstrong\u003e\u003cem\u003eAccuracy\u003c/em\u003e\u003c/strong\u003e is probably the most intuitive metric. The formula for accuracy is:\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Ctext%7BAccuracy%7D%20=%20%5Cfrac%7B%5Ctext%7BNumber%20of%20True%20Positives%20%2b%20True%20Negatives%7D%7D%7B%5Ctext%7BTotal%20Observations%7D%7D\"\u003e \u003c/p\u003e\n\n\u003cp\u003eAccuracy is useful because it allows us to measure the total number of predictions a model gets right, including both \u003cstrong\u003e\u003cem\u003eTrue Positives\u003c/em\u003e\u003c/strong\u003e and \u003cstrong\u003e\u003cem\u003eTrue Negatives\u003c/em\u003e\u003c/strong\u003e. \u003c/p\u003e\n\n\u003cp\u003eSticking with our analogy, accuracy allows us to answer:\u003c/p\u003e\n\n\u003cp\u003e\"Out of all the predictions our model made, what percentage were correct?\"\u003c/p\u003e\n\n\u003cp\u003eAccuracy is the most common metric for classification. It provides a solid holistic view of the overall performance of our model. \u003c/p\u003e\n\n\u003ch3\u003eF1 score\u003c/h3\u003e\n\n\u003cp\u003eThe F1 score is a bit more tricky, but also more informative. F1 score represents the \u003cstrong\u003e\u003cem\u003eHarmonic Mean of Precision and Recall\u003c/em\u003e\u003c/strong\u003e.  In short, this means that the F1 score cannot be high without both precision and recall also being high. When a model's F1 score is high, you know that your model is doing well all around. \u003c/p\u003e\n\n\u003cp\u003eThe formula for F1 score is:\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Ctext%7BF1%20score%7D%20=%202%5C%20%5Cfrac%7BPrecision%5C%20x%5C%20Recall%7D%7BPrecision%20%2b%20Recall%7D\"\u003e \u003c/p\u003e\n\n\u003cp\u003eTo demonstrate the effectiveness of F1 score, let's plug in some numbers and compare F1 score with a regular arithmetic average of precision and recall. \u003c/p\u003e\n\n\u003cp\u003eLet's assume that the model has 98% recall and 6% precision.  \u003c/p\u003e\n\n\u003cp\u003eTaking the arithmetic mean of the two, we get:  \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cfrac%7B0.98%20%2b%200.06%7D%7B2%7D%20=%20%5Cfrac%7B1.04%7D%7B2%7D%20=%200.52\"\u003e \u003c/p\u003e\n\n\u003cp\u003eHowever, using these numbers in the F1 score formula results in:\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Ctext%7BF1%20score%7D%20=%202%20%5Cfrac%7B0.98%20*%200.06%7D%7B0.98%20%2b%200.06%7D%20=%202%20%5Cfrac%7B0.0588%7D%7B1.04%7D%20=%202(0.061152)%20=%200.122304\"\u003e or 12.2%!\u003c/p\u003e\n\n\u003cp\u003eAs you can see, F1 score penalizes models heavily if it skews too hard towards either precision or recall. For this reason, F1 score is generally the most used metric for describing the performance of a model. \u003c/p\u003e\n\n\u003ch2\u003eWhich metric to use?\u003c/h2\u003e\n\n\u003cp\u003eThe metrics that are most important to a project will often be dependent on the business use case or goals for that model. This is why it's \u003cstrong\u003e\u003cem\u003every important\u003c/em\u003e\u003c/strong\u003e to understand why you're doing what you're doing, and how your model will be used in the real world! Otherwise, you may optimize your model for the wrong metric! \u003c/p\u003e\n\n\u003cp\u003eIn general, it is worth noting that it's a good idea to calculate all relevant metrics, when in doubt.  In most classification tasks, you don't know which model will perform best when you start. The common workflow is to train each different type of classifier, and select the best by comparing the performance of each. It's common to make tables like the one below, and highlight the best performer for each metric:\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-evaluation-metrics/master/./images/performance-comparisons.png\"\u003e\u003c/p\u003e\n\n\u003ch2\u003eCalculate evaluation metrics with confusion matrices\u003c/h2\u003e\n\n\u003cp\u003eNote that we can only calculate any of the metrics discussed here if we know the \u003cstrong\u003e\u003cem\u003eTrue Positives, True Negatives, False Positives, and False Negatives\u003c/em\u003e\u003c/strong\u003e resulting from the predictions of a model. If we have a confusion matrix, we can easily calculate \u003cstrong\u003e\u003cem\u003ePrecision\u003c/em\u003e\u003c/strong\u003e, \u003cstrong\u003e\u003cem\u003eRecall\u003c/em\u003e\u003c/strong\u003e and \u003cstrong\u003e\u003cem\u003eAccuracy\u003c/em\u003e\u003c/strong\u003e -- and if we know precision and recall, we can easily calculate \u003cstrong\u003e\u003cem\u003eF1 score\u003c/em\u003e\u003c/strong\u003e!\u003c/p\u003e\n\n\u003ch2\u003eClassification reports\u003c/h2\u003e\n\n\u003cp\u003eScikit-learn has a built-in function that will create a \u003cstrong\u003e\u003cem\u003eClassification Report\u003c/em\u003e\u003c/strong\u003e. This classification report even breaks down performance by individual class predictions for your model. You can find the \u003ccode\u003eclassification_report()\u003c/code\u003e function in the \u003ccode\u003esklearn.metrics\u003c/code\u003e module, which takes labels and predictions and returns the precision, recall, F1 score and support (number of occurrences of each label in \u003ccode\u003ey_true\u003c/code\u003e) for the results of a model. \u003c/p\u003e\n\n\u003ch2\u003eSummary\u003c/h2\u003e\n\n\u003cp\u003eIn this lesson you were introduced to several metrics which can be used to evaluate classification models. In the following lab, you'll write functions to calculate each of these manually, as well as explore how you can use existing functions in scikit-learn to quickly calculate and interpret each of these metrics. \u003c/p\u003e","frontPage":false},{"exportId":"object-oriented-programming-recap","title":"Object-Oriented Programming - Recap","type":"WikiPage","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-oop-recap-v2-1\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-oop-recap-v2-1/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eIn this section, you learned about Object-oriented programming (OOP) as a foundational practice for software development and programming.\u003c/p\u003e\n\u003ch2\u003eOOP overview\u003c/h2\u003e\n\u003cp\u003eYou now know all about OOP and how to define classes. Like functions, using classes in your programming can save you a lot of time by eliminating repetitive tasks. Classes go further than functions by allowing you to persist data. After all, class methods are fairly analogous to functions, while attributes add functionality by acting as data storage containers.\u003c/p\u003e\n\u003ch2\u003eClass structure\u003c/h2\u003e\n\u003cp\u003eAs you saw, the most basic class definition starts off with:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"python\"\u003eclass ClassName:\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eFrom there, you then saw how you can further define class methods:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"python\"\u003eclass ClassName:\n    def method_1(self):\n        pass # Ideally a method does something, but you get the point\n    def method_2(self):\n        print('This is a pretty useless second method.')\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eYou also learned about \u003ccode\u003eself\u003c/code\u003e. Specifically, that \u003ccode\u003eself\u003c/code\u003e is the default parameter used to define methods. This is necessary since instance methods implicitly pass in the object itself as an argument during execution.\u003c/p\u003e\n\u003ch2\u003eCreating instances\u003c/h2\u003e\n\u003cp\u003eRecall that after you define a class, you can then create instances of that class to bring it to life and use it! You're probably wondering what all of this has to do with data science. In turns out you'll use OOP principles when you start working with common data science libraries. For example, you might import the \u003ccode\u003eLinearRegression\u003c/code\u003e class from the scikit-learn library in order to create a regression model!\u003c/p\u003e\n\u003cp\u003eRemember, creating an instance of a class would look like this:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"python\"\u003efrom sklearn.linear_model import LinearRegression() \n\nreg = LinearRegression() \n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eOnce you create an instance object of the class, you can then use all the methods associated with that class!\u003c/p\u003e\n\u003ch2\u003eLevel up\u003c/h2\u003e\n\u003cp\u003eIf you would like to dive deeper into OOP and learn some advanced topics, you can check out the additional OOP lessons and labs in the Appendix.\u003c/p\u003e\n\u003ch2\u003eSummary\u003c/h2\u003e\n\u003cp\u003eCongrats, you now have a solid foundation in OOP! You first learned how to define a class and methods. Then you learned how to create an instance of a class and define instance attributes. These skills will be useful for collaboration and writing concise, modular code!\u003c/p\u003e","frontPage":false},{"exportId":"support-vector-machines-recap","title":"Support Vector Machines - Recap","type":"WikiPage","content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-svm-recap\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-svm-recap\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-svm-recap/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eAs you saw, support vector machines are another classification tool to add to your repertoire. While computationally expensive, they can be powerful tools providing substantial performance gains in many instances.\u003c/p\u003e\n\u003ch2\u003eKernel Functions\u003c/h2\u003e\n\u003cp\u003eProbably the most important information worth reviewing is some of the various kernel functions that you can apply:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eRadial Basis Function (RBF)\n\u003col\u003e\n\u003cli\u003e\u003ccode\u003ec\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e\n\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cgamma\"\u003e , which can be specified using \u003ccode\u003egamma\u003c/code\u003e in scikit-learn\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/li\u003e\n\u003cli\u003ePolynomial Kernel\n\u003col\u003e\n\u003cli\u003e\n\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cgamma\"\u003e , which can be specified using \u003ccode\u003egamma\u003c/code\u003e in scikit-learn\u003c/li\u003e\n\u003cli\u003e\n\u003cimg src=\"https://render.githubusercontent.com/render/math?math=r\"\u003e , which can be specified using \u003ccode\u003ecoef0\u003c/code\u003e in scikit-learn\u003c/li\u003e\n\u003cli\u003e\n\u003cimg src=\"https://render.githubusercontent.com/render/math?math=d\"\u003e , which can be specified using \u003ccode\u003edegree\u003c/code\u003e in scikit-learn\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/li\u003e\n\u003cli\u003eSigmoid Kernel\n\u003col\u003e\n\u003cli\u003e\n\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cgamma\"\u003e , which can be specified using \u003ccode\u003egamma\u003c/code\u003e in scikit-learn\u003c/li\u003e\n\u003cli\u003e\n\u003cimg src=\"https://render.githubusercontent.com/render/math?math=r\"\u003e , which can be specified using \u003ccode\u003ecoef0\u003c/code\u003e in scikit-learn\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eAlso recall that in general, \u003ccode\u003ec\u003c/code\u003e is the parameter for balancing standard accuracy metrics for tuning classifiers with the decision boundary distance.\u003c/p\u003e\n\u003ch2\u003eSummary\u003c/h2\u003e\n\u003cp\u003eWhile it may appear that this section was a bit brief, Support Vector Machines are a powerful algorithm that deserve attention, so make sure you investigate them properly. Moreover, learning to properly tune SVMs using kernels and an appropriate \u003ccode\u003ec\u003c/code\u003e value is critical.\u003c/p\u003e","frontPage":false},{"exportId":"mle-and-logistic-regression-recap","title":"MLE and Logistic Regression - Recap","type":"WikiPage","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-mle-log-reg-recap\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-mle-log-reg-recap/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eWell done! In this section, you reviewed maximum likelihood estimation and logistic regression. This included writing some challenging code, including gradient descent which pushed you to think critically regarding algorithm implementation. \u003c/p\u003e\n\n\u003ch2\u003eLog-likelihoods in Maximum Likelihood Estimation\u003c/h2\u003e\n\n\u003cp\u003eOne of the nuances you saw in maximum likelihood estimation was that of log-likelihoods. Recall that the purpose of taking log-likelihoods as opposed to likelihoods themselves is that it allows us to decompose the product of probabilities as sums of log probabilities. Analytically, this is essential to calculating subsequent gradients in order to find the next steps for our optimization algorithm.\u003c/p\u003e\n\n\u003ch2\u003eLocal minima in Gradient Descent\u003c/h2\u003e\n\n\u003cp\u003eOne of the most important notes from this section is that \u003cstrong\u003egradient descent does not guarantee an optimal solution\u003c/strong\u003e. Gradient descent is meant to find optimal solutions, but it only guarantees a local minimum. For this reason, gradient descent is frequently run multiple times, and the parameters with the lowest loss function then being selected for the final model.\u003c/p\u003e\n\n\u003ch2\u003eLogistic Regression\u003c/h2\u003e\n\n\u003cp\u003eAfter coding logistic regression on your own, you then further investigated tuning such models using regularization. Recall that while precision, recall, and accuracy are useful metrics for evaluating classifiers, determining an appropriate balance between false positives and false negatives will depend on the particular problem application and the relative costs of each. For example, in the context of medical screening, a false negative could be devastating, eliminating the possibility for early intervention of the given disease. On the other hand, in another context, such as finding spam email, the cost of false positives might be much higher than false negatives -- after all, having a spam email sneak its way into your inbox is probably preferable then missing an important time-sensitive email because it was marked as spam. Due to these contextual considerations, you as the practitioner are responsible for selecting appropriate tradeoffs.\u003c/p\u003e\n\n\u003ch2\u003eSummary\u003c/h2\u003e\n\n\u003cp\u003eThis section was designed to give you additional practice coding algorithms in Python, and a deeper understanding of how iterative algorithms such as logistic regression converge to produce underlying model parameters.\u003c/p\u003e\n\u003cfooter class=\"fis-footer\" style=\"visibility: hidden;\"\u003e\u003cdiv class=\"fis-feedback\"\u003e\n\u003ch5\u003eHow do you feel about this lesson?\u003c/h5\u003e\n\u003cimg id=\"thumbs-up\" data-repository=\"dsc-mle-log-reg-recap\" title=\"Thumbs up!\" alt=\"thumbs up\"\u003e\u003cimg id=\"thumbs-down\" data-repository=\"dsc-mle-log-reg-recap\" title=\"Thumbs down!\" alt=\"thumbs down\"\u003e\n\u003c/div\u003e\n\u003ch5\u003eHave specific feedback? \u003ca href=\"https://github.com/learn-co-curriculum/dsc-mle-log-reg-recap/issues/new\"\u003eTell us here!\u003c/a\u003e\n\u003c/h5\u003e\u003c/footer\u003e","frontPage":false},{"exportId":"entropy-and-information-gain","title":"Entropy and Information Gain","type":"WikiPage","content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-entropy-and-information-gain\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-entropy-and-information-gain\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-entropy-and-information-gain/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003e\u003cstrong\u003e\u003cem\u003eInformation gain\u003c/em\u003e\u003c/strong\u003e is calculated using a statistical measure called \u003cstrong\u003e\u003cem\u003eEntropy\u003c/em\u003e\u003c/strong\u003e. Entropy is a widely used concept in the fields of Physics, Mathematics, Computer Science (information theory), and more. You may have come across the idea of entropy in thermodynamics, societal dynamics, and a number of other domains. In electronics and computer science, the idea of entropy is usually derived from \u003cstrong\u003eShannon's\u003c/strong\u003e description of entropy to measure the information gain against some cost incurred in the process. In this lesson, we shall look at how this works with the simple example we introduced in the previous lesson.\u003c/p\u003e\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\u003cp\u003eYou will be able to:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eExplain the process for selecting the best attribute for a split\u003c/li\u003e\n\u003cli\u003eCalculate entropy and information gain by hand for a simple dataset\u003c/li\u003e\n\u003cli\u003eCompare and contrast entropy and information gain\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003eShannon's Entropy\u003c/h2\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003eEntropy is a measure of disorder or uncertainty.\u003c/strong\u003e\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eThe measure is named after \u003cem\u003eClaude Shannon\u003c/em\u003e, who is known as the \"father of information theory\". Information theory provides measures of uncertainty associated with random variables. These measures help calculate the average information content one is missing when one does not know the value of the random variable. This uncertainty is measured in bits, i.e., the amount of information (in bits) contained per average instance in a stream of instances.\u003c/p\u003e\n\u003cp\u003eConceptually, information can be thought of as being stored or transmitted as variables that can take on different values. A variable can be thought of as a unit of storage that can take on, at different times, one of several different specified values, following some process for taking on those values. Informally, we get information from a variable by looking at its value, just as we get information from an email by reading its contents. In the case of the variable, the information is about the process behind the variable.\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eThe entropy of a variable is the \"amount of information\" contained in the variable.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eThis amount is not only determined by the number of different values the variable can take, just as the information in an email is not quantified just by the number of words in the email or the different possible words in the language of the email. Informally, the amount of information in an email is proportional to the amount of surprise its reading causes.\u003c/p\u003e\n\u003cp\u003eFor example, if an email is simply a repeat of an earlier email, then it is not informative at all. On the other hand, if, for example, the email reveals the outcome of an election, then it is highly informative. Similarly, the information in a variable is tied to the amount of surprise the value of that variable causes when revealed.\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eShannons entropy quantifies the amount of information in a variable, thus providing the foundation for a theory around the notion of information.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eIn terms of data, we can informally describe entropy as an indicator of how messy your data is. A high degree of entropy always reflects \"messed-up\" data with low/no information content. The uncertainty about the content of the data, before viewing the data remains the same (or almost the same) as that before the data was available.\u003c/p\u003e\n\u003cp\u003eIn a nutshell, higher entropy means less predictive power when it comes to doing data science with that data.\u003c/p\u003e\n\u003ch2\u003eEntropy and decision trees\u003c/h2\u003e\n\u003cblockquote\u003e\n\u003cp\u003eDecision trees aim to tidy the data by separating the samples and re-grouping them in the classes they belong to.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eBecause decision trees use a supervised learning approach, we know the target variable of our data. So, we maximize the \u003cstrong\u003epurity\u003c/strong\u003e of the classes \u003cstrong\u003eas much as possible\u003c/strong\u003e while making splits, aiming to have \u003cstrong\u003eclarity\u003c/strong\u003e in the leaf nodes. Remember, it may not be possible to remove the uncertainty totally, i.e., to fully clean up the data. Have a look at the image below:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-entropy-and-information-gain/master/images/split_fs.png\" width=\"300\"\u003e\u003c/p\u003e\n\u003cp\u003eWe can see that the split has not \u003cstrong\u003eFULLY\u003c/strong\u003e classified the data above, but the resulting data is \u003cstrong\u003etidier\u003c/strong\u003e than it was before the split. By using a series of such splits that focus on different features, we try to clean up the data as much as possible in the leaf nodes. At each step, we want to decrease the entropy, so \u003cstrong\u003eentropy is computed before and after the split\u003c/strong\u003e. If it decreases, the split is retained and we can proceed to the next step, otherwise, we must try to split with another feature or stop this branch (or quit, in which case we claim that the current tree is the best solution).\u003c/p\u003e\n\u003ch3\u003eCalculating entropy\u003c/h3\u003e\n\u003cp\u003eLet's pretend we have a sample, \u003cimg src=\"https://render.githubusercontent.com/render/math?math=S\"\u003e . This sample contains \u003cimg src=\"https://render.githubusercontent.com/render/math?math=N\"\u003e total items falling into two different categories, \u003ccode\u003eTrue\u003c/code\u003e and \u003ccode\u003eFalse\u003c/code\u003e. Of the \u003cimg src=\"https://render.githubusercontent.com/render/math?math=N\"\u003e total items we have, \u003cimg src=\"https://render.githubusercontent.com/render/math?math=n\"\u003e observations have a target value equal to \u003ccode\u003eTrue\u003c/code\u003e , and \u003cimg src=\"https://render.githubusercontent.com/render/math?math=m\"\u003e observations have a target value equal to \u003ccode\u003eFalse\u003c/code\u003e. Note that if we know \u003cimg src=\"https://render.githubusercontent.com/render/math?math=N\"\u003e and \u003cimg src=\"https://render.githubusercontent.com/render/math?math=n\"\u003e , we can easily calculate \u003cimg src=\"https://render.githubusercontent.com/render/math?math=m\"\u003e to be \u003cimg src=\"https://render.githubusercontent.com/render/math?math=m%20=%20N%20-%20n\"\u003e .\u003c/p\u003e\n\u003cp\u003eLet's assume our boss brings us the dataset \u003cimg src=\"https://render.githubusercontent.com/render/math?math=S\"\u003e , and asks us to group each observation in \u003cimg src=\"https://render.githubusercontent.com/render/math?math=N\"\u003e according to whether their target value is \u003ccode\u003eTrue\u003c/code\u003e or \u003ccode\u003eFalse\u003c/code\u003e. They also want to know the ratio of \u003ccode\u003eTrue\u003c/code\u003es to \u003ccode\u003eFalse\u003c/code\u003es in our dataset. We can calculate this as follows:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=p%20=%20n/N%20-%20(class%201)\"\u003e\u003cbr\u003e\u003cbr\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=q%20=%20m/N%20=%201-p%20-%20(class%202)\"\u003e\u003c/p\u003e\n\u003cp\u003eIf we know these ratios, we can calculate the \u003cem\u003eentropy\u003c/em\u003e of the dataset \u003cimg src=\"https://render.githubusercontent.com/render/math?math=S\"\u003e . This will provide us with an easy way to see how organized or disorganized our dataset is. For instance, let's assume that our boss believes that the dataset should mostly be full of \u003ccode\u003eTrue\u003c/code\u003es, with some occasional \u003ccode\u003eFalse\u003c/code\u003es slipping through. The more \u003ccode\u003eFalse\u003c/code\u003es in with the \u003ccode\u003eTrue\u003c/code\u003es (or \u003ccode\u003eTrue\u003c/code\u003es in with the \u003ccode\u003eFalse\u003c/code\u003es!), the more disorganized our dataset is.\u003c/p\u003e\n\u003cp\u003eWe can calculate entropy using the following equation:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=E%20=%20-p%20.%20log_2(p)%20-%20q%20.%20log_2(q)\"\u003e\u003c/p\u003e\n\u003cp\u003eDon't worry too much about this equation yet -- we'll dig deeper into what it means in a minute.\u003c/p\u003e\n\u003cp\u003eThe equation above tells us that a dataset is considered tidy if it only contains one class (i.e. no uncertainty or confusion). If the dataset contains a mix of classes for our target variable, the entropy increases. This is easier to understand when we visualize it. Consider the following graph of entropy in a dataset that has two classes for our target variable:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-entropy-and-information-gain/master/images/new_entropy_fs.png\" width=\"500\"\u003e\u003c/p\u003e\n\u003cp\u003eAs you can see, when the classes are split equally, \u003cimg src=\"https://render.githubusercontent.com/render/math?math=p%20=%200.5\"\u003e and \u003cimg src=\"https://render.githubusercontent.com/render/math?math=q%20=%201%20-%20p%20=%200.5\"\u003e , the entropy value is at its maximum, 1. Conversely, when the proportion of the split is at 0 (all of one target class) or at 1 (all of the other target class), the entropy value is 0! This means that we can easily think of entropy as follows: the more one-sided the proportion of target classes, the less entropy. Think of a sock drawer that may or may not have some underwear mixed in. If the sock drawer contains only socks (or only underwear), then entropy is 0. If you reach in and pull out an article of clothing, you know exactly what you're going to get. However, if 10% of the items in that sock drawer are actually underwear, you are less certain what that random draw will give you. That uncertainty increases as more and more underwear gets mixed into that sock drawer, right up until there is the exact same amount of socks and underwear in the drawer. When the proportion is exactly equal, you have no way of knowing item of clothing a random draw might give you -- maximum entropy, and perfect chaos!\u003c/p\u003e\n\u003cp\u003eThis is where the logic behind decision trees comes in -- what if we could split the contents of our sock drawer into different subsets, which might divide the drawer into more organized subsets? For instance, let's assume that we've built a laundry robot that can separate items of clothing by color. If a majority of our socks are white, and a majority of our underwear is some other color, then we can safely assume that the two subsets will have a better separation between socks and underwear, even if the original chaotic drawer had a 50/50 mix of the two!\u003c/p\u003e\n\u003ch3\u003eGeneralization of entropy\u003c/h3\u003e\n\u003cp\u003eNow that we have a good real-world example to cling to, let's get back to thinking about the mathematical definition of entropy.\u003c/p\u003e\n\u003cp\u003eEntropy \u003cimg src=\"https://render.githubusercontent.com/render/math?math=H(S)\"\u003e is a measure of the amount of uncertainty in the dataset \u003cimg src=\"https://render.githubusercontent.com/render/math?math=S\"\u003e . We can see this is a measurement or characterization of the amount of information contained within the dataset \u003cimg src=\"https://render.githubusercontent.com/render/math?math=S\"\u003e .\u003c/p\u003e\n\u003cp\u003eWe saw how to calculate entropy for a two-class variable before. However, in the real world we deal with multiclass problems very often, so it would be a good idea to see a general representation of the formula we saw before. The general representation is:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Clarge%20H(S)%20=%20-%5Csum%20(P_i%20.%20log_2(P_i))\"\u003e\u003c/p\u003e\n\u003cp\u003eWhen \u003cimg src=\"https://render.githubusercontent.com/render/math?math=H(S)%20=%200\"\u003e , this means that the set \u003cimg src=\"https://render.githubusercontent.com/render/math?math=S\"\u003e is perfectly classified, meaning that there is no disorganization in our data because all of our data in S is the exact same class. If we know how much entropy exists in a subset (and remember, we can subset our data by just splitting it into 2 or more groups according to whatever metric we choose), then we can easily calculate how much \u003cstrong\u003e\u003cem\u003einformation gain\u003c/em\u003e\u003c/strong\u003e each potential split would give us!\u003c/p\u003e\n\u003ch2\u003eInformation gain\u003c/h2\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003eInformation gain is an impurity/uncertainty based criterion that uses the entropy as the measure of impurity.\u003c/strong\u003e\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eThere are several different algorithms out there for creating decision trees. Of those, the ID3 algorithm is one of the most popular. Information gain is the key criterion that is used by the ID3 classification tree algorithm to construct a decision tree. The decision tree algorithm will always try to \u003cstrong\u003emaximize information gain\u003c/strong\u003e. The entropy of the dataset is calculated using each attribute, and the attribute showing highest information gain is used to create the split at each node. A simple understanding of information gain can be written as:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Ctext%7BInformation%20Gain%7D%20=%20%5Ctext%7BEntropy%7D_%7B%5Ctext%7Bparent%7D%7D%20-%20%5Ctext%7BEntropy%7D_%7B%5Ctext%7Bchild%7D%7D.%5B%5Ctext%7Bchild%20weighted%20average%7D%5D\"\u003e\u003c/p\u003e\n\u003cp\u003eA weighted average based on the number of samples in each class is multiplied by the child's entropy, since most datasets have class imbalance. Thus the information gain calculation for each attribute is calculated and compared, and the attribute showing the highest information gain will be selected for the split.\u003c/p\u003e\n\u003cp\u003eWhen we measure information gain, we're really measuring the difference in entropy from before the split (an untidy sock drawer) to after the split (a group of white socks and underwear, and a group of non-white socks and underwear). Information gain allows us to put a number to exactly how much we've reduced our \u003cem\u003euncertainty\u003c/em\u003e after splitting a dataset \u003cimg src=\"https://render.githubusercontent.com/render/math?math=S\"\u003e on some attribute, \u003cimg src=\"https://render.githubusercontent.com/render/math?math=A\"\u003e.\u003c/p\u003e\n\u003cp\u003eThe equation for information gain is:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Clarge%20IG(A,%20S)%20=%20H(S)%20-%20%5Csum%7B%7D%7Bp(t)H(t)%7D\"\u003e\u003c/p\u003e\n\u003cp\u003eWhere:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cimg src=\"https://render.githubusercontent.com/render/math?math=H(S)\"\u003e is the entropy of set \u003cimg src=\"https://render.githubusercontent.com/render/math?math=S\"\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cimg src=\"https://render.githubusercontent.com/render/math?math=t\"\u003e is a subset of the attributes contained in \u003cimg src=\"https://render.githubusercontent.com/render/math?math=A\"\u003e (we represent all subsets \u003cimg src=\"https://render.githubusercontent.com/render/math?math=t\"\u003e as \u003cimg src=\"https://render.githubusercontent.com/render/math?math=T\"\u003e )\u003c/li\u003e\n\u003cli\u003e\n\u003cimg src=\"https://render.githubusercontent.com/render/math?math=p(t)\"\u003e is the proportion of the number of elements in \u003cimg src=\"https://render.githubusercontent.com/render/math?math=t\"\u003e to the number of elements in \u003cimg src=\"https://render.githubusercontent.com/render/math?math=S\"\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cimg src=\"https://render.githubusercontent.com/render/math?math=H(t)\"\u003e is the entropy of a given subset \u003cimg src=\"https://render.githubusercontent.com/render/math?math=t\"\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eIn the ID3 algorithm, we use entropy to calculate information gain, and then pick the attribute with the largest possible information gain to split our data on at each iteration.\u003c/p\u003e\n\u003ch2\u003eEntropy and information gain example\u003c/h2\u003e\n\u003cp\u003eSo far, we've focused heavily on the math behind entropy and information gain. This usually makes the calculations look scarier than they actually are. To show that calculating entropy/information gain is actually pretty simple, let's take a look at an example problem -- predicting if we want to play tennis or not, based on the weather, temperature, humidity, and windiness of a given day!\u003c/p\u003e\n\u003cp\u003eOur dataset is as follows:\u003c/p\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth style=\"text-align: center;\"\u003eoutlook\u003c/th\u003e\n\u003cth style=\"text-align: center;\"\u003etemp\u003c/th\u003e\n\u003cth style=\"text-align: center;\"\u003ehumidity\u003c/th\u003e\n\u003cth style=\"text-align: center;\"\u003ewindy\u003c/th\u003e\n\u003cth style=\"text-align: center;\"\u003eplay\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd style=\"text-align: center;\"\u003eovercast\u003c/td\u003e\n\u003ctd style=\"text-align: center;\"\u003ecool\u003c/td\u003e\n\u003ctd style=\"text-align: center;\"\u003ehigh\u003c/td\u003e\n\u003ctd style=\"text-align: center;\"\u003eY\u003c/td\u003e\n\u003ctd style=\"text-align: center;\"\u003eyes\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd style=\"text-align: center;\"\u003eovercast\u003c/td\u003e\n\u003ctd style=\"text-align: center;\"\u003emild\u003c/td\u003e\n\u003ctd style=\"text-align: center;\"\u003enormal\u003c/td\u003e\n\u003ctd style=\"text-align: center;\"\u003eN\u003c/td\u003e\n\u003ctd style=\"text-align: center;\"\u003eyes\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd style=\"text-align: center;\"\u003esunny\u003c/td\u003e\n\u003ctd style=\"text-align: center;\"\u003ecool\u003c/td\u003e\n\u003ctd style=\"text-align: center;\"\u003enormal\u003c/td\u003e\n\u003ctd style=\"text-align: center;\"\u003eN\u003c/td\u003e\n\u003ctd style=\"text-align: center;\"\u003eyes\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd style=\"text-align: center;\"\u003eovercast\u003c/td\u003e\n\u003ctd style=\"text-align: center;\"\u003ehot\u003c/td\u003e\n\u003ctd style=\"text-align: center;\"\u003ehigh\u003c/td\u003e\n\u003ctd style=\"text-align: center;\"\u003eY\u003c/td\u003e\n\u003ctd style=\"text-align: center;\"\u003eno\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd style=\"text-align: center;\"\u003esunny\u003c/td\u003e\n\u003ctd style=\"text-align: center;\"\u003ehot\u003c/td\u003e\n\u003ctd style=\"text-align: center;\"\u003enormal\u003c/td\u003e\n\u003ctd style=\"text-align: center;\"\u003eY\u003c/td\u003e\n\u003ctd style=\"text-align: center;\"\u003eyes\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd style=\"text-align: center;\"\u003erain\u003c/td\u003e\n\u003ctd style=\"text-align: center;\"\u003emild\u003c/td\u003e\n\u003ctd style=\"text-align: center;\"\u003ehigh\u003c/td\u003e\n\u003ctd style=\"text-align: center;\"\u003eN\u003c/td\u003e\n\u003ctd style=\"text-align: center;\"\u003eno\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd style=\"text-align: center;\"\u003erain\u003c/td\u003e\n\u003ctd style=\"text-align: center;\"\u003ecool\u003c/td\u003e\n\u003ctd style=\"text-align: center;\"\u003enormal\u003c/td\u003e\n\u003ctd style=\"text-align: center;\"\u003eN\u003c/td\u003e\n\u003ctd style=\"text-align: center;\"\u003eno\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd style=\"text-align: center;\"\u003esunny\u003c/td\u003e\n\u003ctd style=\"text-align: center;\"\u003emild\u003c/td\u003e\n\u003ctd style=\"text-align: center;\"\u003ehigh\u003c/td\u003e\n\u003ctd style=\"text-align: center;\"\u003eN\u003c/td\u003e\n\u003ctd style=\"text-align: center;\"\u003eyes\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd style=\"text-align: center;\"\u003esunny\u003c/td\u003e\n\u003ctd style=\"text-align: center;\"\u003ecool\u003c/td\u003e\n\u003ctd style=\"text-align: center;\"\u003enormal\u003c/td\u003e\n\u003ctd style=\"text-align: center;\"\u003eY\u003c/td\u003e\n\u003ctd style=\"text-align: center;\"\u003eyes\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd style=\"text-align: center;\"\u003esunny\u003c/td\u003e\n\u003ctd style=\"text-align: center;\"\u003emild\u003c/td\u003e\n\u003ctd style=\"text-align: center;\"\u003enormal\u003c/td\u003e\n\u003ctd style=\"text-align: center;\"\u003eY\u003c/td\u003e\n\u003ctd style=\"text-align: center;\"\u003eyes\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd style=\"text-align: center;\"\u003eovercast\u003c/td\u003e\n\u003ctd style=\"text-align: center;\"\u003ecool\u003c/td\u003e\n\u003ctd style=\"text-align: center;\"\u003ehigh\u003c/td\u003e\n\u003ctd style=\"text-align: center;\"\u003eN\u003c/td\u003e\n\u003ctd style=\"text-align: center;\"\u003eyes\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd style=\"text-align: center;\"\u003erain\u003c/td\u003e\n\u003ctd style=\"text-align: center;\"\u003ecool\u003c/td\u003e\n\u003ctd style=\"text-align: center;\"\u003ehigh\u003c/td\u003e\n\u003ctd style=\"text-align: center;\"\u003eY\u003c/td\u003e\n\u003ctd style=\"text-align: center;\"\u003eno\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd style=\"text-align: center;\"\u003esunny\u003c/td\u003e\n\u003ctd style=\"text-align: center;\"\u003ehot\u003c/td\u003e\n\u003ctd style=\"text-align: center;\"\u003enormal\u003c/td\u003e\n\u003ctd style=\"text-align: center;\"\u003eY\u003c/td\u003e\n\u003ctd style=\"text-align: center;\"\u003eno\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd style=\"text-align: center;\"\u003esunny\u003c/td\u003e\n\u003ctd style=\"text-align: center;\"\u003emild\u003c/td\u003e\n\u003ctd style=\"text-align: center;\"\u003ehigh\u003c/td\u003e\n\u003ctd style=\"text-align: center;\"\u003eN\u003c/td\u003e\n\u003ctd style=\"text-align: center;\"\u003eyes\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003cp\u003eLet's apply the formulas we saw earlier to this problem:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=H(S)%20=%20%5Csum%7B%7D%7B-p(C)%20log_2%20p(C)%7D\"\u003e \u003cbr\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=C=%7B%5C%7Byes,%20no%5C%7D%7D\"\u003e\u003c/p\u003e\n\u003cp\u003eOut of 14 instances, 9 are classified as yes, and 5 as no. So:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=p(yes)%20=%20-(9/14)log_2(9/14)%20=%200.28\"\u003e \u003cbr\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=p(no)%20=%20-(5/14)log_2(5/14)%20=%200.37\"\u003e \u003cbr\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=H(S)%20=%20p(yes)%20%2b%20p(no)%20=%200.65\"\u003e\u003c/p\u003e\n\u003cp\u003eThe current entropy of our dataset is 0.65. In the next lesson, we'll see how we can improve this by subsetting our dataset into different groups by calculating the entropy/information gain of each possible split, and then picking the one that performs best until we have a fully fleshed-out decision tree!\u003c/p\u003e\n\u003ch2\u003eSummary\u003c/h2\u003e\n\u003cp\u003eIn this lesson, we looked at calculating entropy and information gain measures for building decision trees. We looked at a simple example and saw how to use these measures to select the best split at each node. Next, we calculate these measures in Python, before digging deeper into decision trees.\u003c/p\u003e","frontPage":false},{"exportId":"bayesian-classification-recap","title":"Bayesian Classification - Recap","type":"WikiPage","content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-bayesian-classification-recap-v2-1\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-bayesian-classification-recap-v2-1\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-bayesian-classification-recap-v2-1/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\u003ch2\u003eKey Takeaways\u003c/h2\u003e\n\u003cp\u003eThe key takeaways from this section include:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eNaive Bayes algorithms extend Bayes' formula to multiple variables by assuming that features are independent of one another. This then allows you to estimate an overall probability by multiplying the conditional probabilities for each of the independent features\u003c/li\u003e\n\u003cli\u003eThis assumption (that the underlying features are independent) is why Naive Bayes algorithm is considered naive -- because this is almost never true. However, Naives Bayes can prove to be quite efficient\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eExpanding to multiple features, the multinomial Bayes' formula is:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5CLarge%20P(y%7Cx_1,%20x_2,%20...,%20x_n)%20=%20%5Cfrac%7BP(y)%5Cdisplaystyle%5Cprod_%7Bi%7D%5E%7Bn%7DP(x_i%7Cy)%7D%7BP(x_1,%20x_2,%20...,%20x_n)%7D\"\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eFinally, you saw how Naive Bayes algorithm can be used for document classification by classifying YouTube videos into the appropriate topic, and classifying documents as \"spam\" or \"no spam\"\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eDue to insufficient text preprocessing (which you will learn how to do in a later module), the performance of this algorithm was trivial\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e","frontPage":false},{"exportId":"k-nearest-neighbors","title":"K-Nearest Neighbors","type":"WikiPage","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-k-nearest-neighbors\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-k-nearest-neighbors/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eIn this lesson, you'll learn about a supervised learning algorithm, \u003cstrong\u003e\u003cem\u003eK-Nearest Neighbors\u003c/em\u003e\u003c/strong\u003e; and how you can use it to make predictions for classification and regression tasks!\u003c/p\u003e\n\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\n\u003cp\u003eYou will be able to:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eDescribe how KNN makes classifications\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch2\u003eWhat is K-Nearest Neighbors?\u003c/h2\u003e\n\n\u003cp\u003e\u003cstrong\u003e\u003cem\u003eK-Nearest Neighbors\u003c/em\u003e\u003c/strong\u003e (or KNN, for short) is a supervised learning algorithm that can be used for both \u003cstrong\u003e\u003cem\u003eClassification\u003c/em\u003e\u003c/strong\u003e and \u003cstrong\u003e\u003cem\u003eRegression\u003c/em\u003e\u003c/strong\u003e tasks. However, in this section, we will cover KNN only in the context of classification. KNN is a distance-based classifier, meaning that it implicitly assumes that the smaller the distance between two points, the more similar they are. In KNN, each column acts as a dimension. In a dataset with two columns, we can easily visualize this by treating values for one column as X coordinates and and the other as Y coordinates. Since this is a \u003cstrong\u003e\u003cem\u003eSupervised learning algorithm\u003c/em\u003e\u003c/strong\u003e, you must also have the labels for each point in the dataset, or else you wouldn't know what to predict!\u003c/p\u003e\n\n\u003ch2\u003eFitting the model\u003c/h2\u003e\n\n\u003cp\u003eKNN is unique compared to other classifiers in that it does almost nothing during the \"fit\" step, and all the work during the \"predict\" step. During the \"fit\" step, KNN just stores all the training data and corresponding labels. No distances are calculated at this point. \u003c/p\u003e\n\n\u003ch2\u003eMaking predictions with K\u003c/h2\u003e\n\n\u003cp\u003eAll the magic happens during the \"predict\" step. During this step, KNN takes a point that you want a class prediction for, and calculates the distances between that point and every single point in the training set. It then finds the \u003ccode\u003eK\u003c/code\u003e closest points, or \u003cstrong\u003e\u003cem\u003eNeighbors\u003c/em\u003e\u003c/strong\u003e, and examines the labels of each. You can think of each of the K-closest points getting to 'vote' about the predicted class. Naturally, they all vote for the same class that they belong to. The majority wins, and the algorithm predicts the point in question as whichever class has the highest count among all of the k-nearest neighbors.\u003c/p\u003e\n\n\u003cp\u003eIn the following animation, K=3: \u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-k-nearest-neighbors/master/images/knn.gif\"\u003e\u003c/p\u003e\n\n\u003cp\u003e\u003ca href=\"https://gfycat.com/wildsorrowfulchevrotain\"\u003egif source\u003c/a\u003e\u003c/p\u003e\n\n\u003ch2\u003eDistance metrics\u003c/h2\u003e\n\n\u003cp\u003eWhen using KNN, you can use \u003cstrong\u003e\u003cem\u003eManhattan\u003c/em\u003e\u003c/strong\u003e, \u003cstrong\u003e\u003cem\u003eEuclidean\u003c/em\u003e\u003c/strong\u003e, \u003cstrong\u003e\u003cem\u003eMinkowski distance\u003c/em\u003e\u003c/strong\u003e, or any other distance metric. Choosing an appropriate distance metric is essential and will depend on the context of the problem at hand.\u003c/p\u003e\n\n\u003ch2\u003eEvaluating model performance\u003c/h2\u003e\n\n\u003cp\u003eHow to evaluate the model performance depends on whether you're using the model for a classification or regression task. KNN can be used for regression (by averaging the target scores from each of the K-nearest neighbors), as well as for both binary and multicategorical classification tasks. \u003c/p\u003e\n\n\u003cp\u003eEvaluating classification performance for KNN works the same as evaluating performance for any other classification algorithm -- you need a set of predictions, and the corresponding ground-truth labels for each of the points you made a prediction on. You can then compute evaluation metrics such as \u003cstrong\u003e\u003cem\u003ePrecision, Recall, Accuracy, F1-Score\u003c/em\u003e\u003c/strong\u003e etc. \u003c/p\u003e\n\n\u003ch2\u003eSummary\u003c/h2\u003e\n\n\u003cp\u003eGreat! Now that you know how the KNN classifier works, you'll implement KNN using Python from scratch in the next lab.\u003c/p\u003e\n\u003cfooter class=\"fis-footer\" style=\"visibility: hidden;\"\u003e\u003cdiv class=\"fis-feedback\"\u003e\n\u003ch5\u003eHow do you feel about this lesson?\u003c/h5\u003e\n\u003cimg id=\"thumbs-up\" data-repository=\"dsc-k-nearest-neighbors\" title=\"Thumbs up!\" alt=\"thumbs up\"\u003e\u003cimg id=\"thumbs-down\" data-repository=\"dsc-k-nearest-neighbors\" title=\"Thumbs down!\" alt=\"thumbs down\"\u003e\n\u003c/div\u003e\n\u003ch5\u003eHave specific feedback? \u003ca href=\"https://github.com/learn-co-curriculum/dsc-k-nearest-neighbors/issues/new\"\u003eTell us here!\u003c/a\u003e\n\u003c/h5\u003e\u003c/footer\u003e","frontPage":false},{"exportId":"gradient-boosting-and-weak-learners","title":"Gradient Boosting and Weak Learners","type":"WikiPage","content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-gradient-boosting-and-weak-learners\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-gradient-boosting-and-weak-learners\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-gradient-boosting-and-weak-learners/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eIn this lesson, we'll explore one of the most powerful ensemble methods around -- gradient boosting!\u003c/p\u003e\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\u003cp\u003eYou will be able to:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eCompare and contrast weak and strong learners and explain the role of weak learners in boosting algorithms\u003c/li\u003e\n\u003cli\u003eDescribe the process of boosting in Adaboost and Gradient Boosting\u003c/li\u003e\n\u003cli\u003eExplain the concept of a learning rate and the role it plays in gradient boosting algorithms\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003eWeak learners and boosting\u003c/h2\u003e\n\u003cp\u003eThe first ensemble technique we learned about was \u003cstrong\u003e\u003cem\u003eBagging\u003c/em\u003e\u003c/strong\u003e, which refers to training different models independently on different subsets of data by sampling with replacement. The goal of bagging is to create variability in the ensemble of models. The next ensemble technique we'll learn about is \u003cstrong\u003e\u003cem\u003eBoosting\u003c/em\u003e\u003c/strong\u003e. This technique is at the heart of some very powerful, top-of-class ensemble methods currently used in machine learning, such as \u003cstrong\u003e\u003cem\u003eAdaboost\u003c/em\u003e\u003c/strong\u003e and \u003cstrong\u003e\u003cem\u003eGradient Boosted Trees\u003c/em\u003e\u003c/strong\u003e.\u003c/p\u003e\n\u003cp\u003eIn order to understand boosting, let's first examine the cornerstone of boosting algorithms -- \u003cstrong\u003e\u003cem\u003eWeak Learners\u003c/em\u003e\u003c/strong\u003e.\u003c/p\u003e\n\u003ch3\u003eWeak learners\u003c/h3\u003e\n\u003cp\u003eAll the models we've learned so far are \u003cstrong\u003e\u003cem\u003eStrong Learners\u003c/em\u003e\u003c/strong\u003e -- models with the goal of doing as well as possible on the classification or regression task they are given. The term \u003cstrong\u003e\u003cem\u003eWeak Learner\u003c/em\u003e\u003c/strong\u003e refers to simple models that do only slightly better than random chance. Boosting algorithms start with a single weak learner (tree methods are overwhelmingly used here), but technically, any model will do. Boosting works as follows:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eTrain a single weak learner\u003c/li\u003e\n\u003cli\u003eFigure out which examples the weak learner got wrong\u003c/li\u003e\n\u003cli\u003eBuild another weak learner that focuses on the areas the first weak learner got wrong\u003c/li\u003e\n\u003cli\u003eContinue this process until a predetermined stopping condition is met, such as until a set number of weak learners have been created, or the model's performance has plateaued\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eIn this way, each new weak learner is specifically tuned to focus on the weak points of the previous weak learner(s). The more often an example is missed, the more likely it is that the next weak learner will be the one that can classify that example correctly. In this way, all the weak learners work together to make up a single strong learner.\u003c/p\u003e\n\u003ch2\u003eBoosting and random forests\u003c/h2\u003e\n\u003ch3\u003eSimilarities\u003c/h3\u003e\n\u003cp\u003eBoosting algorithms share some similarities with random forests, as well as some notable differences. Like random forests, boosting algorithms are an ensemble of many different models with high inter-group diversity. Boosting algorithms also aggregate the predictions of each constituent model into an overall prediction. Both algorithms also make use of tree models (although this isn't strictly required, in the case of boosting).\u003c/p\u003e\n\u003ch3\u003eDifferences\u003c/h3\u003e\n\u003ch4\u003e1: Independent vs. iterative\u003c/h4\u003e\n\u003cp\u003eThe difference is in the approach to training the trees. Whereas a random forest trains each tree independently and at the same time, boosting trains each tree iteratively. In a random forest model, how well or poorly a given tree does has no effect on any of the other trees since they are all trained at the same time. Boosting, on the other hand, trains trees one at a time, identifies the weak points for those trees, and then purposefully creates the next round of trees in such a way as to specialize in those weak points.\u003c/p\u003e\n\u003ch4\u003e2: Weak vs. strong\u003c/h4\u003e\n\u003cp\u003eAnother major difference between random forests and boosting algorithms is the overall size of the trees. In a random forest, each tree is a strong learner -- they would do just fine as a decision tree on their own. In boosting algorithms, trees are artificially limited to a very shallow depth (usually only 1 split), to ensure that each model is only slightly better than random chance. For this reason, boosting algorithms are also highly resilient against noisy data and overfitting. Since the individual weak learners are too simple to overfit, it is very hard to combine them in such a way as to overfit the training data as a whole -- especially when they focus on different things, due to the iterative nature of the algorithm.\u003c/p\u003e\n\u003ch4\u003e3: Aggregate predictions\u003c/h4\u003e\n\u003cp\u003eThe final major difference we'll talk about between the two is the way predictions are aggregated. Whereas in a random forest, each tree simply votes for the final result, boosting algorithms usually employ a system of weights to determine how important the input for each tree is. Since we know how well each weak learner performs on the dataset by calculating its performance at each step, we can see which weak learners do better on hard tasks. Think of it like this -- harder problems deserve more weight. If there are many learners in the overall ensemble that can get the same questions right, then that tree isn't super important -- other trees already provide the same value that it does. This tree will have its overall weight reduced. As more and more trees get a hard problem wrong, the \"reward\" for a tree getting that hard problem correct goes higher and higher. This \"reward\" is actually just a higher weight when calculating the overall vote. Intuitively, this makes sense -- trees that can do what few other trees can do are the ones that we should probably listen to more than others, as they are the most likely to get hard examples correct. Since other trees tend to get this wrong, we can expect to see a general split of about 50/50 among the trees that do not \"specialize\" in the hard problems. Since our \"specialized\" tree has more weight, its correct vote will carry more weight than the combined votes of the half of the \"unspecialized\" trees that get it wrong. It is worth noting that the \"specialized\" trees will often do quite poorly on the examples that are easy to predict. However, since these examples are easier, we can expect a strong majority of the trees in our ensemble to get it right, meaning that the combined, collective weight of their agreement will be enough to overrule the trees with higher weights that get it wrong.\u003c/p\u003e\n\u003ch2\u003eUnderstanding Adaboost and Gradient boosting\u003c/h2\u003e\n\u003cp\u003eThere are two main algorithms that come to mind when Data Scientists talk about boosting: \u003cstrong\u003e\u003cem\u003eAdaboost\u003c/em\u003e\u003c/strong\u003e (short for Adaptive Boosting), and \u003cstrong\u003e\u003cem\u003eGradient Boosted Trees\u003c/em\u003e\u003c/strong\u003e. Both are generally very effective, but they use different methods to achieve their results.\u003c/p\u003e\n\u003ch3\u003eAdaboost\u003c/h3\u003e\n\u003cp\u003eAdaboost was the first boosting algorithm invented. Although there have been marked improvements made to this algorithm, Adaboost still tends to be quite an effective algorithm! More importantly, it's a good starting place for understanding how boosting algorithms actually work.\u003c/p\u003e\n\u003cp\u003eIn Adaboost, each learner is trained on a subsample of the dataset, much like we saw with \u003cstrong\u003e\u003cem\u003eBagging\u003c/em\u003e\u003c/strong\u003e. Initially, the bag is randomly sampled with replacement. However, each data point in the dataset has a weight assigned. As learners correctly classify an example, that example's weight is reduced. Conversely, when learners get an example wrong, the weight for that sample increases. In each iteration, these weights act as the probability that an item will be sampled into the \"bag\" which will be used to train the next weak learner. As the number of learners grows, you can imagine that the examples that are easy to get correct will become less and less prevalent in the samples used to train each new learner. This is a good thing -- if our ensemble already contains multiple learners that can correctly classify that example, then we don't need more that can do this. Instead, the \"bags\" of data will contain multiple instances of the hard examples, thereby increasing the likelihood that the learner will create a split that focuses on getting the hard example correct.\u003c/p\u003e\n\u003cp\u003eThe following diagram demonstrates how the weights change for each example as classifiers get them right and wrong.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-gradient-boosting-and-weak-learners/master/images/new_adaboost.png\" width=\"600\"\u003e\u003c/p\u003e\n\u003cp\u003ePay attention to the colors of the pluses and minuses -- pluses are meant to be in the blue section, and minuses are meant to be in the red. The decision boundary of the tree can be interpreted as the line drawn between the red and blue sections. As you can see above, examples that were misclassified are larger in the next iteration, while examples that were classified correctly are smaller. As we combine the decision boundaries of each new classifier, we end up with a classifier that correctly classifies all of the examples!\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e\u003cem\u003eKey Takeaway:\u003c/em\u003e\u003c/strong\u003e Adaboost creates new classifiers by continually influencing the distribution of the data sampled to train each successive learner.\u003c/p\u003e\n\u003ch3\u003eGradient boosting\u003c/h3\u003e\n\u003cp\u003e\u003cstrong\u003e\u003cem\u003eGradient Boosted Trees\u003c/em\u003e\u003c/strong\u003e are a more advanced boosting algorithm that makes use of \u003cstrong\u003e\u003cem\u003eGradient Descent.\u003c/em\u003e\u003c/strong\u003e Much like Adaboost, gradient boosting starts with a weak learner that makes predictions on the dataset. The algorithm then checks this learner's performance, identifying examples that it got right and wrong. However, this is where the gradient boosting algorithm diverges from Adaboost's methodology. The model then calculates the \u003cstrong\u003e\u003cem\u003eResiduals\u003c/em\u003e\u003c/strong\u003e for each data point, to determine how far off the mark each prediction was. The model then combines these residuals with a \u003cstrong\u003e\u003cem\u003eLoss Function\u003c/em\u003e\u003c/strong\u003e to calculate the overall loss. There are many loss functions that are used -- the thing that matters most is that the loss function is \u003cstrong\u003e\u003cem\u003edifferentiable\u003c/em\u003e\u003c/strong\u003e so that we can use calculus to compute the gradient for the loss, given the inputs of the model. We then use the gradients and the loss as predictors to train the next tree against! In this way, we can use \u003cstrong\u003e\u003cem\u003eGradient Descent\u003c/em\u003e\u003c/strong\u003e to minimize the overall loss.\u003c/p\u003e\n\u003cp\u003eSince the loss is most heavily inflated by examples where the model was wrong, gradient descent will push the algorithm towards creating a new learner that will focus on these harder examples. If the next tree gets these right, then the loss goes down! In this way, gradient descent allows us to continually train and improve on the loss for each model to improve the overall performance of the ensemble as a whole by focusing on the \"hard\" examples that cause the loss to be high.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-gradient-boosting-and-weak-learners/master/images/new_gradient-boosting.png\"\u003e\u003c/p\u003e\n\u003ch3\u003eLearning rates\u003c/h3\u003e\n\u003cp\u003eOften, we want to artificially limit the \"step size\" we take in gradient descent. Small, controlled changes in the parameters we're optimizing with gradient descent will mean that the overall process is slower, but the parameters are more likely to converge to their optimal values. The learning rate for your model is a small scalar meant to artificially reduce the step size in gradient descent. Learning rate is a tunable parameter for your model that you can set -- large learning rates get closer to the optimal values more quickly, but have trouble landing exactly at the optimal values because the step size is too big for the small distances it needs to travel when it gets close. Conversely, small learning rates means the model will take a longer time to get to the optimal parameters, but when it does get there, it will be extremely close to the optimal values, thereby providing the best overall performance for the model.\u003c/p\u003e\n\u003cp\u003eYou'll often see learning rates denoted by the symbol, \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cgamma\"\u003e -- this is the greek letter, \u003cstrong\u003e\u003cem\u003egamma\u003c/em\u003e\u003c/strong\u003e. Don't worry if you're still hazy on the concept of gradient descent -- we'll explore it in much more detail when we start studying deep learning!\u003c/p\u003e\n\u003cp\u003eThe \u003ccode\u003esklearn\u003c/code\u003e library contains some excellent implementations of Adaboost, as well as several different types of gradient boosting classifiers. These classifiers can be found in the \u003ccode\u003eensemble\u003c/code\u003e module, which you will make use of in the upcoming lesson.\u003c/p\u003e\n\u003ch2\u003eSummary\u003c/h2\u003e\n\u003cp\u003eIn this lesson, we learned about \u003cstrong\u003e\u003cem\u003eWeak Learners\u003c/em\u003e\u003c/strong\u003e, and how they are used in various \u003cstrong\u003e\u003cem\u003eGradient Boosting\u003c/em\u003e\u003c/strong\u003e algorithms. We also learned about two specific algorithms -- \u003cstrong\u003e\u003cem\u003eAdaBoost\u003c/em\u003e\u003c/strong\u003e and \u003cstrong\u003e\u003cem\u003eGradient Boosted Trees\u003c/em\u003e\u003c/strong\u003e, and we compared how they are similar and how they are different!\u003c/p\u003e","frontPage":false},{"exportId":"gridsearchcv","title":"GridSearchCV","type":"WikiPage","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-gridsearchcv\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-gridsearchcv/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eIn this lesson, we'll explore the concept of parameter tuning to maximize our model performance using a combinatorial grid search!\u003c/p\u003e\n\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\n\u003cp\u003eYou will be able to:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eDesign a parameter grid for use with scikit-learn's \u003ccode\u003eGridSearchCV\u003c/code\u003e\u003cbr\u003e\n\u003c/li\u003e\n\u003cli\u003eUse \u003ccode\u003eGridSearchCV\u003c/code\u003e to increase model performance through parameter tuning\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch2\u003eParameter tuning\u003c/h2\u003e\n\n\u003cp\u003eBy now, you've seen that the process of building and training a supervised learning model is an iterative one. Your first model rarely performs the best! There are multiple ways we can potentially improve model performance. Thus far, most of the techniques we've used have been focused on our data. We can get better data, or more data, or both. We can engineer certain features, or clean up the data by removing rows/variables that hurt model performance, like multicollinearity. \u003c/p\u003e\n\n\u003cp\u003eThe other major way to potentially improve model performance is to find good parameters to set when creating the model. For example, if we allow a decision tree to have too many leaves, the model will almost certainly overfit the data. Too few, and the model will underfit. However, each modeling problem is unique -- the same parameters could cause either of those situations, depending on the data, the task at hand, and the complexity of the model needed to best fit the data. \u003c/p\u003e\n\n\u003cp\u003eIn this lesson, we'll learn how we can use a \u003cstrong\u003e\u003cem\u003ecombinatorial grid search\u003c/em\u003e\u003c/strong\u003e to find the best combination of parameters for a given model. \u003c/p\u003e\n\n\u003ch2\u003eGrid search\u003c/h2\u003e\n\n\u003cp\u003eWhen we set parameters in a model, the parameters are not independent of one another -- the value set for one parameter can have significant effects on other parameters, thereby affecting overall model performance. Consider the following grid.\u003c/p\u003e\n\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth style=\"text-align: center;\"\u003eParameter\u003c/th\u003e\n\u003cth style=\"text-align: center;\"\u003e1\u003c/th\u003e\n\u003cth style=\"text-align: center;\"\u003e2\u003c/th\u003e\n\u003cth style=\"text-align: center;\"\u003e3\u003c/th\u003e\n\u003cth style=\"text-align: center;\"\u003e4\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd style=\"text-align: center;\"\u003ecriterion\u003c/td\u003e\n\u003ctd style=\"text-align: center;\"\u003e\"gini\"\u003c/td\u003e\n\u003ctd style=\"text-align: center;\"\u003e\"entropy\"\u003c/td\u003e\n\u003ctd style=\"text-align: center;\"\u003e\u003c/td\u003e\n\u003ctd style=\"text-align: center;\"\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd style=\"text-align: center;\"\u003emax_depth\u003c/td\u003e\n\u003ctd style=\"text-align: center;\"\u003e1\u003c/td\u003e\n\u003ctd style=\"text-align: center;\"\u003e2\u003c/td\u003e\n\u003ctd style=\"text-align: center;\"\u003e5\u003c/td\u003e\n\u003ctd style=\"text-align: center;\"\u003e10\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd style=\"text-align: center;\"\u003emin\u003cem\u003esamples\u003c/em\u003esplit\u003c/td\u003e\n\u003ctd style=\"text-align: center;\"\u003e1\u003c/td\u003e\n\u003ctd style=\"text-align: center;\"\u003e5\u003c/td\u003e\n\u003ctd style=\"text-align: center;\"\u003e10\u003c/td\u003e\n\u003ctd style=\"text-align: center;\"\u003e20\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\n\u003cp\u003eAll the parameters above work together to create the framework of the decision tree that will be trained. For a given problem, it may be the case that increasing the value of the parameter for \u003ccode\u003emin_samples_split\u003c/code\u003e generally improves model performance up to a certain point, by reducing overfitting. However, if the value for \u003ccode\u003emax_depth\u003c/code\u003e is too low or too high, this may doom the model to overfitting or underfitting, by having a tree with too many arbitrary levels and splits that overfit on noise, or limiting the model to nothing more than a \"stump\" by only allowing it to grow to one or two levels. \u003c/p\u003e\n\n\u003cp\u003eSo how do we know which combination of parameters is best? The only way we can really know for sure is to try \u003cstrong\u003e\u003cem\u003eevery single combination!\u003c/em\u003e\u003c/strong\u003e For this reason, grid search is sometimes referred to as an \u003cstrong\u003e\u003cem\u003eexhaustive search\u003c/em\u003e\u003c/strong\u003e. \u003c/p\u003e\n\n\u003ch2\u003eUse \u003ccode\u003eGridSearchCV\u003c/code\u003e\n\u003c/h2\u003e\n\n\u003cp\u003eThe \u003ccode\u003esklearn\u003c/code\u003e library provides an easy way to tune model parameters through an exhaustive search by using its \u003ca href=\"https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html\"\u003e\u003ccode\u003eGridSearchCV\u003c/code\u003e\u003c/a\u003e class, which can be found inside the \u003ccode\u003emodel_selection\u003c/code\u003e module. \u003ccode\u003eGridsearchCV\u003c/code\u003e combines \u003cstrong\u003e\u003cem\u003eK-Fold Cross-Validation\u003c/em\u003e\u003c/strong\u003e with a grid search of parameters. In order to do this, we must first create a \u003cstrong\u003e\u003cem\u003eparameter grid\u003c/em\u003e\u003c/strong\u003e that tells \u003ccode\u003esklearn\u003c/code\u003e which parameters to tune, and which values to try for each of those parameters. \u003c/p\u003e\n\n\u003cp\u003eThe following code snippet demonstrates how to use \u003ccode\u003eGridSearchCV\u003c/code\u003e to perform a parameter grid search using a sample parameter grid, \u003ccode\u003eparam_grid\u003c/code\u003e. Our parameter grid should be a dictionary, where the keys are the parameter names, and the values are the different parameter values we want to use in our grid search for each given key. After creating the dictionary, all you need to do is pass it to \u003ccode\u003eGridSearchCV()\u003c/code\u003e along with the classifier. YOu can also use K-fold cross-validation during this process, by specifying the \u003ccode\u003ecv\u003c/code\u003e parameter. In this case, we choose to use 3-fold cross-validation for each model created inside our grid search. \u003c/p\u003e\n\n\u003cpre\u003e\u003ccode class=\"python\"\u003e\nclf = DecisionTreeClassifier()\n\nparam_grid = {\n    'criterion': ['gini', 'entropy'],\n    'max_depth': [1, 2, 5, 10],\n    'min_samples_split': [1, 5, 10, 20]\n}\n\ngs_tree = GridSearchCV(clf, param_grid, cv=3)\ngs_tree.fit(train_data, train_labels)\n\ngs_tree.best_params_\n\u003c/code\u003e\u003c/pre\u003e\n\n\u003cp\u003eThis code will run all combinations of the parameters above. The first model to be trained would be \u003ccode\u003eDecisionTreeClassifier(criterion='gini', max_depth=1, min_samples_split=1)\u003c/code\u003e using a 3-fold cross-validation, and recording the average score. Then, it will change one parameter, and repeat the process (e.g., \u003ccode\u003eDecisionTreeClassifier(criterion='gini', max_depth=1, min_samples_split=5)\u003c/code\u003e, and so on), keeping track of the overall performance of each model. Once it has tried every combination, the \u003ccode\u003eGridSearchCV\u003c/code\u003e object we created will automatically default the model that had the best score. We can even access the best combination of parameters by checking the \u003ccode\u003ebest_params_\u003c/code\u003e attribute! \u003c/p\u003e\n\n\u003ch2\u003eDrawbacks of \u003ccode\u003eGridSearchCV\u003c/code\u003e\n\u003c/h2\u003e\n\n\u003cp\u003eGridSearchCV is a great tool for finding the best combination of parameters. However, it is only as good as the parameters we put in our parameter grid -- so we need to be very thoughtful during this step! \u003c/p\u003e\n\n\u003cp\u003eThe main drawback of an exhaustive search such as \u003ccode\u003eGridsearchCV\u003c/code\u003e is that there is no way of telling what's best until we've exhausted all possibilities! This means training many versions of the same machine learning model, which can be very time consuming and computationally expensive. Consider the example code above -- we have three different parameters, with 2, 4, and 4 variations to try, respectively. We also set the model to use cross-validation with a value of 3, meaning that each model will be built 3 times, and their performances averaged together. If we do some simple math, we can see that this simple grid search we see above actually results in \u003ccode\u003e2 * 4 * 4 * 3 =\u003c/code\u003e \u003cstrong\u003e\u003cem\u003e96 different models trained!\u003c/em\u003e\u003c/strong\u003e For projects that involve complex models and/or very large datasets, the time needed to run a grid search can often be prohibitive. For this reason, be very thoughtful about the parameters you set -- sometimes the extra runtime isn't worth it -- especially when there's no guarantee that the model performance will improve!\u003c/p\u003e\n\n\u003ch2\u003eSummary\u003c/h2\u003e\n\n\u003cp\u003eIn this lesson, you learned about grid search, how to perform grid search, and the drawbacks associated with the method!\u003c/p\u003e\n\u003cfooter class=\"fis-footer\" style=\"visibility: hidden;\"\u003e\u003cdiv class=\"fis-feedback\"\u003e\n\u003ch5\u003eHow do you feel about this lesson?\u003c/h5\u003e\n\u003cimg id=\"thumbs-up\" data-repository=\"dsc-gridsearchcv\" title=\"Thumbs up!\" alt=\"thumbs up\"\u003e\u003cimg id=\"thumbs-down\" data-repository=\"dsc-gridsearchcv\" title=\"Thumbs down!\" alt=\"thumbs down\"\u003e\n\u003c/div\u003e\n\u003ch5\u003eHave specific feedback? \u003ca href=\"https://github.com/learn-co-curriculum/dsc-gridsearchcv/issues/new\"\u003eTell us here!\u003c/a\u003e\n\u003c/h5\u003e\u003c/footer\u003e","frontPage":false},{"exportId":"pipelines-introduction","title":"Pipelines - Introduction","type":"WikiPage","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-pipelines-intro\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-pipelines-intro/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eIn this section, you'll learn about machine learning pipelines. Pipelines are extremely useful for allowing data scientists to quickly and consistently transform data, train, and use machine learning models. This lesson will summarize the key topics you'll be covering.\u003c/p\u003e\n\n\u003ch2\u003eBuilding a Machine Learning Pipeline\u003c/h2\u003e\n\n\u003cp\u003eBy now, you know that the data science process is a flow of activities, from inspecting the data to cleaning it, transforming it, running a model, and discussing the results. Wouldn't it be nice if there was a streamlined process to create nice machine learning workflows? Enter machine learning pipelines in scikit-learn!\u003c/p\u003e\n\n\u003cp\u003eIn this section, you'll learn how you can use a pipeline to integrate several steps of the machine learning workflow. Additionally, you'll compare several classification techniques with each other, and integrate grid search in your pipeline so you can tune several hyperparameters in each of the machine learning models.\u003c/p\u003e\n\n\u003ch2\u003eSummary\u003c/h2\u003e\n\n\u003cp\u003eThis section will quickly introduce how to create pipelines in scikit-learn, but it will then be up to you to explore the magical world of pipelines and practice all your machine learning knowledge gained in this module!\u003c/p\u003e\n\u003cfooter class=\"fis-footer\" style=\"visibility: hidden;\"\u003e\u003cdiv class=\"fis-feedback\"\u003e\n\u003ch5\u003eHow do you feel about this lesson?\u003c/h5\u003e\n\u003cimg id=\"thumbs-up\" data-repository=\"dsc-pipelines-intro\" title=\"Thumbs up!\" alt=\"thumbs up\"\u003e\u003cimg id=\"thumbs-down\" data-repository=\"dsc-pipelines-intro\" title=\"Thumbs down!\" alt=\"thumbs down\"\u003e\n\u003c/div\u003e\n\u003ch5\u003eHave specific feedback? \u003ca href=\"https://github.com/learn-co-curriculum/dsc-pipelines-intro/issues/new\"\u003eTell us here!\u003c/a\u003e\n\u003c/h5\u003e\u003c/footer\u003e","frontPage":false},{"exportId":"linear-algebra-recap","title":"Linear Algebra - Recap","type":"WikiPage","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-linalg-section-recap\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-linalg-section-recap/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eIn this section, you learned the fundamentals of linear algebra. An understanding of linear algebra will help you better understand the underlying mathematics behind some machine learning algorithms.\u003c/p\u003e\n\n\u003ch2\u003eKey Takeaways\u003c/h2\u003e\n\n\u003cp\u003eThe goal of this section was to provide both a conceptual and computational introduction to linear algebra - one of the foundational concepts underlying most machine learning models. Some of the key takeaways include: \u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eOne use case for vectors and matrices is for representing and solving systems of linear equations\u003c/li\u003e\n\u003cli\u003eA scalar is a single, real number. A vector is a one-dimensional array of numbers. A matrix is a 2-dimensional array of numbers \u003c/li\u003e\n\u003cli\u003eA tensor is a generalized term for an n-dimensional rectangular grid of numbers. A vector is a one-dimensional (first-order tensor), a matrix is a two-dimensional (second-order tensor), etc.\u003c/li\u003e\n\u003cli\u003eTwo matrices can be added together if they have the same shape\u003c/li\u003e\n\u003cli\u003eScalars can be added to matrices by adding the scalar (number) to each element\u003c/li\u003e\n\u003cli\u003eTo calculate the dot product for matrix multiplication, the first matrix must have the same number of columns as the number of rows in the second matrix \u003c/li\u003e\n\u003cli\u003eOperating on NumPy data types is substantially more computationally efficient than performing the same operations on native Python data types\u003c/li\u003e\n\u003cli\u003eIt is possible to use linear algebra in NumPy to solve for a linear regression using the OLS method\u003c/li\u003e\n\u003cli\u003eOLS is not computationally efficient, so in practice, we usually perform a gradient descent instead to solve a linear regression\u003c/li\u003e\n\u003c/ul\u003e\n\u003cfooter class=\"fis-footer\" style=\"visibility: hidden;\"\u003e\u003cdiv class=\"fis-feedback\"\u003e\n\u003ch5\u003eHow do you feel about this lesson?\u003c/h5\u003e\n\u003cimg id=\"thumbs-up\" data-repository=\"dsc-linalg-section-recap\" title=\"Thumbs up!\" alt=\"thumbs up\"\u003e\u003cimg id=\"thumbs-down\" data-repository=\"dsc-linalg-section-recap\" title=\"Thumbs down!\" alt=\"thumbs down\"\u003e\n\u003c/div\u003e\n\u003ch5\u003eHave specific feedback? \u003ca href=\"https://github.com/learn-co-curriculum/dsc-linalg-section-recap/issues/new\"\u003eTell us here!\u003c/a\u003e\n\u003c/h5\u003e\u003c/footer\u003e","frontPage":false},{"exportId":"decision-trees-introduction","title":"Decision Trees - Introduction","type":"WikiPage","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-decision-trees-section-intro\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-decision-trees-section-intro/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eIn this section, we're going to introduce another kind of model for predicting values that can be used for both continuous and categorical predictions - decision trees. Decision trees are used to classify (or estimate continuous values) by partitioning the sample space as efficiently as possible into sets with similar data points until you get to (or close to) a homogenous set and can reasonably predict the value for new data points. \u003c/p\u003e\n\n\u003cp\u003eDespite the fact that they've been around for decades, they are still (in conjunction with ensemble methods that we'll learn about in the next section) one of the most powerful modeling tools available in the field of machine learning. They are also highly interpretable when compared to more complex models (they're simple to explain and it's easy to understand how they make their decisions).\u003c/p\u003e\n\n\u003ch3\u003eEntropy and Information Gain\u003c/h3\u003e\n\n\u003cp\u003eDue to the nature of decision trees, you can get very different predictions depending on what questions you ask and in what order. The question then is how to come up with the right questions to ask in the right order. In this section, we also introduce the idea of entropy and information gain as mechanisms for selecting the most promising questions to ask in a decision tree.\u003c/p\u003e\n\n\u003ch3\u003eID3 Classification Trees\u003c/h3\u003e\n\n\u003cp\u003eWe also talk about Ross Quinlan's ID3 (Iterative Dichotomiser 3) algorithm for generating a decision tree from a dataset. \u003c/p\u003e\n\n\u003ch3\u003eBuilding Trees using Scikit-learn\u003c/h3\u003e\n\n\u003cp\u003eNext up, we look at how to build a decision tree using the built-in functions available in scikit-learn, and how to test the accuracy of the predictions using a simple accuracy measure, AUC, and a confusion matrix. We also show how to use the \u003ccode\u003egraph_viz\u003c/code\u003e library to generate a visualization of the resulting decision tree.\u003c/p\u003e\n\n\u003ch3\u003eHyperparameter Tuning and Pruning\u003c/h3\u003e\n\n\u003cp\u003eWe then look at some of the hyperparameters available when optimizing a decision tree. For example, if you're not careful, generated decision trees can lead to overfitting of data (wherein a model is a perfect match for training data, but horrible for test data). There are a number of hyperparameters you can use when generating a tree to minimize overfitting such as maximum depth or minimum leaf sample size. We look at these various \"pruning\" strategies to avoid overfitting of the data and to create a better model. \u003c/p\u003e\n\n\u003ch3\u003eRegression with CART Trees\u003c/h3\u003e\n\n\u003cp\u003eIn addition to building decision tree classifiers, you will also build decision trees for regression problems. \u003c/p\u003e\n\n\u003ch2\u003eSummary\u003c/h2\u003e\n\n\u003cp\u003eDecision trees are highly effective and interpretable. This section will provide you with the skills to create both classifiers and to perform regression using decision trees and to use hyperparameter tuning to optimize your model.\u003c/p\u003e\n\u003cfooter class=\"fis-footer\" style=\"visibility: hidden;\"\u003e\u003cdiv class=\"fis-feedback\"\u003e\n\u003ch5\u003eHow do you feel about this lesson?\u003c/h5\u003e\n\u003cimg id=\"thumbs-up\" data-repository=\"dsc-decision-trees-section-intro\" title=\"Thumbs up!\" alt=\"thumbs up\"\u003e\u003cimg id=\"thumbs-down\" data-repository=\"dsc-decision-trees-section-intro\" title=\"Thumbs down!\" alt=\"thumbs down\"\u003e\n\u003c/div\u003e\n\u003ch5\u003eHave specific feedback? \u003ca href=\"https://github.com/learn-co-curriculum/dsc-decision-trees-section-intro/issues/new\"\u003eTell us here!\u003c/a\u003e\n\u003c/h5\u003e\u003c/footer\u003e","frontPage":false},{"exportId":"the-gradient-in-gradient-descent","title":" The Gradient in Gradient Descent","type":"WikiPage","content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-the-gradient-in-gradient-descent\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-the-gradient-in-gradient-descent\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-the-gradient-in-gradient-descent/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eAs you know, we entered our discussion of derivatives to determine the size and direction of a step with which to move along a cost curve. We first used a derivative in a single variable function to see how the output of our cost curve changed with respect to change a change in one of our regression line's variables. Then we learned about partial derivatives to see how a \u003cem\u003ethree-dimensional cost curve\u003c/em\u003e responded to a change in the regression line.\u003c/p\u003e\n\u003cp\u003eHowever, we have not yet explicitly showed how partial derivatives apply to gradient descent.\u003c/p\u003e\n\u003cp\u003eWell, that's what we hope to show in this lesson: explain how we can use partial derivatives to find the path to minimize our cost function, and thus find our \"best fit\" regression line.\u003c/p\u003e\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\u003cp\u003eYou will be able to:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eDefine a gradient in relation to gradient descent\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003eWhat is the gradient?\u003c/h2\u003e\n\u003cp\u003eNow gradient descent literally means that we are taking the shortest path to \u003cem\u003edescend\u003c/em\u003e towards our minimum. However, it is somewhat easier to understand gradient \u003cem\u003eascent\u003c/em\u003e than descent, and the two are quite related, so that's where we'll begin. Gradient ascent, as you could guess, simply means that we want to move in the direction of steepest ascent.\u003c/p\u003e\n\u003cp\u003eNow moving in the direction of greatest ascent for a function \u003cimg src=\"https://render.githubusercontent.com/render/math?math=f(x,y)\"\u003e , means that our next step is a step some distance in the \u003cimg src=\"https://render.githubusercontent.com/render/math?math=x\"\u003e direction and some distance in the \u003cimg src=\"https://render.githubusercontent.com/render/math?math=y\"\u003e direction which is the steepest upward at that point.\u003c/p\u003e\n\u003cp\u003eNote how this is a different task from what we have previously worked on for multivariable functions. So far, we have used partial derivatives to calculate the \u003cstrong\u003egain\u003c/strong\u003e from moving directly in either the \u003cimg src=\"https://render.githubusercontent.com/render/math?math=x\"\u003e direction or the \u003cimg src=\"https://render.githubusercontent.com/render/math?math=y\"\u003e direction.\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eHere, in finding gradient ascent, our task is not to calculate the gain from a move in either the \u003cimg src=\"https://render.githubusercontent.com/render/math?math=x\"\u003e or \u003cimg src=\"https://render.githubusercontent.com/render/math?math=y\"\u003e direction. Instead, our task is to \u003cstrong\u003efind some combination of a change in \u003cimg src=\"https://render.githubusercontent.com/render/math?math=x\"\u003e , \u003cimg src=\"https://render.githubusercontent.com/render/math?math=y\"\u003e that brings the largest change in output\u003c/strong\u003e.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eSo if you look at the path our climbers are taking in the picture above, \u003cem\u003ethat\u003c/em\u003e is the direction of gradient ascent. If they tilt their path to the right or left, they will no longer be moving along the steepest upward path.\u003c/p\u003e\n\u003cp\u003eThe direction of the greatest rate of increase of a function is called the gradient. We denote the gradient with the nabla, which comes from the Greek word for harp, which is kind of what it looks like: \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cnabla\"\u003e. So we can denote the gradient of a function, \u003cimg src=\"https://render.githubusercontent.com/render/math?math=f(x,%20y)\"\u003e , with \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cnabla%20f(x,%20y)\"\u003e .\u003c/p\u003e\n\u003ch2\u003eCalculating the gradient\u003c/h2\u003e\n\u003cp\u003eNow how do we find the direction for the greatest rate of increase? We use partial derivatives. Here's why.\u003c/p\u003e\n\u003cp\u003eAs we know, the partial derivative \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cfrac%7Bdf%7D%7Bdx%7D\"\u003e calculates the change in output from moving a little bit in the \u003cimg src=\"https://render.githubusercontent.com/render/math?math=x\"\u003e direction, and the partial derivative \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cfrac%7Bdf%7D%7Bdy%7D\"\u003e calculates the change in output from moving in the \u003cimg src=\"https://render.githubusercontent.com/render/math?math=y\"\u003e direction. Because with gradient ascent our goal is to make a nudge in \u003cimg src=\"https://render.githubusercontent.com/render/math?math=x,%20y\"\u003e that produces the greatest change in output, if \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cfrac%7Bdf%7D%7Bdy%7D%20\u003e%20%5Cfrac%7Bdf%7D%7Bdx%7D\"\u003e , we should make that move more in the \u003cimg src=\"https://render.githubusercontent.com/render/math?math=y\"\u003e direction than the \u003cimg src=\"https://render.githubusercontent.com/render/math?math=x\"\u003e direction, and vice versa. That is, we want to get the biggest bang for our buck.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-the-gradient-in-gradient-descent/master/images/Denali.jpg\"\u003e\u003c/p\u003e\n\u003cp\u003eLet's relate this again to mountain climbers. Imagine the vertical edge on the left is our y-axis and the horizontal edge is on the bottom is our x-axis. For the climber in the yellow jacket, imagine his step size is three feet. A step straight along the y-axis will move him further upwards than a step along the x-axis. So in taking that step, he should direct himself more towards the y-axis than the x-axis. That will produce a bigger increase per step size.\u003c/p\u003e\n\u003cp\u003eIn fact, the direction of greatest ascent for a function, \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cnabla%20f(x,%20y)\"\u003e , is the direction which is a proportion of \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cfrac%7Bdf%7D%7Bdy%7D\"\u003e steps in the \u003cimg src=\"https://render.githubusercontent.com/render/math?math=y\"\u003e direction and \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cfrac%7Bdf%7D%7Bdx%7D\"\u003e in the \u003cimg src=\"https://render.githubusercontent.com/render/math?math=x\"\u003e direction. So, for example, if \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cfrac%7Bdf%7D%7Bdy%7D\"\u003e = 5 and \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cfrac%7Bdf%7D%7Bdx%7D\"\u003e = 1, the direction of gradient ascent is five times more in the \u003cimg src=\"https://render.githubusercontent.com/render/math?math=y\"\u003e direction than the \u003cimg src=\"https://render.githubusercontent.com/render/math?math=x\"\u003e direction. And this seems to be the path, more or less that our climbers are taking - some combination of \u003cimg src=\"https://render.githubusercontent.com/render/math?math=x\"\u003e and \u003cimg src=\"https://render.githubusercontent.com/render/math?math=y\"\u003e , but tilted more towards the \u003cimg src=\"https://render.githubusercontent.com/render/math?math=y\"\u003e direction.\u003c/p\u003e\n\u003ch2\u003eApplying Gradient Descent\u003c/h2\u003e\n\u003cp\u003eNow that we have a better understanding of a gradient, let's apply our understanding to a multivariable function. Here is a plot of a function:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=f(x,y)%20=%202x%20%2b%203y\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-the-gradient-in-gradient-descent/master/images/new_gradDescinDesc.png\" width=\"400\"\u003e\u003c/p\u003e\n\u003cp\u003eImagine being at the bottom left of the graph at the point \u003cimg src=\"https://render.githubusercontent.com/render/math?math=x%20=%201\"\u003e , \u003cimg src=\"https://render.githubusercontent.com/render/math?math=y%20=%201\"\u003e . What would be the direction of steepest ascent? It seems, just sizing it up visually, that we should move both in the positive \u003cimg src=\"https://render.githubusercontent.com/render/math?math=y\"\u003e direction and the positive \u003cimg src=\"https://render.githubusercontent.com/render/math?math=x\"\u003e direction. Looking more carefully, it seems we should move \u003cstrong\u003emore\u003c/strong\u003e in the \u003cimg src=\"https://render.githubusercontent.com/render/math?math=y\"\u003e direction than the \u003cimg src=\"https://render.githubusercontent.com/render/math?math=x\"\u003e direction. Let's see what our technique of taking the partial derivative indicates.\u003c/p\u003e\n\u003cp\u003eThe gradient of the function \u003cimg src=\"https://render.githubusercontent.com/render/math?math=f(x,y)\"\u003e , that is \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cnabla%20f(x,y)%20=%202x%20%2b%203y\"\u003e is the following:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cfrac%7Bdf%7D%7Bdx%7D(2x%20%2b%203y)%20=%202\"\u003e and \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cfrac%7Bdf%7D%7Bdy%7D(2x%20%2b%203y)%20=%203\"\u003e .\u003c/p\u003e\n\u003cp\u003eSo what this tells us is to move in the direction of greatest ascent for the function \u003cimg src=\"https://render.githubusercontent.com/render/math?math=f(x,y)%20=%202x%20%2b%203y\"\u003e , is to move up three and to the right two. So we would expect our path of greatest ascent to look like the following.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-the-gradient-in-gradient-descent/master/images/gradient-plot.png\" width=\"400\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-the-gradient-in-gradient-descent/master/images/new_gradDescinDesc.png\" width=\"400\"\u003e\u003c/p\u003e\n\u003cp\u003eSo this path maps up well to what we see visually. That is the idea behind gradient descent. The gradient is the partial derivative with respect to each type of variable of a multivariable function, in this case \u003cimg src=\"https://render.githubusercontent.com/render/math?math=x\"\u003e and \u003cimg src=\"https://render.githubusercontent.com/render/math?math=y\"\u003e . And the importance of the gradient is that its direction is the direction of steepest ascent. The negative gradient, that is the negative of each of the partial derivatives, is the direction of steepest descent. So our direction of gradient descent for the graph above is \u003cimg src=\"https://render.githubusercontent.com/render/math?math=x%20=%20-2\"\u003e , \u003cimg src=\"https://render.githubusercontent.com/render/math?math=y%20=%20-3\"\u003e . And looking at the two graphs above, it seems that the steepest downward direction is just the opposite of the steepest upward direction. We get that by mathematically by simply taking the multiplying our partial derivatives by negative one.\u003c/p\u003e\n\u003ch2\u003eSummary\u003c/h2\u003e\n\u003cp\u003eIn this lesson, you saw how to use gradient descent to find the direction of steepest descent. You saw that the direction of steepest descent is generally some combination of a change in your variables to produce the greatest negative rate of change.\u003c/p\u003e\n\u003cp\u003eYou first how saw how to calculate the gradient \u003cstrong\u003eascent\u003c/strong\u003e, or the gradient \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cnabla\"\u003e , by calculating the partial derivative of a function with respect to the variables of the function. So \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cnabla%20f(x,%20y)%20=%20%5Cfrac%7B%5Cdelta%20f%7D%7B%5Cdelta%20y%7D,%20%5Cfrac%7B%5Cdelta%20f%7D%7B%5Cdelta%20x%7D\"\u003e . This means that to take the path of greatest ascent, you should move \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cfrac%7B%5Cdelta%20f%7D%7B%5Cdelta%20y%7D\"\u003e divided by \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cfrac%7B%5Cdelta%20f%7D%7B%5Cdelta%20x%7D\"\u003e . So for example, when \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cfrac%7B%5Cdelta%20f%7D%7B%5Cdelta%20y%7Df(x,%20y)%20%20=%203\"\u003e , and \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cfrac%7B%5Cdelta%20f%7D%7B%5Cdelta%20x%7Df(x,%20y)%20%20=%202\"\u003e , you traveled in line with a slope of 3/2.\u003c/p\u003e\n\u003cp\u003eFor gradient descent, that is to find the direction of greatest decrease, you simply reverse the direction of your partial derivatives and move in \u003cimg src=\"https://render.githubusercontent.com/render/math?math=-%20%5Cfrac%7B%5Cdelta%20f%7D%7B%5Cdelta%20y%7D,%20-%20%5Cfrac%7B%5Cdelta%20f%7D%7B%5Cdelta%20x%7D\"\u003e .\u003c/p\u003e","frontPage":false},{"exportId":"feature-and-model-selection-aic-and-bic","title":"Feature and Model Selection: AIC and BIC","type":"WikiPage","content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-feature-and-model-selection-aic-and-bic\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-feature-and-model-selection-aic-and-bic\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-feature-and-model-selection-aic-and-bic/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003ePreviously, you've seen how you can select ways of assessing your model fit metrics like MSE, SSE, and \u003cimg src=\"https://render.githubusercontent.com/render/math?math=R%5E2\"\u003e . These values almost always improve when adding more variables, so if you only use these metrics to determine the optimal features of your model, it is highly likely that you will overfit the model to your data. In this lesson you'll be introduced to two new measures: AIC and BIC, which give you a comprehensive measure of model performace taking into account the additional variables.\u003c/p\u003e\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\u003cp\u003eYou will be able to:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eDefine AIC and BIC in the context of assessing model fit\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003eAIC\u003c/h2\u003e\n\u003cp\u003eThe formula for the AIC, invented by Hirotugu Akaike in 1973 and short for \"Akaike's Information Criterion\" is given by:\u003c/p\u003e\n\u003ch4\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Ctext%7BAIC%7D%20=%20-2%5Cln(%5Chat%7BL%7D)%20%2b%202k\"\u003e\u003c/h4\u003e\n\u003cp\u003eWhere:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cimg src=\"https://render.githubusercontent.com/render/math?math=k\"\u003e : length of the parameter space (i.e. the number of features)\u003c/li\u003e\n\u003cli\u003e\n\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Chat%7BL%7D\"\u003e : the maximum value of the likelihood function for the model\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eAnother way to phrase the equation is:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Ctext%7BAIC(model)%7D%20=%20%20-%202%20*%20%5Ctext%7Blog-likelihood(model)%7D%20%2b%202%20*%20%5Ctext%7Blength%20of%20the%20parameter%20space%7D\"\u003e\u003c/p\u003e\n\u003cp\u003eThe AIC is generally used to compare each candidate model. The nice thing about the AIC is that for every model that uses Maximum Likelihood Estimation, the log-likelihood is automatically computed, and as a consequence, the AIC is very easy to calculate.\u003c/p\u003e\n\u003cp\u003eThe AIC acts as a penalized log-likelihood criterion, giving a balance between a good fit (high value of log-likelihood) and complexity (complex models are penalized more than fairly simple ones). The AIC is unbounded so it can take any type of value, but the bottom line is that when comparing models, the model with the \u003cstrong\u003elowest\u003c/strong\u003e AIC should be selected.\u003c/p\u003e\n\u003cp\u003eNote that directly comparing the values of log-likelihood maxima for different models (without including the penalty) is not good enough for model comparison because including more parameters in a model will \u003cem\u003ealways\u003c/em\u003e give rise to an increased value of the maximum likelihood. Due to that reason, searching for the model with maximal log-likelihood would always lead to the model with the most parameters. The AIC balances this by penalizing for the number of parameters, hence searching for models with few parameters but fitting the data well.\u003c/p\u003e\n\u003cp\u003eIn Python, the AIC is built into \u003ccode\u003estatsmodels\u003c/code\u003e and in \u003ccode\u003esklearn\u003c/code\u003e (such as \u003ccode\u003eLassoLarsIC\u003c/code\u003e, which you'll use in the upcoming lab).\u003c/p\u003e\n\u003ch2\u003eBIC\u003c/h2\u003e\n\u003cp\u003eThe BIC (Bayesian Information Criterion) is very similar to the AIC and emerged as a Bayesian response to the AIC, but can be used for the exact same purposes. The idea is to select the candidate model with the highest probability given the data. This idea can be formalized inside a Bayesian framework, involving prior probabilities on candidate models along with prior densities on all parameters in the models. The penalty is slightly changed and depends on the number of rows in the dataset:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Ctext%7BBIC%7D%20=%20-%202%5Cln(%5Chat%7BL%7D)%20%2b%20%5Cln(n)%20*%20k\"\u003e\u003c/p\u003e\n\u003cp\u003eWhere:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Chat%7BL%7D\"\u003e and \u003cimg src=\"https://render.githubusercontent.com/render/math?math=k\"\u003e are the same as in AIC\u003c/li\u003e\n\u003cli\u003e\n\u003cimg src=\"https://render.githubusercontent.com/render/math?math=n\"\u003e : the number of data points (the sample size)\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eAnother way to phrase the equation is:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Ctext%7BBIC(model)%7D%20=%20-2%20*%20%5Ctext%7Blog-likelihood(model)%7D%20%2b%20%5Ctext%7Blog(number%20of%20observations)%7D%20*%20%5Ctext%7B(length%20of%20the%20parameter%20space)%7D\"\u003e\u003c/p\u003e\n\u003cp\u003eLike the AIC, the \u003cstrong\u003elower\u003c/strong\u003e your BIC, the better your model is performing.\u003c/p\u003e\n\u003ch2\u003eUses of AIC and BIC\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003ePerforming feature selection: comparing models with only a few variables and more variables, computing the AIC/BIC and select the features that generated the lowest AIC or BIC\u003c/li\u003e\n\u003cli\u003eSimilarly, selecting or not selecting interactions/polynomial features depending on whether or not the AIC/BIC decreases when adding them in\u003c/li\u003e\n\u003cli\u003eComputing the AIC and BIC for several values of the regularization parameter in Ridge/Lasso models and selecting the best regularization parameter, and many more!\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003eSummary\u003c/h2\u003e\n\u003cp\u003eGreat! In this lesson you learned about AIC and BIC, two measures that are helpful when comparing and evaluating models with varying number of features.\u003c/p\u003e","frontPage":false},{"exportId":"gradient-to-cost-function-appendix","title":"Gradient to Cost Function - Appendix","type":"WikiPage","content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-gradient-to-cost-function-appendix\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-gradient-to-cost-function-appendix\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-gradient-to-cost-function-appendix/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eIn this lesson, you'll find the details on how to compute the partial derivatives in the \"Gradient to cost function\" lesson.\u003c/p\u003e\n\u003ch2\u003eComputing the First Partial Derivative\u003c/h2\u003e\n\u003cp\u003eLet's start with taking the \u003cstrong\u003epartial derivative\u003c/strong\u003e with respect to \u003cimg src=\"https://render.githubusercontent.com/render/math?math=m\"\u003e .\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cfrac%7B%5Cdelta%20J%7D%7B%5Cdelta%20m%7DJ(m,%20b)%20=%20%5Cfrac%7B%5Cdelta%20J%7D%7B%5Cdelta%20m%7D(y%20-%20(mx%20%2b%20b))%5E2\"\u003e\u003c/p\u003e\n\u003cp\u003eNow this is a tricky function to take the derivative of. So we can use functional composition followed by the chain rule to make it easier. Using functional composition, we can rewrite our function \u003cimg src=\"https://render.githubusercontent.com/render/math?math=J\"\u003e as two functions:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=g(m,b)%20=%20y%20-%20(mx%20%2b%20b)%5Cmspace%7B5ex%7D\"\u003e -- set \u003cimg src=\"https://render.githubusercontent.com/render/math?math=g\"\u003e equal to \u003cimg src=\"https://render.githubusercontent.com/render/math?math=y-%5Chat%7By%7D\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=J(g(m,b))%20=%20(g(m,b))%5E2%5Cmspace%7B4ex%7D\"\u003e -- now \u003cimg src=\"https://render.githubusercontent.com/render/math?math=J\"\u003e is a function of \u003cimg src=\"https://render.githubusercontent.com/render/math?math=g\"\u003e and \u003cimg src=\"https://render.githubusercontent.com/render/math?math=J=g%5E2\"\u003e\u003c/p\u003e\n\u003cp\u003eNow using the chain rule to find the partial derivative with respect to a change in the slope, gives us:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5B1%5D%5Cmspace%7B5ex%7D%5Cfrac%7BdJ%7D%7Bdm%7DJ(g)%20=%20%5Cfrac%7BdJ%7D%7Bdg%7DJ(g(m,%20b))*%5Cfrac%7Bdg%7D%7Bdm%7Dg(m,b)\"\u003e\u003c/p\u003e\n\u003cp\u003eBecause \u003cstrong\u003eg\u003c/strong\u003e is a function of \u003cstrong\u003em\u003c/strong\u003e we get \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cboldsymbol%7B%5Cfrac%7Bdg%7D%7Bdm%7D%7D(g)\"\u003e and\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eJ\u003c/strong\u003e is a function of \u003cstrong\u003eg (which is a function of m\u003c/strong\u003e) we get \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cboldsymbol%7B%5Cfrac%7BdJ%7D%7Bdg%7D%7D(J)\"\u003e .\u003c/p\u003e\n\u003cp\u003eOur next step is to solve these derivatives individually.\u003c/p\u003e\n\u003cp\u003eFirst:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cfrac%7BdJ%7D%7Bdg%7DJ(g(m,%20b))%20=%5Cfrac%7BdJ%7D%7Bdg%7Dg(m,b)%5E2\"\u003e\u003c/p\u003e\n\u003cp\u003eSolve \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cboldsymbol%7B%5Cfrac%7BdJ%7D%7Bdg%7D%7D(J)\"\u003e :\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cfrac%7BdJ%7D%7Bdg%7DJ(g(m,%20b))%20=%202*g(m,b)\"\u003e\u003c/p\u003e\n\u003cp\u003eThen:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cfrac%7Bdg%7D%7Bdm%7Dg(m,b)%20=%5Cfrac%7Bdg%7D%7Bdm%7D%20(y%20-%20(mx%20%2bb))\"\u003e\u003c/p\u003e\n\u003cp\u003eSolve \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cboldsymbol%7B%5Cfrac%7Bdg%7D%7Bdm%7D%7D(g)\"\u003e:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cfrac%7Bdg%7D%7Bdm%7Dg(m,b)%20=%5Cfrac%7Bdg%7D%7Bdm%7D%20(y%20-%20mx%20-%20b)\"\u003e \u003cbr\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cmspace%7B9ex%7D=%5Cfrac%7Bdg%7D%7Bdm%7Dy%20-%20%5Cfrac%7Bdg%7D%7Bdm%7Dmx%20-%20%5Cfrac%7Bdg%7D%7Bdm%7Db\"\u003e \u003cbr\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cmspace%7B9ex%7D=%200-x-0\"\u003e \u003cbr\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cmspace%7B9ex%7D=-x\"\u003e\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eEach of the terms are treated as constants, except for the middle term.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eNow plugging these back into our chain rule [1] we have:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Ccolor%7Bblue%7D%7B%5Cfrac%7BdJ%7D%7Bdg%7DJ(g(m,b))%7D%5Ccolor%7Bblack%7D%7B*%7D%5Ccolor%7Bred%7D%7B%5Cfrac%7Bdg%7D%7Bdm%7Dg(m,b)%7D%20%5Ccolor%7Bblack%7D%7B=%7D%5Ccolor%7Bblue%7D%7B(2*g(m,b))%7D%5Ccolor%7Bblack%7D%7B*%7D%5Ccolor%7Bred%7D%7B-x%7D\"\u003e \u003cbr\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cmspace%7B21.75ex%7D=%202*(y%20-%20(mx%20%2b%20b))*-x\"\u003e\u003c/p\u003e\n\u003cp\u003eSo\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5B1%5D%5Cmspace%7B5ex%7D%5Cfrac%7B%5Cdelta%20J%7D%7B%5Cdelta%20m%7DJ(m,%20b)%20=2*(y%20-%20(mx%20%2b%20b))*-x\"\u003e \u003cbr\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cmspace%7B15.75ex%7D=%20-2x*(y%20-%20(mx%20%2b%20b%20))\"\u003e\u003c/p\u003e\n\u003ch2\u003eComputing the Second Partial Derivative\u003c/h2\u003e\n\u003cp\u003eOk, now let's calculate the partial derivative with respect to a change in the y-intercept. We express this mathematically with the following:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cfrac%7B%5Cdelta%20J%7D%7B%5Cdelta%20b%7DJ(m,%20b)%20=%20%5Cfrac%7BdJ%7D%7Bdb%7D(y%20-%20(mx%20%2b%20b))%5E2\"\u003e\u003c/p\u003e\n\u003cp\u003eThen once again, we use functional composition following by the chain rule. So we view our cost function as the same two functions \u003cimg src=\"https://render.githubusercontent.com/render/math?math=g(m,b)\"\u003e and \u003cimg src=\"https://render.githubusercontent.com/render/math?math=J(g(m,b))\"\u003e .\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=g(m,b)%20=%20y%20-%20(mx%20%2b%20b)\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=J(g(m,b))%20=%20(g(m,b))%5E2\"\u003e\u003c/p\u003e\n\u003cp\u003eSo applying the chain rule, to this same function composition, we get:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5B2%5D%5Cmspace%7B5ex%7D%5Cfrac%7BdJ%7D%7Bdb%7DJ(g)%20=%20%5Cfrac%7BdJ%7D%7Bdg%7DJ(g)*%5Cfrac%7Bdg%7D%7Bdb%7Dg(m,b)\"\u003e\u003c/p\u003e\n\u003cp\u003eNow, our next step is to calculate these partial derivatives individually.\u003c/p\u003e\n\u003cp\u003eFrom our earlier calculation of the partial derivative, we know that \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cfrac%7BdJ%7D%7Bdg%7DJ(g(m,b))%20=%20%5Cfrac%7BdJ%7D%7Bdg%7Dg(m,b)%5E2%20=%202*g(m,b)\"\u003e .\u003c/p\u003e\n\u003cp\u003eThe only thing left to calculate is \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cfrac%7Bdg%7D%7Bdb%7Dg(m,b)\"\u003e:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cfrac%7Bdg%7D%7Bdb%7Dg(m,b)%20=%5Cfrac%7Bdg%7D%7Bdb%7D(y%20-%20(mx%20%2b%20b)%20)\"\u003e \u003cbr\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cmspace%7B8.5ex%7D=%5Cfrac%7Bdg%7D%7Bdb%7D(y-mx-b)\"\u003e \u003cbr\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cmspace%7B8.5ex%7D=%5Cfrac%7Bdb%7D%7Bdb%7Dy-%5Cfrac%7Bdb%7D%7Bdb%7Dmx-%5Cfrac%7Bdg%7D%7Bdb%7Db\"\u003e \u003cbr\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cmspace%7B8.5ex%7D=0-0-1\"\u003e \u003cbr\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cmspace%7B8.5ex%7D=%20-1\"\u003e\u003c/p\u003e\n\u003cp\u003eNow we plug our terms into our chain rule [2] and get:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Ccolor%7Bblue%7D%7B%5Cfrac%7BdJ%7D%7Bdg%7DJ(g)%7D%5Ccolor%7Bblack%7D%7B*%7D%5Ccolor%7Bred%7D%7B%5Cfrac%7Bdg%7D%7Bdb%7Dg(m,b)%7D%20%5Ccolor%7Bblack%7D%7B=%7D%20%5Ccolor%7Bblue%7D%7B2*g(m,b)%7D*%5Ccolor%7Bred%7D%7B-1%7D\"\u003e \u003cbr\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cmspace%7B16ex%7D=-2*(y%20-%20(mx%20%2b%20b))\"\u003e\u003c/p\u003e","frontPage":false},{"exportId":"introduction-to-supervised-learning","title":"Introduction to Supervised Learning","type":"WikiPage","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-intro-to-supervised-learning-v2-1\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-intro-to-supervised-learning-v2-1/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eIn this lesson, we'll examine what exactly the term \"Supervised Learning\" means, and where it fits in Data Science. \u003c/p\u003e\n\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\n\u003cul\u003e\n\u003cli\u003eDescribe the components of what makes something a supervised learning task \u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch2\u003eWhat is Supervised Learning?\u003c/h2\u003e\n\n\u003cp\u003eThe term \u003cstrong\u003e\u003cem\u003eSupervised Learning\u003c/em\u003e\u003c/strong\u003e refers to a class of machine learning algorithms that can \"learn\" a task through \u003cstrong\u003e\u003cem\u003elabeled training data\u003c/em\u003e\u003c/strong\u003e. We'll explore this definition more fully in a bit -- but first, it's worth taking some time to understand where supervised learning fits in the overall picture in regards to Data Science. By now, you've probably noticed that many of the things we've learned in Data Science and Computer Science are very hierarchical. This is especially true when it comes to AI and Machine Learning. Let's break down the hierarchy a bit, and see where \u003cstrong\u003e\u003cem\u003eSupervised Learning\u003c/em\u003e\u003c/strong\u003e fits.  \u003c/p\u003e\n\n\u003ch2\u003eArtificial Intelligence\u003c/h2\u003e\n\n\u003cp\u003eAt the top of the hierarchy is \u003cstrong\u003e\u003cem\u003eArtificial Intelligence\u003c/em\u003e\u003c/strong\u003e.  AI is a catch-all term for various kinds of algorithms that can complete tasks that normally require human intelligence to complete. AI is made up of several subcategories, and is also a subcategory itself in the greater hierarchy of Computer Science. When data scientists talk about AI, we're almost focused on a single branch of AI, \u003cstrong\u003e\u003cem\u003eMachine Learning\u003c/em\u003e\u003c/strong\u003e. Machine Learning is responsible for the boom in AI technologies and abilities in the last few decades, but it's worth noting that there are other areas of AI that do not fall under the umbrella of 'Machine Learning'. Other branches of AI include things like \u003cem\u003eGenetic Algorithms\u003c/em\u003e for optimization, or rules-based AI for things like building a bot for players to play against in a video game. While these are still active areas of research, they have little to no application in Data Science, so they're beyond the scope of this lesson. In general, when you see the phrase 'Artificial Intelligence', it's generally safe to assume that that the speaker is probably referring to the subfield of AI known as \u003cstrong\u003e\u003cem\u003eMachine Learning\u003c/em\u003e\u003c/strong\u003e (which is also sometimes referred to by it's older, more traditional name -- \u003cstrong\u003e\u003cem\u003eStatistical Learning\u003c/em\u003e\u003c/strong\u003e).\u003c/p\u003e\n\n\u003cp\u003eThe following graphic shows the breakdown of the 'Machine Learning' branch of AI:\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-intro-to-supervised-learning-v2-1/master/images/new_ml-hierarchy.png\" width=\"600\"\u003e\u003c/p\u003e\n\n\u003ch2\u003eMachine Learning\u003c/h2\u003e\n\n\u003cp\u003eThe field of \u003cem\u003eMachine Learning\u003c/em\u003e can be further divided into two overall categories:\u003c/p\u003e\n\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003e\u003cem\u003eSupervised Learning\u003c/em\u003e\u003c/strong\u003e\u003c/li\u003e\n\u003cli\u003e\u003cem\u003eUnsupervised Learning\u003c/em\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\n\u003cp\u003eThe main difference between these two areas of machine learning is the need for \u003cstrong\u003e\u003cem\u003elabeled training data\u003c/em\u003e\u003c/strong\u003e. In \u003cstrong\u003e\u003cem\u003eSupervised Learning\u003c/em\u003e\u003c/strong\u003e, any data used must have a \u003cstrong\u003e\u003cem\u003elabel\u003c/em\u003e\u003c/strong\u003e. These labels are the \u003cem\u003eground truth\u003c/em\u003e , which allows our supervised learning algorithms to 'check their work'. By comparing its predictions against the actual labels, our algorithm can learn to make less incorrect predictions and improve the overall performance of the task its learning to do. It helps to think of Supervised Learning as close to the type of learning we do as students in grade school. Imagine using practice exams to study for the SAT or ACT test. We can go through all the practice questions we want, but in order to learn from our performance on those practice questions, we need to know what the correct answers are! Without them, we would have no way of knowing which questions we got right and which ones we got wrong, so we wouldn't be able to learn what changes we would need to make to improve our overall performance! \u003c/p\u003e\n\n\u003cblockquote\u003e\n\u003cp\u003e\"A computer program is said to \u003cstrong\u003elearn\u003c/strong\u003e from experience \u003cem\u003eE\u003c/em\u003e with respect to some class of tasks \u003cem\u003eT\u003c/em\u003e and performance measure \u003cem\u003eP\u003c/em\u003e, if its performance at tasks in \u003cem\u003eT\u003c/em\u003e, as measured by \u003cem\u003eP\u003c/em\u003e, improves with experience \u003cem\u003eE\u003c/em\u003e.\"  -- \u003ca href=\"http://www.cs.cmu.edu/%7Etom/\"\u003eTom Mitchell\u003c/a\u003e\u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003cp\u003eLet's pretend we've built and trained a model to detect if a picture contains a cat or not. Using the language from the definition above:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003e\n\u003cstrong\u003eTask (T)\u003c/strong\u003e: predict if a picture contains a cat or not\u003c/li\u003e\n\u003cli\u003e\n\u003cstrong\u003ePerformance Measure (P)\u003c/strong\u003e: The objective function used to score the predictions made by our model for each image\u003c/li\u003e\n\u003cli\u003e\n\u003cstrong\u003eExperience (E)\u003c/strong\u003e: All of our labeled training data. The more training data we provide, the more 'experience' our model gets!\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003eWe'll spend some time learning about \u003cstrong\u003e\u003cem\u003eUnsupervised Learning\u003c/em\u003e\u003c/strong\u003e in the next module, so don't worry about it for now!\u003c/p\u003e\n\n\u003ch2\u003eClassification and Regression\u003c/h2\u003e\n\n\u003cp\u003eThe field of \u003cem\u003eSupervised Learning\u003c/em\u003e can be further broken down into two categories -- \u003cstrong\u003e\u003cem\u003eClassification\u003c/em\u003e\u003c/strong\u003e and \u003cstrong\u003e\u003cem\u003eRegression\u003c/em\u003e\u003c/strong\u003e. At this point in your studies, you already have significant experience with regression -- specifically \u003cstrong\u003eLinear Regression\u003c/strong\u003e , probably the most foundational (and important) machine learning model. Recall that regression allows us to answer questions like \"how much?\" or \"how many?\". If our label is a real-valued number, then the supervised learning problem you're trying to solve is a \u003cem\u003eregression\u003c/em\u003e problem. \u003c/p\u003e\n\n\u003cp\u003eThe other main kind of supervised learning problem is \u003cstrong\u003e\u003cem\u003eClassification\u003c/em\u003e\u003c/strong\u003e. Classification allows us to tell if something belongs to one class or the other. In the case of the \u003ca href=\"https://www.kaggle.com/c/titanic\"\u003etitanic\u003c/a\u003e dataset, this may be something like survival. For example, given various characteristics of a passenger, predict whether they will survive or not. Questions that can be answered in a True/False format (in the titanic example, \"Survived\" or \"Not survived\") are a type of \u003cstrong\u003e\u003cem\u003eBinary Classification\u003c/em\u003e\u003c/strong\u003e. To perform binary classification, you will be introduced to \u003cstrong\u003eLogistic Regression\u003c/strong\u003e. Don't let the name confuse you, although the name contains the word \"regression,\" this important foundational technique is very important in understanding classification problems. There are several other classification techniques you will be learning in this module, but in order to gain a sound understanding of \u003cstrong\u003eClassification\u003c/strong\u003e tasks, this section will be focused exclusively on building and evaluating logistic regression models. \u003c/p\u003e\n\n\u003cp\u003eHowever, we are not limited to only two classes when working with classification algorithms -- we can have as many classes as we see fit. When a supervised learning problem has more than two classes, we refer to it as a \u003cstrong\u003e\u003cem\u003eMulticlass Classification\u003c/em\u003e\u003c/strong\u003e problem. \u003c/p\u003e\n\n\u003ch2\u003eObjective Functions\u003c/h2\u003e\n\n\u003cp\u003eWhenever we're dealing with supervised learning, we have an \u003cstrong\u003e\u003cem\u003eObjective Function\u003c/em\u003e\u003c/strong\u003e (also commonly called a \u003cstrong\u003e\u003cem\u003eLoss Function\u003c/em\u003e\u003c/strong\u003e) that we're trying to optimize against. Regardless of the supervised learning model we're working with, we can be sure that we have some sort of function under the hood that we're using to grade the predictions made by our model against the actual ground-truth labels for each prediction. In the quote from Tom Mitchell listed above, objective functions are \u003cem\u003eP\u003c/em\u003e. While classification and regression models use different kinds of objective functions to evaluate their performance, the concept is the same -- these functions allow the model to evaluate exactly how right or wrong a prediction is, which the algorithm can then \"learn\" from. These objective functions serve an important purpose, because they act as the ground-truth for determining if our model is getting better or not. \u003c/p\u003e\n\n\u003ch3\u003eThe Limitations of Labeled Data\u003c/h3\u003e\n\n\u003cp\u003eBecause supervised learning requires \u003cstrong\u003e\u003cem\u003eLabels\u003c/em\u003e\u003c/strong\u003e for any data used, this severely limits the amount of available data we have for use with supervised learning algorithms.  Of all the data in the world, only a very, very small percentage is labeled. Why? Because labeling data is a purposeful activity that can only be done by humans, and is therefore time-consuming and expensive. In supervised learning, labels are not universal -- they are unique to the problem we're trying to solve. If we're trying to train a model to predict if someone survived the titanic disaster, we need to know the survival results of every passenger in our dataset -- there's no way around it. However, if we're trying to predict how much a person paid for a ticket on the titanic, survival data now no longer works as a label -- instead, we need to know how much each passenger paid for a ticket. In a more generalized sense, this means that for whatever problem we're trying to train a supervised learning model to solve, we need to have a large enough dataset containing examples where humans have already done the things we're trying to get our model to learn how to do.  \u003c/p\u003e\n\n\u003cp\u003eAlthough labeled data is still expensive and time-consuming to get, the internet has made the overall process of getting labeled data a bit easier than it used to be. Nowadays, when companies need to construct a dataset of labeled training data to solve a problem, they typically make use of services like Amazon's \u003ca href=\"https://docs.aws.amazon.com/mturk/index.html\"\u003eAWS Mechanical Turk\u003c/a\u003e, or 'MTurk' for short. Services like this obtain labels by paying people for each label they generate. In this way, a company can crowdsource the work to label the training data needed. The company simply uploads unlabeled training data like an image, and a \"turker\" will then provide a label for that image according to the instructions from the company. Depending on the problem the company is trying to solve, the label for the image might be something as simple as the word \"cat\", or as complex as as boxes drawn around all the cats in the image. \u003c/p\u003e\n\n\u003ch3\u003eNegative Examples\u003c/h3\u003e\n\n\u003cp\u003eWhen creating a labeled dataset for a classification problem, it is worth noting that negative examples are just as important to be included in the dataset as positive examples. If our training data in the titanic dataset only contained data on passengers that all survived, no supervised learning algorithm would be able to learn how to predict if a passenger survived or died with any sort of accuracy. \u003cstrong\u003e\u003cem\u003ePositive Examples\u003c/em\u003e\u003c/strong\u003e are data points that belong to the class we're training our model to recognize. For instance, let's pretend we're building a model to tell if a picture is of a cat or not. All the pictures of cats in our dataset would be positive examples. However, in order to build a good cat classifier, our dataset would also need to contain many different kinds of pictures that don't include cats. Intuitively, this makes sense -- if every picture that our model ever saw had a cat in it, then the only thing that model will learn is that everything is a cat. To truly learn what we need it to learn, this model will also need to learn what a cat \u003cem\u003eisn't\u003c/em\u003e, by looking at pictures that don't include cats -- our \u003cstrong\u003e\u003cem\u003eNegative Examples\u003c/em\u003e\u003c/strong\u003e. In this way, with a complex enough model and enough labeled training data, our classifier will eventually learn that the differentiating factor between images with positive labels and images with negative labels are the shapes and patterns common to cats, but not dogs (or other animals). In this way, supervised learning can be a bit tricky. For instance, if all of the negative examples in our cat classifier dataset are of cars and houses, then the model will almost certainly get a picture of a dog incorrect by predicting that the picture is of a cat. Why does this happen? Because the model hasn't seen a dog before, and therefore has no idea whether this fits. In this particular example, we can guess that any picture of a dog will look more like a cat than it would a house or car, which from the model's perspective means that this is probably a picture of a cat. \u003c/p\u003e\n\n\u003cp\u003eIn summary, this part of supervised learning can often be more art than science -- when creating a dataset, make sure that your dataset contains enough negative examples, and that you are very thoughtful about what those negative examples actually contain! \u003c/p\u003e\n\n\u003ch2\u003eSummary\u003c/h2\u003e\n\n\u003cp\u003eIn this lesson, we learned about \u003cem\u003eSupervised Learning\u003c/em\u003e, and where it fits in relation to Machine Learning and Artificial Intelligence. \u003c/p\u003e\n\u003cfooter class=\"fis-footer\" style=\"visibility: hidden;\"\u003e\u003cdiv class=\"fis-feedback\"\u003e\n\u003ch5\u003eHow do you feel about this lesson?\u003c/h5\u003e\n\u003cimg id=\"thumbs-up\" data-repository=\"dsc-intro-to-supervised-learning-v2-1\" title=\"Thumbs up!\" alt=\"thumbs up\"\u003e\u003cimg id=\"thumbs-down\" data-repository=\"dsc-intro-to-supervised-learning-v2-1\" title=\"Thumbs down!\" alt=\"thumbs down\"\u003e\n\u003c/div\u003e\n\u003ch5\u003eHave specific feedback? \u003ca href=\"https://github.com/learn-co-curriculum/dsc-intro-to-supervised-learning-v2-1/issues/new\"\u003eTell us here!\u003c/a\u003e\n\u003c/h5\u003e\u003c/footer\u003e","frontPage":false},{"exportId":"ensemble-methods-introduction","title":"Ensemble Methods - Introduction","type":"WikiPage","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-ensemble-methods-section-intro\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-ensemble-methods-section-intro/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eIn this section, you'll learn about some of the most powerful machine learning algorithms: ensemble models! This lesson summarizes the topics we'll be covering in this section.\u003c/p\u003e\n\u003ch2\u003eEnsembles\u003c/h2\u003e\n\u003cp\u003eThe idea of ensembles is to bring together multiple models to use them to improve the quality of your predictions when compared to just using a single model. In many real-world problems and Kaggle competitions, ensemble methods tend to outperform any single model.\u003c/p\u003e\n\u003ch3\u003eEnsemble Methods\u003c/h3\u003e\n\u003cp\u003eWe start the section by providing an introduction to the concept of ensemble methods, explaining how they take advantage of the delphic technique (or \"wisdom of crowds\") where the average of multiple independent estimates is usually more consistently accurate than the individual estimates.\u003c/p\u003e\n\u003cp\u003eWe also provide an introduction to the idea of bagging (Bootstrap Aggregation).\u003c/p\u003e\n\u003ch3\u003eRandom Forests\u003c/h3\u003e\n\u003cp\u003eWe then look at random forests - an ensemble method for decision trees that takes advantage of bagging and the subspace sampling method to create a \"forest\" of decision trees that provides consistently better predictions than any single decision tree.\u003c/p\u003e\n\u003ch3\u003eGridsearchCV\u003c/h3\u003e\n\u003cp\u003eWe will also introduce some of the common hyperparameters for tuning decision trees. In this lesson, we look at how you can use GridSearchCV to perform an exhaustive search across multiple hyperparameters and multiple possible values to come up with a better performing model.\u003c/p\u003e\n\u003ch3\u003eGradient Boosting and Weak Learners\u003c/h3\u003e\n\u003cp\u003eNext up, we introduce the concept of boosting which is at the heart of some of the most powerful ensemble methods such as Adaboost and Gradient Boosted Trees.\u003c/p\u003e\n\u003ch3\u003eXGBoost\u003c/h3\u003e\n\u003cp\u003eFinally, we end this section by introducing XGBoost (eXtreme Gradient Boosting) - the top gradient boosting algorithm currently in use.\u003c/p\u003e\n\u003ch2\u003eSummary\u003c/h2\u003e\n\u003cp\u003eYou will often find yourself using a range of ensemble techniques to improve the performance of your models, so this section will introduce you to the techniques that will help you to improve the quality of your models.\u003c/p\u003e\n\u003cfooter class=\"fis-footer\" style=\"visibility: hidden;\"\u003e\n\u003cdiv class=\"fis-feedback\"\u003e\n\u003ch5\u003eHow do you feel about this lesson?\u003c/h5\u003e\n\u003cimg id=\"thumbs-up\" title=\"Thumbs up!\" alt=\"thumbs up\" data-repository=\"dsc-ensemble-methods-section-intro\"\u003e\u003cimg id=\"thumbs-down\" title=\"Thumbs down!\" alt=\"thumbs down\" data-repository=\"dsc-ensemble-methods-section-intro\"\u003e\n\u003c/div\u003e\n\u003ch5\u003eHave specific feedback? \u003ca href=\"https://github.com/learn-co-curriculum/dsc-ensemble-methods-section-intro/issues/new\"\u003eTell us here!\u003c/a\u003e\n\u003c/h5\u003e\n\u003c/footer\u003e","frontPage":false},{"exportId":"k-nearest-neighbors-recap","title":"K-Nearest Neighbors - Recap","type":"WikiPage","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-knn-recap\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-knn-recap/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eIn this lesson you'll briefly review some of the key concepts covered in this section including KNN's computational complexity and how to properly tune a model using scikit-learn. \u003c/p\u003e\n\n\u003ch2\u003eK-Nearest Neighbors\u003c/h2\u003e\n\n\u003cp\u003eAs you saw, KNN is an intuitive algorithm: to generate a prediction for a given data point, it finds the k-nearest data points and then predicts the majority class of these k points.\u003c/p\u003e\n\n\u003ch3\u003eComputational complexity\u003c/h3\u003e\n\n\u003cp\u003eAlso of note is the computational complexity of the KNN algorithm. As the number of data points and features increase, the required calculations increases exponentially! As such, KNN is extremely resource intensive for large datasets.\u003c/p\u003e\n\n\u003ch2\u003eDistance metrics\u003c/h2\u003e\n\n\u003cp\u003eYou learned about Minkowski distance and two cases of Minkowski distance: Euclidean and Manhattan distance. Other distance metrics such as Hamming distance can even be used to compare strings! (Hamming distance can be used to offer typo correction-suggestions for instance by comparing similar words generated by changing only one or two letters from the mistyped word). \u003c/p\u003e\n\n\u003ch2\u003eModel tuning in scikit-learn\u003c/h2\u003e\n\n\u003cp\u003eRemember that model tuning encapsulates the entire gamut of the data science process from problem formulation and preprocessing through hyperparameter tuning. Furthermore, you also need to choose a validation method to determine the model's ability to generalize to new cases such as train-test split or cross-validation. Good models require careful thought, ample preprocessing, and exploration followed by hyperparameter tuning.\u003c/p\u003e\n\n\u003ch2\u003eSummary\u003c/h2\u003e\n\n\u003cp\u003eWell done! You have added another algorithm in your toolset. Even though KNN doesn't scale well to larger datasets, it has many useful applications from recommendations to classification. \u003c/p\u003e\n\u003cfooter class=\"fis-footer\" style=\"visibility: hidden;\"\u003e\u003cdiv class=\"fis-feedback\"\u003e\n\u003ch5\u003eHow do you feel about this lesson?\u003c/h5\u003e\n\u003cimg id=\"thumbs-up\" data-repository=\"dsc-knn-recap\" title=\"Thumbs up!\" alt=\"thumbs up\"\u003e\u003cimg id=\"thumbs-down\" data-repository=\"dsc-knn-recap\" title=\"Thumbs down!\" alt=\"thumbs down\"\u003e\n\u003c/div\u003e\n\u003ch5\u003eHave specific feedback? \u003ca href=\"https://github.com/learn-co-curriculum/dsc-knn-recap/issues/new\"\u003eTell us here!\u003c/a\u003e\n\u003c/h5\u003e\u003c/footer\u003e","frontPage":false},{"exportId":"introduction-to-pipelines","title":"Introduction to Pipelines","type":"WikiPage","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-pipelines-v2-1\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-pipelines-v2-1/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eYou've learned a substantial number of different supervised learning algorithms. Now, it's time to learn about a handy tool used to integrate these algorithms into a single manageable pipeline.\u003c/p\u003e\n\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\n\u003cp\u003eYou will be able to:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eExplain how pipelines can be used to combine various parts of a machine learning workflow\u003cbr\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch2\u003eWhy Use Pipelines?\u003c/h2\u003e\n\n\u003cp\u003ePipelines are extremely useful tools to write clean and manageable code for machine learning. Recall how we start preparing our dataset: we want to clean our data, transform it, potentially use feature selection, and then run a machine learning algorithm. Using pipelines, you can do all these steps in one go!\u003c/p\u003e\n\n\u003cp\u003ePipeline functionality can be found in scikit-learn's \u003ccode\u003ePipeline\u003c/code\u003e module. Pipelines can be coded in a very simple way:\u003c/p\u003e\n\n\u003cpre\u003e\u003ccode class=\"python\"\u003efrom sklearn.pipeline import Pipeline\n\npipe = Pipeline([('mms', MinMaxScaler()),\n                 ('tree', DecisionTreeClassifier(random_state=123))])\n\u003c/code\u003e\u003c/pre\u003e\n\n\u003cp\u003eThis pipeline will ensure that first we'll apply a Min-Max scaler on our data before fitting a decision tree. However, the \u003ccode\u003ePipeline()\u003c/code\u003e function above is only defining the sequence of actions to perform. In order to actually fit the model, you need to call the \u003ccode\u003e.fit()\u003c/code\u003e method like so: \u003c/p\u003e\n\n\u003cpre\u003e\u003ccode class=\"python\"\u003epipe.fit(X_train, y_train)\n\u003c/code\u003e\u003c/pre\u003e\n\n\u003cp\u003eThen, to score the model on test data, you can call the \u003ccode\u003e.score()\u003c/code\u003e method like so: \u003c/p\u003e\n\n\u003cpre\u003e\u003ccode class=\"python\"\u003epipe.score(X_test, y_test)\n\u003c/code\u003e\u003c/pre\u003e\n\n\u003cp\u003eA really good blog post on the basic ideas of pipelines can be found \u003ca href=\"https://www.kdnuggets.com/2017/12/managing-machine-learning-workflows-scikit-learn-pipelines-part-1.html\"\u003ehere\u003c/a\u003e.\u003c/p\u003e\n\n\u003ch2\u003eIntegrating Grid Search in Pipelines\u003c/h2\u003e\n\n\u003cp\u003eNote that the above pipeline simply creates one pipeline for a training set, and evaluates on a test set. Is it possible to create a pipeline that performs grid search? And cross-validation? Yes, it is!\u003c/p\u003e\n\n\u003cp\u003eFirst, you define the pipeline in the same way as above. Next, you create a parameter grid. When this is all done, you use the function \u003ccode\u003eGridSearchCV()\u003c/code\u003e, which you've seen before, and specify the pipeline as the estimator and the parameter grid. You also have to define how many folds you'll use in your cross-validation. \u003c/p\u003e\n\n\u003cpre\u003e\u003ccode class=\"python\"\u003epipe = Pipeline([('mms', MinMaxScaler()),\n                 ('tree', DecisionTreeClassifier(random_state=123))])\n\ngrid = [{'tree__max_depth': [None, 2, 6, 10], \n         'tree__min_samples_split': [5, 10]}]\n\n\ngridsearch = GridSearchCV(estimator=pipe, \n                          param_grid=grid, \n                          scoring='accuracy', \n                          cv=5)\n\ngridsearch.fit(X_train, y_train)\n\ngridsearch.score(X_test, y_test)\n\u003c/code\u003e\u003c/pre\u003e\n\n\u003cp\u003eAn article with a detailed workflow can be found \u003ca href=\"https://www.kdnuggets.com/2018/01/managing-machine-learning-workflows-scikit-learn-pipelines-part-2.html\"\u003ehere\u003c/a\u003e.\u003c/p\u003e\n\n\u003ch2\u003eSummary\u003c/h2\u003e\n\n\u003cp\u003eGreat, this wasn't too difficult! The proof of all this is in the pudding. In the next lab, you'll use this workflow to build pipelines applying classification algorithms you have learned so far in this module. \u003c/p\u003e\n\u003cfooter class=\"fis-footer\" style=\"visibility: hidden;\"\u003e\u003cdiv class=\"fis-feedback\"\u003e\n\u003ch5\u003eHow do you feel about this lesson?\u003c/h5\u003e\n\u003cimg id=\"thumbs-up\" data-repository=\"dsc-pipelines-v2-1\" title=\"Thumbs up!\" alt=\"thumbs up\"\u003e\u003cimg id=\"thumbs-down\" data-repository=\"dsc-pipelines-v2-1\" title=\"Thumbs down!\" alt=\"thumbs down\"\u003e\n\u003c/div\u003e\n\u003ch5\u003eHave specific feedback? \u003ca href=\"https://github.com/learn-co-curriculum/dsc-pipelines-v2-1/issues/new\"\u003eTell us here!\u003c/a\u003e\n\u003c/h5\u003e\u003c/footer\u003e","frontPage":false},{"exportId":"the-kernel-trick","title":"The Kernel Trick","type":"WikiPage","content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-the-kernel-trick\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-the-kernel-trick\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-the-kernel-trick/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eIn this lesson, you'll learn how to create SVMs with non-linear decision boundaries data using kernels!\u003c/p\u003e\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\u003cp\u003eYou will be able to:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eDefine the kernel trick and explain why it is important in an SVM model\u003c/li\u003e\n\u003cli\u003eDescribe a radial basis function kernel\u003c/li\u003e\n\u003cli\u003eDescribe a sigmoid kernel\u003c/li\u003e\n\u003cli\u003eDescribe a polynomial kernel\u003c/li\u003e\n\u003cli\u003eDetermine when it is best to use specific kernels within SVM\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003eNon-linear problems: The Kernel trick\u003c/h2\u003e\n\u003cp\u003eIn the previous lab, you looked at a plot where a linear boundary was clearly not sufficient to separate the two classes cleanly. Another example where a linear boundary would not work well is shown below. How would you draw a max margin classifier here? The intuitive solution is to draw an arc around the circles, separating them from the surrounding diamonds. To generate non-linear boundaries such as this, you use what is known as a kernel.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-the-kernel-trick/master/images/new_SVM_nonlin.png\" width=\"500\"\u003e\u003c/p\u003e\n\u003cp\u003eThe idea behind kernel methods is to create (nonlinear) combinations of the original features and project them onto a higher-dimensional space. For example, take a look at how this dataset could be transformed with an appropriate kernel from a two-dimensional dataset onto a new three-dimensional feature space.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-the-kernel-trick/master/images/new_SVM_kernel.png\" width=\"500\"\u003e\u003c/p\u003e\n\u003ch2\u003eTypes of kernels\u003c/h2\u003e\n\u003cp\u003eThere are several kernels, and an overview can be found in this lesson, as well as in the scikit-learn documentation \u003ca href=\"https://scikit-learn.org/stable/modules/svm.html#kernel-functions\"\u003ehere\u003c/a\u003e. The idea is that kernels are inner products in a transformed space.\u003c/p\u003e\n\u003ch3\u003eThe Linear kernel\u003c/h3\u003e\n\u003cp\u003eThe linear kernel is, as you've seen, the default kernel and simply creates linear decision boundaries. The linear kernel is represented by the inner product of the \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Clangle%20x,%20x'%20%5Crangle\"\u003e . It is important to note that some kernels have additional parameters that can be specified and knowing how these parameters work is critical to tuning SVMs.\u003c/p\u003e\n\u003ch3\u003eThe RBF kernel\u003c/h3\u003e\n\u003cp\u003eThere are two parameters when training an SVM with the \u003cem\u003eR\u003c/em\u003eadial \u003cem\u003eB\u003c/em\u003easis \u003cem\u003eF\u003c/em\u003eunction: \u003cimg src=\"https://render.githubusercontent.com/render/math?math=C\"\u003e and \u003cimg src=\"https://render.githubusercontent.com/render/math?math=gamma\"\u003e .\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eThe parameter \u003cimg src=\"https://render.githubusercontent.com/render/math?math=C\"\u003e is common to all SVM kernels. Again, by tuning the \u003cimg src=\"https://render.githubusercontent.com/render/math?math=C\"\u003e parameter when using kernels, you can provide a trade-off between misclassification of the training set and simplicity of the decision function. A high \u003cimg src=\"https://render.githubusercontent.com/render/math?math=C\"\u003e will classify as many samples correctly as possible (and might potentially lead to overfitting)\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=gamma\"\u003e defines how much influence a single training example has. The larger \u003cimg src=\"https://render.githubusercontent.com/render/math?math=gamma\"\u003e is, the closer other examples must be to be affected\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThe RBF kernel is specified as:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cexp%7B(-%5Cgamma%20%5ClVert%20%20x%20-%20%20x'%20%5CrVert%5E2)%7D\"\u003e\u003c/p\u003e\n\u003cp\u003eGamma has a strong effect on the results: a \u003cimg src=\"https://render.githubusercontent.com/render/math?math=gamma\"\u003e that is too large will lead to overfitting, while a \u003cimg src=\"https://render.githubusercontent.com/render/math?math=gamma\"\u003e which is too small will lead to underfitting (kind of like a simple linear boundary for a complex problem).\u003c/p\u003e\n\u003cp\u003eIn scikit-learn, you can specify a value for \u003cimg src=\"https://render.githubusercontent.com/render/math?math=gamma\"\u003e using the parameter \u003ccode\u003egamma\u003c/code\u003e. The default \u003ccode\u003egamma\u003c/code\u003e value is \"auto\", if no other gamma is specified, gamma is set to \u003cimg src=\"https://render.githubusercontent.com/render/math?math=1/%5Ctext%7Bnumber_of_features%7D\"\u003e . You can find more on parameters in the RBF kernel \u003ca href=\"https://scikit-learn.org/stable/auto_examples/svm/plot_rbf_parameters.html\"\u003ehere\u003c/a\u003e.\u003c/p\u003e\n\u003ch3\u003eThe Polynomial kernel\u003c/h3\u003e\n\u003cp\u003eThe Polynomial kernel is specified as\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=(%5Cgamma%20%5Clangle%20%20x%20-%20%20x'%20%5Crangle%2br)%5Ed\"\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cimg src=\"https://render.githubusercontent.com/render/math?math=d\"\u003e can be specified by the parameter \u003ccode\u003edegree\u003c/code\u003e. The default degree is 3.\u003c/li\u003e\n\u003cli\u003e\n\u003cimg src=\"https://render.githubusercontent.com/render/math?math=r\"\u003e can be specified by the parameter \u003ccode\u003ecoef0\u003c/code\u003e. The default is 0.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003eThe Sigmoid kernel\u003c/h3\u003e\n\u003cp\u003eThe sigmoid kernel is specified as:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Ctanh%20(%20%5Cgamma%5Clangle%20%20x%20-%20%20x'%20%5Crangle%2br)\"\u003e\u003c/p\u003e\n\u003cp\u003eThis kernel is similar to the signoid function in logistic regression.\u003c/p\u003e\n\u003ch2\u003eSome more notes on SVC, NuSVC, and LinearSVC\u003c/h2\u003e\n\u003ch3\u003eNuSVC\u003c/h3\u003e\n\u003cp\u003eNuSVC is similar to SVC, but adds an additional parameter, \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cnu\"\u003e , which controls the number of support vectors and training errors. \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cnu\"\u003e jointly creates an upper bound on training errors and a lower bound on support vectors.\u003c/p\u003e\n\u003cp\u003eJust like SVC, NuSVC implements the \"one-against-one\" approach when there are more than 2 classes. This means that when there are n classes, \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cdfrac%7Bn*(n-1)%7D%7B2%7D\"\u003e classifiers are created, and each one classifies samples in 2 classes.\u003c/p\u003e\n\u003ch3\u003eLinearSVC\u003c/h3\u003e\n\u003cp\u003eLinearSVC is similar to SVC, but instead of the \"one-versus-one\" method, a \"one-vs-rest\" method is used. So in this case, when there are \u003cimg src=\"https://render.githubusercontent.com/render/math?math=n\"\u003e classes, just \u003cimg src=\"https://render.githubusercontent.com/render/math?math=n\"\u003e classifiers are created, and each one classifies samples in 2 classes, the one of interest, and all the other classes. This means that SVC generates more classifiers, so in cases with many classes, LinearSVC actually tends to scale better.\u003c/p\u003e\n\u003ch2\u003eProbabilities and predictions\u003c/h2\u003e\n\u003cp\u003eYou can make predictions using support vector machines. The SVC decision function gives a probability score per class. However, this is not done by default. You'll need to set the \u003ccode\u003eprobability\u003c/code\u003e argument equal to \u003ccode\u003eTrue\u003c/code\u003e. Scikit-learn internally performs cross-validation to compute the probabilities, so you can expect that setting \u003ccode\u003eprobability\u003c/code\u003e to \u003ccode\u003eTrue\u003c/code\u003e makes the calculations longer. For large datasets, computation can take considerable time to execute.\u003c/p\u003e\n\u003ch2\u003eSummary\u003c/h2\u003e\n\u003cp\u003eGreat! You now have a basic understanding of how to use kernel functions in Support Vector Machines. You'll do just that in the upcoming lab!\u003c/p\u003e","frontPage":false},{"exportId":"computational-complexity-from-ols-to-gradient-descent","title":"Computational Complexity: From OLS to Gradient Descent","type":"WikiPage","content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-computational-complexity\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-computational-complexity\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-computational-complexity/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eIn this lesson, youll be introduced to computational complexity. You'll learn about this idea in relationship with OLS regression and see how this may not be the most efficient algorithm to calculate the regression parameters when performing regression with large datasets. You'll set the stage for an optimization algorithm called \"Gradient Descent\" which will be covered in detail later.\u003c/p\u003e\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\u003cp\u003eYou will be able to:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eDescribe computational complexity and how it is related to Big O notation\u003c/li\u003e\n\u003cli\u003eDescribe why OLS with matrix algebra would become problematic for large/complex data\u003c/li\u003e\n\u003cli\u003eExplain how optimizing techniques such as gradient descent can solve complexity issues\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003eComplexities in OLS\u003c/h2\u003e\n\u003cp\u003eRecall the OLS formula for calculating the beta vector:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cbeta%20=(X%5ETX)%5E%7B-1%7DX%5ET%20y\"\u003e\u003c/p\u003e\n\u003cp\u003eThis formula looks very simple, elegant, and intuitive. It works perfectly fine for the case of simple linear regression due to a limited number of computed dimensions, but with datasets that are very large or \u003cstrong\u003ebig data\u003c/strong\u003e sets, it becomes computationally very expensive as it can potentially involve a huge number of complex mathematical operations.\u003c/p\u003e\n\u003cp\u003eFor this formula, we need to find \u003cimg src=\"https://render.githubusercontent.com/render/math?math=(X%5ETX)\"\u003e , and invert it as well, which makes it very expensive. Imagine the matrix \u003cimg src=\"https://render.githubusercontent.com/render/math?math=X_%7B(N%20%5Ctimes%20M%2b1)%7D\"\u003e has \u003cimg src=\"https://render.githubusercontent.com/render/math?math=(M%2b1)\"\u003e columns where \u003cimg src=\"https://render.githubusercontent.com/render/math?math=M\"\u003e is the number of predictors and \u003cimg src=\"https://render.githubusercontent.com/render/math?math=N\"\u003e is the number of rows of observations. In machine learning, you will often find datasets with \u003cimg src=\"https://render.githubusercontent.com/render/math?math=M\u003e1000\"\u003e and \u003cimg src=\"https://render.githubusercontent.com/render/math?math=N\u003e1,000,000\"\u003e . The \u003cimg src=\"https://render.githubusercontent.com/render/math?math=(X%5ETX)\"\u003e matrix itself takes a while to calculate, then you have to invert an \u003cimg src=\"https://render.githubusercontent.com/render/math?math=M%5Ctimes%20M\"\u003e matrix which adds more to the complexity - making it very expensive. You'll also come across situations where the input matrix grows so large that it cannot fit into your computer's memory.\u003c/p\u003e\n\u003ch2\u003eThe Big O Notation\u003c/h2\u003e\n\u003cp\u003eIn computer science, Big O notation is used to describe how \"fast\" an algorithm grows, by comparing the number of operations within the algorithm. Big O notation helps you see the worst-case scenario for an algorithm. Typically, we are most concerned with the Big O time because we are interested in how slowly a given algorithm will possibly run at worst.\u003c/p\u003e\n\u003ch4\u003eExample\u003c/h4\u003e\n\u003cp\u003eImagine you need to find a person you only know the name of. What's the most straightforward way of finding this person? Well, you could go through every single name in the phone book until you find your target. This is known as a simple search. If the phone book is not very long, with say, only 10 names, this is a fairly fast process. But what if there are 10,000 names in the phone book?\u003c/p\u003e\n\u003cp\u003eAt best, your target's name is at the front of the list and you only need to need to check the first item. At worst, your target's name is at the very end of the phone book and you will need to have searched all 10,000 names. As the \"dataset\" (or the phone book) increases in size, the maximum time it takes to run a simple search also linearly increases.\u003c/p\u003e\n\u003cp\u003eBig O notation allows you to describe what the worst case is. The worst case is that you will have to search through all elements ( \u003cimg src=\"https://render.githubusercontent.com/render/math?math=n\"\u003e ) in the phone book. You can describe the run-time as:\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=O(n)\"\u003e where \u003cimg src=\"https://render.githubusercontent.com/render/math?math=n\"\u003e is the number of operations\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eBecause the maximum number of operations is equal to the maximum number of elements in our phone book, we say the Big \u003cimg src=\"https://render.githubusercontent.com/render/math?math=O\"\u003e of a simple search is \u003cimg src=\"https://render.githubusercontent.com/render/math?math=O(n)\"\u003e . \u003cstrong\u003eA simple search will never be slower than \u003cimg src=\"https://render.githubusercontent.com/render/math?math=O(n)\"\u003e time.\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eDifferent algorithms have different run-times. That is, algorithms grow at different rates. The most common Big O run-times, from fastest to slowest, are:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cimg src=\"https://render.githubusercontent.com/render/math?math=O(%5Clog%20n)\"\u003e : aka \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Clog\"\u003e time\u003c/li\u003e\n\u003cli\u003e\n\u003cimg src=\"https://render.githubusercontent.com/render/math?math=O(n)\"\u003e : aka linear time\u003c/li\u003e\n\u003cli\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=O(n%5E2)\"\u003e\u003c/li\u003e\n\u003cli\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=O(n%5E3)\"\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThese rates, as well as some other rates, can be visualized in the following figure:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-computational-complexity/master/images/big_o.png\" width=\"500\"\u003e\u003c/p\u003e\n\u003ch3\u003eOLS and Big O notation\u003c/h3\u003e\n\u003cp\u003eInverting a matrix costs \u003cimg src=\"https://render.githubusercontent.com/render/math?math=O(n%5E3)\"\u003e for computation where n is the number of rows in \u003cimg src=\"https://render.githubusercontent.com/render/math?math=X\"\u003e matrix, i.e., the observations. Here is an explanation of how to calculate Big O for OLS.\u003c/p\u003e\n\u003cp\u003eOLS linear regression is computed as \u003cimg src=\"https://render.githubusercontent.com/render/math?math=(X%5ETX)%5E%7B-1%7DX%5ET%20y\"\u003e .\u003c/p\u003e\n\u003cp\u003eIf \u003cimg src=\"https://render.githubusercontent.com/render/math?math=X\"\u003e is an \u003cimg src=\"https://render.githubusercontent.com/render/math?math=(n%20%5Ctimes%20k)\"\u003e matrix:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cimg src=\"https://render.githubusercontent.com/render/math?math=(X%5ETX)\"\u003e takes \u003cimg src=\"https://render.githubusercontent.com/render/math?math=O(n*k%5E2)\"\u003e time and produces a \u003cimg src=\"https://render.githubusercontent.com/render/math?math=(k%20%5Ctimes%20k)\"\u003e matrix\u003c/li\u003e\n\u003cli\u003eThe matrix inversion of a (k x k) matrix takes \u003cimg src=\"https://render.githubusercontent.com/render/math?math=O(k%5E3)\"\u003e time\u003c/li\u003e\n\u003cli\u003e\n\u003cimg src=\"https://render.githubusercontent.com/render/math?math=(X%5ETY)\"\u003e takes \u003cimg src=\"https://render.githubusercontent.com/render/math?math=O(n*k%5E2)\"\u003e time and produces a \u003cimg src=\"https://render.githubusercontent.com/render/math?math=(k%20%5Ctimes%20k)\"\u003e matrix\u003c/li\u003e\n\u003cli\u003eThe final matrix multiplication of two \u003cimg src=\"https://render.githubusercontent.com/render/math?math=(k%20%5Ctimes%20k)\"\u003e matrices takes \u003cimg src=\"https://render.githubusercontent.com/render/math?math=O(k%5E3)\"\u003e time\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003eSo the Big O running time for OLS is \u003cimg src=\"https://render.githubusercontent.com/render/math?math=O(k%5E%7B2*(n%20%2b%20k)%7D)\"\u003e - which is pretty expensive\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eMoreover, if \u003cimg src=\"https://render.githubusercontent.com/render/math?math=X\"\u003e is ill-conditioned (i.e. it isn't a square matrix), there will be computational errors in the estimation. Another common problem is overfitting and underfitting in estimation of regression coefficients.\u003c/p\u003e\n\u003cp\u003eSo, this leads us to the gradient descent kind of optimization algorithm which can save us from this type of problem. The main reason why gradient descent is used for linear regression is the computational complexity: it's computationally cheaper (faster) to find the solution using the gradient descent in most cases.\u003c/p\u003e\n\u003ch2\u003eGradient Descent\u003c/h2\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-computational-complexity/master/images/gradient_descent.png\" width=\"850\"\u003e\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eGradient Descent is an iterative approach to minimize the model loss (error), used while training a machine learning model like linear regression. It is an optimization algorithm based on a convex function as shown in the figure above, that tweaks its parameters iteratively to minimize a given function to its local minimum.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eIn regression, it is used to find the values of model parameters (coefficients, or the \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cbeta\"\u003e matrix) that minimize a cost function (like RMSE) as far as possible.\u003c/p\u003e\n\u003cp\u003eIn order to fully understand how this works, you need to know what a gradient is and how is it calculated. And for this, you would need some Calculus. It may sound a bit intimidating at this stage, but don't worry. The next few sections will introduce you to the basics of calculus with gradients and derivatives.\u003c/p\u003e\n\u003ch2\u003eFurther Reading\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e\u003ca href=\"https://en.wikipedia.org/wiki/Computational_complexity_of_mathematical_operations\"\u003eWiki: Computational complexity of mathematical operations\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003ca href=\"https://medium.com/karuna-sehgal/a-simplified-explanation-of-the-big-o-notation-82523585e835\"\u003eSimplified Big O notation\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003ca href=\"https://towardsdatascience.com/gradient-descent-in-a-nutshell-eaf8c18212f0\"\u003eGradient descent in a nutshell\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003eSummary\u003c/h2\u003e\n\u003cp\u003eIn this lesson, you learned about the shortcomings and limitations of OLS and matrix inverses. You looked at the Big O notation to explain how calculating inverses and transposes for large matrix might make our analysis unstable and computationally very expensive. This lesson sets a stage for your next section on calculus and gradient descent. You will have a much better understanding of the gradient descent diagram shown above and how it all works by the end of next section.\u003c/p\u003e","frontPage":false},{"exportId":"mle-review","title":"MLE Review","type":"WikiPage","content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-mle-review\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-mle-review\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-mle-review/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eYou've seen MLE (Maximum Likelihood Estimation) when discussing Bayesian statistics, but did you know logistic regression can also be seen from this statistical perspective? In this section, you'll gain a deeper understanding of logistic regression by coding it from scratch and analyzing the statistical motivations backing it. But first take some time to review maximum likelihood estimation.\u003c/p\u003e\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\u003cp\u003eYou will be able to:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eDescribe how to take MLE of a binomial variable\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003eMLE\u003c/h2\u003e\n\u003cp\u003eMaximum likelihood estimation can often sound academic, confusing, and cryptic when first introduced. It is often presented and introduced with complex integrals of statistical distributions that scare away many readers. Hopefully, this hasn't been your experience. While the mathematics can quickly become complex, the underlying concepts are actually quite intuitive.\u003c/p\u003e\n\u003cp\u003eTo demonstrate this, imagine a simple coin flipping example. Let's say that you flip a coin 100 times and get 55 heads. Maximum likelihood estimation attempts to uncover the underlying theoretical probability of this coin landing on heads given your observations. In other words, given the observations, what is the chance that the coin was fair and had a 0.5 chance of landing on heads each time? Or what is the chance that the coin actually had a 0.75 probability of lands of heads, given what we observed? It turns out that the answer to these questions is rather intuitive. If you observe 55 out of 100 coin flips, the underlying probability which maximizes the chance of us observing 55 out of 100 coin flips is 0.55. In this simple example, MLE simply returns the current sample mean as the underlying parameter that makes the observations most probable. Slight deviations to this would be almost as probable but slightly less so, and large deviations from our sample mean should be rare. This intuitively makes some sense; as your sample size increases, you expect the sample mean to converge to the true underlying parameter. MLE takes a flipped perspective, asking what underlying parameter is most probable given the observations.\u003c/p\u003e\n\u003ch2\u003eLog-likelihood\u003c/h2\u003e\n\u003cp\u003eWhen calculating maximum likelihood, it is common to use the log-likelihood, as taking the logarithm can simplify calculations. For example, taking the logarithm of a set of products allows you to decompose the problem from products into sums. (You may recall from high school mathematics that \u003cimg src=\"https://render.githubusercontent.com/render/math?math=x%5E%7B(a%2bb)%7D%20=%20x%5Ea%20*%20x%5Eb\"\u003e . Similarly, taking the logarithm of both sides of a function allows you to transform products into sums.\u003c/p\u003e\n\u003ch2\u003eMLE for a binomial variable\u003c/h2\u003e\n\u003cp\u003eLet's take a deeper mathematical investigation into the coin flipping example above.\u003c/p\u003e\n\u003cp\u003eIn general, if you were to observe \u003cimg src=\"https://render.githubusercontent.com/render/math?math=n\"\u003e flips, you would have observations \u003cimg src=\"https://render.githubusercontent.com/render/math?math=y_1,%20y_2,%20...,%20y_n\"\u003e .\u003c/p\u003e\n\u003cp\u003eIn maximum likelihood estimation, you are looking to maximize the likelihood:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=L(p)%20=%20L(y_1,%20y_2,%20...,%20y_n%20%7C%20p)%20=%20p%5Ey%20(1-p)%5E%7Bn-y%7D\"\u003e where \u003cimg src=\"https://render.githubusercontent.com/render/math?math=y%20=%20%5Csum_%7Bi=1%7D%5E%7Bn%7Dy_i\"\u003e\u003c/p\u003e\n\u003cp\u003eTaking the log of both sides:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=ln%5BL(p)%5D%20=%20ln%5Bp%5Ey%20(1-p)%5E%7Bn-y%7D%5D%20=%20y%20ln(p)%2b(n-y)ln(1-p)\"\u003e\u003c/p\u003e\n\u003cp\u003eIf \u003cimg src=\"https://render.githubusercontent.com/render/math?math=y%20=%201,%202,%20...,%20n-1\"\u003e the derivative of \u003cimg src=\"https://render.githubusercontent.com/render/math?math=ln%5BL(p)%5D\"\u003e with respect to \u003cimg src=\"https://render.githubusercontent.com/render/math?math=p\"\u003e is:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cfrac%7Bd%5C,ln%5BL(p)%5D%7D%7Bdp%7D%20=%20y%20(%5Cfrac%7B1%7D%7Bp%7D)%2b(n-y)(%5Cfrac%7B-1%7D%7B1-p%7D)\"\u003e\u003c/p\u003e\n\u003cp\u003eAs you've seen previously, the maximum will then occur when the derivative equals zero:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=0%20=%20y%20(%5Cfrac%7B1%7D%7Bp%7D)%2b(n-y)(%5Cfrac%7B-1%7D%7B1-p%7D)\"\u003e\u003c/p\u003e\n\u003cp\u003eDistributing, you have\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=0%20=%20%5Cfrac%7By%7D%7Bp%7D%20-%20%5Cfrac%7Bn-y%7D%7B1-p%7D\"\u003e\u003c/p\u003e\n\u003cp\u003eAnd solving for p:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cfrac%7Bn-y%7D%7B1-p%7D%20=%20%5Cfrac%7By%7D%7Bp%7D\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=p(n-y)%20=%20%5Cfrac%7By(1-p)%7D%7Bp%7D\"\u003e\u003cbr\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cfrac%7Bn-y%7D%7By%7D%20=%20%5Cfrac%7B1-p%7D%7Bp%7D\"\u003e\u003cbr\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cfrac%7Bn%7D%7By%7D-1%20=%20%5Cfrac%7B1%7D%7Bp%7D-1\"\u003e\u003cbr\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cfrac%7Bn%7D%7By%7D%20=%20%5Cfrac%7B1%7D%7Bp%7D\"\u003e\u003cbr\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=p%20=%20%5Cfrac%7By%7D%7Bn%7D\"\u003e\u003c/p\u003e\n\u003cp\u003eAnd voil, you've verified the intuitive solution discussed above; the maximum likelihood for a binomial sample is the observed frequency!\u003c/p\u003e\n\u003ch2\u003eSummary\u003c/h2\u003e\n\u003cp\u003eIn this lesson, you briefly reviewed maximum likelihood estimation. In the upcoming lesson, you'll see how logistic regression can also be interpreted from this framework, which will help set the stage for you to code a logistic regression function from scratch using NumPy. Continue on to the next lesson to take a look at how this works for logistic regression.\u003c/p\u003e","frontPage":false},{"exportId":"building-an-object-oriented-simulation","title":"Building an Object-Oriented Simulation","type":"WikiPage","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-building-an-object-oriented-simulation\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-building-an-object-oriented-simulation/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eIn this lesson, you'll learn a bit more about best practices for running simulations in the real world using object-oriented programming!\u003c/p\u003e\n\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\n\u003cp\u003eYou will be able to:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eUse inheritance to write nonredundant code\u003c/li\u003e\n\u003cli\u003eCreate methods that calculate statistics of the attributes of an object\u003c/li\u003e\n\u003cli\u003eCreate object-oriented data models that describe the real world with multiple classes and subclasses and interaction between classes\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch2\u003eCreating Stochastic Simulations\u003c/h2\u003e\n\n\u003cp\u003eAs a capstone for everything you've learned about object-oriented programming, you'll be creating a \u003cstrong\u003e\u003cem\u003eHerd Immunity Simulation\u003c/em\u003e\u003c/strong\u003e.\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-building-an-object-oriented-simulation/master/images/herd_immunity.gif\"\u003e\u003c/p\u003e\n\n\u003cp\u003e\u003ca href=\"https://www.reddit.com/r/dataisbeautiful/comments/5v72fw/how_herd_immunity_works_oc/\"\u003egif created by u/theotheredmund\u003c/a\u003e\u003c/p\u003e\n\n\u003cp\u003eThis simulation is meant to model the effects that vaccinations have on the way a communicable disease spreads through a population. The simulation you're building depends on just a few statistics from the CDC (Center for Disease Control):\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003e\n\u003ccode\u003er0\u003c/code\u003e, the average number of people a contagious person infects before they are no longer contagious (because they got better or they died)\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003emortality_rate\u003c/code\u003e, the percentage chance a person infected with a disease will die from it \u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003eThe main workflow of this simulation is as follows:\u003c/p\u003e\n\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003eCreate a \u003ccode\u003ePerson()\u003c/code\u003e class with the following attributes:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003e\u003ccode\u003ealive\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003evaccinated\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eis_infected\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003ehas_been_infected\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003enewly_infected\u003c/code\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eCreate a \u003ccode\u003eSimulation()\u003c/code\u003e class with the following attributes:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003e\u003ccode\u003epopulation\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003evirus_name\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003enum_time_steps\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003er0\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003epercent_pop_vaccinated\u003c/code\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cp\u003eCreate methods for our \u003ccode\u003eSimulation()\u003c/code\u003e class that will cover each step of the simulation. \u003c/p\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\n\u003cp\u003eIn order for our simulation to work, you'll need to define some rules for it:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003e\u003cp\u003eEach infected person will \"interact\" with 100 random people from the \u003ccode\u003epopulation\u003c/code\u003e. If the person the infected individual interacts with is sick, vaccinated, or has had the disease before, nothing happens. However, if the person the infected individual interacts with is healthy, unvaccinated, and has not been infected yet, then that person becomes infected. \u003c/p\u003e\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eAt the end of each round, the following things happen:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eAll currently infected people either get better from the disease or die, with the chance of death corresponding to the mortality rate of the disease \u003c/li\u003e\n\u003cli\u003eAll people that were newly infected during this round become the new infected for the next round\u003cbr\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003eThe simulation continues for the set number of time steps.  Any time someone dies or gets infected, log it in a text file called \u003ccode\u003e'simulation_logs.txt'\u003c/code\u003e.  Once the simulation is over, write some code to quickly parse the text logs into data and visualize the results, so that you can run multiple simulations and answer questions like: \u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eIf vaccination rates for {disease x} dropped by 5%, how many more people become infected in an epidemic? How many more die?\u003c/li\u003e\n\u003cli\u003eWhat does the spread of {disease x} through a population look like?\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003eIf this all seems a bit daunting, don't worry! You'll be provided with much more detail as you build this step-by-step during the lab. \u003c/p\u003e\n\n\u003cp\u003eWith that, go ahead and take a look at this cool simulation lab!\u003c/p\u003e\n\n\u003ch2\u003eSummary\u003c/h2\u003e\n\n\u003cp\u003eIn this lesson, you reviewed some best practices for simulating things in the real world using object-oriented programming!\u003c/p\u003e\n\u003cfooter class=\"fis-footer\" style=\"visibility: hidden;\"\u003e\u003cdiv class=\"fis-feedback\"\u003e\n\u003ch5\u003eHow do you feel about this lesson?\u003c/h5\u003e\n\u003cimg id=\"thumbs-up\" data-repository=\"dsc-building-an-object-oriented-simulation\" title=\"Thumbs up!\" alt=\"thumbs up\"\u003e\u003cimg id=\"thumbs-down\" data-repository=\"dsc-building-an-object-oriented-simulation\" title=\"Thumbs down!\" alt=\"thumbs down\"\u003e\n\u003c/div\u003e\n\u003ch5\u003eHave specific feedback? \u003ca href=\"https://github.com/learn-co-curriculum/dsc-building-an-object-oriented-simulation/issues/new\"\u003eTell us here!\u003c/a\u003e\n\u003c/h5\u003e\u003c/footer\u003e","frontPage":false},{"exportId":"pipelines-recap","title":"Pipelines - Recap","type":"WikiPage","content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-pipelines-recap\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-pipelines-recap\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-pipelines-recap/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\u003ch2\u003eKey Takeaways\u003c/h2\u003e\n\u003cp\u003eThe key takeaways from this section include:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eMachine Learning Pipelines create a nice workflow to combine data manipulations, preprocessing, and modeling\u003c/li\u003e\n\u003cli\u003eMachine Learning Pipelines can be used along with grid search to evaluate several parameter settings\u003c/li\u003e\n\u003cli\u003eGrid search can considerably blow up computation time when computing for several parameters along with cross-validation\u003c/li\u003e\n\u003cli\u003eSome models are very sensitive to hyperparameter changes, so they should be chosen with care, and even with big grids a good outcome isn't always guaranteed\u003c/li\u003e\n\u003c/ul\u003e","frontPage":false},{"exportId":"regularization-introduction","title":"Regularization - Introduction","type":"WikiPage","content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-regularization-intro-v2-1\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-regularization-intro-v2-1\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-regularization-intro-v2-1/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eIn an attempt to fit a good model to data, we often tend to overfit. In this section, you will learn about Regularization, a technique used to avoid overfitting. Regularization discourages overly complex models by penalizing the loss function.\u003c/p\u003e\n\u003ch2\u003eRidge and Lasso\u003c/h2\u003e\n\u003cp\u003eRidge and Lasso regression are two examples of penalized estimation. Penalized estimation makes some or all of the coefficients smaller in magnitude (closer to zero). Some of the penalties have the property of performing both variable selection (setting some coefficients exactly equal to zero) and shrinking the other coefficients.\u003c/p\u003e\n\u003cp\u003eIn Ridge regression, the cost function is changed by adding a penalty term to the square of the magnitude of the coefficients.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Ctext%7BRidge%20cost%20function%7D=%20%5Csum_%7Bi=1%7D%5En(y_i%20-%20%5Chat%7By%7D)%5E2%20=%20%5Csum_%7Bi=1%7D%5En(y_i%20-%20%5Csum_%7Bj=1%7D%5Ek(m_jx_%7Bij%7D)-b)%5E2%20%2b%20%5Clambda%20%5Csum_%7Bj=1%7D%5Ep%20m_j%5E2\"\u003e\u003c/p\u003e\n\u003cp\u003eLasso regression is very similar to Ridge regression, except that the magnitude of the coefficients are not squared in the penalty term.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Ctext%7BLASSO%20cost%20function%7D=%20%5Csum_%7Bi=1%7D%5En(y_i%20-%20%5Chat%7By%7D)%5E2%20=%20%5Csum_%7Bi=1%7D%5En(y_i%20-%20%5Csum_%7Bj=1%7D%5Ek(m_jx_%7Bij%7D)-b)%5E2%20%2b%20%5Clambda%20%5Csum_%7Bj=1%7D%5Ep%20%5Cmid%20m_j%20%5Cmid\"\u003e\u003c/p\u003e\n\u003ch2\u003eAIC and BIC\u003c/h2\u003e\n\u003cp\u003eIn this section you'll also be introduced to two new measures: AIC and BIC, which give you a comprehensive measure of model performace taking into account the additional variables.\u003c/p\u003e\n\u003cp\u003eThe formula for the AIC, invented by Hirotugu Akaike in 1973 and short for \"Akaike's Information Criterion\" is given by:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Ctext%7BAIC(model)%7D%20=%20-%202%20*%20%5Ctext%7Blog-likelihood(model)%7D%20%2b%202%20*%20%5Ctext%7Blength%20of%20the%20parameter%20space%7D\"\u003e\u003c/p\u003e\n\u003cp\u003eThe BIC (Bayesian Information Criterion) is very similar to the AIC and emerged as a Bayesian response to the AIC, but can be used for the exact same purposes.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Ctext%7BBIC(model)%7D%20=%20-2%20*%20%5Ctext%7Blog-likelihood(model)%7D%20%2b%20%5Ctext%7Blog(number%20of%20observations)%7D%20*%20%5Ctext%7B(length%20of%20the%20parameter%20space)%7D\"\u003e\u003c/p\u003e\n\u003cp\u003eLower the values of AIC and BIC, the better your model is performing.\u003c/p\u003e\n\u003ch2\u003eGenerating Data\u003c/h2\u003e\n\u003cp\u003eFinally, you will also see how you can generate practice datasets that allow testing and debugging of algorithms.\u003c/p\u003e\n\u003ch2\u003eSummary\u003c/h2\u003e\n\u003cp\u003eRegularization helps prevent overfitting your models. It is also useful when performing feature selection. On the other hand, if you are comparing multiple models with varying number of features, you can use the AIC and BIC measures. Remember that the lower the AIC (or BIC), the better the model.\u003c/p\u003e","frontPage":false},{"exportId":"hyperparameter-tuning-and-pruning-in-decision-trees","title":"Hyperparameter Tuning and Pruning in Decision Trees","type":"WikiPage","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-tuning-decision-trees\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-tuning-decision-trees/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eHyperparameter tuning relates to how we sample candidate model architectures from the space of all possible hyperparameter values. This is often referred to as \u003cstrong\u003esearching the hyperparameter space for the optimum values\u003c/strong\u003e. In this lesson, we'll look at some of the key hyperparameters for decision trees and how they affect the learning and prediction processes. \u003c/p\u003e\n\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\n\u003cul\u003e\n\u003cli\u003eIdentify the role of pruning while training decision trees\u003cbr\u003e\n\u003c/li\u003e\n\u003cli\u003eList the different hyperparameters for tuning decision trees \u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch2\u003eHyperparameter Optimization\u003c/h2\u003e\n\n\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003eIn machine learning, a hyperparameter is a parameter whose value is set before the learning process begins.\u003c/strong\u003e\u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003cp\u003eBy contrast, the values of model parameters are derived via training as we have seen previously.\nDifferent model training algorithms require different hyperparameters, some simple algorithms (such as ordinary least squares regression) require none. Given these hyperparameters, the training algorithm learns the parameters from the data. For instance, Lasso is an algorithm that adds a regularization hyperparameter to ordinary least squares regression, which has to be set before estimating the parameters through the training algorithm. \u003c/p\u003e\n\n\u003cp\u003eIn this lesson, we'll look at these sorts of optimizations in the context of decision trees and see how these can affect the predictive performance as well as the computational complexity of the tree. \u003c/p\u003e\n\n\u003ch2\u003eTree pruning\u003c/h2\u003e\n\n\u003cp\u003eNow that we know how to grow a decision tree using Python and scikit-learn, let's move on and practice \u003cstrong\u003eoptimizing\u003c/strong\u003e a classifier. We can tweak a few parameters in the decision tree algorithm before the actual learning takes place. \u003c/p\u003e\n\n\u003cp\u003eA decision tree, grown beyond a certain level of complexity leads to overfitting. If we grow our tree and carry on using poor predictors that don't have any impact on the accuracy, we will eventually a) slow down the learning, and b) cause overfitting.  Different tree pruning parameters can adjust the amount of overfitting or underfitting in order to optimize for increased accuracy, precision, and/or recall.\u003c/p\u003e\n\n\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003eThis process of trimming decision trees to optimize the learning process is called \"tree pruning\".\u003c/strong\u003e\u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003cp\u003eWe can prune our trees using:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003e\u003cp\u003eMaximum depth: Reduce the depth of the tree to build a generalized tree. Set the depth of the tree to 3, 5, 10 depending after verification on test data\u003c/p\u003e\u003c/li\u003e\n\u003cli\u003e\u003cp\u003eMinimum samples leaf with split: Restrict the size of sample leaf\u003c/p\u003e\u003c/li\u003e\n\u003cli\u003e\u003cp\u003eMinimum leaf sample size: Size in terminal nodes can be fixed to 30, 100, 300 or 5% of total\u003c/p\u003e\u003c/li\u003e\n\u003cli\u003e\u003cp\u003eMaximum leaf nodes: Reduce the number of leaf nodes\u003c/p\u003e\u003c/li\u003e\n\u003cli\u003e\u003cp\u003eMaximum features: Maximum number of features to consider when splitting a node\u003c/p\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003eLet's look at a few hyperparameters and learn about their impact on classifier performance:  \u003c/p\u003e\n\n\u003ch2\u003e\u003ccode\u003emax_depth\u003c/code\u003e\u003c/h2\u003e\n\n\u003cp\u003eThe parameter for decision trees that we normally tune first is \u003ccode\u003emax_depth\u003c/code\u003e. This parameter indicates how deep we want our tree to be. If the tree is too deep, it means we are creating a large number of splits in the parameter space and capturing more information about underlying data. This may result in \u003cstrong\u003eoverfitting\u003c/strong\u003e as it will lead to learning granular information from given data, which makes it difficult for our model to generalize on unseen data. \nGenerally speaking, a low training error but a large testing error is a strong indication of this. \u003c/p\u003e\n\n\u003cp\u003eIf, on the other hand, the tree is too shallow, we may run into \u003cstrong\u003eunderfitting\u003c/strong\u003e, i.e., we are not learning enough information about the data and the accuracy of the model stays low for both the test and training samples. The following example shows the training and test AUC scores for a decision tree with depths ranging from 1 to 32.\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-tuning-decision-trees/master/images/depth.png\" width=\"400\"\u003e\u003c/p\u003e\n\n\u003cp\u003eIn the above example, we see that as the tree depth increases, our validation/test accuracy starts to go down after a depth of around 4. But with even greater depths, the training accuracy keeps on rising, as the classifier learns more information from the data. However this information can not be mapped onto unseen examples, hence the validation accuracy falls down constantly. Finding the sweet spot (e.g. depth = 4) in this case would be the first hyperparameter that we need to tune. \u003c/p\u003e\n\n\u003ch2\u003e\u003ccode\u003emin_samples_split\u003c/code\u003e\u003c/h2\u003e\n\n\u003cp\u003eThe hyperparameter \u003ccode\u003emin_samples_split\u003c/code\u003e is used to set the \u003cstrong\u003eminimum number of samples required to split an internal node\u003c/strong\u003e. This can vary between two extremes, i.e., considering only one sample at each node vs. considering all of the samples at each node - for a given attribute. \u003c/p\u003e\n\n\u003cp\u003eWhen we increase this parameter value, the tree becomes more constrained as it has to consider more samples at each node. Here we will vary the parameter from 10% to 100% of the samples.\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-tuning-decision-trees/master/images/split.png\" width=\"500\"\u003e\u003c/p\u003e\n\n\u003cp\u003eIn the above plot, we see that the training and test accuracy stabilize at a certain minimum sample split size, and stays the same even if we carry on increasing the size of the split. This means that we will have a complex model, with similar accuracy than a much simpler model could potentially exhibit. Therefore, it is imperative that we try to identify the optimal sample size during the training phase. \u003c/p\u003e\n\n\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003eNote\u003c/strong\u003e: \u003ccode\u003emax_depth\u003c/code\u003e and \u003ccode\u003emin_samples_split\u003c/code\u003e are also both related to the computational cost involved with growing the tree. Large values for these parameters can create complex, dense, and long trees. For large datasets, it may become extremely time-consuming to use default values.  \u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003ch2\u003e\u003ccode\u003emin_samples_leaf\u003c/code\u003e\u003c/h2\u003e\n\n\u003cp\u003eThis hyperparameter is used to identify the minimum number of samples that we want a leaf node to contain. When this minimum size is achieved at a node, it does not get split any further.  This parameter is similar to \u003ccode\u003emin_samples_splits\u003c/code\u003e, however, this describes the minimum number of samples at the leaves, the base of the tree.\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-tuning-decision-trees/master/images/leaf.png\" width=\"400\"\u003e\u003c/p\u003e\n\n\u003cp\u003eThe above plot shows the impact of this parameter on the accuracy of the classifier. We see that increasing this parameter value after an optimal point reduces accuracy. That is due to underfitting again, as keeping too many samples in our leaf nodes means that there is still a high level of uncertainty in the data. \u003c/p\u003e\n\n\u003cp\u003eThe main difference between the two is that \u003ccode\u003emin_samples_leaf\u003c/code\u003e guarantees a minimum number of samples in a leaf, while \u003ccode\u003emin_samples_split\u003c/code\u003e can create arbitrary small leaves, though \u003ccode\u003emin_samples_split\u003c/code\u003e is more common in practice. These two hyperparameters make the distinction between a leaf (terminal/external node) and an internal node. An internal node will have further splits (also called children), while a leaf is by definition a node without any children (without any further splits).\u003c/p\u003e\n\n\u003cp\u003eFor instance, if \u003ccode\u003emin_samples_split = 5\u003c/code\u003e, and there are 7 samples at an internal node, then the split is allowed. But let's say the split results in two leaves, one with 1 sample, and another with 6 samples. If \u003ccode\u003emin_samples_leaf = 2\u003c/code\u003e, then the split won't be allowed (even if the internal node has 7 samples) because one of the leaves resulted will have less than the minimum number of samples required to be at a leaf node.\u003c/p\u003e\n\n\u003ch3\u003eAre there more hyperparameters?\u003c/h3\u003e\n\n\u003cp\u003eYes, there are! Scikit-learn offers a number of other hyperparameters for further fine-tuning the learning process. \u003ca href=\"https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html\"\u003eConsult the official doc\u003c/a\u003e to look at them in detail. The hyperparameters mentioned here are directly related to the complexity which may arise in decision trees and are normally tuned when growing trees. We'll shortly see this in action with a real dataset. \u003c/p\u003e\n\n\u003ch2\u003eAdditional Resources\u003c/h2\u003e\n\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://cloud.google.com/ml-engine/docs/tensorflow/hyperparameter-tuning-overview\"\u003eOverview of hyperparameter tuning\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://towardsdatascience.com/demystifying-hyper-parameter-tuning-acb83af0258f\"\u003eDemystifying hyperparameter tuning\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://www.displayr.com/machine-learning-pruning-decision-trees/\"\u003ePruning decision trees\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch2\u003eSummary\u003c/h2\u003e\n\n\u003cp\u003eIn this lesson, we looked at the idea of optimizing hyperparameters and how pruning plays an important role in restricting the growth of a decision tree. We looked at a few hyperparameters which directly impact the potential overfitting/underfitting in trees. Next, we'll see these in practice using scikit-learn.   \u003c/p\u003e\n\u003cfooter class=\"fis-footer\" style=\"visibility: hidden;\"\u003e\u003cdiv class=\"fis-feedback\"\u003e\n\u003ch5\u003eHow do you feel about this lesson?\u003c/h5\u003e\n\u003cimg id=\"thumbs-up\" data-repository=\"dsc-tuning-decision-trees\" title=\"Thumbs up!\" alt=\"thumbs up\"\u003e\u003cimg id=\"thumbs-down\" data-repository=\"dsc-tuning-decision-trees\" title=\"Thumbs down!\" alt=\"thumbs down\"\u003e\n\u003c/div\u003e\n\u003ch5\u003eHave specific feedback? \u003ca href=\"https://github.com/learn-co-curriculum/dsc-tuning-decision-trees/issues/new\"\u003eTell us here!\u003c/a\u003e\n\u003c/h5\u003e\u003c/footer\u003e","frontPage":false},{"exportId":"linear-algebra-introduction","title":"Linear Algebra - Introduction","type":"WikiPage","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-linalg-introduction\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-linalg-introduction/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eIn this section, we're going to take a step back to learn some of the basics of linear algebra - the math that powers most machine learning models. You may not need to know linear algebra just to call a method in scikit-learn to do some modeling, but this introduction to linear algebra should give you a much better understanding of how your models are working \"under the hood\".\u003c/p\u003e\n\n\u003ch2\u003eThe importance of linear algebra\u003c/h2\u003e\n\n\u003cp\u003eWe're going to kick this section off by looking at some of the many places that linear algebra is used in machine learning - from deep learning through Natural Language Processing and dimensionality reduction techniques such as Principle Component Analysis.\u003c/p\u003e\n\n\u003ch2\u003eSystems of linear equations\u003c/h2\u003e\n\n\u003cp\u003eWe then start to dig into the math! We look at the idea of linear simultaneous equations - a set of two or more equations each of which is linear (can be plotted on a graph as a straight line). We then see how such equations can be represented as vectors or matrices to represent such systems efficiently.\u003c/p\u003e\n\n\u003ch2\u003eScalars, vectors, matrices, and tensors\u003c/h2\u003e\n\n\u003cp\u003eIn a code along, we'll introduce the concepts and concrete representations (in NumPy) of scalars, vectors, matrices, and tensors - why they are important and how to create them. \u003c/p\u003e\n\n\u003ch2\u003eVector/matrix operations\u003c/h2\u003e\n\n\u003cp\u003eWe then start to build up the basic skills required to perform matrix operations such as addition and multiplication.  You will also cover key techniques used by many machine learning models to perform their calculations covering both the Hadamard product and the (more common) dot product. \u003c/p\u003e\n\n\u003ch2\u003eSolving systems of linear equations using NumPy\u003c/h2\u003e\n\n\u003cp\u003eWe then bring the previous work together to look at how to use NumPy to solve systems of linear equations, introducing the identity and inverse matrices along the way.\u003c/p\u003e\n\n\u003ch2\u003eRegression analysis using linear algebra and NumPy\u003c/h2\u003e\n\n\u003cp\u003eHaving built up a basic mathematical and computational foundation for linear algebra, you will solve a real data problem - looking at how to use NumPy to solve a linear regression using the ordinary least squares (OLS) method.\u003c/p\u003e\n\n\u003ch2\u003eComputational complexity\u003c/h2\u003e\n\n\u003cp\u003eFinally, we look at the idea of computational complexity and Big O notation, showing why OLS is computationally inefficient, and that a gradient descent algorithm can instead be used to solve a linear regression much more efficiently.\u003c/p\u003e\n\n\u003ch2\u003eSummary\u003c/h2\u003e\n\n\u003cp\u003eLinear Algebra is so foundational to machine learning that you're going to see it referenced many times as the course progresses. In this section, the goal is to give you both a theoretical introduction and some computational practice, solving a real-life problem by writing the code required to solve a linear regression using OLS.\u003c/p\u003e\n\u003cfooter class=\"fis-footer\" style=\"visibility: hidden;\"\u003e\u003cdiv class=\"fis-feedback\"\u003e\n\u003ch5\u003eHow do you feel about this lesson?\u003c/h5\u003e\n\u003cimg id=\"thumbs-up\" data-repository=\"dsc-linalg-introduction\" title=\"Thumbs up!\" alt=\"thumbs up\"\u003e\u003cimg id=\"thumbs-down\" data-repository=\"dsc-linalg-introduction\" title=\"Thumbs down!\" alt=\"thumbs down\"\u003e\n\u003c/div\u003e\n\u003ch5\u003eHave specific feedback? \u003ca href=\"https://github.com/learn-co-curriculum/dsc-linalg-introduction/issues/new\"\u003eTell us here!\u003c/a\u003e\n\u003c/h5\u003e\u003c/footer\u003e","frontPage":false},{"exportId":"systems-of-linear-equations","title":"Systems of Linear Equations","type":"WikiPage","content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-lingalg-linear-equations\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-lingalg-linear-equations\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-lingalg-linear-equations/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eLinear algebra is a sub-field of mathematics concerned with vectors, matrices, and linear transforms between them. The first step towards developing a good understanding of linear algebra is to get a good sense of \u003cem\u003ewhat linear mappings and linear equations\u003c/em\u003e are, \u003cem\u003ehow these relate to vectors and matrices\u003c/em\u003e and \u003cem\u003ewhat this has to do with data analysis\u003c/em\u003e. Let's try to develop a basic intuition around these ideas by first understanding what linear equations are.\u003c/p\u003e\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\u003cp\u003eYou will be able to:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eDescribe a system of linear equations for solving analytical problems\u003c/li\u003e\n\u003cli\u003eDescribe how matrices and vectors can be used to solve linear equations\u003c/li\u003e\n\u003cli\u003eSolve a system of equations using elimination and substitution\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003eWhat are linear equations?\u003c/h2\u003e\n\u003cp\u003eIn mathematics, a system of linear equations (or linear system) is a collection of two or more linear equations involving the same set of variables. For example, look at the following equations:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=3x%20%2b%202y%20-%20z%20=%200\"\u003e\u003c/li\u003e\n\u003cli\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=2x-%202y%20%2b%204z%20=%20-2\"\u003e\u003c/li\u003e\n\u003cli\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=-x%20%2b%200.5y%20-%20z%20=%200\"\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThis is a system of three equations in the three variables \u003cimg src=\"https://render.githubusercontent.com/render/math?math=x\"\u003e , \u003cimg src=\"https://render.githubusercontent.com/render/math?math=y\"\u003e , and \u003cimg src=\"https://render.githubusercontent.com/render/math?math=z\"\u003e . A solution to a linear system is an assignment of values to the variables in a way that \u003cem\u003eall the equations are simultaneously satisfied\u003c/em\u003e. A solution to the system above is given by:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=x%20=%201\"\u003e\u003c/li\u003e\n\u003cli\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=y%20=%20-8/3\"\u003e\u003c/li\u003e\n\u003cli\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=z%20=%20-7/3\"\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThese values make all three equations valid. The word \"system\" indicates that the equations are to be considered collectively, rather than individually.\u003c/p\u003e\n\u003ch2\u003eSolving linear equations\u003c/h2\u003e\n\u003cp\u003eA system of linear equations can always be expressed in a matrix form. Algebraically, both of these express the same thing. Let's work with an example to see how this works:\u003c/p\u003e\n\u003ch3\u003eExample\u003c/h3\u003e\n\u003cp\u003eLet's say you go to a market and buy 2 apples and 1 banana. For this, you end up paying 35 pence. If you denote apples by \u003cimg src=\"https://render.githubusercontent.com/render/math?math=a\"\u003e and bananas by \u003cimg src=\"https://render.githubusercontent.com/render/math?math=b\"\u003e , the relationship between items bought and the price paid can be written down as an equation - let's call it Eq. A:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=2a%20%2b%20b%20=%2035\"\u003e (Eq. A)\u003c/p\u003e\n\u003cp\u003eOn your next trip to the market, you buy 3 apples and 4 bananas, and the cost is 65 pence. Just like above, this can be written as Eq. B:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=3a%20%2b%204b%20=%2065\"\u003e (Eq. B)\u003c/p\u003e\n\u003cp\u003eThese two equations (known as a simultaneous equations) form a system that can be solved by hand for values of \u003cimg src=\"https://render.githubusercontent.com/render/math?math=a\"\u003e and \u003cimg src=\"https://render.githubusercontent.com/render/math?math=b\"\u003e i.e., price of a single apple and banana.\u003c/p\u003e\n\u003cp\u003eLet's solve this system for individual prices using a series of eliminations and substitutions:\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eStep 1:\u003c/strong\u003e Multiply Eq. A by 4\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=8a%20%2b%204b%20=%20140\"\u003e (Eq. C)\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eStep 2 :\u003c/strong\u003e Subtract Eq. B from Eq. C\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=5a%20=%2075\"\u003e which leads to \u003cimg src=\"https://render.githubusercontent.com/render/math?math=a%20=%2015\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eStep 3:\u003c/strong\u003e Substitute the value of \u003cimg src=\"https://render.githubusercontent.com/render/math?math=a\"\u003e in Eq. A\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=30%20%2b%20b%20=%2035\"\u003e which leads to \u003cimg src=\"https://render.githubusercontent.com/render/math?math=b%20=%205\"\u003e\u003c/p\u003e\n\u003cp\u003eSo the price of an apple is 15 pence and the price of the banana is 5 pence.\u003c/p\u003e\n\u003ch2\u003eFrom equations to vectors and matrices\u003c/h2\u003e\n\u003cp\u003eNow, as your number of shopping trips increase along with the number of items you buy at each trip, the system of equations will become more complex and solving a system for individual price may become very expensive in terms of time and effort. In these cases, you can use a computer to find the solution.\u003c/p\u003e\n\u003cp\u003eThe above example is a classic linear algebra problem. The numbers 2 and 1 from Eq. A and 3 and 4 from Eq. B are linear coefficients that relate input variables a and b to the known output 15 and 5.\u003c/p\u003e\n\u003cp\u003eUsing linear algebra, we can write this system of equations as shown below:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-lingalg-linear-equations/master/images/ss.png\" width=\"320\"\u003e\u003c/p\u003e\n\u003cp\u003eYou see that in order for a computational algorithm to solve this (and other similar) problems, we need to first convert the data we have into a set of matrix and vector objects. Machine learning involves building up these objects from the given data, understanding their relationships and how to process them for a particular problem.\u003c/p\u003e\n\u003cp\u003eSolving these equations requires knowledge of defining these vectors and matrices in a computational environment and of operations that can be performed on these entities to solve for unknown variables as we saw above. We'll look into how to do this in upcoming lessons.\u003c/p\u003e\n\u003ch2\u003eSummary\u003c/h2\u003e\n\u003cp\u003eIn this lesson, you learned how a system of linear (simultaneous) equations can be solved using elimination and substitution, and also, how to covert these problems into matrices and vectors to be processed by computational algorithms. In the next couple of lessons, we'll look at how to describe these entities in Python and NumPy and also how to perform arithmetic operations to solve these types of equations.\u003c/p\u003e","frontPage":false},{"exportId":"finding-the-best-value-for-k","title":"Finding the Best Value for K","type":"WikiPage","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-finding-the-best-value-for-k\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-finding-the-best-value-for-k/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eIn this lesson, you'll investigate how changing the value for K can affect the performance of the model, and how to use this to find the best value for K.\u003c/p\u003e\n\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\n\u003cul\u003e\n\u003cli\u003eConduct a parameter search to find the optimal value for K \u003c/li\u003e\n\u003cli\u003eExplain how KNN is related to the curse of dimensionality \u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch2\u003eFinding the optimal number of neighbors\u003c/h2\u003e\n\n\u003cp\u003eBy now, you've got a strong understanding of how the K-Nearest Neighbors algorithm works, but you likely have at least one lingering question\u003cstrong\u003e\u003cem\u003ewhat is the best value to use for K\u003c/em\u003e\u003c/strong\u003e? There's no set number that works best. If there was, it wouldn't be called \u003cstrong\u003e\u003cem\u003eK\u003c/em\u003e\u003c/strong\u003e-nearest neighbors. While the best value for K is not immediately obvious for any problem, there are some strategies that you can use to select a good or near optimal value. \u003c/p\u003e\n\n\u003ch2\u003eK, overfitting, and underfitting\u003c/h2\u003e\n\n\u003cp\u003eIn general, the smaller K is, the tighter the \"fit\" of the model. Remember that with supervised learning, you want to fit a model to the data as closely as possible without \u003cstrong\u003e\u003cem\u003eoverfitting\u003c/em\u003e\u003c/strong\u003e to patterns in the training set that don't generalize.  This can happen if your model pays too much attention to every little detail and makes a very complex decision boundary. Conversely, if your model is overly simplistic, then you may have \u003cstrong\u003e\u003cem\u003eunderfit\u003c/em\u003e\u003c/strong\u003e the model, limiting its potential. A visual explanation helps demonstrate this concept in practice:\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-finding-the-best-value-for-k/master/images/fit_fs.png\" width=\"700\"\u003e\u003c/p\u003e\n\n\u003cp\u003eWhen K is small, any given prediction only takes into account a very small number of points around it to make the prediction. If K is too small, this can end up with a decision boundary that looks like the overfit picture on the right. \u003c/p\u003e\n\n\u003cp\u003eConversely, as K grows larger, it takes into account more and more points, that are farther and farther away from the point in question, increasing the overall size of the region taken into account. If K grows too large, then the model begins to underfit the data. \u003c/p\u003e\n\n\u003cp\u003eIt's important to try to find the best value for K by iterating over a multiple values and comparing performance at each step. \u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-finding-the-best-value-for-k/master/images/best_k_fs.png\" width=\"550\"\u003e\u003c/p\u003e\n\n\u003cp\u003eAs you can see from the image above, \u003ccode\u003ek=1\u003c/code\u003e and \u003ccode\u003ek=3\u003c/code\u003e will provide different results! \u003c/p\u003e\n\n\u003ch2\u003eIterating over values of K\u003c/h2\u003e\n\n\u003cp\u003eSince the model arrives at a prediction by voting, it makes sense that you should only use odd values for k, to avoid ties and subsequent arbitrary guesswork. By adding this constraint (an odd value for k) the model will never be able to evenly split between two classes. From here, finding an optimal value of K requires some iterative investigation.\u003c/p\u003e\n\n\u003cp\u003eThe best way to find an optimal value for K is to choose a minimum and maximum boundary and try them all! In practice, this means:\u003c/p\u003e\n\n\u003col\u003e\n\u003cli\u003eFit a KNN classifier for each value of K \u003c/li\u003e\n\u003cli\u003eGenerate predictions with that model\u003cbr\u003e\n\u003c/li\u003e\n\u003cli\u003eCalculate and evaluate a performance metric using the predictions the model made \u003c/li\u003e\n\u003cli\u003eCompare the results for every model and find the one with the lowest overall error, or highest overall score!\u003c/li\u003e\n\u003c/ol\u003e\n\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-finding-the-best-value-for-k/master/images/plot_fs.png\" width=\"550\"\u003e\u003c/p\u003e\n\n\u003cp\u003eA common way to find the best value for K at a glance is to plot the error for each value of K. Find the value for K where the error is lowest. If this graph continued into higher values of K, we would likely see the error numbers go back up as K increased. \u003c/p\u003e\n\n\u003ch2\u003eKNN and the curse of dimensionality\u003c/h2\u003e\n\n\u003cp\u003eNote that KNN isn't the best choice for extremely large datasets, and/or models with high dimensionality. This is because the time complexity (what computer scientists call \"Big O\", which you saw briefly earlier) of this algorithm is exponential. As you add more data points to the dataset, the number of operations needed to complete all the steps of the algorithm grows exponentially! That said, for smaller datasets, KNN often works surprisingly well, given the simplicity of the overall algorithm. However, if your dataset contains millions of rows and thousands of columns, you may want to choose another algorithm, as the algorithm may not run in any reasonable amount of time;in some cases, it could quite literally take years to complete! \u003c/p\u003e\n\n\u003ch2\u003eSummary\u003c/h2\u003e\n\n\u003cp\u003eIn this lesson you learned how to determine the best value for K and that the KNN algorithm may not necessarily be the best choice for large datasets due to the large amount of time it can take for the algorithm to run. \u003c/p\u003e\n\u003cfooter class=\"fis-footer\" style=\"visibility: hidden;\"\u003e\u003cdiv class=\"fis-feedback\"\u003e\n\u003ch5\u003eHow do you feel about this lesson?\u003c/h5\u003e\n\u003cimg id=\"thumbs-up\" data-repository=\"dsc-finding-the-best-value-for-k\" title=\"Thumbs up!\" alt=\"thumbs up\"\u003e\u003cimg id=\"thumbs-down\" data-repository=\"dsc-finding-the-best-value-for-k\" title=\"Thumbs down!\" alt=\"thumbs down\"\u003e\n\u003c/div\u003e\n\u003ch5\u003eHave specific feedback? \u003ca href=\"https://github.com/learn-co-curriculum/dsc-finding-the-best-value-for-k/issues/new\"\u003eTell us here!\u003c/a\u003e\n\u003c/h5\u003e\u003c/footer\u003e","frontPage":false},{"exportId":"bayesian-classification-introduction","title":"Bayesian Classification - Introduction","type":"WikiPage","content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-bayesian-classification-intro-v2-1\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-bayesian-classification-intro-v2-1\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-bayesian-classification-intro-v2-1/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eIn an earlier section, you learned about Bayesian statistics with plenty of theory and application of Bayes theorem. You'll now take a look at using Bayes theorem to perform some classification tasks. Here, you'll see that the Bayes theorem can be applied to multiple variables simultaneously.\u003c/p\u003e\n\u003ch2\u003eBayes Classification\u003c/h2\u003e\n\u003cp\u003eNaive Bayes algorithms extend Bayes' formula to multiple variables by assuming that these features are independent of one another, which may not be met, (hence its naivety) it can nonetheless provide strong results in scenarios with clean and well normalized datasets. This then allows you to estimate an overall probability by multiplying the conditional probabilities for each of the independent features.\u003c/p\u003e\n\u003cp\u003eBayes' formula extended to multiple features is:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5CLarge%20P(y%7Cx_1,%20x_2,%20...,%20x_n)%20=%20%5Cfrac%7BP(y)%5Cdisplaystyle%5Cprod_%7Bi%7D%5E%7Bn%7DP(x_i%7Cy)%7D%7BP(x_1,%20x_2,%20...,%20x_n)%7D\"\u003e\u003c/p\u003e\n\u003ch2\u003eDocument Classification\u003c/h2\u003e\n\u003cp\u003eAn interesting application of Bayes' theorem is to use \u003cem\u003ebag of words\u003c/em\u003e for document classification. A bag of words representation takes a text document and converts it into a word frequency representation. In this section, you'll use bag of words and Naive Bayes to classify YouTube videos into appropriate topics.\u003c/p\u003e\n\u003ch2\u003eSummary\u003c/h2\u003e\n\u003cp\u003eOver the next few lessons you will learn about another fundamental classification algorithm which has many practical applications. It's time to jump into the wonderful Bayesian world again! This section will help you solidify your understanding of Bayesian stats.\u003c/p\u003e","frontPage":false},{"exportId":"mle-and-logistic-regression","title":"MLE and Logistic Regression","type":"WikiPage","content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-mle-logistic-regression\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-mle-logistic-regression\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-mle-logistic-regression/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eIn this lesson, you'll further investigate the connections between maximum likelihood estimation and logistic regression. This is a common perspective for logistic regression and will be the underlying intuition for upcoming lessons where you'll code the algorithm from the ground up using NumPy.\u003c/p\u003e\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\u003cp\u003eYou will be able to:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eDetermine how MLE is tied into logistic regression\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003eMLE formulation\u003c/h2\u003e\n\u003cp\u003eAs discussed, maximum likelihood estimation finds the underlying parameters of an assumed distribution to maximize the likelihood of the observations. Logistic regression expands upon this by investigating the conditional probabilities associated with the various features, treating them as independent probabilities and calculating the respective total probability.\u003c/p\u003e\n\u003cp\u003eFor example, when predicting an individual's risk for heart disease, you might consider various factors such as their family history, weight, diet, exercise routines, blood pressure, and cholesterol. When looked at individually, each of these has an associated conditional probability that the individual has heart disease based on each of these factors. Mathematically, you can write each of these probabilities for each factor \u003cimg src=\"https://render.githubusercontent.com/render/math?math=X\"\u003e as:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cpi_i%20=%20Pr(Y_i%20=%201%7CX_i%20=%20x_i)%20=%20%5Cdfrac%7B%5Ctext%7Bexp%7D(%5Cbeta_0%20%2B%20%5Cbeta_1%20x_i)%7D%7B1%20%2B%20%5Ctext%7Bexp%7D(%5Cbeta_0%20%2B%20%5Cbeta_1%20x_i)%7D\"\u003e\u003c/p\u003e\n\u003cp\u003eThis is the standard linear regression model \u003cimg src=\"https://render.githubusercontent.com/render/math?math=(%5Cbeta_0%2B%5Cbeta_1%20x_i)\"\u003e you have seen previously, modified to have a range of 0 to 1. The range is modified and constrained by applying the sigmoid function since you're predicting probabilities.\u003c/p\u003e\n\u003cp\u003eThen, combining these conditional probabilities from multiple features, you maximize the likelihood function of each of those independent conditional probabilities, giving you:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=L(%5Cbeta_0%2C%5Cbeta_1)%3D%5Cdisplaystyle%5Cprod_%7Bi=1%7D%5EN%5Cpi_i%5E%7By_i%7D(1-%5Cpi_i)%5E%7Bn_i-y_i%7D%3D%5Cdisplaystyle%5Cprod_%7Bi=1%7D%5EN%5Cdfrac%7B%5Ctext%7Bexp%7D%5C%7By_i(%5Cbeta_0%20%2B%20%5Cbeta_1x_i)%5C%7D%7D%7B1%2B%5Ctext%7Bexp%7D(%5Cbeta_0%20%2B%20%5Cbeta_1x_i)%7D\"\u003e\u003c/p\u003e\n\u003ch2\u003eNotes on mathematical symbols\u003c/h2\u003e\n\u003cp\u003eRecall that the \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cprod\"\u003e sign stands for a product of each of these individual probabilities. (Similar to how \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Csum\"\u003e stands for the sum of a series.) Since this is a monotonically increasing function, its maximum will be the same as the logarithm of the function, which is typically used in practice in order to decompose this product of probabilities into a sum of log probabilities for easier calculation of the derivative. In future sections, you'll investigate the derivative of this function and then use that in order to code up our own function for logistic regression.\u003c/p\u003e\n\u003ch2\u003eAlgorithm bias and ethical concerns\u003c/h2\u003e\n\u003cp\u003eIt should also be noted that while this is mathematically sound and a powerful tool, the model will simply reflect the data that is fed in. For example, logistic regression and other algorithms are used to inform a wide range of decisions including whether to provide someone with a loan, the degree of criminal sentencing, or whether to hire an individual for a job. In all of these scenarios, it is again important to remember that the algorithm is simply reflective of the underlying data itself. If an algorithm is trained on a dataset where African Americans have had disproportionate criminal prosecution, the algorithm will continue to perpetuate these racial injustices. Similarly, algorithms trained on data that reflect a gender pay-gap will also continue to promote this bias unless adequately accounted for through careful preprocessing and normalization. With this, substantial thought and analysis regarding problem set up and the resulting model is incredibly important. While future lessons and labs in this section return to underlying mathematical theory and how to implement logistic regression on your own, it is worthwhile to investigate some of the current problems regarding some of these algorithms, and how naive implementations can perpetuate unjust biases.\u003c/p\u003e\n\u003ch2\u003eAdditional resources\u003c/h2\u003e\n\u003cp\u003eBelow are a handful of resources providing further information regarding some of the topics discussed here. Be sure to check out some of the news articles describing how poor safeguards and problem formulation surrounding algorithms such as logistic regression can lead to unjust biases:\u003c/p\u003e\n\u003ch3\u003eAlgorithm bias and ethical concerns\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e\u003ca href=\"https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing\"\u003eMachine Bias\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003ca href=\"https://www.bloomberg.com/opinion/articles/2018-10-16/amazon-s-gender-biased-algorithm-is-not-alone\"\u003eAmazons Gender-Biased Algorithm Is Not Alone\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003ca href=\"https://www.bostonglobe.com/business/2017/12/21/the-software-that-runs-our-lives-can-bigoted-and-unfair-but-can-fix/RK4xG4gYxcVNVTIubeC1JI/story.html\"\u003eThe software that runs our lives can be bigoted and unfair. But we can fix it\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003ca href=\"https://www.bostonglobe.com/ideas/2017/07/07/why-artificial-intelligence-far-too-human/jvG77QR5xPbpwBL2ApAFAN/story.html\"\u003eWhy artificial intelligence is far too human\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003ca href=\"https://www.npr.org/2016/03/14/470427605/can-computers-be-racist-the-human-like-bias-of-algorithms\"\u003eCan Computers Be Racist? The Human-Like Bias Of Algorithms\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003eAdditional mathematical resources\u003c/h3\u003e\n\u003cp\u003eFor a more in-depth discussion of the mathematical ideas, check out Penn State's lecture \u003ca href=\"https://onlinecourses.science.psu.edu/stat504/node/150/\"\u003ehere\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eIf you want to really go down the math rabbit-hole, check out section 4.4 on Logistic Regression from the Elements of Statistical Learning which can be found here: \u003ca href=\"https://web.stanford.edu/%7Ehastie/ElemStatLearn//\"\u003ehttps://web.stanford.edu/~hastie/ElemStatLearn//\u003c/a\u003e.\u003c/p\u003e\n\u003ch2\u003eSummary\u003c/h2\u003e\n\u003cp\u003eIn this lesson, you further analyzed logistic regression from the perspective of maximum likelihood estimation. Additionally, there was a brief pause to consider the setup and interpretation of algorithms such as logistic regression. In particular, remember that issues regarding racial and gender bias that can be perpetuated by these algorithms. Always try to ensure your models are ethically sound. In the proceeding labs and lessons, you will continue to formalize your knowledge of logistic regression, implementing gradient descent and then a full logistic regression algorithm using Python packages in order to give you a deeper understanding of how logistic regression works.\u003c/p\u003e","frontPage":false},{"exportId":"object-oriented-programming-introduction","title":"Object-Oriented Programming - Introduction","type":"WikiPage","content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-introduction-to-object-orientation\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-introduction-to-object-orientation\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-introduction-to-object-orientation/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\u003ch2\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eIn this section, you'll be introduced to the concept of Object-oriented programming (OOP) in Python. OOP has become a foundational practice in much of software development and programming, allowing developers to build upon each other's code in a fluent manner.\u003c/p\u003e\n\u003ch2\u003eObject-oriented programming (OOP)\u003c/h2\u003e\n\u003cp\u003eWhen programmers started writing code, originally they took a procedural approach. They'd write a series of sequential steps, using conditional statements and even the dreaded \u003ca href=\"https://en.wikipedia.org/wiki/Goto\"\u003eGOTO\u003c/a\u003e statements to branch their logic and their program flow. There would be a set of global variables (variables with values that anyone could change anywhere within the application) for keeping track of the \"state\" of the application.\u003c/p\u003e\n\u003cp\u003eUnfortunately, as they got bigger such programs were really hard to manage. Firstly, if they ever wanted to repeat the same business logic, they either had to copy it or loop back to where it was in the program, and secondly, it became really hard to keep track of where those variables got changed, making it really hard to reason about the program and to avoid small changes to the program breaking huge pieces of the application.\u003c/p\u003e\n\u003cp\u003eOne solution was to break the common subroutines into separate \"functions\". This was a huge step forward. Now you could write a short script, and by calling it multiple times, passing various parameters you could reuse it safely. You could also write small automated tests (unit tests) to verify the behavior of those functions so you'd know how they would behave with various types of inputs. There is a whole branch of software development devoted just to functional programming and it can be a very effective way to write and reason about complex code bases. Languages such as Haskell and Clojure are primarily \"Functional Programming languages\". They are extremely powerful but have a reputation for being a little harder for developers to learn and use and because of that are less popular than languages that are primarily object-oriented.\u003c/p\u003e\n\u003cp\u003eLanguages like Ruby and Python are often considered to be primarily \"Object-oriented\" (OO) programming languages. In part, OO programming came from the question \"Where do we put our functions and our data?\". One way to organize functions is in libraries. This is still done even in OO programming languages, so you might well have a library like \u003ccode\u003estatsmodels\u003c/code\u003e and use \u003ccode\u003eimport statsmodels.formula.api as smf\u003c/code\u003e to get access to a series of functions related to formulae.\u003c/p\u003e\n\u003cp\u003eHowever, in OO programming languages, you also get the ability to create objects. Objects are a logical bundle of functions (they're called methods if they are associated to an object) and variables (often called properties when they're associated to an object), and for many classes of programming, once you get used to them, they provide a really useful abstraction.\u003c/p\u003e\n\u003cp\u003eSo, you might have a \u003ccode\u003ePerson\u003c/code\u003e class with its code saved in a file called \u003ccode\u003eperson.py\u003c/code\u003e which describes both the properties of a person (height, weight, date of birth) and their behaviors (everything from their \u003ccode\u003efull_name()\u003c/code\u003e to their \u003ccode\u003ecurrent_age()\u003c/code\u003e ).\u003c/p\u003e\n\u003cp\u003ePython comes with a few basic built-in objects to get us started, things like \u003ccode\u003eint\u003c/code\u003e for integer, \u003ccode\u003estr\u003c/code\u003e for string, \u003ccode\u003elist\u003c/code\u003e for list, etc. We call these base types of objects \"Primitives.\" Primitives already have methods we can call on them, for example: \u003ccode\u003e.title()\u003c/code\u003e for a string. But what if we wanted to create a new type of object in our programming universe, a new kind of object for our code? With Object-oriented programming, we can do just that by using the \u003ccode\u003eclass\u003c/code\u003e keyword!\u003c/p\u003e\n\u003ch2\u003eClasses and methods\u003c/h2\u003e\n\u003cp\u003eIn this section, you'll learn a lot about classes. A Python class can be thought of as the blueprint for creating a code object. These objects are known as an instance objects. Since nearly everything in Python is an object, understanding how to work with objects is critical to developing strong Python skills. You'll also learn about instance methods which are analogous to functions but are \"bound\" to instance objects. Don't worry if this sounds confusing right now, you'll have plenty of opportunities to explore classes and methods in this section.\u003c/p\u003e\n\u003ch2\u003eSummary\u003c/h2\u003e\n\u003cp\u003eObject-oriented programming (OOP) is a way of organizing your code that can make many types of applications easier to write by combining related variables/properties and functions/methods into objects containing both behavior and state.\u003c/p\u003e","frontPage":false},{"exportId":"regularization-recap","title":"Regularization - Recap","type":"WikiPage","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-regularization-recap-v2-1\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-regularization-recap-v2-1/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\u003ch2\u003eKey Takeaways\u003c/h2\u003e\n\n\u003cul\u003e\n\u003cli\u003eRegularization discourages overly complex models by penalizing the loss function \u003c/li\u003e\n\u003cli\u003eLasso and Ridge are two commonly used so-called regularization techniques \u003c/li\u003e\n\u003cli\u003eIn Ridge regression, the cost function is changed by adding a penalty term to the square of the magnitude of the coefficients \u003c/li\u003e\n\u003cli\u003eRidge regression is often also referred to as L2 Norm Regularization \u003c/li\u003e\n\u003cli\u003eLasso regression is very similar to Ridge regression, except that the magnitude of the coefficients are not squared in the penalty term \u003c/li\u003e\n\u003cli\u003eLasso regression is often also referred to as L1 Norm Regularization \u003c/li\u003e\n\u003cli\u003eAIC and BIC are two measures which give you a comprehensive measure of model performace taking into account the varying number of features\u003cbr\u003e\n\u003c/li\u003e\n\u003cli\u003eThe lower the AIC and/or BIC, the better the model \u003c/li\u003e\n\u003c/ul\u003e\n\u003cfooter class=\"fis-footer\" style=\"visibility: hidden;\"\u003e\u003cdiv class=\"fis-feedback\"\u003e\n\u003ch5\u003eHow do you feel about this lesson?\u003c/h5\u003e\n\u003cimg id=\"thumbs-up\" data-repository=\"dsc-regularization-recap-v2-1\" title=\"Thumbs up!\" alt=\"thumbs up\"\u003e\u003cimg id=\"thumbs-down\" data-repository=\"dsc-regularization-recap-v2-1\" title=\"Thumbs down!\" alt=\"thumbs down\"\u003e\n\u003c/div\u003e\n\u003ch5\u003eHave specific feedback? \u003ca href=\"https://github.com/learn-co-curriculum/dsc-regularization-recap-v2-1/issues/new\"\u003eTell us here!\u003c/a\u003e\n\u003c/h5\u003e\u003c/footer\u003e","frontPage":false},{"exportId":"logistic-regression-recap","title":"Logistic Regression - Recap","type":"WikiPage","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-logistic-regression-section-recap\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-logistic-regression-section-recap/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\u003ch2\u003eKey Takeaways\u003c/h2\u003e\n\n\u003cul\u003e\n\u003cli\u003eIn this section you learned about a different supervised learning technique: classification! Specifically, you practiced building a very basic classification model from scratch - a logistic regression model \u003c/li\u003e\n\u003cli\u003eLogistic regression uses a sigmoid function which helps to plot an \"s\"-like curve that enables a linear function to act as a binary classifier\u003c/li\u003e\n\u003cli\u003eYou can evaluate logistic regression models using some combination of precision, recall, and accuracy\u003c/li\u003e\n\u003cli\u003eA confusion matrix is another common way to visualize the performance of a classification model\u003c/li\u003e\n\u003cli\u003eReceiver Operating Characteristic (ROC) curve and the Area Under the Curve (AUC) can be used to help determine the best precision-recall tradeoff for a given classifier\u003c/li\u003e\n\u003cli\u003eClass weights, under/oversampling, and SMOTE can be used to deal with class imbalance problems\u003c/li\u003e\n\u003c/ul\u003e\n\u003cfooter class=\"fis-footer\" style=\"visibility: hidden;\"\u003e\u003cdiv class=\"fis-feedback\"\u003e\n\u003ch5\u003eHow do you feel about this lesson?\u003c/h5\u003e\n\u003cimg id=\"thumbs-up\" data-repository=\"dsc-logistic-regression-section-recap\" title=\"Thumbs up!\" alt=\"thumbs up\"\u003e\u003cimg id=\"thumbs-down\" data-repository=\"dsc-logistic-regression-section-recap\" title=\"Thumbs down!\" alt=\"thumbs down\"\u003e\n\u003c/div\u003e\n\u003ch5\u003eHave specific feedback? \u003ca href=\"https://github.com/learn-co-curriculum/dsc-logistic-regression-section-recap/issues/new\"\u003eTell us here!\u003c/a\u003e\n\u003c/h5\u003e\u003c/footer\u003e","frontPage":false},{"exportId":"decision-trees-recap","title":"Decision Trees - Recap","type":"WikiPage","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-decision-trees-section-recap\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-decision-trees-section-recap/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\u003ch2\u003eKey Takeaways\u003c/h2\u003e\n\n\u003cp\u003eThe key takeaways from this section include:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eDecision trees can be used for both categorization and regression tasks\u003c/li\u003e\n\u003cli\u003eThey are a powerful and interpretable technique for many machine learning problems (especially when combined with ensemble methods)\u003c/li\u003e\n\u003cli\u003eDecision trees are a form of Directed Acyclic Graphs (DAGs) - you traverse them in a specified direction, and there are no \"loops\" in the graphs to go backward\u003c/li\u003e\n\u003cli\u003eAlgorithms for generating decision trees are designed to maximize the information gain from each split\u003c/li\u003e\n\u003cli\u003eA popular algorithm for generating decision trees is ID3 - the Iterative Dichotomiser 3 algorithm\u003c/li\u003e\n\u003cli\u003eThere are several hyperparameters for decision trees to reduce overfitting - including maximum depth, minimum samples to split a node that is currently a leaf, minimum leaf sample size, maximum leaf nodes, and maximum features \u003c/li\u003e\n\u003c/ul\u003e\n\u003cfooter class=\"fis-footer\" style=\"visibility: hidden;\"\u003e\u003cdiv class=\"fis-feedback\"\u003e\n\u003ch5\u003eHow do you feel about this lesson?\u003c/h5\u003e\n\u003cimg id=\"thumbs-up\" data-repository=\"dsc-decision-trees-section-recap\" title=\"Thumbs up!\" alt=\"thumbs up\"\u003e\u003cimg id=\"thumbs-down\" data-repository=\"dsc-decision-trees-section-recap\" title=\"Thumbs down!\" alt=\"thumbs down\"\u003e\n\u003c/div\u003e\n\u003ch5\u003eHave specific feedback? \u003ca href=\"https://github.com/learn-co-curriculum/dsc-decision-trees-section-recap/issues/new\"\u003eTell us here!\u003c/a\u003e\n\u003c/h5\u003e\u003c/footer\u003e","frontPage":false},{"exportId":"mle-and-logistic-regression-introduction","title":"MLE and Logistic Regression - Introduction","type":"WikiPage","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-mle-logistic-reg-intro\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-mle-logistic-reg-intro/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eIn this section, you'll review maximum likelihood estimation and logistic regression. In fact, you'll see that logistic regression can be seen through a statistical point of view with MLE. This should provide you some additional time to wrangle with statistical concepts and improve your overall coding abilities.\u003c/p\u003e\n\n\u003ch2\u003eMaximum Likelihood Estimation\u003c/h2\u003e\n\n\u003cp\u003eMaximum likelihood estimation is a statistical procedure for determining underlying parameter distributions. As the name implies, the underlying motivation is to find parameters that maximize the theoretical chances of observing the actual observations.\u003c/p\u003e\n\n\u003ch2\u003eLogistic Regression\u003c/h2\u003e\n\n\u003cp\u003eLogistic regression, despite its name, is a classification algorithm. An interesting nuance is that it provides confidence values with its predictions since the raw output is a probability of a class between 0 and 1. The general process for this is similar to linear regression, where coefficients for various feature weights are altered in order to optimize the accuracy of subsequent predictions from the model. \u003c/p\u003e\n\n\u003ch2\u003eSummary\u003c/h2\u003e\n\n\u003cp\u003eIn this lesson, you got a brief overview of the concepts that will covered in this section. With that, dive in and continue to review your knowledge!\u003c/p\u003e\n\u003cfooter class=\"fis-footer\" style=\"visibility: hidden;\"\u003e\u003cdiv class=\"fis-feedback\"\u003e\n\u003ch5\u003eHow do you feel about this lesson?\u003c/h5\u003e\n\u003cimg id=\"thumbs-up\" data-repository=\"dsc-mle-logistic-reg-intro\" title=\"Thumbs up!\" alt=\"thumbs up\"\u003e\u003cimg id=\"thumbs-down\" data-repository=\"dsc-mle-logistic-reg-intro\" title=\"Thumbs down!\" alt=\"thumbs down\"\u003e\n\u003c/div\u003e\n\u003ch5\u003eHave specific feedback? \u003ca href=\"https://github.com/learn-co-curriculum/dsc-mle-logistic-reg-intro/issues/new\"\u003eTell us here!\u003c/a\u003e\n\u003c/h5\u003e\u003c/footer\u003e","frontPage":false},{"exportId":"introduction","title":"Introduction","type":"WikiPage","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-calculus-introduction\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-calculus-introduction/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eIn this section, you'll learn about the mechanism behind many machine learning optimization algorithms: gradient descent!\u003c/p\u003e\n\n\u003ch2\u003eCalculus and Solving a Linear Regression Using Gradient Descent\u003c/h2\u003e\n\n\u003cp\u003eIn this section, we're going to see how you can apply a \"gradient descent\" to solve a linear regression. Along the way, we'll also look at cost functions and will provide a foundation in calculus that will be valuable to you throughout your career as a data scientist.\u003c/p\u003e\n\n\u003ch3\u003eAn Introduction to Derivatives\u003c/h3\u003e\n\n\u003cp\u003eWe're going to start off by introducing derivatives - the \"instantaneous rate of change of a function\" or (more graphically) the \"slope of a curve\". We'll start off by looking at how to calculate the slope of a curve for a straight line, and then we'll explore how to calculate the rate of change for more complex (non-linear) functions.\u003c/p\u003e\n\n\u003ch3\u003eGradient Descent\u003c/h3\u003e\n\n\u003cp\u003eNow that we know how to calculate the slope of a curve - and, by extension, to find a local minima (low point) or maxima (high point) where the curve is flat (the slope of the curve is zero), we'll look at the idea of a gradient descent to step from some random point on a cost curve to find the local optima to solve for a given linear equation. We'll also look at how best to select the step sizes for descending the cost function, and how to use partial derivatives to optimize both slope and offset to more effectively solve a linear regression using gradient descent.\u003c/p\u003e\n\n\u003ch2\u003eSummary\u003c/h2\u003e\n\n\u003cp\u003eJust as we used solving a linear regression using OLS as an excuse to introduce you to linear algebra - one of the foundational elements of mathematics underpinning machine learning, we're now using the idea of gradient descent to introduce enough calculus to both understand and have good intuitions about many of the machine learning models that you're going to learn throughout the rest of the course.\u003c/p\u003e\n\u003cfooter class=\"fis-footer\" style=\"visibility: hidden;\"\u003e\u003cdiv class=\"fis-feedback\"\u003e\n\u003ch5\u003eHow do you feel about this lesson?\u003c/h5\u003e\n\u003cimg id=\"thumbs-up\" data-repository=\"dsc-calculus-introduction\" title=\"Thumbs up!\" alt=\"thumbs up\"\u003e\u003cimg id=\"thumbs-down\" data-repository=\"dsc-calculus-introduction\" title=\"Thumbs down!\" alt=\"thumbs down\"\u003e\n\u003c/div\u003e\n\u003ch5\u003eHave specific feedback? \u003ca href=\"https://github.com/learn-co-curriculum/dsc-calculus-introduction/issues/new\"\u003eTell us here!\u003c/a\u003e\n\u003c/h5\u003e\u003c/footer\u003e","frontPage":false},{"exportId":"xgboost","title":"XGBoost","type":"WikiPage","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-xgboost\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-xgboost/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eNow that you are familiar with gradient boosting, you'll learn about the top gradient boosting algorithm currently in use -- XGBoost!\u003c/p\u003e\n\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\n\u003cp\u003eYou will be able to:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eCompare XGBoost to other boosting algorithms \u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch2\u003eWhat is XGBoost?\u003c/h2\u003e\n\n\u003cp\u003eGradient boosting is one of the most powerful concepts in machine learning right now. As you've seen, the term \u003cem\u003egradient boosting\u003c/em\u003e refers to a class of algorithms, rather than any single one. The version with the highest performance right now is \u003cstrong\u003e\u003cem\u003eXGBoost\u003c/em\u003e\u003c/strong\u003e, which is short for \u003cstrong\u003e\u003cem\u003eeXtreme Gradient Boosting\u003c/em\u003e\u003c/strong\u003e. \u003c/p\u003e\n\n\u003cp\u003e\u003ccode\u003eXGBoost\u003c/code\u003e is a stand-alone library that implements popular gradient boosting algorithms in the fastest, most performant way possible. There are many under-the-hood optimizations that allow XGBoost to train more quickly than any other library implementations of gradient boosting algorithms. For instance, XGBoost is configured in such a way that it parallelizes the construction of trees across all your computer's CPU cores during the training phase. It also allows for more advanced use cases, such as distributing training across a cluster of computers, which is often a technique used to speed up computation. The algorithm even automatically handles missing values!\u003c/p\u003e\n\n\u003ch2\u003eInstalling \u003ccode\u003eXGBoost\u003c/code\u003e\n\u003c/h2\u003e\n\n\u003cp\u003e\u003ccode\u003eXGBoost\u003c/code\u003e is an independent library that provides implementations in C++, Python, and other languages. Luckily, the open-source community has had the good sense to make the Python API for \u003ccode\u003eXGBoost\u003c/code\u003e mirror that of \u003ccode\u003esklearn\u003c/code\u003e, so using \u003ccode\u003eXGBoost\u003c/code\u003e feels no different than using any other supervised learning algorithm from \u003ccode\u003esklearn\u003c/code\u003e. The only downside is that it does not come packaged with \u003ccode\u003esklearn\u003c/code\u003e, so we must install it ourselves. \u003cstrong\u003econda\u003c/strong\u003e makes this quite easy. \u003c/p\u003e\n\n\u003cp\u003eAll you need to do is run the command \u003ccode\u003econda install py-xgboost\u003c/code\u003e in your terminal, and \u003ccode\u003econda\u003c/code\u003e will take care of the rest!\u003c/p\u003e\n\n\u003ch2\u003eUse cases\u003c/h2\u003e\n\n\u003cp\u003eXGBoost has risen to prominence by being the go-to algorithm for winning competitions on \u003ca href=\"https://www.kaggle.com/\"\u003eKaggle\u003c/a\u003e, a competitive data science platform. It is so common to see XGBoost cited as an algorithm used by the winners of Kaggle competitions that it has become a bit of a running joke in the community. \u003ca href=\"https://github.com/dmlc/xgboost/tree/master/demo#machine-learning-challenge-winning-solutions\"\u003eThis page\u003c/a\u003e contains an (incomplete) list of all the recent competitions with place winners that used XGBoost for their solution!\u003c/p\u003e\n\n\u003cp\u003eXGBoost is a great choice for classification tasks. It provides best-in-class performance compared to other classification algorithms (with the exception of Deep Learning, which we'll learn more about soon).\u003c/p\u003e\n\n\u003ch2\u003eTakeaways\u003c/h2\u003e\n\n\u003cp\u003eWhen approaching a supervised learning problem, you should always use multiple algorithms, and compare the performances of the various models. There will always be use cases where some classes of models tend to outperform others. However, there are some models that generally outperform all the others -- XGBoost is at the top of this list! Make sure that this is an algorithm you're familiar with, as there are many situations where you'll find it quite useful!\u003c/p\u003e\n\n\u003cp\u003eYou can find the full documentation for XGBoost \u003ca href=\"https://xgboost.readthedocs.io/en/latest/\"\u003ehere\u003c/a\u003e. \u003c/p\u003e\n\n\u003ch2\u003eSummary\u003c/h2\u003e\n\n\u003cp\u003eIn this lesson, we learned about what XGBoost is, and why it is so powerful and useful to Data Scientists!\u003c/p\u003e\n\u003cfooter class=\"fis-footer\" style=\"visibility: hidden;\"\u003e\u003cdiv class=\"fis-feedback\"\u003e\n\u003ch5\u003eHow do you feel about this lesson?\u003c/h5\u003e\n\u003cimg id=\"thumbs-up\" data-repository=\"dsc-xgboost\" title=\"Thumbs up!\" alt=\"thumbs up\"\u003e\u003cimg id=\"thumbs-down\" data-repository=\"dsc-xgboost\" title=\"Thumbs down!\" alt=\"thumbs down\"\u003e\n\u003c/div\u003e\n\u003ch5\u003eHave specific feedback? \u003ca href=\"https://github.com/learn-co-curriculum/dsc-xgboost/issues/new\"\u003eTell us here!\u003c/a\u003e\n\u003c/h5\u003e\u003c/footer\u003e","frontPage":false},{"exportId":"introduction-to-decision-trees","title":"Introduction to Decision Trees","type":"WikiPage","content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-introduction-to-decision-trees\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-introduction-to-decision-trees\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-introduction-to-decision-trees/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eIn this lesson, we'll take a look at \u003cstrong\u003e\u003cem\u003edecision tree classifiers\u003c/em\u003e\u003c/strong\u003e. These are rule-based classifiers and belong to the first generation of modern AI. Despite the fact that this algorithm has been used in practice for decades, its simplicity and effectiveness for routine classification tasks is still on par with more sophisticated approaches. They are quite common in the business world because they have decent effectiveness without sacrificing explainability. Let's get started!\u003c/p\u003e\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\u003cp\u003eYou will be able to:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eDescribe a decision tree algorithm in terms of graph architecture\u003c/li\u003e\n\u003cli\u003eDescribe how decision trees are used to create partitions in a sample space\u003c/li\u003e\n\u003cli\u003eDescribe the training and prediction process of a decision tree\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003eFrom graphs to decision trees\u003c/h2\u003e\n\u003cp\u003eWe have seen basic classification algorithms (a.k.a classifiers), including Naive Bayes and logistic regression, in earlier sections. A decision tree is a different type of classifier that performs a \u003cstrong\u003erecursive partition of the sample space\u003c/strong\u003e. In this lesson, you will get a conceptual understanding of how this is achieved.\u003c/p\u003e\n\u003cp\u003eA decision tree comprises of decisions that originate from a chosen point in sample space. If you are familiar with Graph theory, a tree is a \u003cstrong\u003edirected acyclic graph with a root called \"root node\" that has no incoming edges\u003c/strong\u003e. All other nodes have one (and only one) incoming edge. Nodes having outgoing edges are known as \u003cstrong\u003einternal\u003c/strong\u003e nodes. All other nodes are called \u003cstrong\u003eleaves\u003c/strong\u003e. Nodes with an incoming edge, but no outgoing edges, are called \u003cstrong\u003eterminal nodes\u003c/strong\u003e.\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003eDirected Acyclic Graphs\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eIn computer science and mathematics, a directed graph is a collection of nodes and edges such that edges can be traversed only in a specified direction (eg, from node A to node B, but not from node B to node A). An acyclic graph is a graph such that it is impossible for a node to be visited twice along any path from one node to another. So, a directed acyclic graph (or, a DAG) is a directed graph with no cycles. A DAG has a \u003cstrong\u003etopological ordering\u003c/strong\u003e, or, a sequence of the nodes such that every edge is directed from earlier to later in the sequence.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch2\u003ePartitioning the sample space\u003c/h2\u003e\n\u003cp\u003eSo, a decision tree is effectively a DAG, such as the one seen below where \u003cstrong\u003eeach internal node partitions the sample space into two (or more) sub-spaces\u003c/strong\u003e. These nodes are partitioned according to some discrete function that takes the attributes of the sample space as input.\u003c/p\u003e\n\u003cp\u003eIn the simplest and most frequent case, each internal node considers a single attribute so that space is partitioned according to the attributes value. In the case of numeric attributes, the condition refers to a range.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-introduction-to-decision-trees/master/images/dt1.png\" width=\"600\"\u003e\u003c/p\u003e\n\u003cp\u003eThis is the basic idea behind decision trees: every internal node checks for a condition and performs a decision, and every terminal node (AKA leaf node) represents a discrete class. Decision tree induction is closely related to \u003cstrong\u003erule induction\u003c/strong\u003e. In essence, a decision tree is a just series of IF-ELSE statements (rules). Each path from the root of a decision tree to one of its leaves can be transformed into a rule simply by combining the decisions along the path to form the antecedent, and taking the leafs class prediction as the consequence (IF-ELSE statements follow the form: IF \u003cem\u003eantecedent\u003c/em\u003e THEN \u003cem\u003econsequence\u003c/em\u003e ).\u003c/p\u003e\n\u003ch2\u003eDefinition\u003c/h2\u003e\n\u003cblockquote\u003e\n\u003cp\u003eA decision tree is a DAG type of classifier where each internal node represents a choice between a number of alternatives and each leaf node represents a classification. An unknown (or test) instance is routed down the tree according to the values of the attributes in the successive nodes. When the instance reaches a leaf, it is classified according to the label assigned to the corresponded leaf.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-introduction-to-decision-trees/master/images/dt2.png\" width=\"850\"\u003e\u003c/p\u003e\n\u003cp\u003eA real dataset would usually have a lot more features than the example above and will create much bigger trees, but the idea will remain exactly the same. The idea of feature importance is crucial to decision trees, since selecting the correct feature to make a split on will affect the complexity and efficacy of the classification process. Regression trees are represented in the same manner, but instead they predict continuous values like the price of a house.\u003c/p\u003e\n\u003ch2\u003eTraining process\u003c/h2\u003e\n\u003cp\u003eThe process of training a decision tree and predicting the target features of a dataset is as follows:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003ePresent a dataset of training examples containing features/predictors and a target (similar to classifiers we have seen earlier).\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eTrain the tree model by making splits for the target using the values of predictors. Which features to use as predictors gets selected following the idea of feature selection and uses measures like \"\u003cstrong\u003einformation gain\u003c/strong\u003e\" and \"\u003cstrong\u003eGini Index\u003c/strong\u003e\". We shall cover these shortly.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eThe tree is grown until some \u003cstrong\u003estopping criteria\u003c/strong\u003e is achieved. This could be a set depth of the tree or any other similar measure.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eShow a new set of features to the tree, with an unknown class and let the example propagate through a trained tree. The resulting leaf node represents the class prediction for this example datum.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-introduction-to-decision-trees/master/images/dt3.png\" width=\"650\"\u003e\u003c/p\u003e\n\u003ch2\u003eSplitting criteria\u003c/h2\u003e\n\u003cp\u003eThe training process of a decision tree can be generalized as \"\u003cstrong\u003erecursive binary splitting\u003c/strong\u003e\".\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eIn this procedure, all the features are considered and different split points are tried and tested using some \u003cstrong\u003ecost function\u003c/strong\u003e. The split with the lowest cost is selected.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eThere are a couple of algorithms used to build a decision tree:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cstrong\u003eCART (Classification and Regression Trees)\u003c/strong\u003e uses the Gini Index as a metric\u003c/li\u003e\n\u003cli\u003e\n\u003cstrong\u003eID3 (Iterative Dichotomiser 3)\u003c/strong\u003e uses the entropy function and information gain as metrics\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003eGreedy search\u003c/h2\u003e\n\u003cp\u003eWe need to determine the attribute that \u003cstrong\u003ebest\u003c/strong\u003e classifies the training data, and use this attribute at the root of the tree. At each node, we repeat this process creating further splits, until a leaf node is achieved, i.e., all data gets classified.\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eThis means we are performing a top-down, greedy search through the space of possible decision trees.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eIn order to identify the best attribute for ID3 classification trees, we use the \"information gain\" criteria. Information gain (IG) measures how much \"information\" a feature gives us about the class. Decision trees always try to maximize information gain. So, the attribute with the highest information gain will be split on first.\u003c/p\u003e\n\u003cp\u003eLet's move on to the next lesson where we shall look into these criteria with simple examples.\u003c/p\u003e\n\u003ch2\u003eAdditional resources\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ca href=\"http://www.r2d3.us/visual-intro-to-machine-learning-part-1/\"\u003eR2D3\u003c/a\u003e: This is highly recommended for getting a visual introduction to decision trees. Excellent animations explaining the training and prediction stages shown above.\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003ca href=\"http://www.dataversity.net/introduction-machine-learning-decision-trees/\"\u003eDataversity: Decision Trees Intro\u003c/a\u003e: A quick and visual introduction to DTs.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003ca href=\"https://cran.r-project.org/web/packages/ggdag/vignettes/intro-to-dags.html\"\u003eDirected Acyclic Graphs\u003c/a\u003e: This would help relate early understanding of graph computation to decision tree architectures.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003eSummary\u003c/h2\u003e\n\u003cp\u003eIn this lesson, we saw an introduction to decision trees as simple yet effective classifiers. We looked at how decision trees partition the sample space based on learning rules from a given dataset. We also looked at how feature selection for splitting the tree is of such high importance. Next, we shall look at information gain criteria used for feature selection.\u003c/p\u003e","frontPage":false}],"assignments":[{"exportId":"ga83ba2495b2eb7cadc1a80606b3fb8e3","title":"Applying Gradient Descent - Lab","type":"Assignment","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-applying-gradient-descent-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-applying-gradient-descent-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"g1b40d2f2e5c34c44aea75f2dfbf13898","title":"Building an Object-Oriented Simulation - Lab","type":"Assignment","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-building-an-object-oriented-simulation-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-building-an-object-oriented-simulation-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"g20e461cd69734561fae01407a52fb4ba","title":"Building an SVM from Scratch - Lab","type":"Assignment","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-building-an-svm-from-scratch-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-building-an-svm-from-scratch-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"g1c2d3b6e28d3da996a0e47a884010444","title":"Building an SVM using scikit-learn - Lab","type":"Assignment","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-building-an-svm-using-scikit-learn-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-building-an-svm-using-scikit-learn-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"g6d29959318dd25b71c0d335a961f5a90","title":"Building Trees using scikit-learn","type":"Assignment","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-decision-trees-with-sklearn-codealong\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-decision-trees-with-sklearn-codealong/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"g3c2aa2f625d8be27c9fbab09bb2c9221","title":"Building Trees using scikit-learn - Lab","type":"Assignment","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-decision-trees-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-decision-trees-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"ge6e90d2504f02461b5762a851c0580c8","title":"Classes and Instances","type":"Assignment","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-classes-and-instances\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-classes-and-instances/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"gf587d4088978e189faff23320a25aad4","title":"Classes and Instances - Lab","type":"Assignment","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-classes-and-instances-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-classes-and-instances-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"g66166d60e0c5f0d6e14ab83e4483c695","title":"Class Imbalance Problems","type":"Assignment","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-class-imbalance-problems\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-class-imbalance-problems/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"ga286fdb3f1a20bbcc89b41b52975bdde","title":"Class Imbalance Problems - Lab","type":"Assignment","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-class-imbalance-problems-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-class-imbalance-problems-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"g94ccb73ca3226687e0b2b7aecf2ba753","title":"Coding Logistic Regression from Scratch - Lab","type":"Assignment","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-coding-logistic-regression-from-scratch\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-coding-logistic-regression-from-scratch/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"g377bdea0b6c3635c7c42197af9f2c581","title":"Confusion Matrices","type":"Assignment","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-confusion-matrices\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-confusion-matrices/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"gcd9ac0d2c29d37405f643e362e213e27","title":" Deeper Dive into \"self\"","type":"Assignment","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-understanding-self\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-understanding-self/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"gef19edea98453a15d1e86a5cd962db4b","title":"Derivatives: Conclusion","type":"Assignment","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-derivatives-conclusion\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-derivatives-conclusion/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"geb52af323d1b29002c44e7a37a5aa534","title":"Derivatives of Non-Linear Functions","type":"Assignment","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-derivatives-of-non-linear-functions\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-derivatives-of-non-linear-functions/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"g3dd0fd184b054c5e13d45821d403d737","title":"Derivatives: the Chain Rule","type":"Assignment","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-derivatives-chain-rule\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-derivatives-chain-rule/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"gff21df4a7b65d3895a0c59921e08c5a6","title":"Distance Metrics","type":"Assignment","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-distance-metrics\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-distance-metrics/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"g43db40e3744b4e42bbeb39ea8342dada","title":"Distance Metrics - Lab","type":"Assignment","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-distance-metrics-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-distance-metrics-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"g0aa558e0a5cd6c4eb73c11beeb1d394f","title":"Document Classification with Naive Bayes","type":"Assignment","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-document-classification-with-naive-bayes\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-document-classification-with-naive-bayes/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"g490b04613e6dbbafe98301e1aee554ee","title":"Document Classification with Naive Bayes - Lab","type":"Assignment","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-document-classification-with-naive-bayes-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-document-classification-with-naive-bayes-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"g015f7ecd2ca8f6372c1e2b59c0d13d21","title":"Evaluating Logistic Regression Models - Lab","type":"Assignment","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-evaluating-logistic-regression-models-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-evaluating-logistic-regression-models-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"gde5a6dd6ef6b31fc0a79a676100531bf","title":"Extensions to Linear Models - Lab","type":"Assignment","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-extensions-to-linear-models-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-extensions-to-linear-models-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"g5d3f046c022e8266135a7dcdb886e34e","title":"Feature Selection Methods","type":"Assignment","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-feature-selection-methods\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-feature-selection-methods/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"gf94b2c5c0d20aa16341b6f6956eadbab","title":"Fitting a Logistic Regression Model - Lab","type":"Assignment","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-fitting-a-logistic-regression-model-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-fitting-a-logistic-regression-model-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"ge43739e7a344d7fc8d0e9c3e93358249","title":"Gaussian Naive Bayes","type":"Assignment","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-gaussian-naive-bayes\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-gaussian-naive-bayes/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"gf3a619c5dd8f7ed6ca56d3a886b961f1","title":"Gaussian Naive Bayes - Lab","type":"Assignment","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-gaussian-naive-bayes-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-gaussian-naive-bayes-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"g33d17fe2b6113510b4cff5de22005e96","title":"Generating Data","type":"Assignment","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-generating-data\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-generating-data/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"ga71184eb3d0d5c34ffb33847b710fa21","title":"Generating Data - Lab","type":"Assignment","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-generating-data-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-generating-data-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"g402c8fa1483e3cb383bc414c7ed728b1","title":"Gradient Boosting - Lab","type":"Assignment","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-gradient-boosting-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-gradient-boosting-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"g76b9f7bdb0655f18fcc3459eda791388","title":"Gradient Descent in 3D","type":"Assignment","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-gradient-descent-in-3d\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-gradient-descent-in-3d/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"gb06cecd53d5d21b04187b921a72fd028","title":"Gradient Descent - Lab","type":"Assignment","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-gradient-descent-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-gradient-descent-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"gbdf1eac42db439d31c1d341787937fd4","title":"Gradient Descent: Step Sizes","type":"Assignment","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-gradient-descent-step-sizes\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-gradient-descent-step-sizes/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"g681a923c77330be3e649a1c400dbc5be","title":"Gradient Descent: Step Sizes - Lab","type":"Assignment","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-gradient-descent-step-sizes-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-gradient-descent-step-sizes-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"gfa3bd7bfa3ef7de7e8cd69de1c88abae","title":"Gradient to Cost Function","type":"Assignment","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-gradient-to-cost-function-v2-1\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-gradient-to-cost-function-v2-1/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"g99518519ccfe706ed4e9a8718be7ff1a","title":"GridSearchCV - Lab","type":"Assignment","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-gridsearchcv-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-gridsearchcv-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"gb53019a69149a0bca741d35cf2fc001a","title":"Hyperparameter Tuning and Pruning in Decision Trees - Lab","type":"Assignment","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-tuning-decision-trees-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-tuning-decision-trees-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"g92a79bdb58c268e3896971638328cfda","title":"ID3 Classification Trees: Perfect Split with Information Gain - Lab","type":"Assignment","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-ID3-trees-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-ID3-trees-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"g8177ee3cf3c6fb4202d8eb28958b0c6d","title":"Inheritance","type":"Assignment","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-inheritance\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-inheritance/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"g733da2d7d63ad4f2f766abbccc60d5a1","title":"Inheritance - Lab","type":"Assignment","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-inheritance-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-inheritance-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"g416e8b49941bbf47fd5ac89857046570","title":"Instance Methods","type":"Assignment","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-instance-methods\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-instance-methods/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"g411e53067859cfc82a917fe8970317e5","title":"Instance Methods - Lab","type":"Assignment","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-instance-methods-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-instance-methods-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"g1b96a50ee73471e91e73a34dd33376e9","title":"Instance Variables - Lab","type":"Assignment","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-instance-variables-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-instance-variables-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"geeea38a45e9f7a0543cf69879ca83ca9","title":"Introduction to Derivatives","type":"Assignment","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-derivatives-intro\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-derivatives-intro/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"g16a3298cfad293e535ed3e9d09e66a57","title":"Introduction To Derivatives - Lab","type":"Assignment","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-derivatives-intro-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-derivatives-intro-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"gd4c4efe9ae315b769375d0e0f70ae2d0","title":"Introduction to Gradient Descent","type":"Assignment","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-gradient-descent-intro\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-gradient-descent-intro/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"g9899dfdc4a8c9f30195910663bdc2a62","title":"Kernels in scikit-learn - Lab","type":"Assignment","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-kernels-in-scikit-learn-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-kernels-in-scikit-learn-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"g30eef00ddca4c05b9dc1e562aa33a729","title":"K-Nearest Neighbors - Lab","type":"Assignment","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-k-nearest-neighbors-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-k-nearest-neighbors-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"ge848c6abcaec82f3f37cd659ee8878de","title":"KNN with scikit-learn - Lab","type":"Assignment","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-knn-with-scikit-learn-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-knn-with-scikit-learn-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"g63126a9cacec1131b896047af1e9d4e8","title":"Linear to Logistic Regression","type":"Assignment","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-linear-to-logistic-regression\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-linear-to-logistic-regression/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"g24774b938d5ffb2fe4a4d631ff1dbbb6","title":"Logistic Regression in scikit-learn","type":"Assignment","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-logistic-regression-in-scikit-learn\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-logistic-regression-in-scikit-learn/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"gfdff295f713f47ee6682e21a117d40ff","title":"Logistic Regression in scikit-learn - Lab","type":"Assignment","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-logistic-regression-in-scikit-learn-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-logistic-regression-in-scikit-learn-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"gb11df9755baa2dbf94ca86b32d3c5242","title":"Logistic Regression Model Comparisons - Lab","type":"Assignment","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-logistic-regression-model-comparisons-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-logistic-regression-model-comparisons-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"g45d1530438268d58175ba1cafdfc0240","title":"Matrix Multiplication - Code Along","type":"Assignment","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-linalg-mat-multiplication-codealong\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-linalg-mat-multiplication-codealong/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"g8388d6bfa6485f82364470c866bc74f9","title":"Object Attributes - Lab","type":"Assignment","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-object-attributes-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-object-attributes-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"gd791ce89a33d32e127ad6502bbda4ffc","title":"Object Initialization","type":"Assignment","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-object-initialization\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-object-initialization/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"g73eefc80499c5a2aed9e98957e2895dd","title":"Object Initialization - Lab","type":"Assignment","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-object-initialization-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-object-initialization-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"gc2998173ced52b848ddc6f055e66bed8","title":"Object Oriented Attributes with Functions","type":"Assignment","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-object-oriented-attributes-with-functions\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-object-oriented-attributes-with-functions/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"g7f35e07eb391ea34a37089b8d1254a6f","title":"Object Oriented Attributes With Functions - Lab","type":"Assignment","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-object-oriented-attributes-with-functions-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-object-oriented-attributes-with-functions-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"ge089addce181f8509e2ac7905e8c4b22","title":"Object Oriented Shopping Cart - Lab","type":"Assignment","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-object-oriented-shopping-cart-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-object-oriented-shopping-cart-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"g17c854c55744c3b81c57c0c6229504cf","title":"Phase 3 Blog Post","type":"Assignment","content":"\u003cp\u003e\u003cspan\u003ePlease put the URL to your Phase 3 Blog Post here. \u003c/span\u003e\u003cspan\u003eRefer to the \u003c/span\u003e\u003ca title=\"Blogging Overview\" href=\"pages/blogging-overview\"\u003eBlogging Overview\u003c/a\u003e\u003cspan\u003e to learn about how to write good blog posts that\u003c/span\u003e\u003cspan style=\"font-family: inherit; font-size: 1rem;\"\u003e meet Flatiron Schools requirements.\u003c/span\u003e\u003c/p\u003e","submissionTypes":"a website url","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"gc6ad6eb095b74767a3900ea52caf102b","title":"Phase 3 Project","type":"Assignment","content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-phase-3-project\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-phase-3-project\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-phase-3-project/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\u003cp\u003eCongratulations! You've made it through another \u003cem\u003eintense\u003c/em\u003e module, and now you're ready to show off your newfound Machine Learning skills!\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-phase-3-project/main/images/smart.gif\" alt=\"awesome\"\u003e\u003c/p\u003e\n\u003cp\u003eAll that remains in Phase 3 is to put your new skills to use with another large project! This project should take 20 to 30 hours to complete.\u003c/p\u003e\n\u003ch2\u003eProject Overview\u003c/h2\u003e\n\u003cp\u003eFor this project, you will engage in the full data science process from start to finish, solving a classification problem using a dataset of your choice.\u003c/p\u003e\n\u003ch3\u003eThe Data\u003c/h3\u003e\n\u003cp\u003eYou have the option to either \u003cstrong\u003echoose a dataset from a curated list\u003c/strong\u003e or \u003cstrong\u003echoose your own dataset \u003cem\u003enot on the list\u003c/em\u003e\u003c/strong\u003e. The goal is to choose a dataset appropriate to the type of business problem and/or classification methods that most interests you. It is up to you to define a stakeholder and business problem appropriate to the dataset you choose. If you are feeling overwhelmed or behind, we recommend you choose dataset #2 or #3 from the curated list.\u003c/p\u003e\n\u003cp\u003eIf you choose a dataset from the curated list, \u003cstrong\u003einform your instructor which dataset you chose\u003c/strong\u003e and jump right into the project. If you choose your own dataset, \u003cstrong\u003erun the dataset and business problem by your instructor for approval\u003c/strong\u003e before starting your project.\u003c/p\u003e\n\u003ch3\u003eCurated List of Datasets\u003c/h3\u003e\n\u003cp\u003eYou may select any of the four datasets below - we provide brief descriptions of each. Follow the links to learn more about the dataset and business problems before making a final decision.\u003c/p\u003e\n\u003ch4\u003e1) \u003ca href=\"https://data.cityofchicago.org/Transportation/Traffic-Crashes-Crashes/85ca-t3if\"\u003eChicago Car Crashes\u003c/a\u003e\u003c/h4\u003e\n\u003cp\u003eNote this links also to \u003ca href=\"https://data.cityofchicago.org/Transportation/Traffic-Crashes-Vehicles/68nd-jvt3\"\u003eVehicle Data\u003c/a\u003e and to \u003ca href=\"https://data.cityofchicago.org/Transportation/Traffic-Crashes-People/u6pd-qa9d\"\u003eDriver/Passenger Data\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eBuild a classifier to predict the primary contributory cause of a car accident, given information about the car, the people in the car, the road conditions etc. You might imagine your audience as a Vehicle Safety Board who's interested in reducing traffic accidents, or as the City of Chicago who's interested in becoming aware of any interesting patterns. Note that there is a \u003cstrong\u003emulti-class\u003c/strong\u003e classification problem. You will almost certainly want to bin or trim or otherwise limit the number of target categories on which you ultimately predict. Note e.g. that some primary contributory causes have very few samples.\u003c/p\u003e\n\u003ch4\u003e2) \u003ca href=\"https://catalog.data.gov/dataset/terry-stops\"\u003eTerry Traffic Stops\u003c/a\u003e\u003c/h4\u003e\n\u003cp\u003eIn \u003ca href=\"https://www.oyez.org/cases/1967/67\"\u003e\u003cem\u003eTerry v. Ohio\u003c/em\u003e\u003c/a\u003e, a landmark Supreme Court case in 1967-8, the court found that a police officer was not in violation of the \"unreasonable search and seizure\" clause of the Fourth Amendment, even though he stopped and frisked a couple of suspects only because their behavior was suspicious. Thus was born the notion of \"reasonable suspicion\", according to which an agent of the police may e.g. temporarily detain a person, even in the absence of clearer evidence that would be required for full-blown arrests etc. Terry Stops are stops made of suspicious drivers.\u003c/p\u003e\n\u003cp\u003eBuild a classifier to predict whether an arrest was made after a Terry Stop, given information about the presence of weapons, the time of day of the call, etc. Note that this is a \u003cstrong\u003ebinary\u003c/strong\u003e classification problem.\u003c/p\u003e\n\u003cp\u003eNote that this dataset also includes information about gender and race. You \u003cstrong\u003emay\u003c/strong\u003e use this data as well. You may, e.g. pitch your project as an inquiry into whether race (of officer or of subject) plays a role in whether or not an arrest is made.\u003c/p\u003e\n\u003cp\u003eIf you \u003cstrong\u003edo\u003c/strong\u003e elect to make use of race or gender data, be aware that this can make your project a highly sensitive one; your discretion will be important, as well as your transparency about how you use the data and the ethical issues surrounding it.\u003c/p\u003e\n\u003ch4\u003e3) \u003ca href=\"https://www.kaggle.com/becksddf/churn-in-telecoms-dataset\"\u003eSyriaTel Customer Churn\u003c/a\u003e\u003c/h4\u003e\n\u003cp\u003eBuild a classifier to predict whether a customer will (\"soon\") stop doing business with SyriaTel, a telecommunications company. Note that this is a \u003cstrong\u003ebinary\u003c/strong\u003e classification problem.\u003c/p\u003e\n\u003cp\u003eMost naturally, your audience here would be the telecom business itself, interested in losing money on customers who don't stick around very long. Are there any predictable patterns here?\u003c/p\u003e\n\u003ch4\u003e4) \u003ca href=\"https://www.drivendata.org/competitions/7/pump-it-up-data-mining-the-water-table/page/23/\"\u003eTanzanian Water Well Data\u003c/a\u003e\u003c/h4\u003e\n\u003cp\u003eThis dataset is part of an \u003cem\u003eactive competition\u003c/em\u003e until April 31, 2021!\u003c/p\u003e\n\u003cp\u003eTanzania, as a developing country, struggles with providing clean water to its population of over 57,000,000. There are many waterpoints already established in the country, but some are in need of repair while others have failed altogether.\u003c/p\u003e\n\u003cp\u003eBuild a classifier to predict the condition of a water well, using information about the sort of pump, when it was installed, etc. Note that this is a \u003cstrong\u003eternary\u003c/strong\u003e classification problem.\u003c/p\u003e\n\u003ch3\u003eSourcing Your Own Data\u003c/h3\u003e\n\u003cp\u003eSourcing new data is a valuable skill for data scientists, but it requires a great deal of care. An inappropriate dataset or an unclear business problem can lead you spend a lot of time on a project that delivers underwhelming results. The guidelines below will help you complete a project that demonstrates your ability to engage in the full data science process.\u003c/p\u003e\n\u003cp\u003eYour dataset must be...\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eAppropriate for classification.\u003c/strong\u003e It should have a categorical outcome or the data needed to engineer one.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eUsable to solve a specific business problem.\u003c/strong\u003e This solution must rely on your classification model.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eSomewhat complex.\u003c/strong\u003e It should contain a minimum of 1000 rows and 10 features.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eUnfamiliar.\u003c/strong\u003e It can't be one we've already worked with during the course or that is commonly used for demonstration purposes (e.g. MNIST).\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eManageable.\u003c/strong\u003e Stick to datasets that you can model using the techniques introduced in Phase 3.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eOnce you've sourced your own dataset and identified the business problem you want to solve with it, you must to \u003cstrong\u003erun them by your instructor for approval\u003c/strong\u003e.\u003c/p\u003e\n\u003ch4\u003eProblem First, or Data First?\u003c/h4\u003e\n\u003cp\u003eThere are two ways that you can source your own dataset: \u003cstrong\u003e\u003cem\u003eProblem First\u003c/em\u003e\u003c/strong\u003e or \u003cstrong\u003e\u003cem\u003eData First\u003c/em\u003e\u003c/strong\u003e. The less time you have to complete the project, the more strongly we recommend a Data First approach to this project.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e\u003cem\u003eProblem First\u003c/em\u003e\u003c/strong\u003e: Start with a problem that you are interested in that you could potentially solve with a classification model. Then look for data that you could use to solve that problem. This approach is high-risk, high-reward: Very rewarding if you are able to solve a problem you are invested in, but frustrating if you end up sinking lots of time in without finding appropriate data. To mitigate the risk, set a firm limit for the amount of time you will allow yourself to look for data before moving on to the Data First approach.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e\u003cem\u003eData First\u003c/em\u003e\u003c/strong\u003e: Take a look at some of the most popular internet repositories of cool data sets we've listed below. If you find a data set that's particularly interesting for you, then it's totally okay to build your problem around that data set.\u003c/p\u003e\n\u003cp\u003eThere are plenty of amazing places that you can get your data from. We recommend you start looking at data sets in some of these resources first:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://archive.ics.uci.edu/ml/datasets.html\"\u003eUCI Machine Learning Datasets Repository\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://www.kaggle.com/datasets\"\u003eKaggle Datasets\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/awesomedata/awesome-public-datasets\"\u003eAwesome Datasets Repo on Github\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://opendata.cityofnewyork.us/\"\u003eNew York City Open Data Portal\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"http://insideairbnb.com/\"\u003eInside AirBNB\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003eThe Deliverables\u003c/h2\u003e\n\u003cp\u003eThere are three deliverables for this project:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eA \u003cstrong\u003eGitHub repository\u003c/strong\u003e\u003c/li\u003e\n\u003cli\u003eA \u003cstrong\u003eJupyter Notebook\u003c/strong\u003e\u003c/li\u003e\n\u003cli\u003eA \u003cstrong\u003enon-technical presentation\u003c/strong\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eReview the \"Project Submission \u0026amp; Review\" page in the \"Milestones Instructions\" topic for instructions on creating and submitting your deliverables. Refer to the rubric associated with this assignment for specifications describing high-quality deliverables.\u003c/p\u003e\n\u003ch3\u003eKey Points\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eYour deliverables should explicitly address each step of the data science process.\u003c/strong\u003e Refer to \u003ca href=\"https://github.com/learn-co-curriculum/dsc-data-science-processes\"\u003ethe Data Science Process lesson\u003c/a\u003e from Topic 19 for more information about process models you can use.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eYour Jupyter Notebook should demonstrate an iterative approach to modeling.\u003c/strong\u003e This means that you begin with a basic model, evaluate it, and then provide justification for and proceed to a new model. We encourage you to try a bunch of different models: logistic regression, decision trees, or anything else you think would be appropriate.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eYou must choose appropriate classification metrics and use them to evaluate your models.\u003c/strong\u003e Choosing the right classification metrics is a key data science skill, and should be informed by data exploration and the business problem itself. You must then use this metric to evaluate your model performance using both training and testing data.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003eGetting Started\u003c/h2\u003e\n\u003cp\u003eCreate a new repository for your project to get started. We recommend structuring your project repository similar to the structure in \u003ca href=\"https://github.com/learn-co-curriculum/dsc-project-template\"\u003ethe Phase 1 Project Template\u003c/a\u003e. You can do this either by creating a new fork of that repository to work in or by building a new repository from scratch that mimics that structure.\u003c/p\u003e\n\u003ch2\u003eProject Submission and Review\u003c/h2\u003e\n\u003cp\u003eReview the \"Project Submission \u0026amp; Review\" page in the \"Milestones Instructions\" topic to learn how to submit your project and how it will be reviewed. Your project must pass review for you to progress to the next Phase.\u003c/p\u003e\n\u003ch2\u003eSummary\u003c/h2\u003e\n\u003cp\u003eThis project is an opportunity to expand your data science toolkit by evaluating, choosing, and working with new datasets. Spending time up front making sure you have a good dataset for a solvable problem will help avoid the major problems that can sometimes derail data science projects. You've got this!\u003c/p\u003e","submissionTypes":"a file upload","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"gc8125a8313e71955700ce06b0b50f54c","title":"Phase 3 Project - GitHub Repository URL","type":"Assignment","content":"\u003cp\u003e\u003cspan\u003ePlease put the URL to your Phase 3 Project GitHub Repository here.\u0026nbsp;\u003c/span\u003e\u003c/p\u003e","submissionTypes":"a website url","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"g5cebdd8af99a364284986be844159457","title":"Pipelines in scikit-learn - Lab","type":"Assignment","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-pipelines-lab-v2-1\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-pipelines-lab-v2-1/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"gd719bc423b04acae1066e5cad7066e58","title":"Properties of Dot Product - Lab","type":"Assignment","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-linalg-dot-product-properties-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-linalg-dot-product-properties-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"g3dc50dc7142e35c5b3d7776c252a2bda","title":"Pure Python vs. Numpy - Lab","type":"Assignment","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-linalg-python-vs-numpy-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-linalg-python-vs-numpy-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"g5fc09481bd2ed62bc9f8f94ffaa91b30","title":"Regression Analysis using Linear Algebra and NumPy - Code Along","type":"Assignment","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-linalg-regression-codealong\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-linalg-regression-codealong/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"g67e1252db6cff2589c8f39e5a6841e8c","title":"Regression Trees and Model Optimization - Lab","type":"Assignment","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-tuning-regression-trees-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-tuning-regression-trees-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"g788207398f92235119a984d3e4791ebd","title":"Regression with CART Trees","type":"Assignment","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-regression-cart-trees-codealong\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-regression-cart-trees-codealong/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"g7df89b6489cd90836a98f6c229d019a8","title":"Regression with CART Trees - Lab","type":"Assignment","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-regression-cart-trees-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-regression-cart-trees-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"gcfcaaf171073f826019e8468eaeee7a8","title":"Regression with Linear Algebra - Lab","type":"Assignment","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-linalg-regression-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-linalg-regression-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"g02cf089874c3fd09715748e0c0550454","title":"Ridge and Lasso Regression","type":"Assignment","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-ridge-and-lasso-regression\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-ridge-and-lasso-regression/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"g2972e2db8767234f30f672e218a5337f","title":"Ridge and Lasso Regression - Lab","type":"Assignment","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-ridge-and-lasso-regression-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-ridge-and-lasso-regression-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"g4ce8d159a0531ff71494a2d07f275387","title":"ROC Curves and AUC","type":"Assignment","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-roc-curves-and-auc\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-roc-curves-and-auc/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"gafbf3bc1cbeaddedc666b5a4ee36b509","title":"ROC Curves and AUC - Lab","type":"Assignment","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-roc-curves-and-auc-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-roc-curves-and-auc-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"g37914033df1e7de2e707f964f79dcdbe","title":"Rules for Derivatives","type":"Assignment","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-rules-for-derivatives\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-rules-for-derivatives/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"g838615764c7af308be8585611ad63a78","title":"Rules for Derivatives - Lab","type":"Assignment","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-rules-for-derivatives-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-rules-for-derivatives-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"g3547e21885702235e64882937631c05a","title":"Scalars, Vectors, Matrices, and Tensors - Code Along","type":"Assignment","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-scalars-vectors-matrices-tensors-codealong\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-scalars-vectors-matrices-tensors-codealong/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"gfbb1f52ae29185c60a7943bb30dc61ba","title":"Solving Systems of Linear Equations with NumPy - Code Along","type":"Assignment","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-lineq-numpy-codealong\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-lineq-numpy-codealong/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"g12f688c84032b3b1e7e7230f4d52594f","title":"Solving Systems of Linear Equations with NumPy - Lab","type":"Assignment","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-lineq-numpy-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-lineq-numpy-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"gac95296474eef5ec7a7ea3d8dd6059ab","title":"Systems of Linear Equations - Lab","type":"Assignment","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-lingalg-linear-equations-quiz\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-lingalg-linear-equations-quiz/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"g386828ef7d8aebe42e3aa5618b551fe5","title":"Tree Ensembles and Random Forests - Lab","type":"Assignment","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-tree-ensembles-random-forests-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-tree-ensembles-random-forests-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"gdb4d8a2f135ea18f696e94af952f5c08","title":"Vector Addition and Broadcasting in NumPy - Code Along","type":"Assignment","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-linalg-vector-addition-codealong\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-linalg-vector-addition-codealong/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"g8426ab3f21931cbe23df251e82e6c068","title":"Vectors and Matrices in Numpy - Lab","type":"Assignment","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-linalg-vector-matrices-numpy-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-linalg-vector-matrices-numpy-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"ge106a5e4ee58121697da7f759dec429f","title":"Visualizing Confusion Matrices - Lab","type":"Assignment","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-visualizing-confusion-matrices-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-visualizing-confusion-matrices-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"gdbf5e9577a46c704d4f83f451f46a921","title":"XGBoost - Lab","type":"Assignment","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-xgboost-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-xgboost-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null}],"discussion_topics":[],"quizzes":[],"files":null}