window.COURSE_DATA = {"language":"en","lastDownload":"2021-06-18T14:43:24Z","title":"Phase 4","modules":[{"id":21078,"name":"Topic 33: Principal Component Analysis","status":"started","unlockDate":null,"prereqs":[],"requirement":"all","sequential":false,"exportId":"g2b40ca149a509603539e1cf3b89ea684","items":[{"id":197196,"title":"PCA - Introduction","type":"WikiPage","indent":0,"locked":false,"requirement":"must_mark_done","completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-pca-introduction\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-pca-introduction/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eIn this section, you'll learn about principal component analysis, or PCA, one of the most famous \u003cem\u003eUnsupervised Learning Techniques\u003c/em\u003e. PCA is a dimensionality reduction technique. It allows you to compress a dataset into a lower dimensional space with fewer features while maintaining as much of the original information as possible.\u003c/p\u003e\n\n\u003ch2\u003eThe Curse of Dimensionality\u003c/h2\u003e\n\n\u003cp\u003eThe curse of dimensionality is a general mathematical problem relating to the exploding size of space as you continue to add additional dimensions. This can be particularly problematic when dealing with large datasets. The more features you have, the more data you have about the scenario, but the more difficult it might be to exhaustively explore combinations of these features.\u003c/p\u003e\n\n\u003ch2\u003ePCA Use Cases\u003c/h2\u003e\n\n\u003cp\u003eThe curse of dimensionality is certainly one motivating factor for PCA. If you can't process all of the information at your disposal, then an alternative path around is necessary. Dimensionality reduction techniques such as PCA can be essential in such situations. PCA can also help improve regression and classification algorithms in many cases. In particular, algorithms are less prone to overfitting when the underlying data itself has first been compressed, reducing noise or other anomalies. Finally, PCA can also be helpful for visualizing the structure of large datasets. After all, you are limited to 2 or 3 dimensions when visualizing data. As such, reducing a dataset to 2 or 3 primary features is monumental in creating a visualization.\u003c/p\u003e\n\n\u003ch2\u003eSummary\u003c/h2\u003e\n\n\u003cp\u003eIn this section, you'll explore PCA in depth using scikit-learn, and coding your own version from scratch using NumPy. Throughout this section, keep in mind use cases for PCA such as the curse of dimensionality.\u003c/p\u003e\n\u003cfooter class=\"fis-footer\" style=\"visibility: hidden;\"\u003e\u003cdiv class=\"fis-feedback\"\u003e\n\u003ch5\u003eHow do you feel about this lesson?\u003c/h5\u003e\n\u003cimg id=\"thumbs-up\" data-repository=\"dsc-pca-introduction\" title=\"Thumbs up!\" alt=\"thumbs up\"\u003e\u003cimg id=\"thumbs-down\" data-repository=\"dsc-pca-introduction\" title=\"Thumbs down!\" alt=\"thumbs down\"\u003e\n\u003c/div\u003e\n\u003ch5\u003eHave specific feedback? \u003ca href=\"https://github.com/learn-co-curriculum/dsc-pca-introduction/issues/new\"\u003eTell us here!\u003c/a\u003e\n\u003c/h5\u003e\u003c/footer\u003e","exportId":"pca-introduction"},{"id":197199,"title":"Unsupervised Learning","type":"WikiPage","indent":0,"locked":false,"requirement":"must_mark_done","completed":true,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-unsupervised-learning\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-unsupervised-learning/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eIn this lesson, you'll get a high-level overview of unsupervised learning, an entire class of algorithms in machine learning. To date, you've only seen examples of supervised learning tasks such as regression and classification. As the name implies, unsupervised learning is a bit different than these tasks. In supervised learning, you define an \u003ccode\u003eX\u003c/code\u003e and \u003ccode\u003ey\u003c/code\u003e, and the algorithm attempts to generalize this transformation in order to predict \u003ccode\u003ey\u003c/code\u003e given \u003ccode\u003eX\u003c/code\u003e. In unsupervised learning, you do not define an \u003ccode\u003eX\u003c/code\u003e or \u003ccode\u003ey\u003c/code\u003e. Instead, you feed in a given dataset and the unsupervised learning algorithm returns some new representation of the data based on the structure and patterns within the data itself.\u003c/p\u003e\n\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\n\u003cp\u003eYou will be able to: \u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eDefine unsupervised learning \u003c/li\u003e\n\u003cli\u003eCompare and contrast supervised and unsupervised learning \u003c/li\u003e\n\u003cli\u003eIdentify real-world scenarios in which you would use unsupervised learning \u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch2\u003eSupervised vs. Unsupervised Learning\u003c/h2\u003e\n\n\u003cp\u003eThe main difference between supervised and unsupervised learning are their goals. Supervised learning needs concrete, ground-truth labels to train models that answer very specific questions. Unsupervised learning differs in that the task it is trying to accomplish is much less well-defined - it can usually be summed up as \"are there any natural patterns in this data that are recognizable?\"  To illustrate this, assume that you have a basket of various different kinds of fruit. A supervised learning task would be building an apple classifier that tells us if a given fruit is or isn't an apple, based on the size, shape, color, texture, taste, and any other data that you've encoded for each piece of fruit. An unsupervised learning task on the same data would analyze only the features, and sort them into groups without being told what type of fruit each was. In general, supervised learning uses data to accomplish a clear task while unsupervised learning has no clear task, but is instead used to identify patterns.\u003c/p\u003e\n\n\u003ch2\u003eUnsupervised Learning Tasks\u003c/h2\u003e\n\n\u003cp\u003eThe two most common unsupervised learning tasks are clustering and dimensionality reduction. Clustering groups data into homogeneous groups, where members share common traits. Dimensionality reduction attempts to reduce the overall number of features of a dataset while preserving as much information as possible. With that, let's take a deeper look into some general notes on each.\u003c/p\u003e\n\n\u003ch3\u003eClustering\u003c/h3\u003e\n\n\u003cp\u003eThere are a few different kinds of clustering algorithms, but they all do the same thing - finding different ways to group a dataset based on patterns in the data.  One common use-case for clustering is market segmentation. In market segmentation, you would try to decompose an audience into subsets for more precise targeting for business purposes, such as advertising. Even though there's no way to verify that these groups are correct, in practice it usually does quite well, often providing useful subgroups which can then be individually examined.\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-unsupervised-learning/master/images/kmeans.gif\"\u003e\u003c/p\u003e\n\n\u003cblockquote\u003e\n\u003cp\u003eSource: \u003ca href=\"https://dashee87.github.io/data%20science/general/Clustering-with-Scikit-with-GIFs/\"\u003eGIF by David Sheehan\u003c/a\u003e\u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003ch3\u003eDimensionality Reduction\u003c/h3\u003e\n\n\u003cp\u003eThe most common dimensionality reduction algorithm is Principal Component Analysis (PCA). Dimensionality reduction algorithms work by projecting data from its current n-dimensional subspace into a smaller subspace, while losing as little information as possible in the process. Dimensionality reduction algorithms still lose \u003cem\u003esome\u003c/em\u003e information, but you can quantify this information loss to make an informed decision about the number of dimensions reduced versus the overall information lost. Dimensionality reduction algorithms are a must-have in any data scientist's toolbox, because they provide a way for us to deal with the \u003cstrong\u003eCurse of Dimensionality\u003c/strong\u003e. The curse of dimensionality is a key concept as datasets scale. In short, as the number of features in a dataset increases, the processing power and search space required to optimize a given machine learning algorithm explodes exponentially. Because this often creates intractable computational problems, dimensionality reduction techniques such as PCA can be an essential preprocessing technique.\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-unsupervised-learning/master/images/pca.gif\"\u003e\u003c/p\u003e\n\n\u003cblockquote\u003e\n\u003cp\u003eSource: \u003ca href=\"https://stats.stackexchange.com/questions/2691/making-sense-of-principal-component-analysis-eigenvectors-eigenvalues/140579#140579\"\u003eGIF by amoeba\u003c/a\u003e\u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003ch2\u003eSummary\u003c/h2\u003e\n\n\u003cp\u003eIn this lesson, you explored the differences between supervised and unsupervised learning. You also learned about the types of problems we can solve with unsupervised learning, including clustering and dimensionality reduction. \u003c/p\u003e\n\u003cfooter class=\"fis-footer\" style=\"visibility: hidden;\"\u003e\u003cdiv class=\"fis-feedback\"\u003e\n\u003ch5\u003eHow do you feel about this lesson?\u003c/h5\u003e\n\u003cimg id=\"thumbs-up\" data-repository=\"dsc-unsupervised-learning\" title=\"Thumbs up!\" alt=\"thumbs up\"\u003e\u003cimg id=\"thumbs-down\" data-repository=\"dsc-unsupervised-learning\" title=\"Thumbs down!\" alt=\"thumbs down\"\u003e\n\u003c/div\u003e\n\u003ch5\u003eHave specific feedback? \u003ca href=\"https://github.com/learn-co-curriculum/dsc-unsupervised-learning/issues/new\"\u003eTell us here!\u003c/a\u003e\n\u003c/h5\u003e\u003c/footer\u003e","exportId":"unsupervised-learning"},{"id":197201,"title":"The Curse of Dimensionality","type":"WikiPage","indent":0,"locked":false,"requirement":"must_mark_done","completed":true,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-curse-of-dimensionality\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-curse-of-dimensionality/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eThe curse of dimensionality is an interesting paradox for data scientists. On the one hand, one often hopes to garner more information to improve the accuracy of a machine learning algorithm. However, there are also some interesting phenomena that come along with larger datasets. In particular, the curse of dimensionality is based on the exploding volume of n-dimensional spaces as the number of dimensions, n, increases.\u003c/p\u003e\n\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\n\u003cp\u003eYou will be able to:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eExplain what is meant by the curse of dimensionality and its implications when training machine learning algorithms \u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch2\u003eSparseness in N-Dimensional Space\u003c/h2\u003e\n\n\u003cp\u003ePoints in n-dimensional space become increasingly sparse as the number of dimensions increases. That is, the distance between points will continue to grow as the number of dimensions grows. This can be problematic in a number of machine learning algorithms, in particular, when clustering points into groups. Due to the exploding nature of n-dimensional space, there is also an unwieldy number of possible combinations when searching for optimal parameters for a machine learning algorithm. \u003c/p\u003e\n\n\u003cp\u003eTo demonstrate this, you'll generate this graph in the upcoming lab:  \u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-curse-of-dimensionality/master/images/sparsity.png\"\u003e\u003c/p\u003e\n\n\u003cp\u003eThis image demonstrates how the average distance between points and the origin continues to grow as the number of dimensions increases, even though each dimension has a fixed range. Simply increasing the number of dimensions continues to make individual points more and more sparse.\u003c/p\u003e\n\n\u003ch2\u003eImplications\u003c/h2\u003e\n\n\u003cp\u003eThe main implication of the curse dimensionality is that optimization problems can become infeasible as the number of features increases. The practical limit will vary based on your particular computer and the time that you have to invest in a problem. As you'll see in the upcoming lab, this relationship is exponential. For machine learning algorithms that involve backpropagation, or iterative convergence, including Lasso and Ridge regression, this will drastically impact the size of feasible solvable problems.\u003c/p\u003e\n\n\u003cp\u003eThe sparsity of points also has additional consequences. Due to the sheer scale of potential points in an n-dimensional space, as n continues to grow, the probability of seeing a particular point (or even nearby point) continues to plummet. Therefore, it is likely that there are entire regions of an n-dimensional space that have yet to be explored. As such, if no such information from the training set is available regarding such cases, then making predictions regarding these cases will be guesswork. Put another way, with the increasing sparsity of points, you have an ever decreasing proportionate sample of the space. For example, a thousand observations in a 3-dimensional space might be quite powerful and provide sufficient information to determine a relevant classification or regression model. However, a thousand observations in a million-dimensional space is likely to be utterly useless in determining which features are most influential and to what degree. \u003c/p\u003e\n\n\u003ch2\u003eSummary\u003c/h2\u003e\n\n\u003cp\u003eThe curse of dimensionality presents an intriguing paradox. On the one hand, more features allow one to account for variance and nuances required to accurately model a given machine learning model. On the other hand, as the number of dimensions increases, the accompanying volume of the hyperspace explodes exponentially. As such, the potential amount of information required to accurately model such a space becomes increasingly complex. (This is not always the case; a simple line can still exist in a 10-dimensional space, but the problems one is likely to be tackling when employing 10 features are most likely more complex than a 2-dimensional model.) With this, more and more observations will be required to produce an adequate model.\u003c/p\u003e\n\u003cfooter class=\"fis-footer\" style=\"visibility: hidden;\"\u003e\u003cdiv class=\"fis-feedback\"\u003e\n\u003ch5\u003eHow do you feel about this lesson?\u003c/h5\u003e\n\u003cimg id=\"thumbs-up\" data-repository=\"dsc-curse-of-dimensionality\" title=\"Thumbs up!\" alt=\"thumbs up\"\u003e\u003cimg id=\"thumbs-down\" data-repository=\"dsc-curse-of-dimensionality\" title=\"Thumbs down!\" alt=\"thumbs down\"\u003e\n\u003c/div\u003e\n\u003ch5\u003eHave specific feedback? \u003ca href=\"https://github.com/learn-co-curriculum/dsc-curse-of-dimensionality/issues/new\"\u003eTell us here!\u003c/a\u003e\n\u003c/h5\u003e\u003c/footer\u003e","exportId":"the-curse-of-dimensionality"},{"id":197204,"title":"Curse of Dimensionality - Lab","type":"Assignment","indent":1,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_mark_done","completed":true,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-curse-of-dimensionality-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-curse-of-dimensionality-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","exportId":"g5653177efd941d689ec4c48330ea0c79"},{"id":197208,"title":"PCA in scikit-learn","type":"Assignment","indent":0,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_mark_done","completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-pca-in-scikitlearn\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-pca-in-scikitlearn/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","exportId":"gef7dcb9d05c4072a9f49d7aee1c49a3e"},{"id":197210,"title":"PCA in scikit-learn - Lab","type":"Assignment","indent":1,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_mark_done","completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-pca-in-scikitlearn-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-pca-in-scikitlearn-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","exportId":"g6cf4693bcea5b2ce7d90b156cf14056c"},{"id":197213,"title":"Integrating PCA in Pipelines - Lab","type":"Assignment","indent":1,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_mark_done","completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-pca-and-pipelines-v2-1\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-pca-and-pipelines-v2-1/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","exportId":"ge86474f485a622c725ebcd07e1f92cac"},{"id":197216,"title":"PCA Background: Covariance Matrix and Eigendecomposition","type":"Assignment","indent":0,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_mark_done","completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-pca-covariance-matrix-eigendecomp\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-pca-covariance-matrix-eigendecomp/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","exportId":"g17b335a023f0ff43e5f2dab37c1cf7b2"},{"id":197218,"title":"Performing Principal Component Analysis","type":"Assignment","indent":0,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_mark_done","completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-performing-principle-component-analysis\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-performing-principle-component-analysis/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","exportId":"g1b186fdf4181788a31260a9296a62568"},{"id":197220,"title":"Performing Principal Component Analysis - Lab","type":"Assignment","indent":1,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_mark_done","completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-pca-numpy-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-pca-numpy-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","exportId":"g791d0e61061e39932162ce861f82e51e"},{"id":197222,"title":"PCA and Digital Image Processing","type":"Assignment","indent":0,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_mark_done","completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-pca-and-digital-image-processing\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-pca-and-digital-image-processing/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","exportId":"ge1abb17e54a9ceb85fe1539297e7e4a1"},{"id":197225,"title":"PCA and Digital Image Processing - Lab","type":"Assignment","indent":1,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_mark_done","completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-pca-and-digital-image-processing-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-pca-and-digital-image-processing-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","exportId":"g4e184b24c353e4bea0bababe338be61d"},{"id":197228,"title":"PCA - Recap","type":"WikiPage","indent":0,"locked":false,"requirement":"must_mark_done","completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-pca-summary\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-pca-summary/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\u003ch2\u003eKey Takeaways\u003c/h2\u003e\n\n\u003cp\u003eThe key takeaways from this section include: \u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003ePCA is an \u003cem\u003eunsupervised learning technique\u003c/em\u003e which does not require labeled data \u003c/li\u003e\n\u003cli\u003eIt is also a dimensionality reduction technique which can be used to compress data and experiment with its effects on machine learning algorithms as a preprocessing step \u003c/li\u003e\n\u003cli\u003eThere are four steps to conducting PCA:\n\n\u003cul\u003e\n\u003cli\u003eCenter each feature by subtracting the feature mean\u003c/li\u003e\n\u003cli\u003eCalculate the covariance matrix for your normalized dataset\u003c/li\u003e\n\u003cli\u003eCalculate the eigenvectors/eigenvalues for the covariance matrix\n\n\u003cul\u003e\n\u003cli\u003eReorder your eigenvectors based on their accompanying eigenvalues (in descending order of the eigenvalues)\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eTake the dot product of the transpose of the eigenvectors with the transpose of the normalized data\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eYou can also easily implement PCA using scikit-learn \u003c/li\u003e\n\u003c/ul\u003e\n\u003cfooter class=\"fis-footer\" style=\"visibility: hidden;\"\u003e\u003cdiv class=\"fis-feedback\"\u003e\n\u003ch5\u003eHow do you feel about this lesson?\u003c/h5\u003e\n\u003cimg id=\"thumbs-up\" data-repository=\"dsc-pca-summary\" title=\"Thumbs up!\" alt=\"thumbs up\"\u003e\u003cimg id=\"thumbs-down\" data-repository=\"dsc-pca-summary\" title=\"Thumbs down!\" alt=\"thumbs down\"\u003e\n\u003c/div\u003e\n\u003ch5\u003eHave specific feedback? \u003ca href=\"https://github.com/learn-co-curriculum/dsc-pca-summary/issues/new\"\u003eTell us here!\u003c/a\u003e\n\u003c/h5\u003e\u003c/footer\u003e","exportId":"pca-recap"}]},{"id":21080,"name":"Topic 34: Clustering","status":"started","unlockDate":null,"prereqs":[],"requirement":"all","sequential":false,"exportId":"g1a4062045be2c5862e4b850f018dd6b4","items":[{"id":197232,"title":"Clustering - Introduction","type":"WikiPage","indent":0,"locked":false,"requirement":"must_mark_done","completed":true,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-clustering-intro\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-clustering-intro/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eIn this section, you'll learn about a useful unsupervised learning technique: clustering. This lesson summarizes the topics you'll be covering in this section.\u003c/p\u003e\n\u003ch2\u003eClustering\u003c/h2\u003e\n\u003cp\u003eClustering techniques are very powerful when you want to group data with similar characteristics together, but have no pre-specified labels. The main goal of clustering is to create clusters that have a high similarity between the data belonging to one cluster while aiming for minimal similarity between clusters.\u003c/p\u003e\n\u003ch3\u003eK-Means Clustering\u003c/h3\u003e\n\u003cp\u003eWe start by providing a basic intuition of the K-means clustering algorithm. When using the K-means clustering algorithm, the number of clusters that you want to obtain is specified upfront and the algorithm aims at the most \"optimal\" cluster centers, given that there are \u003cimg src=\"https://render.githubusercontent.com/render/math?math=K\"\u003e clusters.\u003c/p\u003e\n\u003ch3\u003eHierarchical Agglomerative Clustering\u003c/h3\u003e\n\u003cp\u003eA second branch of clustering algorithms is hierarchical agglomerative clustering. Using hierarchical clustering, unlike K-means clustering, you don't decide on the number of clusters beforehand. Instead, you start with \u003cimg src=\"https://render.githubusercontent.com/render/math?math=n\"\u003e clusters, where \u003cimg src=\"https://render.githubusercontent.com/render/math?math=n\"\u003e is the number of data points, and at each step you join two clusters. You stop joining clusters when a certain criterion is reached.\u003c/p\u003e\n\u003ch3\u003eMarket Segmentation with Clustering\u003c/h3\u003e\n\u003cp\u003eA very common and useful application of clustering is market segmentation. You'll practice your clustering skills on a market segmentation dataset!\u003c/p\u003e\n\u003ch3\u003eSemi-Supervised Learning\u003c/h3\u003e\n\u003cp\u003eAt the end of this section, you'll learn how semi-supervised learning techniques, which are increasingly popular in machine learning, combines both concepts of supervised and unsupervised learning.\u003c/p\u003e\n\u003ch2\u003eSummary\u003c/h2\u003e\n\u003cp\u003eIn this section, you'll learn how to use clustering techniques which are very useful for finding patterns and grouping unlabeled data together.\u003c/p\u003e","exportId":"clustering-introduction"},{"id":197233,"title":"K-means Clustering","type":"WikiPage","indent":0,"locked":false,"requirement":"must_mark_done","completed":true,"content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-k-means-clustering\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-k-means-clustering\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-k-means-clustering/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eIn this lesson, we'll learn about the most popular and widely-used clustering algorithm, K-means clustering. \u003c/p\u003e\n\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\n\u003cp\u003eYou will be able to:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eCompare the different approaches to clustering networks \u003c/li\u003e\n\u003cli\u003eExplain the steps behind the K-means clustering algorithm \u003c/li\u003e\n\u003cli\u003ePerform k-means clustering in scikit-learn \u003c/li\u003e\n\u003cli\u003eExplain how clusters are evaluated \u003c/li\u003e\n\u003cli\u003eDefine an \"elbow plot\" and how to interpret it \u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch2\u003eClustering\u003c/h2\u003e\n\n\u003cp\u003e\u003cstrong\u003e\u003cem\u003eClustering\u003c/em\u003e\u003c/strong\u003e techniques are among the most popular unsupervised machine learning algorithms. The main idea behind clustering is that you want to group objects into similar classes, in a way that:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eintra-class similarity is high (similarity amongst members of the same group is high)\u003c/li\u003e\n\u003cli\u003einter-class similarity is low (similarity of different groups is low)\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003eWhat does \u003cem\u003esimilarity\u003c/em\u003e mean? You should be thinking of it in terms of \u003cem\u003edistance\u003c/em\u003e, just like we did with the k-nearest-neighbors algorithm. The closer two points are, the more similar they are. It is useful to make a distinction between \u003cem\u003ehierarchical\u003c/em\u003e and \u003cem\u003enonhierarchical\u003c/em\u003e clustering algorithms:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003e\u003cp\u003eIn cluster analysis, an \u003cstrong\u003e\u003cem\u003eagglomerative hierarchical\u003c/em\u003e\u003c/strong\u003e algorithm starts with \u003cem\u003en\u003c/em\u003e clusters (where \u003cem\u003en\u003c/em\u003e is the number of observations, so each observation is a cluster), then combines the two most similar clusters, combines the next two most similar clusters, and so on. A \u003cstrong\u003e\u003cem\u003edivisive\u003c/em\u003e\u003c/strong\u003e hierarchical algorithm does the exact opposite, going from 1 to \u003cem\u003en\u003c/em\u003e clusters.\u003c/p\u003e\u003c/li\u003e\n\u003cli\u003e\u003cp\u003eA \u003cstrong\u003e\u003cem\u003enonhierarchical\u003c/em\u003e\u003c/strong\u003e algorithm chooses \u003cem\u003ek\u003c/em\u003e initial clusters and reassigns observations until no improvement can be obtained. How initial clusters and reassignments are done depends on the specific type of algorithm.\u003c/p\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003eAn essential understanding when using clustering methods is that you are basically trying to group data points together without knowing what the \u003cem\u003eactual\u003c/em\u003e cluster/classes are. This is also the main distinction between clustering and classification (which is a supervised learning method). This is why technically, you also don't know how many clusters you're looking for.\u003c/p\u003e\n\n\u003ch2\u003eNon-Hierarchical Clustering With K-Means Clustering\u003c/h2\u003e\n\n\u003cp\u003e\u003cstrong\u003e\u003cem\u003eK-means clustering\u003c/em\u003e\u003c/strong\u003e is the most well-known clustering technique, and it belongs to the class of non-hierarchical clustering methods. When performing k-means clustering, you're essentially trying to find  \u003cimg src=\"https://render.githubusercontent.com/render/math?math=k\"\u003e cluster centers as the mean of the data points that belong to these clusters. One challenging aspect of k-means is that the number \u003cem\u003ek\u003c/em\u003e needs to be decided upon before you start running the algorithm.\u003c/p\u003e\n\n\u003cp\u003eThe k-means clustering algorithm is an iterative algorithm that reaches for a pre-determined number of clusters within an unlabeled dataset, and basically works as follows:\u003c/p\u003e\n\n\u003col\u003e\n\u003cli\u003eSelect  \u003cimg src=\"https://render.githubusercontent.com/render/math?math=k\"\u003e initial seeds \u003c/li\u003e\n\u003cli\u003eAssign each observation to the cluster to which it is \"closest\"\u003c/li\u003e\n\u003cli\u003eRecompute the cluster centroids\u003c/li\u003e\n\u003cli\u003eReassign the observations to one of the clusters according to some rule\u003c/li\u003e\n\u003cli\u003eStop if there is no reallocation \u003c/li\u003e\n\u003c/ol\u003e\n\n\u003cp\u003eTwo assumptions are of main importance for the k-means clustering algorithm:\u003c/p\u003e\n\n\u003col\u003e\n\u003cli\u003eTo compute the \"cluster center\", you calculate the (arithmetic) mean of all the points belonging to the cluster.  Each cluster center is recalculated in the beginning of each new iteration\u003cbr\u003e\n\u003c/li\u003e\n\u003cli\u003eAfter the cluster center has been recalculated, if a given point is now closer to a different cluster center than the center of its current cluster, then that point is reassigned to the clostest center \u003c/li\u003e\n\u003c/ol\u003e\n\n\u003ch2\u003eVisualization of K-means Clustering Algorithm\u003c/h2\u003e\n\n\u003cp\u003eIn the animation below, the green dots are the centroids. Notice how they are randomly assigned at the beginning, and shift with each iteration as they are recalculated to match the center of the points assigned to their cluster. The clustering ends when the centroids find a position in which points are no longer reassigned, meaning that the centroids no longer need to move. \u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-k-means-clustering/master/images/good-centroid-start.gif\"\u003e\u003c/p\u003e\n\n\u003ch2\u003eImplementing K-means Clustering in scikit-learn\u003c/h2\u003e\n\n\u003cp\u003eImplementing k-means clustering with scikit-learn is quite simple because the API mirrors the same functionality that we've seen before. The same preprocessing steps used for supervised learning methods are required -- missing values must be dealt with and all data must be in numerical format (meaning that non-numerical columns must be dropped or one-hot encoded). \u003c/p\u003e\n\n\u003cpre\u003e\u003ccode class=\"python\"\u003efrom sklearn.cluster import KMeans\n\nk_means = KMeans(n_clusters=3) \n\nk_means.fit(some_df) \n\ncluster_assignments = k_means.predict(some_df) \n\n\u003c/code\u003e\u003c/pre\u003e\n\n\u003ch2\u003eEvaluating Cluster Fitness\u003c/h2\u003e\n\n\u003cp\u003eRunning K-means on a dataset is easy enough, but how do we know if we have the best value for  \u003cimg src=\"https://render.githubusercontent.com/render/math?math=k\"\u003e ?  The best bet is to use an accepted metric for evaluating cluster fitness such as \u003ca href=\"https://scikit-learn.org/stable/modules/generated/sklearn.metrics.calinski_harabasz_score.html\"\u003e\u003cstrong\u003e\u003cem\u003eCalinski Harabasz Score\u003c/em\u003e\u003c/strong\u003e\u003c/a\u003e, which is more often referred to by a simpler, \u003cstrong\u003e\u003cem\u003eVariance Ratio\u003c/em\u003e\u003c/strong\u003e.\u003c/p\u003e\n\n\u003ch3\u003eComputing Variance Ratios\u003c/h3\u003e\n\n\u003cp\u003eThe \u003cem\u003evariance ratio\u003c/em\u003e is a ratio of the variance of the points within a cluster, to the variance of a point to points in other clusters. Intuitively, we can understand that we want intra-cluster variance to be low (suggesting that the clusters are tightly knit), and inter-cluster variance to be high (suggesting that there is little to no ambiguity about which cluster the points belong to). \u003c/p\u003e\n\n\u003cp\u003eWe can easily calculate the variance ratio by importing a function from scikit-learn to calculate it for us, as shown below. To use this metric, we just need to pass in the points themselves, and the predicted labels given to each point by the clustering algorithm. The higher the score, the better the fit.\u003c/p\u003e\n\n\u003cpre\u003e\u003ccode class=\"python\"\u003efrom sklearn.metrics import calinski_harabasz_score\n\nprint(calinski_harabasz_score(some_df, cluster_assignments))\n\u003c/code\u003e\u003c/pre\u003e\n\n\u003cp\u003eThere are other metrics that can also be used to evaluate the fitness, such as \u003ca href=\"https://scikit-learn.org/stable/modules/generated/sklearn.metrics.silhouette_score.html#sklearn.metrics.silhouette_score\"\u003eSilhouette Score\u003c/a\u003e. No one metric is best -- they all have slightly different strengths and weaknesses depending on the given dataset and goals. Because of this, it's generally accepted that it's best to pick one metric and stick to it. \u003c/p\u003e\n\n\u003ch3\u003eFinding the Optimal Value of K\u003c/h3\u003e\n\n\u003cp\u003eNow that we have a way to evaluate how well our clusters fit the dataset, we can use this to find the optimal value for  \u003cimg src=\"https://render.githubusercontent.com/render/math?math=k\"\u003e . The best way to do this is to create and fit different k-means clustering objects for every value of  \u003cimg src=\"https://render.githubusercontent.com/render/math?math=k\"\u003e that we want to try, and then compare the variance ratio scores for each. \u003c/p\u003e\n\n\u003cp\u003eWe can then visualize the scores using an \u003cstrong\u003e\u003cem\u003eElbow Plot\u003c/em\u003e\u003c/strong\u003e:\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-k-means-clustering/master/images/new_elbow-method.png\" width=\"500\"\u003e\u003c/p\u003e\n\n\u003cp\u003eAn \u003cem\u003eelbow plot\u003c/em\u003e is a general term for plots like this where we can easily see where we hit a point of diminishing returns. In the plot above, we can see that performance peaks at \u003cem\u003ek=6\u003c/em\u003e, and then begins to drop off. That tells us that our data most likely has 6 naturally occurring clusters in our data. \u003c/p\u003e\n\n\u003cp\u003eElbow plots aren't exclusively used with variance ratios -- it's also quite common to calculate something like distortion (another clustering metric), which will result in a graph with a negative as opposed to a positive slope. \u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-k-means-clustering/master/images/new_elbow_2.png\" width=\"500\"\u003e\u003c/p\u003e\n\n\u003ch4\u003eUnderstanding the Elbow\u003c/h4\u003e\n\n\u003cp\u003eA note on elbow plots: higher scores aren't always better. Higher values of  \u003cimg src=\"https://render.githubusercontent.com/render/math?math=k\"\u003e mean introducing more overall complexity -- we will sometimes see elbow plots that look like this:\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-k-means-clustering/master/images/new_dim_returns.png\" width=\"500\"\u003e\u003c/p\u003e\n\n\u003cp\u003eIn the example above, although k=20 technically scores better than k=4, we choose k=4 because it is the \u003cstrong\u003e\u003cem\u003eElbow\u003c/em\u003e\u003c/strong\u003e on the graph. After the elbow, the metric we're trying to optimize for gets better at a much slower rate. Dealing with 20 clusters, when the fit is only slightly better, isn't worth it -- it's better to treat our data as having only 4 clusters, because that is the simplest overall model that provides the most value with the least complexity!\u003c/p\u003e\n\n\u003ch2\u003eSummary\u003c/h2\u003e\n\n\u003cp\u003eIn this lesson, we learned about different kinds of clustering and explored how the k-means clustering algorithm works. We also learned about how we can quantify the performance of a clustering algorithm using metrics such as variance ratios, and how we can use these metrics to find the optimal value for  \u003cimg src=\"https://render.githubusercontent.com/render/math?math=k\"\u003e by creating elbow plots!\u003c/p\u003e","exportId":"k-means-clustering"},{"id":197235,"title":"K-Means Clustering - Lab","type":"Assignment","indent":1,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_mark_done","completed":true,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-k-means-clustering-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-k-means-clustering-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","exportId":"g04296a1eec45d2008f3fecb687111576"},{"id":197238,"title":"Hierarchical Agglomerative Clustering","type":"WikiPage","indent":0,"locked":false,"requirement":"must_mark_done","completed":false,"content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-hierarchical-agglomerative-clustering\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-hierarchical-agglomerative-clustering\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-hierarchical-agglomerative-clustering/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eIn this lesson, we'll learn about another popular class of clustering algorithms -- hierarchical agglomerative clustering!\u003c/p\u003e\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\u003cp\u003eYou will be able to:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eExplain the process behind hierarchical agglomerative clustering\u003c/li\u003e\n\u003cli\u003eDescribe the three different linkage criteria for hierarchical agglomerative clustering\u003c/li\u003e\n\u003cli\u003eDefine the purpose of a dendrogram\u003c/li\u003e\n\u003cli\u003eCompare and contrast k-means and hierarchical agglomerative clustering methodologies\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003eUnderstanding Hierarchical Clustering\u003c/h2\u003e\n\u003cp\u003eSo far, we've worked with a non-hierarchical clustering algorithm, k-means clustering. K-means works by taking a set parameter that tells it how many clusters we think exist in the data, and then uses the Expectation-Maximization (EM) algorithm to iteratively shift each cluster centroid to the best possible position by constantly calculating and recalculating the centroid's position by assigning each point to the cluster centroid they are closest to with each new step, and then moving the centroid to the center of all the points currently assigned to that centroid. With non-hierarchical algorithms, there can be no subgroups -- that is, no clusters within clusters.\u003c/p\u003e\n\u003cp\u003eThis is where agglomerative clustering algorithms come in. In agglomerative clustering, the algorithm starts with \u003cimg src=\"https://render.githubusercontent.com/render/math?math=n\"\u003e clusters (where \u003cimg src=\"https://render.githubusercontent.com/render/math?math=n\"\u003e is the number of data points) and proceeds by merging the most similar clusters, until some stopping criterion. in \u003ccode\u003escikit-learn\u003c/code\u003e, the stopping criterion that is implemented is \"number of clusters\". If left alone, the algorithm will work until it has merged every cluster into one giant cluster. We can also set the limit, if we want, to stop when there are only [x] clusters remaining.\u003c/p\u003e\n\u003ch3\u003eLinking Similar Clusters Together\u003c/h3\u003e\n\u003cp\u003eSeveral linkage criteria that have different definitions for \"most similar clusters\" can be used. The measure is always defined between two existing clusters up until that point, so the later in the algorithms, the bigger the clusters get.\u003c/p\u003e\n\u003cp\u003eScikit-learn provides three linkage criteria:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eward\u003c/strong\u003e (default): picks the two clusters to merge in a way that the variance within all clusters increases the least. Generally, this leads to clusters that are fairly equally sized.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eaverage\u003c/strong\u003e: merges the two clusters that have the smallest \u003cstrong\u003e\u003cem\u003eaverage\u003c/em\u003e\u003c/strong\u003e distance between all the points.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003ecomplete\u003c/strong\u003e (or maximum linkage): merges the two clusters that have the smallest \u003cstrong\u003e\u003cem\u003emaximum\u003c/em\u003e\u003c/strong\u003e distance between their points.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eAs we'll see in the next lab, these linkage criteria can definitely have an effect on how the clustering algorithm performs. As always seems to be the case, no one of these is \"best\" -- which one you should use often depends on the structure of your data, and/or your own goals.\u003c/p\u003e\n\u003ch3\u003eA Visual Example\u003c/h3\u003e\n\u003cp\u003eIt's often easier to understand what the HAC algorithm is doing when we look at the decisions it makes at each given step. The following diagram demonstrates the clusters created at each step for a dataset of 16 points. Take a look at the diagram and see if you can figure out what the algorithm is doing at each step as it merges clusters together:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-hierarchical-agglomerative-clustering/master/images/new_hac_iterative.png\"\u003e\u003c/p\u003e\n\u003cp\u003eAs we can see from the diagram above, in each step, the algorithm takes the two clusters that are closest together (and remember, we define \"closest together\" according to whichever linkage criteria we choose to use), and then \u003cstrong\u003e\u003cem\u003emerge\u003c/em\u003e\u003c/strong\u003e those two clusters together into a single cluster. We don't move the data points or anything like that -- we just consider them as a single unit, as opposed to two separate ones. This works at every stage because in the beginning, we treat each data point as a unique cluster.\u003c/p\u003e\n\u003cp\u003eThis becomes very intuitive when we look at the following gif -- pay attention to the image on the left:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-hierarchical-agglomerative-clustering/master/images/dendrogram_gif.gif\"\u003e\u003c/p\u003e\n\u003cp\u003eAs the dots disappear, the visualization is replacing them with the newly calculated center of that cluster, which will be used for linkage purposes. Now, let's end this lesson by talking about visualizations we can use to interpret results!\u003c/p\u003e\n\u003ch3\u003eDendrograms and Clustergrams\u003c/h3\u003e\n\u003cp\u003eOne advantage of HAC is that we can easily visualize the results \u003cstrong\u003e\u003cem\u003eat any given step\u003c/em\u003e\u003c/strong\u003e using visualizations such as \u003cstrong\u003e\u003cem\u003eDendrograms\u003c/em\u003e\u003c/strong\u003e and \u003cstrong\u003e\u003cem\u003eClustergrams\u003c/em\u003e\u003c/strong\u003e. Take another look at the gif above, but this time, pay attention to the image on the right. This is a \u003cem\u003edendrogram,\u003c/em\u003e which is used to visualize the hierarchical relationship between the various clusters that are computed throughout each step. Dendrograms are very useful to decide how clusters change depending on the euclidian distance. If you decide that your intra-cluster euclidian distance should be smaller than 3, you can draw a horizontal line at euclidian distance 3, and define which points belong to which cluster by looking at the dendrogram. For the gif above, this means that there are three clusters: cluster one contains \u003cimg src=\"https://render.githubusercontent.com/render/math?math=p_0\"\u003e , \u003cimg src=\"https://render.githubusercontent.com/render/math?math=p_1\"\u003e and \u003cimg src=\"https://render.githubusercontent.com/render/math?math=p_2\"\u003e , cluster two contains \u003cimg src=\"https://render.githubusercontent.com/render/math?math=p_3\"\u003e , cluster three contains \u003cimg src=\"https://render.githubusercontent.com/render/math?math=p_4\"\u003e , \u003cimg src=\"https://render.githubusercontent.com/render/math?math=p_5\"\u003e and \u003cimg src=\"https://render.githubusercontent.com/render/math?math=p_6\"\u003e .\u003c/p\u003e\n\u003cp\u003eWe can also visualize the same information by drawing lines representing each cluster at each step to create a \u003cem\u003eclustergram\u003c/em\u003e. Take a look at the following diagram below, which shows both a dendrogram and clustergram of the same HAC results:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-hierarchical-agglomerative-clustering/master/images/new_clustergram.png\" width=\"600\"\u003e\u003c/p\u003e\n\u003ch3\u003eHow is HAC used?\u003c/h3\u003e\n\u003cp\u003eHAC algorithms are used in generally the same way that K-means and other clustering algorithms are used: for tasks such as market segmentation, or for gaining a deeper understanding of a dataset through cluster analysis. However, there are special cases of things that fit quite well in a hierarchical agglomerative structure -- one of the most common use cases you'll see for HAC is the way that smartphones naturally sort photos inside their photos app! Take a look at your photos app on your phone, and the albums that it creates for you -- you'll likely see that the albums are sorted in a \u003cstrong\u003e\u003cem\u003ehierarchical\u003c/em\u003e\u003c/strong\u003e fashion! Perhaps the phone chooses to group photos by date first, and then by location, or even content! In this way, these can be viewed as natural clusters within clusters, in a way that makes intuitive sense to users. When we browse, we likely want to see photos that were taken around the same time, and then at the same place, and then narrow it down to photos about the same things, to quickly browse and find what we're looking for. This is a great example of HAC being used in the wild!\u003c/p\u003e\n\u003ch2\u003eSummary\u003c/h2\u003e\n\u003cp\u003eIn this lesson, we learned about how the HAC algorithm derives its clusters, including different linkage criteria that can be used to determine which clusters should be merged at any given point. We also examined some visualizations of HAC algorithms, in the forms of dendrograms and clustergrams!\u003c/p\u003e","exportId":"hierarchical-agglomerative-clustering"},{"id":197240,"title":"Common Problems with Clustering Algorithms","type":"WikiPage","indent":0,"locked":false,"requirement":"must_mark_done","completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-common-problems-with-clustering\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-common-problems-with-clustering/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eIn this lesson, we'll discuss some of the common problems often seen when attempting clustering with k-means or hierarchical agglomerative clustering.\u003c/p\u003e\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\u003cp\u003eYou will be able to:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eIdentify the problems that can arise from bad centroid initializations in k-means and bad initial groups in HAC\u003c/li\u003e\n\u003cli\u003eCompare and contrast k-means and hierarchical agglomerative clustering methodologies\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003eCommon Problems with Clustering\u003c/h2\u003e\n\u003cp\u003eWhen working with clustering algorithms, there are certain problems that we should always be aware of in order to help us prevent situations where we unknowingly accept the results of a bad clustering. Understanding the potential problems that can arise with each clustering algorithm also tends to provide greater insight into how each algorithm works.\u003c/p\u003e\n\u003cp\u003eThe most common issue is one that is applicable to all forms of clustering -- we have no way of verifying if the results of the cluster analysis are correct or not! Always try to keep this in mind when working with clustering algorithms, and never make the mistake of treating the results of a cluster analysis as ground-truth.\u003c/p\u003e\n\u003cp\u003eTo further drive this point home, let's spend some time looking at common problems with the two kinds of clustering algorithms we've talked about so far so that we can gain insight into the situations where clustering algorithms fall short.\u003c/p\u003e\n\u003ch2\u003eAdvantages \u0026amp; Disadvantages of K-Means Clustering\u003c/h2\u003e\n\u003cp\u003eThe advantages of the k-means clustering approach are:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eVery easy to implement!\u003c/li\u003e\n\u003cli\u003eWith many features, k-means is usually faster than HAC (as long as \u003cimg src=\"https://render.githubusercontent.com/render/math?math=k\"\u003e is reasonably small)\u003c/li\u003e\n\u003cli\u003eObjects are locked into the cluster they are first assigned to and can change as the centroids move around\u003c/li\u003e\n\u003cli\u003eClusters are often tighter than those formed by HAC\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eHowever, this algorithm often comes with several disadvantages:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eQuality of results depends on picking the right value for \u003cimg src=\"https://render.githubusercontent.com/render/math?math=k\"\u003e . This can be a problem when we don't know how many clusters to expect in our dataset\u003c/li\u003e\n\u003cli\u003eScaling our dataset will completely change the results\u003c/li\u003e\n\u003cli\u003eInitial start points of each centroid have a very strong impact on our final results. A bad start point can cause sub-optimal clusters (see example below)\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-common-problems-with-clustering/master/images/bad-centroid-start.gif\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"http://shabal.in/visuals/kmeans/right.gif\"\u003egif courtesy of Andrey A. Shabalin\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eThe animation above shows what can happen when we get a bad centroid initialization. Because of the random points that the centroids were initialized at, this led to one centroid cluster containing no points, while another cluster centroid has combined two clusters by being located in between them! Even though we had the correct value for \u003cimg src=\"https://render.githubusercontent.com/render/math?math=k\"\u003e (since we have 4 centroids, and data clearly contains 4 clusters), we ended up with incorrect results.\u003c/p\u003e\n\u003cp\u003eSince every dataset is different, and centroids are generated randomly, there is no way to make sure that we have good centroid initialization every time. One way to deal with this is to run a clustering algorithm multiple times, and keep track of how many times the same results come up. The good news here is that bad centroid initializations are typically much less likely than good centroid initializations, so the chances of getting bad results due to poor centroid initialization multiple times in a row are somewhat unlikely.\u003c/p\u003e\n\u003cp\u003eNow, let's take a look at HAC.\u003c/p\u003e\n\u003ch2\u003eAdvantages \u0026amp; Disadvantages of HAC\u003c/h2\u003e\n\u003cp\u003eHAC is useful as a clustering algorithm because:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eIt produces an ordered relationship between clusters, which can be useful when visualized\u003c/li\u003e\n\u003cli\u003eSmaller clusters are created. This allows us to get a very granular understanding of our dataset, and zoom in at the level where the clusters make the most sense to us (note the coloration of the lines in the example dendrogram above)\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eHowever, this algorithm is also built on some assumptions which can be disadvantages:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eResults are usually dependent upon the distance metric used\u003c/li\u003e\n\u003cli\u003eObjects can be grouped 'incorrectly' early on, with no way to relocate them. For instance, consider two points that belong to separate clusters, but are both nearer to each other than the center of the cluster they actually belong to (both are near the \"boundary\" between their cluster and the opposing cluster). These will be incorrectly grouped as a cluster, which will throw off the clustering of the groups they actually belong to, as well\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eLet's look at an example. Consider the circled points in the following plot:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-common-problems-with-clustering/master/images/new_bad-hac.png\" width=\"600\"\u003e\u003c/p\u003e\n\u003cp\u003eThe two points circled are from different clusters. However, they are right on the boundary between the two clusters, which has significant overlap between them. Because of this, there is a good chance that the clusters will meet the linkage criteria, and the HAC algorithm will group them together. The centroid of this new (incorrect) cluster is also close to many points on the boundary, meaning that it is quite likely that those points will be merged and the incorrect cluster will grow bigger. Early mistakes with the HAC algorithm tend to act as a bit of a slippery slope, and since HAC doesn't constantly reassign points like k-means does, this means that things can go from bad to worse if mistakes are made early on.\u003c/p\u003e\n\u003ch2\u003eA Note on Visualization\u003c/h2\u003e\n\u003cp\u003eSo far, we've checked our work by looking at visualizations of the clusters and using our eyes and our judgment to check if we agree with the results of the algorithm. However, it's worth remembering that this is highly unlikely to be an option on real-world data since we can't visualize any data with more than 3 dimensions. Because of this, it's often much harder to tell when a clustering algorithm has made a mistake, because we aren't able to use our eyes to confirm or deny the results!\u003c/p\u003e\n\u003ch2\u003eSummary\u003c/h2\u003e\n\u003cp\u003eIn this lesson, we learned about some of the challenges that come with clustering, and the relative advantages and disadvantages of k-means and hierarchical agglomerative clustering.\u003c/p\u003e","exportId":"common-problems-with-clustering-algorithms"},{"id":197242,"title":"Hierarchical Agglomerative Clustering - Codealong","type":"Assignment","indent":0,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_mark_done","completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-hierarchical-agglomerative-clustering-codealong\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-hierarchical-agglomerative-clustering-codealong/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","exportId":"gb241a59232b1afda854cb830e82a967c"},{"id":197245,"title":"Market Segmentation with Clustering","type":"WikiPage","indent":0,"locked":false,"requirement":"must_mark_done","completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-market-segmentation-clustering\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-market-segmentation-clustering/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eIn this lesson, we'll learn about one of the most popular use cases for clustering in the business world -- market segmentation!\u003c/p\u003e\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\u003cp\u003eYou will be able to:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eExplain market segmentation and how clustering can be used for it\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003eWhat is Market Segmentation?\u003c/h2\u003e\n\u003cp\u003ePerhaps the most common use case for clustering algorithms in the real world, \u003cstrong\u003e\u003cem\u003eMarket Segmentation\u003c/em\u003e\u003c/strong\u003e refers to using \u003cstrong\u003e\u003cem\u003eCluster Analysis\u003c/em\u003e\u003c/strong\u003e to segment a customer base into different \u003cem\u003emarket segments\u003c/em\u003e using the clustering techniques we've learned.\u003c/p\u003e\n\u003cp\u003eConsider the following scenario: You're a movie executive, and you have a new superhero film coming out. You need to decide how to best allocate your advertising budget in order to attract the most customers. This film is a sequel, so you have good demographic data on who went to see the last film. The advertising options available to you are TV, newspaper, radio, and internet. How do you best allocate your advertising budget to ensure that the movie does as well as possible?\u003c/p\u003e\n\u003cp\u003eThe answer depends on your data. A regression analysis on last year's data can give you a general idea of how much you can expect to make overall, assuming that there aren't major differences between last year and this year. However, regression just tells you what you can expect \u003cem\u003eoverall\u003c/em\u003e -- what if we're trying to optimize where we spend our money, rather than just predict what the returns will be, based on the overall amount of money we spent?\u003c/p\u003e\n\u003cp\u003eThe answer lies in knowing who your customer is. All forms of advertising are not consumed equally by every age group or demographic. By identifying \u003cstrong\u003e\u003cem\u003esegments\u003c/em\u003e\u003c/strong\u003e in our customer data, we can look for trends that identify one group or another, and create personalized regression models for each group.\u003c/p\u003e\n\u003cp\u003eIn order to understand this better, let's take a sample question that market segmentation can help us answer. For our TV advertising budget, we still have to decide what channel to run our commercials on. What effect will advertising on the Disney channel have on a person's likelihood of coming to see our superhero movie? If the person in question is 12 years old, then it's probably very likely that our commercial convinces this person to see our movie. But what about if they're 68 years old? In that case, advertising during a cartoon on the Disney channel might not be the most effective way to reach that person. If we're worried about reaching this customer, the first question we should ask is what kind of customer they are. In the case of a superhero movie, we can likely assume that all things equal, a 12-year-old child is more likely to be interested in seeing a superhero movie after seeing our commercial than a 68-year-old, so we should probably pay attention to what the data tells us about how 12-year-olds are affected by each type of media advertisement we can use!\u003c/p\u003e\n\u003cp\u003e\u0026nbsp;\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-market-segmentation-clustering/master/images/new_old-man-little-boy-talking.png\" width=\"600\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e\u003cem\u003eTwo potential customers deep in conversation about what movie to see\u003c/em\u003e\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eYou can bet that movie studio executives have complex, well-defined models to predict their Return on Investment (RoI) for things as granular as advertising on Disney, versus advertising on the History channel. This is because different market segments of customers behave differently, and market segmentation allows us to zoom in on those groups!\u003c/p\u003e\n\u003ch3\u003eIdentifying Market Segmentation\u003c/h3\u003e\n\u003cp\u003eAt the most basic level, market segmentation allows us to look at our data and identify which customers belong to which groups. Once we have this information, we can examine each individual segment and use it to answer important questions and build individual, targeted models for each segment.\u003c/p\u003e\n\u003cp\u003eOnce we understand our market segments, then we can begin making informed decisions that are specific to each segment. So how do we find these market segments?\u003c/p\u003e\n\u003cp\u003eWith clustering, of course! By definition, market segments are groups within our dataset with substantive differences between them. A segment only matters to us if it is different from other groups -- we don't really care about identifying the segment of children with red hair versus children with brown hair in our data if they both act the same way and have the same level of interest in our movie. Before data scientists became commonplace, this sort of segmentation was usually handled by marketers using their intuition about their customer base to create \u003cem\u003ecustomer personas\u003c/em\u003e, and then seek out data to back up their assumptions. As data scientists, we know that the best option is not to seek data to confirm our beliefs -- instead, it is to pull our beliefs from evidence in the data. Clustering provides a great way for us to allow the data to tell us what is and isn't significant -- lest we get caught up chasing down market segments that aren't actually all that different -- or worse, don't actually exist at all!\u003c/p\u003e\n\u003ch2\u003eSegmentation and Targeting\u003c/h2\u003e\n\u003cp\u003eIn modern business analytics, segmentation is only the first step.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-market-segmentation-clustering/master/images/new_marketing-strategy.png\" width=\"700\"\u003e\u003c/p\u003e\n\u003cp\u003eAfter we've identified the different market segments, the next step is to build individualized strategies to \u003cstrong\u003e\u003cem\u003eTarget\u003c/em\u003e\u003c/strong\u003e them! In the movie example we used above, we would first start by answering questions such as \"which market segment is most valuable to us?\" This can be answered through research or through analyzing our data, or a combination of both. Once we realized that the 12-to-18-year-old demographic is most valuable to us, we can then decide how to target them in the most effective way possible. This brings us back to our earlier question -- how do we allocate our advertising budget? If we've used regression to determine that we're most likely to get the return on investment for our advertising dollars with the 12-to-18-year-old age group, then our next step is to determine which ad channels are most effective to us. We'll likely find that TV advertisements and internet ads are very effective at reaching this particular market segment, but radio is less effective (since a solid portion of the target segment can't yet drive), and newspaper ads are unlikely to reach them at all (because when is the last time you saw a 12-year-old read a newspaper?).\u003c/p\u003e\n\u003cp\u003eThe third step in this process is a bit outside the scope of clustering. This is where the marketing team really shines -- figuring out how to position our product to make it both as desirable as possible to a given segment, while also making our product stand out from competitors.\u003c/p\u003e\n\u003cp\u003eLet's look at one more example to consider what this looks like: car advertisements!\u003c/p\u003e\n\u003cp\u003eScenario: You are the newest data scientist at Tesla Motors. Next year, you are introducing a new SUV in the $30-50k price range. The SUV is roomy, spacious, fast, and affordable, in addition to having a very high safety rating and a ton of technological bells and whistles. One day, Elon Musk asks you (presumably, on Twitter) who your valuable market segments are, and what parts of the car he should highlight in several upcoming interviews. How do you answer this question?\u003c/p\u003e\n\u003cp\u003ePresumably, the first thing you would do is to look at the results of your market segmentation and identify the most profitable market segments to target. Once you know who these segments are, you can target them with ads -- but this only brings us to the second step in our diagram above. The third step means personalizing these ads to have maximum impact on a given targeted segment. Is your target market middle-class families? Then maybe it makes sense to highlight the car's affordability, space, and safety rating. What about if your target is upper-middle-class customers between 30 and 40 years of age that enjoy luxury cars? In that case, you'd probably focus on the speed, luxury, and looks of the car, because they're more likely to care about these qualities than the others.\u003c/p\u003e\n\u003cp\u003e\u0026nbsp;\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-market-segmentation-clustering/master/images/new_market_seg.png\" width=\"70%\" height=\"70%\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e\u003cem\u003eWhen you know your market segment, you can market to them in the most effective way possible!\u003c/em\u003e\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eStep 3 in this process is usually done with the help of survey data, under the umbrella of \u003cem\u003eUser Research\u003c/em\u003e. This is not something that data scientists typically have to worry about too much, as it is a different domain of expertise. However, the first two stages are very much something that data scientists can expect to do multiple times in their career!\u003c/p\u003e\n\u003ch2\u003eSummary\u003c/h2\u003e\n\u003cp\u003eIn this lesson, we learned about how cluster analysis can be applied to determine market segmentation, and how these market segments are used in the real world to plan and execute effective business strategies!\u003c/p\u003e\n\u003cfooter class=\"fis-footer\" style=\"visibility: hidden;\"\u003e\n\u003cdiv class=\"fis-feedback\"\u003e\n\u003ch5\u003eHow do you feel about this lesson?\u003c/h5\u003e\n\u003cimg id=\"thumbs-up\" title=\"Thumbs up!\" alt=\"thumbs up\" data-repository=\"dsc-market-segmentation-clustering\"\u003e\u003cimg id=\"thumbs-down\" title=\"Thumbs down!\" alt=\"thumbs down\" data-repository=\"dsc-market-segmentation-clustering\"\u003e\n\u003c/div\u003e\n\u003ch5\u003eHave specific feedback? \u003ca href=\"https://github.com/learn-co-curriculum/dsc-market-segmentation-clustering/issues/new\"\u003eTell us here!\u003c/a\u003e\n\u003c/h5\u003e\n\u003c/footer\u003e","exportId":"market-segmentation-with-clustering"},{"id":197249,"title":"Market Segmentation with Clustering - Lab","type":"Assignment","indent":1,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_mark_done","completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-market-segmentation-clustering-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-market-segmentation-clustering-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","exportId":"gb889aa7446b061678725d79be8b38b4c"},{"id":197253,"title":"Semi-Supervised Learning and Look-Alike Models","type":"WikiPage","indent":0,"locked":false,"requirement":"must_mark_done","completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-semi-supervised-learning-and-look-alike-models\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-semi-supervised-learning-and-look-alike-models/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eIn this lesson, we'll learn about some unsupervised learning techniques we can use to supplement our supervised learning techniques.\u003c/p\u003e\n\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\n\u003cp\u003eYou will be able to:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eIdentify appropriate use cases for semi-supervised learning \u003c/li\u003e\n\u003cli\u003eIdentify appropriate use cases for look-alike models \u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch2\u003eCombining Supervised and Unsupervised Learning\u003c/h2\u003e\n\n\u003cp\u003eFor the majority of this section, we've focused exclusively on popular unsupervised learning techniques and their most common use cases. However, in the real world, there are also plenty of examples where it works to our advantage to bring supervised and unsupervised learning algorithms together to supplement each other. In this lesson, we'll look at two common areas combining supervised and unsupervised learning algorithms that allow us to be more effective than just using them on their own. \u003c/p\u003e\n\n\u003ch2\u003eUse Case 1: Look-Alike Models\u003c/h2\u003e\n\n\u003cp\u003eAs we've learned when working with clustering algorithms, one of their most common use cases is for market segmentation. A more advanced, but similar use case is to then use these market segments to create \u003cstrong\u003e\u003cem\u003elook-alike models\u003c/em\u003e\u003c/strong\u003e to help us identify more customers or market segments that we can plausibly assume are equally valuable, due to their similarity with valuable customers or market segments we've already identified. \u003c/p\u003e\n\n\u003cp\u003eTake a look at the following infographic that provides a visual representation of look-alike modeling:\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-semi-supervised-learning-and-look-alike-models/master/images/new_look-alike-model.png\"\u003e\u003c/p\u003e\n\n\u003cp\u003eIn the example above, the dark blue smiley faces represent customer segments that we already know are valuable. These are customers that we have identified in our data, and know for a fact have been good for us. Under normal circumstances, this would mean that we can divide our customers (or, more often, potential customers) into two groups: the group we know is valuable, and everyone else, who are all unknown to us. \u003c/p\u003e\n\n\u003cp\u003eThis is where \u003cem\u003elook-alike modeling\u003c/em\u003e comes in. A look-alike model uses a distance metric of our choice to rate the similarity of each customer in our group of unknowns to customers in our known, valuable group. For customers that look extremely similar to customers in own known valuable group, we can assume with a very high likelihood that these customers will also be valuable, and should direct resources at capturing them! We'll likely also see customers that are only somewhat similar to our valuable group, which tells us that they \u003cem\u003ecould possibly be valuable\u003c/em\u003e, but we aren't sure. And finally, customers that look nothing like our known valuable customers segment, should probably be left alone.  \u003c/p\u003e\n\n\u003cp\u003eIf this sounds suspiciously like clustering to you, you are absolutely correct! Although this could also be framed as a classification or regression problem, it's quite common to see clustering used to help determine similarity. After all, if we want to build a supervised learning model to predict if an unknown customer looks like our known valuable customers, then we need plenty of labeled examples, and we don't always have that luxury! \u003c/p\u003e\n\n\u003cp\u003eIn the real-world, using look-alike models to find other customers that could potentially be valuable to us is often referred to as \u003cstrong\u003e\u003cem\u003eprospecting\u003c/em\u003e\u003c/strong\u003e. Viewed in terms of the infographic above, we would choose direct resources to market to the customers that look like our valuable customers to increase our \u003cstrong\u003e\u003cem\u003etop-of-funnel\u003c/em\u003e\u003c/strong\u003e, meaning that we are trying to increase the number of potential customers that haven't shown interest in our product or company yet but are likely to, due to their similarity to customers that already have. \u003c/p\u003e\n\n\u003ch2\u003eUse Case 2: Semi-Supervised Learning\u003c/h2\u003e\n\n\u003cp\u003eThe second use case we'll talk about combines supervised and unsupervised learning to allow us access to more (pseudo) labeled data so that we can better train our supervised learning models. This technique is called \u003cstrong\u003e\u003cem\u003esemi-supervised learning\u003c/em\u003e\u003c/strong\u003e.  You may also hear it commonly referred to as \u003cstrong\u003e\u003cem\u003eweakly supervised learning\u003c/em\u003e\u003c/strong\u003e, but it means the same thing. \u003c/p\u003e\n\n\u003cp\u003ePicture the following scenario: \u003c/p\u003e\n\n\u003cp\u003eWe are trying to build a supervised learning model, and we have 100,000 observations in our dataset. However, labels are exceedingly expensive, so only 5,000 of these 100,000 observations are labeled. In traditional supervised learning, this means that in a practical sense, we really only have a dataset of 5,000 observations, because we can't do anything with the 95,000 unlabeled examples -- or can we?\u003c/p\u003e\n\n\u003cp\u003eThe main idea behind \u003cem\u003esemi-supervised learning\u003c/em\u003e is to generate \u003cstrong\u003e\u003cem\u003epseudo-labels\u003c/em\u003e\u003c/strong\u003e that are possibly correct (at least better than random chance). To do this, we don't usually use clustering algorithms -- instead, we use our supervised learning algorithms in an unsupervised way. \u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-semi-supervised-learning-and-look-alike-models/master/images/new_semi-supervised.png\"\u003e\u003c/p\u003e\n\n\u003cp\u003eSupervised learning typically follows a set pattern:\u003c/p\u003e\n\n\u003col\u003e\n\u003cli\u003e\u003cp\u003e\u003cstrong\u003e\u003cem\u003eTrain your model on your labeled training data\u003c/em\u003e\u003c/strong\u003e. In the case of our example above, we would build the best model possible with our tiny dataset of 5,000 labeled examples. \u003c/p\u003e\u003c/li\u003e\n\u003cli\u003e\u003cp\u003e\u003cstrong\u003e\u003cem\u003eUse your trained model to generate pseudo-labels for your unlabeled data\u003c/em\u003e\u003c/strong\u003e. This means having our trained model make predictions on our 95,000 unlabeled examples. Since our trained model does better than random chance, this means that our generated pseudo-labels will be at least somewhat more correct than random chance. We can even put a number to this, by looking at the performance our trained model had on the test set. For example, if our trained model had an accuracy of ~70%, then we can assume that ~70% of the pseudo-labels will be correct, ~30% will be incorrect. \u003c/p\u003e\u003c/li\u003e\n\u003cli\u003e\u003cp\u003e\u003cstrong\u003e\u003cem\u003eCombine your labeled data and your pseudo-labeled data into a single, new dataset.\u003c/em\u003e\u003c/strong\u003e. This means that we concatenate all our labeled data of 5,000 examples with the 95,000 pseudo-labeled examples. \u003c/p\u003e\u003c/li\u003e\n\u003cli\u003e\u003cp\u003e\u003cstrong\u003e\u003cem\u003eRetrain your model on the new dataset\u003c/em\u003e\u003c/strong\u003e. Although some of the pseudo-labeled data will certainly be wrong, it's likely that the amount that is correct will be more useful, and the signal that these correctly pseudo-labeled examples provide will outweigh the incorrectly labeled ones, thereby resulting in better overall model performance. \u003c/p\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\n\u003ch3\u003eBenefits and Drawbacks of Semi-Supervised Learning\u003c/h3\u003e\n\n\u003cp\u003eIf semi-supervised learning sounds a bit risky to you, you're not wrong. When done correctly, semi-supervised learning can increase overall model performance by opening up access to much more data than we would have access to, and more data almost always results in better performance, but without the exorbitant costs of paying to have humans generate labels for the data needed. \u003c/p\u003e\n\n\u003cp\u003eHowever, there are definitely some problems that can arise from using a semi-supervised learning approach, if we're not careful and thoughtful throughout.\u003c/p\u003e\n\n\u003ch4\u003eFeedback Loops and Self-Fulfilling Prophecies\u003c/h4\u003e\n\n\u003cp\u003eSemi-supervised learning tends to work fairly well in many use cases and has become quite a popular technique in the field of Deep Learning, which requires massive amounts of labeled data that is often very expensive to obtain. But what happens when our dataset is extremely noisy to begin with? In that case, our incorrect pseudo-labels may skew the model by introducing more \"noise\" than \"signal\". This is partially because we can end up in a feedback loop of sorts. Think about an example where the model has generated an incorrect pseudo-label. If a model trained only on the real data with no pseudo-labels got this example wrong, then what happens when you train the model on the same example, but this time provide a pseudo-label that \"confirms\" this incorrect belief? When done correctly, we can hope that the signal provided by all the correctly pseudo-labeled examples will generalize to help the model correct its mistakes on the ones it got wrong. However, if the dataset is noisy, or the original model wasn't that good to begin with (or both), then it can be quite likely that we are introducing even more incorrect information than correct information, moving the model in the wrong direction.\u003c/p\u003e\n\n\u003cp\u003eSo how do we make sure that we're not making these mistakes when using a semi-supervised approach? \u003cstrong\u003e\u003cem\u003eUse a holdout set!\u003c/em\u003e\u003c/strong\u003e You should definitely have a test set that the model has never seen before to check the performance of your semi-supervised model. Obviously, make sure that your test set only contains actual, ground-truth labeled examples, no pseudo-labels allowed! Also, the noisier your dataset or more complicated your problem, the more likely you are to run into trouble with semi-supervised learning. When possible, try to structure your tasks as binary classification tasks, rather than multi-categorical, and make sure that your dataset is as clean as possible before attempting semi-supervised learning. Although it seems risky, there's a reason companies that are heavy into deep learning and AI research such as Google, Microsoft, and Facebook make heavy use of semi-supervised learning -- when done correctly, it works wonders, without costing an arm and a leg to pay for labeling!\u003c/p\u003e\n\n\u003ch2\u003eSummary\u003c/h2\u003e\n\n\u003cp\u003eIn this lesson, we learned about two popular methodologies for using unsupervised learning in applied, focused ways to help companies generate more revenue, get more customers, or increase model performance without paying for more labeled training data!\u003c/p\u003e\n\u003cfooter class=\"fis-footer\" style=\"visibility: hidden;\"\u003e\u003cdiv class=\"fis-feedback\"\u003e\n\u003ch5\u003eHow do you feel about this lesson?\u003c/h5\u003e\n\u003cimg id=\"thumbs-up\" data-repository=\"dsc-semi-supervised-learning-and-look-alike-models\" title=\"Thumbs up!\" alt=\"thumbs up\"\u003e\u003cimg id=\"thumbs-down\" data-repository=\"dsc-semi-supervised-learning-and-look-alike-models\" title=\"Thumbs down!\" alt=\"thumbs down\"\u003e\n\u003c/div\u003e\n\u003ch5\u003eHave specific feedback? \u003ca href=\"https://github.com/learn-co-curriculum/dsc-semi-supervised-learning-and-look-alike-models/issues/new\"\u003eTell us here!\u003c/a\u003e\n\u003c/h5\u003e\u003c/footer\u003e","exportId":"semi-supervised-learning-and-look-alike-models"},{"id":197255,"title":"Clustering - Recap","type":"WikiPage","indent":0,"locked":false,"requirement":"must_mark_done","completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-clustering-recap\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-clustering-recap/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\u003ch2\u003eKey Takeaways\u003c/h2\u003e\n\u003cp\u003eThe key takeaways from this section include:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eThere are two main types of clustering algorithms: non-hierarchical clustering (k-means) and hierarchical agglomerative clustering\u003c/li\u003e\n\u003cli\u003eYou can quantify the performance of a clustering algorithm using metrics such as variance ratios\u003c/li\u003e\n\u003cli\u003eWhen working with the k-means clustering algorithm, it is useful to create elbow plots to find an optimal value for \u003cimg src=\"https://render.githubusercontent.com/render/math?math=k\"\u003e\n\u003c/li\u003e\n\u003cli\u003eWhen using hierarchical agglomerative clustering, different linkage criteria can be used to determine which clusters should be merged and at what point\u003c/li\u003e\n\u003cli\u003eDendrograms and clustergrams are very useful visual tools in hierarchical agglomerative clustering\u003c/li\u003e\n\u003cli\u003eAdvantages of k-means clustering include easy implementation and speed, whereas the main disadvantage is that it isn't always straightforward how to pick the \"right\" value for \u003cimg src=\"https://render.githubusercontent.com/render/math?math=k\"\u003e\n\u003c/li\u003e\n\u003cli\u003eAdvantages of hierarchical agglomerative clustering include easy visualization and intuitiveness, whereas the main disadvantage is that the result is very distance-metric-dependent\u003c/li\u003e\n\u003cli\u003eYou can use supervised and unsupervised learning together in a few different ways. Applications of this are look-alike models in market segmentation and semi-supervised learning\u003c/li\u003e\n\u003c/ul\u003e","exportId":"clustering-recap"}]},{"id":21083,"name":"Topic 35: Big Data in PySpark","status":"unlocked","unlockDate":null,"prereqs":[],"requirement":"all","sequential":false,"exportId":"g1eae13b61f7538355bf260a8305ee166","items":[{"id":197257,"title":"Apache Spark - Introduction","type":"WikiPage","indent":0,"locked":false,"requirement":"must_mark_done","completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-spark-introduction\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-spark-introduction/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eIn this section, you will be introduced to the idea of big data and the tools data scientists use to manage it.\u003c/p\u003e\n\n\u003ch2\u003eBig Data in PySpark\u003c/h2\u003e\n\n\u003cp\u003eBig data is undoubtedly one of the most hyped terms in data science these days. Big data analytics involves dealing with data that is large in volume and high in variety and velocity, making it challenging for data scientists to run their routine analysis activities. In this section, you'll learn the basics of dealing with big data through parallel and distributed computing. In particular, you will be introduced to Apache Spark, an open-source distributed cluster-computing framework. You'll learn how to use the popular Apache Spark Python API, \u003cstrong\u003ePySpark\u003c/strong\u003e.\u003c/p\u003e\n\n\u003ch3\u003eParallel and Distributed Computing with MapReduce\u003c/h3\u003e\n\n\u003cp\u003eBefore diving into PySpark, we start this section by providing more context on the ideas of parallel and distributed computing and MapReduce. When talking about distributed and parallel computing, we refer to the fact that complex (and big) data science tasks can be executed over a cluster of interconnected computers instead of on just one machine. You'll learn that MapReduce allows us to convert these big datasets into sets of tuples as key:value pairs, as we'll cover in more detail in this section.\u003c/p\u003e\n\n\u003ch3\u003eApache Spark\u003c/h3\u003e\n\n\u003cp\u003eAs mentioned before, Apache Spark makes it easier (and feasible) to use huge amounts of data! You'll read a scientific article on the advantages of Apache Spark to understand its use and benefits better.\u003c/p\u003e\n\n\u003ch3\u003eInstalling and Configuring PySpark with Docker\u003c/h3\u003e\n\n\u003cp\u003eA big part of PySpark is actually getting PySpark up and running on your machine. You'll get an overview of how to do this so you can get started exploring distributed computing!\u003c/p\u003e\n\n\u003ch3\u003ePySpark\u003c/h3\u003e\n\n\u003cp\u003eYou'll learn about distributed and parallel computing and the different PySpark modules needed to create this parallelization.\u003c/p\u003e\n\n\u003ch3\u003eRDDs (Resilient Distributed Datasets)\u003c/h3\u003e\n\n\u003cp\u003eResilient Distributed Datasets (RDDs) are the core concept in PySpark. RDDs are immutable distributed collections of data objects. Each dataset in RDD is divided into logical partitions, which may be computed on different computers (so-called \"nodes\") in the Spark cluster. In this section, you'll learn how RDDs in Spark work. Additionally, you'll learn that RDD operations can be split into actions and transformations. \u003c/p\u003e\n\n\u003ch3\u003eWord Count with MapReduce\u003c/h3\u003e\n\n\u003cp\u003eYou'll use MapReduce to solve a basic NLP task where you compare the attributes of different authors of various texts.\u003c/p\u003e\n\n\u003ch3\u003eMachine Learning with Spark\u003c/h3\u003e\n\n\u003cp\u003eAfter you've solved a basic MapReduce problem, you will learn about employing the machine learning modules of PySpark. You will perform both a regression and classification problem and get the chance to build a full parallelizable data science pipeline that can scale to work with big data. In this section, you'll also get a chance to work with PySpark DataFrames.\u003c/p\u003e\n\n\u003ch2\u003eSummary\u003c/h2\u003e\n\n\u003cp\u003eIn this section, you'll learn the foundations of Big Data and how to manage it with Apache Spark!\u003c/p\u003e\n\u003cfooter class=\"fis-footer\" style=\"visibility: hidden;\"\u003e\u003cdiv class=\"fis-feedback\"\u003e\n\u003ch5\u003eHow do you feel about this lesson?\u003c/h5\u003e\n\u003cimg id=\"thumbs-up\" data-repository=\"dsc-spark-introduction\" title=\"Thumbs up!\" alt=\"thumbs up\"\u003e\u003cimg id=\"thumbs-down\" data-repository=\"dsc-spark-introduction\" title=\"Thumbs down!\" alt=\"thumbs down\"\u003e\n\u003c/div\u003e\n\u003ch5\u003eHave specific feedback? \u003ca href=\"https://github.com/learn-co-curriculum/dsc-spark-introduction/issues/new\"\u003eTell us here!\u003c/a\u003e\n\u003c/h5\u003e\u003c/footer\u003e","exportId":"apache-spark-introduction"},{"id":197262,"title":"Introduction to Big Data","type":"WikiPage","indent":0,"locked":false,"requirement":"must_mark_done","completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-big-data-introduction\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-big-data-introduction/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eIn the information age, data in huge quantities has become available to analysts and decision-makers. Due to a vast increase in the amount of such data in recent times, a number of specialized platforms and development paradigms have been developed that can handle big data. Using such specialist approaches allows data scientists to gain valuable insights from complex data, ranging from daily transactions to customer interactions and social network data.\u003c/p\u003e\n\n\u003cp\u003eThis section aims to focus on some of the different analytical approaches and tools data scientists apply to big data in order to gain valuable insights that aid business decision making. \u003c/p\u003e\n\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\n\u003cp\u003eYou will be able to:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eList the domain areas where big data is particularly useful \u003c/li\u003e\n\u003cli\u003eList the technologies associated with big data \u003c/li\u003e\n\u003cli\u003eDescribe the 3 V's of big data and how they differentiate big data from routine data \u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch2\u003eWhat is Big Data\u003c/h2\u003e\n\n\u003cp\u003eThe topic of \"big data\" has received a lot of hype lately, accompanied by a huge amount of interest from big businesses as it can potentially provide them data-driven decision-making abilities. Big data is one of the most discussed topics in business today across industry sectors, although it was barely known a few years ago. This lesson will focus on what big data is, why it is important, and the benefits it brings. \u003c/p\u003e\n\n\u003cp\u003eBig data is no different than normal data that we have seen so far; it's only \"bigger.\" This changes the analytical landscape that must be used as the huge size increase of the data requires specialist tools, techniques, and platforms. It helps us solve new problems and find improved ways to find answers to old problems. \u003c/p\u003e\n\n\u003ch3\u003eDefining Big Data\u003c/h3\u003e\n\n\u003cp\u003eDespite all the hype around this topic, there is no clear consensus on how to define  \u003cstrong\u003ebig data\u003c/strong\u003e. The term often gets related to business analytics and data mining for identifying relationships and associations present in huge amounts of transaction data.  \u003c/p\u003e\n\n\u003cp\u003eIn the data science domain, big data usually refers to datasets that grow so large that they become awkward to work with using traditional database management systems and analytical approaches. They are datasets whose size is beyond the ability of commonly used software tools and storage systems to capture, store, manage, as well as process the data within a tolerable elapsed time.\u003c/p\u003e\n\n\u003ch4\u003eHow Big is \"Big\" Data?\u003c/h4\u003e\n\n\u003cp\u003eBig data sizes are constantly increasing, currently ranging from a few terabytes (TB) to many petabytes (PB) of data in a single dataset. Consequently, some of the difficulties related to big data include capturing, storing, searching, sharing, analyzing, and visualizing. Today, enterprises are exploring large volumes of highly detailed data to discover trends and pieces of information considered incapable of being captured before. \u003c/p\u003e\n\n\u003cp\u003eHere are some of the examples of big data:\n- Web traffic data: Data points such as number of page views, previous web page, user information, advertisement click-through rate, pages per visit, average visit duration\n- Text data: Emails, tweets, news reports, voice recordings, and text gathered from crawling the web can make massive datasets that are valuable to data scientists\n- Location and time data: GPS data helps Google determine which roads have higher traffic and which businesses will be busier at certain hours\n- Social network data: Using the information of relationships between users on Facebook, LinkedIn, Twitter, Reddit, and countless other websites and apps\n- Smart grid and sensor data: With the advent of the Internet of Things (IoT), more and more devices are able to record data at all times, making it possible to gather lots of data instantaneously\u003c/p\u003e\n\n\u003ch2\u003e3 V's of Big Data\u003c/h2\u003e\n\n\u003cp\u003eDoug Laney published a \u003ca href=\"https://blogs.gartner.com/doug-laney/files/2012/01/ad949-3D-Data-Management-Controlling-Data-Volume-Velocity-and-Variety.pdf\"\u003epaper\u003c/a\u003e on three defining characteristics of big data. Three main features characterize big data: volume, variety, and velocity, or the three Vs. The volume of the data is its size, and how enormous it is. Velocity refers to the rate with which data is changing, or how often it is created. Finally, variety includes the different formats and types of data, as well as the different kinds of uses and ways of analyzing the data:\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-big-data-introduction/master/./images/3_components.png\" width=\"500\"\u003e\u003c/p\u003e\n\n\u003cp\u003eLet's look a bit deeper into what these 3 V's refer to:\u003c/p\u003e\n\n\u003ch3\u003eVOLUME\u003c/h3\u003e\n\n\u003cp\u003eVolume refers to the \u003cstrong\u003eamount of data\u003c/strong\u003e generated through websites, portals, and online applications in a data-driven business. Especially for online retailers, volume encompasses the available data that are out there and need to be assessed for relevance. \u003c/p\u003e\n\n\u003cp\u003eConsider the following:\u003c/p\u003e\n\n\u003cp\u003eAs of 2019, Facebook has 2.32 billion users, Youtube: 1.9 billion users, WhatsApp: 1.6 billion users and Instagram: 1 billion users. Every day, these users contribute to billions of images, posts, videos, tweets, etc. You can now imagine the insanely large amount (or \u003cstrong\u003ev\u003c/strong\u003eolume) of data that is generated every minute around the world.\nData volume is the primary attribute of big data. Big data can be quantified by size in Terabytes (TBs) or Petabytes (PBs), as well as even the number of records, transactions, tables, or files. Additionally, one of the things that makes big data really big is that its coming from a greater variety of sources than ever before, including logs, clickstreams, and social media as we will see below. \u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-big-data-introduction/master/./images/rank_users.png\" width=\"650\"\u003e\u003c/p\u003e\n\n\u003ch3\u003eVELOCITY\u003c/h3\u003e\n\n\u003cp\u003eVelocity refers to the speed with which data is generated, and as internet speeds have increased and the number of users has increased, the velocity has also increased substantially.\u003c/p\u003e\n\n\u003cp\u003eThe following image created by \u003ca href=\"https://www.allaccess.com/merge/archive/29580/2019-this-is-what-happens-in-an-internet-minute\"\u003eLori Lewis and Chadd Callahan\u003c/a\u003e shows what happens on major social media platforms in one minute. \u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-big-data-introduction/master/./images/internet_minute.jpg\" width=\"500\"\u003e\u003c/p\u003e\n\n\u003cp\u003eVelocity is basically the frequency of data generation or the frequency of data delivery. The leading edge of\nbig data is streaming data, which is collected in real-time from the websites. \u003c/p\u003e\n\n\u003cp\u003eTools within the big data stack help companies hold this explosion in velocity, accept the incoming flow of data, and at the same time process it quickly enough so that it does not create bottlenecks.\u003c/p\u003e\n\n\u003ch3\u003eVARIETY\u003c/h3\u003e\n\n\u003cp\u003eVariety in big data refers to all the structured and unstructured data that has the possibility of getting generated either by humans or by machines. Structured data is whatever data you could store in a spreadsheet. It can easily be cataloged and summary statistics can be calculated for it. Unstructured data are raw things like texts, tweets, pictures, videos, emails, voice mails, hand-written text, ECG readings, and audio recordings. Humans can only make sense of data that is structured, and it is usually up to data scientists to create some organization and structure to unstructured data.\u003c/p\u003e\n\n\u003cp\u003eVariety is all about the ability to classify the incoming data into various categories and turn unstructured data into something with more structure.\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-big-data-introduction/master/./images/unstructured_data.png\" width=\"500\"\u003e\u003c/p\u003e\n\n\u003cp\u003eThis leads us to the most widely used definition in the industry by Gartner: \u003c/p\u003e\n\n\u003ch3\u003e\n\u003cem\u003eBig data is high-volume, high-velocity and/or high-variety information assets that demand cost-effective, innovative forms of information processing that enable enhanced insight, decision making, and process automation\u003c/em\u003e.\u003c/h3\u003e\n\n\u003cp\u003eAny data sources that fall under those 3 Vs are sources of big data, no matter how you define it.\u003c/p\u003e\n\n\u003cp\u003e\u003cstrong\u003eNOTE\u003c/strong\u003e: \u003cem\u003eSome researchers have discussed the addition of a fourth V, or \u003cstrong\u003eVeracity\u003c/strong\u003e. Veracity focuses on the quality of the data. This characterizes big data quality as good, bad, or undefined due to data inconsistency, incompleteness, ambiguity, latency, deception, and approximations\u003c/em\u003e\u003c/p\u003e\n\n\u003cp\u003eThe important thing to remember from this three-pronged definition of Big Data is that there is not a single component that makes data \"big\" or not. It is also a futile effort to try and make a definition of what the threshold is to make something \"big data\" rather than normal data. As technologies evolve and new distributed algorithms are created, what was once big data will no longer be big, and we will raise the bar.\u003c/p\u003e\n\n\u003ch2\u003eBig Data Analytics\u003c/h2\u003e\n\n\u003cp\u003eWith the evolution of technology and the increased amounts of data, as discussed above, the need for faster and more efficient ways of analyzing such data has also grown exponentially. Having big data \u003cstrong\u003ealone\u003c/strong\u003e is no longer enough to make efficient decisions at the right time. As we mentioned above, Big Data cannot be easily analyzed with traditional data management and analysis techniques and infrastructures. Therefore, there arises a need for new\ntools and methods specialized for big data analytics, as well as the required architectures for storing and managing such data. Accordingly, the emergence of big data has an effect on everything from the data itself to its collection, analysis, and visualization, as well as the final extracted decisions.\u003c/p\u003e\n\n\u003cp\u003eThe image below shows the technology stack, or the key tools and platforms heavily being employed in big data analytics today. \u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-big-data-introduction/master/./images/tech_stack.png\" width=\"800\"\u003e\u003c/p\u003e\n\n\u003cp\u003eExplaining each one of these tools/platforms etc. is outside the scope of this lesson. You are, however, encouraged to look up these technologies and see their role in big data analytics. Such a stack maps the different big data storage, management, analytics tools/methods, visualization, and evaluation tools to the different phases of the decision-making process. \u003c/p\u003e\n\n\u003cp\u003eThe key activities associated with big data analytics are reflected in four main areas: \u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eBig data warehousing and distribution\u003c/li\u003e\n\u003cli\u003eBig data storage\u003c/li\u003e\n\u003cli\u003eBig data computational platforms\u003c/li\u003e\n\u003cli\u003eBig data analyses, visualization, and evaluation\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003eSuch a framework can be applied for knowledge discovery and informed decision-making in big data-driven organizations.\u003c/p\u003e\n\n\u003ch3\u003eExample Business Applications of Big Data Analytics\u003c/h3\u003e\n\n\u003cp\u003eAlong with some of the most common advanced data analytics methods such as regression analysis, association rules, clustering, and classification, some additional analyses have become common with big data.\u003c/p\u003e\n\n\u003cp\u003eFor example, social media has recently become important for social networking and content sharing. Yet, the content that is generated from social media websites is enormous and remains largely unexploited. However, social media analytics can be used to analyze such data and extract useful information and predictions.\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-big-data-introduction/master/./images/social_media.png\" width=\"600\"\u003e\u003c/p\u003e\n\n\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003eSocial media analytics\u003c/strong\u003e is based on developing and evaluating informatics frameworks and tools in order to collect, monitor, summarize, analyze, as well as visualize social media data. \u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003cp\u003eSocial media analytics facilitates understanding the reactions and conversations between people in online communities, as well as extracting useful patterns and intelligence from their interactions and what they share on social media websites.\u003c/p\u003e\n\n\u003cp\u003eOn the other hand, \u003cstrong\u003etext mining\u003c/strong\u003e and \u003cstrong\u003eNLP\u003c/strong\u003e techniques are used to analyze a document or set of documents in order to understand the content within and the meaning of the information contained. Text mining has become very important nowadays since much of the information stored consists of text - in the form of emails, SMS texts, social media feeds, blogs, etc. While data mining deals with structured data, text presents special characteristics which basically follow a non-relational form and require wisely thought-out schemas to grant it more structure.\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-big-data-introduction/master/./images/nlp.png\" width=\"200\"\u003e\u003c/p\u003e\n\n\u003cp\u003e\u003cstrong\u003eSentiment analysis/opinion mining\u003c/strong\u003e is also becoming more and more important as online opinion data, such as blogs, product reviews, forums, and social data from social media sites, like Twitter and Facebook, grow tremendously. \u003c/p\u003e\n\n\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003eSentiment Analysis\u003c/strong\u003e focuses on analyzing and understanding emotions from subjective text patterns and is enabled through text mining. It identifies the opinions and attitudes of individuals towards certain topics, and it is useful in classifying viewpoints as positive or negative. \u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-big-data-introduction/master/./images/sentiment_2.png\" width=\"500\"\u003e\u003c/p\u003e\n\n\u003cp\u003eSentiment analysis uses NLP and text analytics in order to identify and extract information by finding words that are indicative of certain sentiments, as well as relationships between words so that sentiments can be accurately identified.\u003c/p\u003e\n\n\u003cp\u003eAnd finally, one of the leading applications in big data analytics is \u003cstrong\u003erecommendation systems\u003c/strong\u003e. Powerful recommendation engines can be built for anything from movies and videos to music, books, and products as offered by Netflix, Pandora, or Amazon. As customers of an online retailer browse through products, the Recommendation system offers recommendations of products they might be interested in. In our daily online browsing and shopping routine, most of us often come across messages like the one shown below. This is a recommendation system doing its job. \u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-big-data-introduction/master/./images/rec.png\" width=\"800\"\u003e\u003c/p\u003e\n\n\u003cp\u003eRecommendation systems have been immensely beneficial for both businesses and consumers. Big data is the driving force behind recommendation systems. A typical recommendation system cannot do its job without sufficient data and big data supplies plenty of user data such as past purchases, browsing history, and feedback for the recommendation systems to provide relevant and effective recommendations. In a nutshell, even the most advanced recommendations cannot be effective without big data.\u003c/p\u003e\n\n\u003ch3\u003eSo what's next?\u003c/h3\u003e\n\n\u003cp\u003eAfter this quick introduction, we will look at Map-Reduce, a distributed computation platform designed to incorporate big data analytics and how it is used by Hadoop/Apache Spark development environments to analyze big data. \u003c/p\u003e\n\n\u003ch2\u003eAdditional Reading\u003c/h2\u003e\n\n\u003cp\u003eBig data is a huge subject and incorporates a lot of underlying technologies and principles. You are advised to visit the following resources and read up on big data to develop a sound and holistic understanding of the domain. \u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003e\u003cp\u003e\u003ca href=\"https://www.youtube.com/watch?v=0cizsKDn3TI\"\u003eYoutube: Big Data Trap\u003c/a\u003e - Highly recommended, an excellent lecture on the social dimension of big data.\u003c/p\u003e\u003c/li\u003e\n\u003cli\u003e\u003cp\u003e\u003ca href=\"https://www.educba.com/big-data-vs-data-science/\"\u003eBig Data vs Data Science\u003c/a\u003e - How to relate big data analytics to routine analytics that we have so far!\u003c/p\u003e\u003c/li\u003e\n\u003cli\u003e\u003cp\u003e\u003ca href=\"https://pdfs.semanticscholar.org/d392/0f02dbb15da19b04d782fc0546ef113e0bf7.pdf\"\u003eBig Data Analytics\u003c/a\u003e - A great paper summarizing big-data-related terms, ideas, etc. \u003c/p\u003e\u003c/li\u003e\n\u003cli\u003e\u003cp\u003e\u003ca href=\"https://www.ntnu.no/iie/fag/big/lessons/lesson2.pdf\"\u003eIntroduction to Big Data\u003c/a\u003e - A paper discussing the basics of big data\u003c/p\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch2\u003eSummary\u003c/h2\u003e\n\n\u003cp\u003eIn this introductory lesson on big data, we looked at what data qualifies at \"big data\". We looked at how it is hard to come up with a standard definition of big data due to the variety of its applications and use cases. Up next, we will get into how we actually make parallelizable applications that are efficient with big data. \u003c/p\u003e\n\u003cfooter class=\"fis-footer\" style=\"visibility: hidden;\"\u003e\u003cdiv class=\"fis-feedback\"\u003e\n\u003ch5\u003eHow do you feel about this lesson?\u003c/h5\u003e\n\u003cimg id=\"thumbs-up\" data-repository=\"dsc-big-data-introduction\" title=\"Thumbs up!\" alt=\"thumbs up\"\u003e\u003cimg id=\"thumbs-down\" data-repository=\"dsc-big-data-introduction\" title=\"Thumbs down!\" alt=\"thumbs down\"\u003e\n\u003c/div\u003e\n\u003ch5\u003eHave specific feedback? \u003ca href=\"https://github.com/learn-co-curriculum/dsc-big-data-introduction/issues/new\"\u003eTell us here!\u003c/a\u003e\n\u003c/h5\u003e\u003c/footer\u003e","exportId":"introduction-to-big-data"},{"id":197266,"title":"Parallel and Distributed Computing with MapReduce","type":"WikiPage","indent":0,"locked":false,"requirement":"must_mark_done","completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-parallel-and-distributed-computing-with-mapreduce\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-parallel-and-distributed-computing-with-mapreduce/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eMapReduce is a programming paradigm that enables the ability to scale across hundreds or thousands of servers for big data analytics. The underlying concept can be somewhat difficult to grasp, because this paradigm differs from the traditional programming practices. This lesson aims to present a simple yet intuitive account of MapReduce that we shall put into practice in upcoming labs.\u003c/p\u003e\n\u003cp\u003e\u003cem\u003eIn a nutshell, the term \"MapReduce\" refers to two distinct tasks. The first is the \u003cstrong\u003eMap\u003c/strong\u003e job, which takes one set of data and transforms it into another set of data, where individual elements are broken down into tuples \u003cstrong\u003e(key/value pairs)\u003c/strong\u003e, while the \u003cstrong\u003eReduce\u003c/strong\u003e job takes the output from a map as input and combines those data tuples into a smaller set of tuples.\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003eWe'll see this with help of some simple examples in this lesson.\u003c/p\u003e\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\u003cp\u003eYou will be able to:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eExplain how the MapReduce paradigm works and how it differs from traditional programming approaches\u003c/li\u003e\n\u003cli\u003eExplain what is meant by distributed and parallel processing\u003c/li\u003e\n\u003cli\u003eUse MapReduce with a simple word count example\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003eParallel and Distributed Processing\u003c/h2\u003e\n\u003cp\u003eThe MapReduce programming paradigm is designed to allow \u003cstrong\u003eparallel and distributed processing\u003c/strong\u003e of large sets of data (also known as big data). MapReduce allows us to convert such big datasets into sets of \u003cstrong\u003etuples\u003c/strong\u003e as \u003cstrong\u003ekey:value\u003c/strong\u003e pairs, as we'll see shortly. These pairs are analogous to the data structures we saw with dictionaries and JSON files etc. These tuples are \u003cstrong\u003emapped\u003c/strong\u003e and \u003cstrong\u003ereduced\u003c/strong\u003e in a computational environment to allow distributed execution of complex tasks on a group (cluster) of interconnected computers.\u003c/p\u003e\n\u003cp\u003eSo in simpler terms, \u003cem\u003eMapReduce uses parallel distributed computing to turn big data into regular data.\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003eLet's first see what we mean by parallel and distributed processing below.\u003c/p\u003e\n\u003ch3\u003eDistributed Processing Systems\u003c/h3\u003e\n\u003cblockquote\u003e\n\u003cp\u003eA distributed processing system is a group of computers in a network working in tandem to accomplish a task\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eWhen computers are in a distributed system, they do not share hard drive memory or processing memory; they communicate with one other through messages, which are transferred over a network. The individual computers in this network are referred to as \u003cstrong\u003enodes\u003c/strong\u003e. As you've seen before, computers can send requests as well as packets of data to one another.\u003c/p\u003e\n\u003cp\u003eThe two most common ways of organizing computers into a distributed system are the client-server system and peer-to-peer system.\u003c/p\u003e\n\u003cp\u003eThe client-server architecture has nodes that make requests to a central server. The server will then decide to accept or reject these requests and send additional methods out to the outer nodes.\u003c/p\u003e\n\u003cp\u003ePeer-to-peer systems allow nodes to communicate with one another directly without requiring approval from a server.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-parallel-and-distributed-computing-with-mapreduce/master/images/types_of_network.png\"\u003e\u003c/p\u003e\n\u003ch3\u003eParallel Processing Systems\u003c/h3\u003e\n\u003cp\u003eThese networks are useful for many applications all over the web, but they are generally ill-suited for dealing with the processing power required for very large sets of data and complex problems.\u003c/p\u003e\n\u003cp\u003eJust like in the workplace, whenever there is an extremely complex task, it is best to divide and conquer. In the world of big data, if the data is \"big\" enough, it is generally better to take the approach of splitting up the larger task into smaller pieces.\u003c/p\u003e\n\u003cp\u003eEven though individual processors are getting faster (remember \u003ca href=\"https://en.wikipedia.org/wiki/Moore%27s_law\"\u003eMoore's Law\u003c/a\u003e), they will never have the ability to keep up with the amount of data we are able to produce. The best solution computer scientists have developed has been to use the power of \u003cstrong\u003emultiple processors\u003c/strong\u003e to put them to the same task. When using a well-developed distributed system, multiple processors can accomplish tasks at a fraction of the time it would take for a single processor to accomplish. As noted in the picture below, if you can divide the work between multiple processors, everything will be more efficient.\u003c/p\u003e\n\u003cp\u003eWith parallel computing:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003ea larger problem is broken up into smaller pieces\u003c/li\u003e\n\u003cli\u003eevery part of the problem follows a series of instructions\u003c/li\u003e\n\u003cli\u003eeach one of the instructions is executed simultaneously on different processors\u003c/li\u003e\n\u003cli\u003eall of the answers are collected from the small problems and combined into one final answer\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eIn the image below, you can see a simple example of a process being broken up and completed both sequentially and in parallel.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-parallel-and-distributed-computing-with-mapreduce/master/images/parallel.png\"\u003e\u003c/p\u003e\n\u003cp\u003eOf course, not all problems can be parallelized, but there are some that are formally called \u003ca href=\"https://en.wikipedia.org/wiki/Embarrassingly_parallel\"\u003eembarrassingly parallel\u003c/a\u003e problems that require hardly any effort to ensure that a certain task is able to easily parallelizable. One example of something that would be embarrassingly parallelizable would be password cracking. Another example would be a movie production company trying to calculate the total profit they made from all of the movies they released in a given year. Let's think about all of the components that go into determining whether or not a movie is profitable.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003estory rights\u003c/li\u003e\n\u003cli\u003eproducer\u003c/li\u003e\n\u003cli\u003edirector\u003c/li\u003e\n\u003cli\u003ecast\u003c/li\u003e\n\u003cli\u003eproduction costs\u003c/li\u003e\n\u003cli\u003evisual effects\u003c/li\u003e\n\u003cli\u003emusic\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eand of course\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003ebox office revenue\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eHere is what this would look like if it was calculated sequentially.\u003c/p\u003e\n\u003cp\u003eIf a movie studio was to compute each one it's movie's profits sequentially, it would take far more time than if it calculated each movie's profit and combined them in parallel.\u003c/p\u003e\n\u003cp\u003eHere is a diagram of what parallel processing looks like in action:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-parallel-and-distributed-computing-with-mapreduce/master/images/parallel_movies_.png\"\u003e\u003c/p\u003e\n\u003cp\u003eSo how can we make all these nodes communicate with one another? By using a programming paradigm called MapReduce!\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eMapReduce\u003c/strong\u003e is a software framework developed for processing datasets that qualify as \"Big Data\", in a \u003cstrong\u003edistributed and parallel\u003c/strong\u003e processing environment over several computers/nodes connected to each other as part of a \u003cstrong\u003ecluster\u003c/strong\u003e. It is a specific instance of the generalized split-apply-combine technique used to perform different data analyses.\u003c/p\u003e\n\u003cp\u003eWe will soon look into a simple example that is shown to introduce MapReduce, \u003cstrong\u003eThe Word Count Problem\u003c/strong\u003e. The overall concept of MapReduce is very simple yet very powerful as:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eSomehow, all data can be mapped to \u003cstrong\u003ekey:value\u003c/strong\u003e pairs\u003c/li\u003e\n\u003cli\u003eKeys and values themselves can be of ANY data type\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eFor our example, let's say a national association of zoos wants to determine the total number of species of animals in the country. After receiving responses from every zoo in the country, a data scientist in charge of receives a large file that has a different zoo located on each line with the species at that location.\u003c/p\u003e\n\u003cp\u003eHere are the first five zoos the data scientist reads over in the data document they receive:\u003c/p\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003eAnimals\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd\u003elion tiger bear\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003elion giraffe\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003egiraffe penguin\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003epenguin lion giraffe\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003ekoala giraffe\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003cp\u003eLet's now look at how you would use the MapReduce framework in this simple word count example that could be generalized to much more data.\u003c/p\u003e\n\u003cp\u003eWe'll take a look at an image of this process in action and determine what's actually going on.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-parallel-and-distributed-computing-with-mapreduce/master/images/word_count.png\"\u003e\u003c/p\u003e\n\u003ch3\u003e1. MAP Task (Splitting \u0026amp; Mapping)\u003c/h3\u003e\n\u003cp\u003eThe dataset that needs processing must first be transformed into \u003cstrong\u003ekey:value\u003c/strong\u003e pairs and split into fragments, which are then assigned to map tasks. Each computing cluster is assigned a number of map tasks, which are subsequently distributed among its nodes. In this example, let's assume that we are using 5 nodes (a server with 5 different workers).\u003c/p\u003e\n\u003cp\u003eFirst, split the data from one file or files into however many nodes are being used.\u003c/p\u003e\n\u003cp\u003eWe will then use the map function to create key:value pairs represented by:\u003cbr\u003e\u003cem\u003e{animal}\u003c/em\u003e , \u003cem\u003e{# of animals per zoo}\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003eAfter processing of the original key:value pairs, some \u003cstrong\u003eintermediate\u003c/strong\u003e key:value pairs are generated. The intermediate key:value pairs are \u003cstrong\u003esorted by their key values\u003c/strong\u003e to create a new list of key:value pairs.\u003c/p\u003e\n\u003ch3\u003e2. Shuffling\u003c/h3\u003e\n\u003cp\u003eThis list from the map task is divided into a new set of fragments that sorts and shuffles the mapped objects into an order or grouping that will make it easier to reduce them. \u003cstrong\u003eThe number of these new fragments will be the same as the number of the reduce tasks\u003c/strong\u003e.\u003c/p\u003e\n\u003ch3\u003e3. REDUCE Task (Reducing)\u003c/h3\u003e\n\u003cp\u003eNow, every properly shuffled segment will have a reduce task applied to it. After the task is completed, the final output is written onto a file system. The underlying file system is usually HDFS (Hadoop Distributed File System).\u003c/p\u003e\n\u003cp\u003eIt's important to note that MapReduce will generally only be powerful when dealing with large amounts of data. When working with a small dataset, it will be faster not to perform operations in the MapReduce framework.\u003c/p\u003e\n\u003cp\u003eThere are two groups of entities in this process to ensuring that the MapReduce task gets done properly:\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eJob Tracker\u003c/strong\u003e: a \"master\" node that informs the other nodes which map and reduce jobs to complete\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eTask Tracker\u003c/strong\u003e: the \"worker\" nodes that complete the map and reduce operations\u003c/p\u003e\n\u003cp\u003eThere are different names for these components depending on the technology used, but there will always be a master node that informs worker nodes what tasks to perform.\u003c/p\u003e\n\u003cp\u003eA general pseudocode for a word count map and reduce tasks would look like\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"python\"\u003edef map( doc ) :\n    for word in doc.split( ' ' ) :\n    emit ( word , 1 )\n\ndef reduce( key , values ) :\n    emit ( key , sum( values ) )\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eSimilarly, we can discuss combining several MapReduce jobs in order to complete a given task. This means that once the first MapReduce job is finished, the output will become an input for the second MapReduce job and that output could be the final result (or fed into another MapReduce job).\u003c/p\u003e\n\u003cp\u003eLet's assume that we would like to extend the word count program and we would like to count all words in a given Twitter dataset. The first MapReduce will read our twitter data and extract the tweets' text. The second MapReduce is the word count Map-Reduce which will analyze the Twitter dataset and produce the statistics about it. So it is simply chaining together multiple jobs.\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003eInputFile -\u0026gt; Map-1 -\u0026gt; Reduce-1 -\u0026gt; output-1 -\u0026gt; Map-2 - \u0026gt; Reduce-2 -\u0026gt; output-2 -\u0026gt; ... Map-x -\u0026gt; Reduce-x\u003c/strong\u003e\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eNext, we are going to look at Apache Spark, which adds extra features of security and fault tolerance to its MapReduce offering, making it an industry standard. We will also look at programming for the aforementioned word count problem.\u003c/p\u003e\n\u003ch2\u003eAdditional Resources\u003c/h2\u003e\n\u003cp\u003eVisit following external links to read about the previous descriptions and examples in more detail.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e\u003ca href=\"https://www.tutorialspoint.com/map_reduce/map_reduce_introduction.htm\"\u003eMapReduce Introduction\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003ca href=\"https://www.guru99.com/introduction-to-mapreduce.html\"\u003eWhat is MapReduce? How it Works\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003eSummary\u003c/h2\u003e\n\u003cp\u003eIn this lesson, we looked at how MapReduce allows a programming paradigm quite different than traditional programming practices, yet very powerful and effective towards processing large amounts of data. Next, we will look at the Spark programming environment and some coding exercises to get grips with PySpark programming.\u003c/p\u003e","exportId":"parallel-and-distributed-computing-with-mapreduce"},{"id":197267,"title":"Big Data Analytics on Apache Spark","type":"WikiPage","indent":0,"locked":false,"requirement":"must_mark_done","completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-big-data-analytics-apache-spark\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-big-data-analytics-apache-spark/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eBig data analytics is an emerging area of interest both for business and academia. There are a lot of details around the characteristics of big data and how Apache Spark eases up the job of analyzing huge amounts of data using a simple programming paradigm. In this section, we will look at understanding and implementing a simple problem using MapReduce in PySpark. Real world problems, however, are much more complicated than this and you should be able to scale up the takeaways from the simple word count example we will complete to much bigger problems. This lesson aims to provide you with a wider understanding of MapReduce and big data computation in the Apache Spark environment.\u003c/p\u003e\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\u003cp\u003eYou will be able to:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eDescribe the role of Apache Spark in Big data analytics\u003c/li\u003e\n\u003cli\u003eList some of the Spark functionalities\u003c/li\u003e\n\u003cli\u003eDescribe the role of RDDs in spark\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eFor this lesson, you are required to read the following review article:\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://link.springer.com/article/10.1007/s41060-016-0027-9\"\u003ehttps://link.springer.com/article/10.1007/s41060-016-0027-9\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eInternational Journal of Data Science and Analytics, November 2016, Volume 1, Issue 34, pp 145164. Authors: Salman Salloum, Ruslan Dautov, Xiaojun Chen, Patrick Xiaogang Peng, Joshua Zhexue Huang\u003c/p\u003e\n\u003cp\u003e\u003cem\u003e\"In this paper, we present a technical review on big data analytics using Apache Spark. This review focuses on the key components, abstractions and features of Apache Spark. More specifically, it shows what Apache Spark has for designing and implementing big data algorithms and pipelines for machine learning, graph analysis and stream processing. In addition, we highlight some research and development directions on Apache Spark for big data analytics.\"\u003c/em\u003e - from the abstract. Here is an image from the paper giving a general overview of how the spark ecosystem functions:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-big-data-analytics-apache-spark/master/images/spark.gif\"\u003e\u003c/p\u003e\n\u003cp\u003eYou are expected to spend around 90 - 120 minutes reading this article. It is an excellent article and all the key aspects of spark computational environment are summarized and presented in an excellent manner.\u003c/p\u003e\n\u003ch2\u003eSummary\u003c/h2\u003e\n\u003cp\u003eIn this lesson, you read the scientific article \"Big Data Analytics on Apache Spark\", which covers the key aspects of Spark's computational environment. You'll now move on to working with Spark through Python.\u003c/p\u003e","exportId":"big-data-analytics-on-apache-spark"},{"id":197270,"title":"Installing and Configuring PySpark with Docker","type":"Assignment","indent":0,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_mark_done","completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-spark-docker-installation\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-spark-docker-installation/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","exportId":"g7cb9fa827af71c29f528e875448b7abf"},{"id":197274,"title":"Understanding Spark Context - Lab","type":"Assignment","indent":1,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_mark_done","completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-sparkcontext-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-sparkcontext-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","exportId":"g968c20c24c5b4f090ed4f1eb53ee98df"},{"id":197277,"title":"Resilient Distributed Datasets (RDDs) - Lab","type":"Assignment","indent":1,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_mark_done","completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-resilient-distributed-datasets-rdd-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-resilient-distributed-datasets-rdd-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","exportId":"gb8717a1aec7738ff1870579ddf51b133"},{"id":197280,"title":"Word Count with MapReduce - Lab","type":"Assignment","indent":1,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_mark_done","completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-word-count-with-map-reduce-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-word-count-with-map-reduce-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","exportId":"g0e8c2fc0851dfc42e4d00c386b41ff1a"},{"id":197285,"title":"Machine Learning with Spark","type":"Assignment","indent":0,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_mark_done","completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-machine-learning-with-spark\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-machine-learning-with-spark/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","exportId":"g84ed5ca72401403a935312139bcef300"},{"id":197289,"title":"Machine Learning with Spark -  Lab","type":"Assignment","indent":1,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_mark_done","completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-machine-learning-with-spark-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-machine-learning-with-spark-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","exportId":"g395b717b16a2407d6667a519246a8e78"},{"id":197291,"title":"Apache Spark - Recap","type":"WikiPage","indent":0,"locked":false,"requirement":"must_mark_done","completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-spark-section-recap\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-spark-section-recap/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\u003ch2\u003eKey Takeaways\u003c/h2\u003e\n\n\u003cp\u003eThe key takeaways from this section include:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eBig Data usually refers to datasets that grow so large that they become awkward to work with using traditional database management systems and analytical approaches\u003c/li\u003e\n\u003cli\u003eBig data refers to data that is terabytes (TB) to petabytes (PB) in size\u003c/li\u003e\n\u003cli\u003eMapReduce can be used to split big datasets up in smaller sets to be distributed over several machines to deal with Big Data Analytics \u003c/li\u003e\n\u003cli\u003eBefore starting to work, you need to install Docker and Kinematic on your environment\u003c/li\u003e\n\u003cli\u003eMake sure to test your installation so you're sure everything is working\u003c/li\u003e\n\u003cli\u003eWhen you start working with PySpark, you have to create a \u003ccode\u003eSparkContext()\u003c/code\u003e \u003c/li\u003e\n\u003cli\u003eThe creation or RDDs is essential when working with PySpark\u003c/li\u003e\n\u003cli\u003eExamples of actions and transformations include \u003ccode\u003ecollect()\u003c/code\u003e, \u003ccode\u003ecount()\u003c/code\u003e, \u003ccode\u003efilter()\u003c/code\u003e, \u003ccode\u003efirst()\u003c/code\u003e, \u003ccode\u003etake()\u003c/code\u003e, and \u003ccode\u003ereduce()\u003c/code\u003e \u003c/li\u003e\n\u003cli\u003eMachine Learning on the scale of big data can be done with Spark using the \u003ccode\u003eml\u003c/code\u003e library\u003c/li\u003e\n\u003c/ul\u003e\n\u003cfooter class=\"fis-footer\" style=\"visibility: hidden;\"\u003e\u003cdiv class=\"fis-feedback\"\u003e\n\u003ch5\u003eHow do you feel about this lesson?\u003c/h5\u003e\n\u003cimg id=\"thumbs-up\" data-repository=\"dsc-spark-section-recap\" title=\"Thumbs up!\" alt=\"thumbs up\"\u003e\u003cimg id=\"thumbs-down\" data-repository=\"dsc-spark-section-recap\" title=\"Thumbs down!\" alt=\"thumbs down\"\u003e\n\u003c/div\u003e\n\u003ch5\u003eHave specific feedback? \u003ca href=\"https://github.com/learn-co-curriculum/dsc-spark-section-recap/issues/new\"\u003eTell us here!\u003c/a\u003e\n\u003c/h5\u003e\u003c/footer\u003e","exportId":"apache-spark-recap"}]},{"id":21086,"name":"Topic 36: Recommendation Systems","status":"unlocked","unlockDate":null,"prereqs":[],"requirement":"all","sequential":false,"exportId":"ge064c31631220ee3c4dd014f4a52379d","items":[{"id":197298,"title":"Recommendation Systems - Introduction","type":"WikiPage","indent":0,"locked":false,"requirement":"must_mark_done","completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-recommender-section-intro\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-recommender-section-intro/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eIn this section, you'll learn about machine learning algorithms that we encounter every day in our lives:  recommendation systems! \u003c/p\u003e\n\n\u003ch2\u003eDeveloping a Recommendation System in PySpark\u003c/h2\u003e\n\n\u003cp\u003eIn this section you'll learn about recommendation system modeling approaches and you'll even build your own recommendation system in PySpark! \u003c/p\u003e\n\n\u003ch3\u003eRecommendation Systems\u003c/h3\u003e\n\n\u003cp\u003eA recommendation system allows predicting the future preference list for a certain customer or user, and recommends the top preference for this user. Examples include: which books would a customer prefer to buy on Amazon, which Netflix movie or series would a user watch next, etc. You'll learn about several different types of recommendation system algorithms and how they work.\u003c/p\u003e\n\n\u003ch3\u003eCollaborative Filtering with Singular Value Decomposition\u003c/h3\u003e\n\n\u003cp\u003eCollaborative Filtering (CF) is currently the most widely used approach to build recommendation systems. It often uses matrix factorization under the hood, which you'll learn about in this section.\u003c/p\u003e\n\n\u003ch3\u003eImplementing Recommender Systems with \u003ccode\u003esurprise\u003c/code\u003e\n\u003c/h3\u003e\n\n\u003cp\u003e\u003ccode\u003esurprise\u003c/code\u003e is a library that is optimized to efficiently create recommendations. You'll get a chance to use this library to code up different implementations of collaborative filtering recommendation systems.\u003c/p\u003e\n\n\u003ch3\u003eMatrix Factorization with Alternating Least Squares\u003c/h3\u003e\n\n\u003cp\u003eWe'll also look at another matrix factorization technique called Alternating Least Squares (ALS). This method can prove to be much more effective and robust than SVD. You'll also learn how ALS is implemented in Spark's machine learning library \u003ccode\u003eml\u003c/code\u003e.\u003c/p\u003e\n\n\u003ch3\u003eBuilding a Recommendation System in PySpark\u003c/h3\u003e\n\n\u003cp\u003eYou'll end this section by building your own recommendation system in PySpark using the \u003ccode\u003eMovieLens\u003c/code\u003e rating dataset!\u003c/p\u003e\n\n\u003ch2\u003eSummary\u003c/h2\u003e\n\n\u003cp\u003eIn this section, you'll learn the basics of recommendation systems and how to implement them in \u003ccode\u003esurprise\u003c/code\u003e and Spark!\u003c/p\u003e\n\u003cfooter class=\"fis-footer\" style=\"visibility: hidden;\"\u003e\u003cdiv class=\"fis-feedback\"\u003e\n\u003ch5\u003eHow do you feel about this lesson?\u003c/h5\u003e\n\u003cimg id=\"thumbs-up\" data-repository=\"dsc-recommender-section-intro\" title=\"Thumbs up!\" alt=\"thumbs up\"\u003e\u003cimg id=\"thumbs-down\" data-repository=\"dsc-recommender-section-intro\" title=\"Thumbs down!\" alt=\"thumbs down\"\u003e\n\u003c/div\u003e\n\u003ch5\u003eHave specific feedback? \u003ca href=\"https://github.com/learn-co-curriculum/dsc-recommender-section-intro/issues/new\"\u003eTell us here!\u003c/a\u003e\n\u003c/h5\u003e\u003c/footer\u003e","exportId":"recommendation-systems-introduction"},{"id":197301,"title":"Introduction to Recommendation Systems","type":"WikiPage","indent":0,"locked":false,"requirement":"must_mark_done","completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-recommendation-system-introduction\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-recommendation-system-introduction/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eThis lesson will give you a brief introduction to recommendation system modeling approaches. We will develop intuition into how these systems work and how collaborative filtering is used to make accurate recommendation systems that can harness the power of big data.\u003c/p\u003e\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\u003cp\u003eYou will be able to:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eDescribe the role and rationale for recommendation systems\u003c/li\u003e\n\u003cli\u003eDescribe collaborative filtering recommender systems and their benefits/limitations\u003c/li\u003e\n\u003cli\u003eDescribe content-based recommenders and their benefits/limitations\u003c/li\u003e\n\u003cli\u003eDefine implicit and explicit rating systems\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003eProblem Domain\u003c/h2\u003e\n\u003cp\u003eThe goal of a recommendation system is to expose people to items that they will like. In exact terms, a recommendation system predicts the future preference of a set of items for a user, and recommends the top items from this set. In today's world, due to the internet and its global reach, people have more options to choose from than ever before.\u003c/p\u003e\n\u003cp\u003eConsider buying an album from a traditional music store where the options are always limited and mainly depend upon size and type of the store. There is a physical limitation to how many songs, albums, and artists can be offered. An online product like Spotify, however has a much higher ceiling in terms of storage space. With this new method of selecting products, recommendation systems are a popular way for users to sort through millions of songs to find the ones that are customized exactly for them. Recommendation systems cast a direct impact on profitability and customer satisfaction for most businesses today. With the nearly limitless options consumers have for products online, they need some guidance!\u003c/p\u003e\n\u003cp\u003eThis idea can be represented by a concept called the \"Long Tail,\" which is a set of statistical distributions that have a very long \"tail\" of the distribution, representing many occurrences of low frequency things. In the context of consumer products, there are some products that everyone is going to buy: light bulbs, toilet paper, bread etc. There are also items that are far more obscure: specific toys, sports equipment, movies. Recommendation systems are made to help consumers tap into this long tail to assist them in picking from the endless number of options that are made available to them via the internet.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-recommendation-system-introduction/master/images/LongTailConcept.png\" width=\"500\"\u003e\u003c/p\u003e\n\u003cp\u003eHere's a formal definition of recommendation systems from authors \u003ca href=\"https://misq.org/e-commerce-product-recommendation-agents-use-characteristics-and-impact.html\"\u003eBo Xiao and Izak Benbasat, 2017\u003c/a\u003e :\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\"Recommendation Systems are software agents that elicit the interests and preferences of individual consumers [] and make recommendations accordingly. They have the potential to support and improve the quality of the decisions consumers make while searching for and selecting products online.\"\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch2\u003eApplications of Recommendation Systems\u003c/h2\u003e\n\u003cp\u003eLets understand what all recommendation systems can do for businesses:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eHelp in suggesting the merchants/items which a customer might be interested in after buying a product in a marketplace\u003c/li\u003e\n\u003cli\u003eEstimate profit \u0026amp; loss of many competing items and make recommendations to the customer (e.g. buying and selling stocks)\u003c/li\u003e\n\u003cli\u003eBased on the experience of the customer, recommend a customer centric or product centric offering\u003c/li\u003e\n\u003cli\u003eEnhance customer engagement by providing offers which can be highly appealing to the customer\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003eRecommendation Systems Approaches\u003c/h2\u003e\n\u003cp\u003eThere are two main types of recommendation systems: unpersonalized and personalized. In the majority of this section, we will focus on personalized recommendation systems because that's where data scientists can provide the most value to companies, but to start off, let's investigate some unpersonalized systems because they can be productive in their own right.\u003c/p\u003e\n\u003ch3\u003eUnpersonalized Recommendations\u003c/h3\u003e\n\u003cp\u003eUnpersonalized recommendation systems have been happening since way before machine learning was ever in the public knowledge base. An example of an unpersonalized recommendation would be on YouTube when it recommends the most viewed videos. These are videos that the most people have watched. For the most part, these recommendations aren't too bad. After all, there's a reason why things are popular. This approach, however, is not going to help more niche videos get exposure. It also won't be immensely beneficial to those who have very particular tastes. Of course, there are times when a simple approach like this might be best. An example of a simple popularity recommender working well is with the news. There's a high chance that everyone who visits a news website is going to want to see whatever is the most popular at that moment in time.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-recommendation-system-introduction/master/images/news.png\" width=\"900\"\u003e\u003c/p\u003e\n\u003cp\u003eBecause unpersonalized recommendations are based on the entire user pool, whatever item is the most popular at any given time would be recommended to you, even if it's something you are completely uninterested in. There are so many items that are far too obscure to be the \"most popular\" item that might make someone's day. To make more informed recommendations, personalized recommendation systems make use of big data to ensure that users are getting items tailored towards there personal interests, no matter how niche they are.\u003c/p\u003e\n\u003ch3\u003ePersonalized Recommendations\u003c/h3\u003e\n\u003cp\u003eThe general problem of personalized recommendation systems can be summarized as:\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eGiven\u003c/strong\u003e:\u003c/p\u003e\n\u003cp\u003eThe profile of the \"active\" user and possibly some situational context, i.e. user browsing a product or making a purchase etc.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eRequired\u003c/strong\u003e:\u003c/p\u003e\n\u003cp\u003eCreating a set of items, and a score for each recommendable item in that set\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eProfile\u003c/strong\u003e:\u003c/p\u003e\n\u003cp\u003eUser profile may contain past purchases, ratings in either implicit or explicit form, demographics and interest scores for item features\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eThere are two ways to gather such data. The first method is to ask for explicit ratings from a user, typically on a concrete rating scale (such as rating a movie from one to five stars). The second is to gather data implicitly as the user is in the domain of the system - that is, to log the actions of a user on the site.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003e\u003cstrong\u003eProblem\u003c/strong\u003e:\u003c/p\u003e\n\u003cp\u003eWe want to learn a function that predicts the relevance score for a given (typically unseen) item based on user user profile and context\u003c/p\u003e\n\u003cp\u003eWithin personalized recommendation systems there are many different possible algorithms. We're going to go over the important ones now.\u003c/p\u003e\n\u003cp\u003eEach of these techniques make use of different similarity metrics to determine how \"similar\" items are to one another. The most common similarity metrics are \u003ca href=\"https://en.wikipedia.org/wiki/Euclidean_distance\"\u003eEuclidean distance\u003c/a\u003e, \u003ca href=\"https://en.wikipedia.org/wiki/Cosine_similarity\"\u003ecosine similarity\u003c/a\u003e, \u003ca href=\"https://en.wikipedia.org/wiki/Pearson_correlation_coefficient\"\u003ePearson correlation\u003c/a\u003e and the \u003ca href=\"https://en.wikipedia.org/wiki/Jaccard_index\"\u003eJaccard index (useful with binary data)\u003c/a\u003e. Each one of these distance metrics has its advantages and disadvantages depending on the type of ratings you are using and the characteristics of your data.\u003c/p\u003e\n\u003ch3\u003eContent-Based Recommenders\u003c/h3\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003eMain Idea\u003c/strong\u003e: If you like an item, you will also like \"similar\" items.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-recommendation-system-introduction/master/images/content_based.png\" width=\"500\"\u003e\u003c/p\u003e\n\u003cp\u003eThese systems are based on the characteristics of the items themselves. If you ever see a banner ad saying \"try other items like this\", it is most likely a content-based recommender systems. The advantages of content-based recommender systems is that it is a recommender system that gives the user a bit more information as to why they are seeing these recommendations. If they are on a page of a book they very much like, they will be happy to see another book that is similar to it. If they are told that this book is similar to their favorite book, they're more than likely to get that book. A disadvantage of content-based recommender systems is that they often require manual or semi-manual tagging of each of products. More advanced versions of content-based recommender systems allow for the development of an average of all the items a user has liked. This allows for a more nuanced approach to incorporate more than one item when calculating which items are most similar.\u003c/p\u003e\n\u003ch3\u003eCollaborative Filtering Systems\u003c/h3\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003eMain Idea\u003c/strong\u003e: If user A likes items 5, 6, 7, and 8 and user B likes items 5, 6, and 7, then it is highly likely that user B will also like item 8.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-recommendation-system-introduction/master/images/collaborative_filtering.png\" width=\"450\"\u003e\u003c/p\u003e\n\u003cp\u003eCollaborative filtering systems use a collection of user rating of items to make recommendations. The issue with collaborative filtering is that you have what is called the \"cold start problem.\" The idea behind it is, how to recommend something based off of user activity if you do not have any user activity to begin with! This can be overcome through various techniques. The most important thing to realize is that there is no one best recommendation system technique. In the end, what matters most is what system actually gets people to get recommendations that they will act upon. It might be that on the aggregate, recommending the most popular items is the most cost effective way to introduce users to new products.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eThe key idea behind collaborative filtering is that similar users share similar interests and that users tend to like items that are similar to one another.\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eWhile this may not be completely true on every occasion, if we have a large enough dataset, if there are patterns present, they will start to emerge.\u003c/p\u003e\n\u003cp\u003eAssume there are some users who have bought certain items, we can use a matrix with size \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Ctext%7Bnum_users%7D%20*%20%5Ctext%7Bnum_items%7D\"\u003e to denote the past behavior of users. Each cell in the matrix represents the associated opinion that a user holds. Such matrix is called a \u003cstrong\u003eUtility Matrix\u003c/strong\u003e. For instance, \u003cimg src=\"https://render.githubusercontent.com/render/math?math=M_%7Bi,%20j%7D\"\u003e denotes how user \u003cimg src=\"https://render.githubusercontent.com/render/math?math=u\"\u003e likes item \u003cimg src=\"https://render.githubusercontent.com/render/math?math=i\"\u003e . Sometimes these individual ratings are written as \u003cimg src=\"https://render.githubusercontent.com/render/math?math=r_%7Bui%7D\"\u003e for a rating for a given user and a given item. Using the table below as a reference point, if we replaced the \u003cimg src=\"https://render.githubusercontent.com/render/math?math=u\"\u003e and \u003cimg src=\"https://render.githubusercontent.com/render/math?math=i\"\u003e variable subscripts with actual values it would look like \u003cimg src=\"https://render.githubusercontent.com/render/math?math=r_%7B%5Ctext%7BMike%7D,%5Ctext%7BLittle%20Mermaid%7D%7D%20=%203\"\u003e .\u003c/p\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003e\u003c/th\u003e\n\u003cth\u003eToy Story\u003c/th\u003e\n\u003cth\u003eCinderella\u003c/th\u003e\n\u003cth\u003eLittle Mermaid\u003c/th\u003e\n\u003cth\u003eLion King\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd\u003eMatt\u003c/td\u003e\n\u003ctd\u003e\u003c/td\u003e\n\u003ctd\u003e2\u003c/td\u003e\n\u003ctd\u003e\u003c/td\u003e\n\u003ctd\u003e5\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eLore\u003c/td\u003e\n\u003ctd\u003e2\u003c/td\u003e\n\u003ctd\u003e\u003c/td\u003e\n\u003ctd\u003e4\u003c/td\u003e\n\u003ctd\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eMike\u003c/td\u003e\n\u003ctd\u003e\u003c/td\u003e\n\u003ctd\u003e5\u003c/td\u003e\n\u003ctd\u003e3\u003c/td\u003e\n\u003ctd\u003e2\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eForest\u003c/td\u003e\n\u003ctd\u003e5\u003c/td\u003e\n\u003ctd\u003e\u003c/td\u003e\n\u003ctd\u003e1\u003c/td\u003e\n\u003ctd\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eTaylor\u003c/td\u003e\n\u003ctd\u003e1\u003c/td\u003e\n\u003ctd\u003e5\u003c/td\u003e\n\u003ctd\u003e\u003c/td\u003e\n\u003ctd\u003e2\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003cp\u003eThe task of a recommendation system would be to come up with ratings for users in the spots that are currently empty. As you can imagine, most of the time, these values will be largely empty. For user 1, our recommendation system would try and predict what user 1 would rate Toy Story and the Little Mermaid and then recommend whichever product our model predicts they would rate the highest. The utility matrix above is what's known as an explicit rating. Each person has rated the movies that they've seen. Frequently, we must infer some meaning from the data and use our own judgment to determine how to use it for a recommendation system. Assume that rather than ratings, we only knew whether or not users bought a movie from a streaming website. Let's take a look at what this table would look like:\u003c/p\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003e\u003c/th\u003e\n\u003cth\u003eToy Story\u003c/th\u003e\n\u003cth\u003eCinderella\u003c/th\u003e\n\u003cth\u003eLittle Mermaid\u003c/th\u003e\n\u003cth\u003eLion King\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd\u003eMatt\u003c/td\u003e\n\u003ctd\u003e\u003c/td\u003e\n\u003ctd\u003e1\u003c/td\u003e\n\u003ctd\u003e\u003c/td\u003e\n\u003ctd\u003e1\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eLore\u003c/td\u003e\n\u003ctd\u003e1\u003c/td\u003e\n\u003ctd\u003e\u003c/td\u003e\n\u003ctd\u003e1\u003c/td\u003e\n\u003ctd\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eMike\u003c/td\u003e\n\u003ctd\u003e\u003c/td\u003e\n\u003ctd\u003e1\u003c/td\u003e\n\u003ctd\u003e1\u003c/td\u003e\n\u003ctd\u003e1\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eForest\u003c/td\u003e\n\u003ctd\u003e1\u003c/td\u003e\n\u003ctd\u003e\u003c/td\u003e\n\u003ctd\u003e1\u003c/td\u003e\n\u003ctd\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eTaylor\u003c/td\u003e\n\u003ctd\u003e1\u003c/td\u003e\n\u003ctd\u003e1\u003c/td\u003e\n\u003ctd\u003e\u003c/td\u003e\n\u003ctd\u003e1\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003cp\u003eThese are \u003cstrong\u003eimplicit\u003c/strong\u003e ratings because we are assuming that because a person has bought something, they would like to buy other items like it. Of course, this is not necessarily true, but it's better than nothing!\u003c/p\u003e\n\u003cp\u003eWithin the domain of collaborative filtering, there are both memory-based approaches and model-based approaches that you will learn about in the upcoming lessons.\u003c/p\u003e\n\u003ch2\u003eFurther Reading\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"http://infolab.stanford.edu/%7Eullman/mmds/ch9.pdf\"\u003eChapter 9: Mining of Massive Datasets (MMDS)\u003c/a\u003e - A must read for in-depth knowledge about how recommendation systems work, their underlying algorithms and evaluation approaches. This covers most of the topic from this lesson and also the upcoming lessons in great detail.\u003c/p\u003e\n\u003ch2\u003eSummary\u003c/h2\u003e\n\u003cp\u003eIn this lesson, we looked at an overview of recommendation systems. Focusing on collaborative filtering systems, we will move on to developing user-based engines.\u003c/p\u003e","exportId":"introduction-to-recommendation-systems"},{"id":197306,"title":"Collaborative Filtering and Singular Value Decomposition","type":"Assignment","indent":0,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_mark_done","completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-collaborative-filtering-singular-value-decomposition\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-collaborative-filtering-singular-value-decomposition/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","exportId":"g685fa164fdd87e600d0d9c69caade1d3"},{"id":197309,"title":"Collaborative Filtering with Surprise","type":"Assignment","indent":0,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_mark_done","completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-implementing-recommender-systems\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-implementing-recommender-systems/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","exportId":"g9e1da4e964e6ef1d671b812ed34cfd86"},{"id":197313,"title":"Collaborative Filtering with Surprise - Lab","type":"Assignment","indent":1,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_mark_done","completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-implementing-recommender-systems-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-implementing-recommender-systems-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","exportId":"g856ffd862141af76250734dab1429013"},{"id":197317,"title":"Matrix Factorization with Alternating Least Squares","type":"Assignment","indent":0,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_mark_done","completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-matrix-factorization-als\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-matrix-factorization-als/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","exportId":"gec228d629172fb18ee30e8df90f2a7d4"},{"id":197321,"title":"Building a Recommendation System in PySpark - Lab","type":"Assignment","indent":1,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_mark_done","completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/als-recommender-system-pyspark-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/als-recommender-system-pyspark-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","exportId":"gd6ed89fe9dc5314f1389f740f4a3ad88"},{"id":197323,"title":"Recommendation Systems - Recap","type":"WikiPage","indent":0,"locked":false,"requirement":"must_mark_done","completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-recommendation-section-recap\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-recommendation-section-recap/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\u003ch2\u003eKey Takeaways\u003c/h2\u003e\n\n\u003cp\u003eThe key takeaways from this section include:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eRecommendation approaches can consist of simply recommending popular items (without personalization), or using algorithms which takes into account past customer behavior\u003c/li\u003e\n\u003cli\u003eWhen using algorithms, the two main types are content-based algorithms (recommending new content based on similar \u003cem\u003econtent\u003c/em\u003e), or collaborative filtering based (recommending new content based on similar types of \u003cem\u003eusers\u003c/em\u003e)\u003c/li\u003e\n\u003cli\u003eCollaborative Filtering (CF) is currently the most widely used approach to build recommendation systems\u003c/li\u003e\n\u003cli\u003eThe key idea behind CF is that similar users have similar interests and that a user generally likes items that are similar to other items they like\u003c/li\u003e\n\u003cli\u003eCF is filling an \"empty cell\" in the utility matrix based on the similarity between users or item. Matrix factorization or decomposition can help us solve this problem by determining what the overall \"topics\" are when a matrix is factored\u003c/li\u003e\n\u003cli\u003eMatrix decomposition can be reformulated as an optimization problem with loss functions and constraints\u003c/li\u003e\n\u003cli\u003eMatrix decomposition can be done using either Singular Value Decomposition (SVD) or Alternating Least Squares (ALS)\u003c/li\u003e\n\u003cli\u003eSpark's ALS implementation can be used to build a scalable and efficient recommendation system \u003c/li\u003e\n\u003c/ul\u003e\n\u003cfooter class=\"fis-footer\" style=\"visibility: hidden;\"\u003e\u003cdiv class=\"fis-feedback\"\u003e\n\u003ch5\u003eHow do you feel about this lesson?\u003c/h5\u003e\n\u003cimg id=\"thumbs-up\" data-repository=\"dsc-recommendation-section-recap\" title=\"Thumbs up!\" alt=\"thumbs up\"\u003e\u003cimg id=\"thumbs-down\" data-repository=\"dsc-recommendation-section-recap\" title=\"Thumbs down!\" alt=\"thumbs down\"\u003e\n\u003c/div\u003e\n\u003ch5\u003eHave specific feedback? \u003ca href=\"https://github.com/learn-co-curriculum/dsc-recommendation-section-recap/issues/new\"\u003eTell us here!\u003c/a\u003e\n\u003c/h5\u003e\u003c/footer\u003e","exportId":"recommendation-systems-recap"}]},{"id":21091,"name":"Topic 37: Exploring Time Series Data","status":"unlocked","unlockDate":null,"prereqs":[],"requirement":"all","sequential":false,"exportId":"g9fddcbbdfc63e4aa30e3755054190209","items":[{"id":197329,"title":"Exploring Time Series Data - Introduction","type":"WikiPage","indent":0,"locked":false,"requirement":"must_mark_done","completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-time-series-section-intro\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-time-series-section-intro/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eIn this section, you will learn about working with an important and ever-present type of data: time series! Stock market prices, weather, and economic indicators like GDP are a few examples of time series data.\u003c/p\u003e\n\n\u003ch2\u003eTime Series Data\u003c/h2\u003e\n\n\u003cp\u003e\"Time series\" data refers to datasets where the progress of time is an important dimension in the dataset. For example, working with the changes in stock prices, oil flow through a pipeline or even climate data over time requires an understanding of how to work with time series data. We introduce the concept of time series data, look at how to manage and visualize time series data, introduce the types of trends and the idea of \"time series decomposition\". In the next section, we'll introduce techniques for modeling time series data.\u003c/p\u003e\n\n\u003ch3\u003eIntroduction to Time Series\u003c/h3\u003e\n\n\u003cp\u003eWe start by importing daily minimum temperatures for Melbourne, Australia and introduce the importance of using dates as index values when importing time series data into Pandas. We then go through how to downsample and upsample a dataset and show some of the built-in methods for easily selecting and slicing time series data. We also provide an introduction to some of the most common plots for time series such as a line plot and a dot plot, and approaches to grouping and visualizing time series data.\u003c/p\u003e\n\n\u003cp\u003eWe also introduce the use of time series histograms and density plots for visualizing the distribution of the values without considering the times at which the values were measured and suggest time series box and whisker plots on a per-year basis to get a sense of trends over time. Finally, we introduce time series heat maps which can be a great way of getting a sense of how time series data changes across a couple of dimensions (e.g. month to month and year to year).\u003c/p\u003e\n\n\u003ch3\u003eTypes of Trends\u003c/h3\u003e\n\n\u003cp\u003eBasic regression tests are often not capable of capturing and predicting time-dependent patterns, so we introduce the concept of trends and stationarity, and explain the Dickey-Fuller test for performing statistical testing for time series stationarity.\u003c/p\u003e\n\n\u003ch3\u003eRemoving Trends\u003c/h3\u003e\n\n\u003cp\u003eMost time series modeling techniques assume stationarity, so we look at some of the techniques available for removing (or reducing) trends and/or seasonality using techniques such as a log transformation, rolling means, and differencing.\u003c/p\u003e\n\n\u003ch3\u003eTime Series Decomposition\u003c/h3\u003e\n\n\u003cp\u003eFinally, we end the section by introducing the concept of decomposition - another approach to removing trends and seasonality from a time series dataset.\u003c/p\u003e\n\n\u003ch2\u003eSummary\u003c/h2\u003e\n\n\u003cp\u003eThis section will provide you with the foundational knowledge for loading and working with time series data, so you'll have the skills required to start to perform time series modeling!\u003c/p\u003e\n\u003cfooter class=\"fis-footer\" style=\"visibility: hidden;\"\u003e\u003cdiv class=\"fis-feedback\"\u003e\n\u003ch5\u003eHow do you feel about this lesson?\u003c/h5\u003e\n\u003cimg id=\"thumbs-up\" data-repository=\"dsc-time-series-section-intro\" title=\"Thumbs up!\" alt=\"thumbs up\"\u003e\u003cimg id=\"thumbs-down\" data-repository=\"dsc-time-series-section-intro\" title=\"Thumbs down!\" alt=\"thumbs down\"\u003e\n\u003c/div\u003e\n\u003ch5\u003eHave specific feedback? \u003ca href=\"https://github.com/learn-co-curriculum/dsc-time-series-section-intro/issues/new\"\u003eTell us here!\u003c/a\u003e\n\u003c/h5\u003e\u003c/footer\u003e","exportId":"exploring-time-series-data-introduction"},{"id":197332,"title":"Introduction to Time Series","type":"Assignment","indent":0,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_mark_done","completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-introduction-to-time-series\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-introduction-to-time-series/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","exportId":"g47191c997c95223ee0f5ab16ae5df781"},{"id":197337,"title":"Managing Time Series Data - Lab","type":"Assignment","indent":1,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_mark_done","completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-managing-time-series-data-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-managing-time-series-data-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","exportId":"gd94d0e7feb4fabba41d06eaea1e409a3"},{"id":197339,"title":"Visualizing Time Series Data - Lab","type":"Assignment","indent":1,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_mark_done","completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-visualizing-time-series-data-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-visualizing-time-series-data-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","exportId":"g9f78f4552bfdc85602573b4d20e581d3"},{"id":197342,"title":"Types of Trends","type":"Assignment","indent":0,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_mark_done","completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-types-of-trends\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-types-of-trends/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","exportId":"g5dee489c6bfeb0ed4ebcabc43c337ac5"},{"id":197346,"title":"Testing for Trends - Lab","type":"Assignment","indent":1,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_mark_done","completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-testing-for-trends-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-testing-for-trends-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","exportId":"g81908fe02dffb8e98b7ebb5cc4074411"},{"id":197348,"title":"Removing Trends","type":"Assignment","indent":0,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_mark_done","completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-removing-trends\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-removing-trends/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","exportId":"ge609caa4ce192937728849d3e8579757"},{"id":197351,"title":"Removing Trends - Lab","type":"Assignment","indent":1,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_mark_done","completed":false,"content":"\u003cheader class=\"fis-header\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-removing-trends-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-removing-trends-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","exportId":"gf1d75b5dd1effc263db5285e973d0a50"},{"id":197354,"title":"Time Series Decomposition","type":"Assignment","indent":0,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_mark_done","completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-time-series-decomposition\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-time-series-decomposition/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","exportId":"ge39f9b78a9073aac955e8962d85e3ae9"},{"id":197357,"title":"Exploring Time Series Data - Recap","type":"WikiPage","indent":0,"locked":false,"requirement":"must_mark_done","completed":false,"content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-time-series-section-recap\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-time-series-section-recap\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-time-series-section-recap/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\u003ch2\u003eKey Takeaways\u003c/h2\u003e\n\u003cp\u003eThe key takeaways from this section include:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eWhen you import time series data into Pandas, make sure to use the time/date information as index values using either a Pandas \u003ccode\u003etimestamp\u003c/code\u003e or Python \u003ccode\u003edatetime\u003c/code\u003e data type\u003c/li\u003e\n\u003cli\u003eThere are a range of built-in methods in Pandas for easily downsampling or upsampling time series data\u003c/li\u003e\n\u003cli\u003eLine plots and dot plots can be useful for getting a sense of how a time series dataset changes over time\u003c/li\u003e\n\u003cli\u003eHistograms and density plots can be useful for getting a sense of the time-independent distribution of a time series\u003c/li\u003e\n\u003cli\u003eBox and whisker plots per year (or other seasonality period - day, week, month, etc) can be a great way to easily see trends in the distribution of time series data over time\u003c/li\u003e\n\u003cli\u003eHeat maps can also be useful for comparing changes of time series data across a couple of dimensions. For example, with months on one axis and years on another, they can be a great way to see both seasonality and year on year trends\u003c/li\u003e\n\u003cli\u003eA time series is said to be stationary if its statistical properties such as mean and variance remain constant over time\u003c/li\u003e\n\u003cli\u003eMost time series models work on the assumption that the time series are stationary (assumption of homoscedasticity)\u003c/li\u003e\n\u003cli\u003eMany time series datasets \u003cem\u003edo\u003c/em\u003e have trends, violating the assumption of homoscedasticity\u003c/li\u003e\n\u003cli\u003eCommon examples are trends that include linear (straight line over time), exponential, and periodic. Some datasets also have increasing (or decreasing) variance over time\u003c/li\u003e\n\u003cli\u003eAny given dataset may exhibit multiple trends (e.g. linear, periodic, and reduction of variance)\u003c/li\u003e\n\u003cli\u003eRolling statistics can be used to test for trends to see whether the centrality and/or dispersion of time series changes over time\u003c/li\u003e\n\u003cli\u003eThe Dickey-Fuller test is a common test for determining whether a time series contains trends\u003c/li\u003e\n\u003cli\u003eCommon approaches for removing trends and seasonality include taking a log-transform, subtracting the rolling mean, and differencing\u003c/li\u003e\n\u003cli\u003eDecomposing allows you to separately view \u003cem\u003eseasonality\u003c/em\u003e (which could be daily, weekly, annual, etc), \u003cem\u003etrend\u003c/em\u003e, and \u003cem\u003erandom\u003c/em\u003e, which is the variability in time series after removing the effects of the seasonality and trend\u003c/li\u003e\n\u003c/ul\u003e","exportId":"exploring-time-series-data-recap"}]},{"id":21093,"name":"Topic 38: Time Series Models","status":"unlocked","unlockDate":null,"prereqs":[],"requirement":"all","sequential":false,"exportId":"g1542dfe88645e5922946ff1e27e85889","items":[{"id":197364,"title":"Modeling Time Series Data - Introduction","type":"WikiPage","indent":0,"locked":false,"requirement":"must_mark_done","completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-time-series-models-introduction\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-time-series-models-introduction/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eIn this section, you'll learn about modeling for time series data. \u003c/p\u003e\n\n\u003ch2\u003eTime Series Modeling\u003c/h2\u003e\n\n\u003cp\u003eIn the previous section, we introduced the idea of time series data and provided some best practices for importing, managing, and visualizing time series data along with a number of techniques for removing trends and/or seasonality from a time series dataset. In this section, we're going to look at various types of models for time series data.\u003c/p\u003e\n\n\u003ch3\u003eBasic Time Series Models\u003c/h3\u003e\n\n\u003cp\u003eWe start off by introducing two basic time series models -- the white noise and random walk models.\u003c/p\u003e\n\n\u003ch3\u003eCorrelation, Autocorrelation, and Partial Autocorrelation\u003c/h3\u003e\n\n\u003cp\u003eWe will then move on to the concept of correlation as it relates to time series datasets, and plot the Autocorrelation Function (ACF) and Partial Autocorrelation Function (PACF) for a time series. \u003c/p\u003e\n\n\u003ch3\u003eARMA Models\u003c/h3\u003e\n\n\u003cp\u003eWe then move on to introduce two other key time series models that are widely used for predicting future values for time series data - the auto regressive (AR) and moving average (MA) models.\u003c/p\u003e\n\n\u003ch2\u003eSummary\u003c/h2\u003e\n\n\u003cp\u003eLet's get started! This section wraps up our introduction to time series analysis, giving you the modeling tools required to effectively forecast time series data.\u003c/p\u003e\n\u003cfooter class=\"fis-footer\" style=\"visibility: hidden;\"\u003e\u003cdiv class=\"fis-feedback\"\u003e\n\u003ch5\u003eHow do you feel about this lesson?\u003c/h5\u003e\n\u003cimg id=\"thumbs-up\" data-repository=\"dsc-time-series-models-introduction\" title=\"Thumbs up!\" alt=\"thumbs up\"\u003e\u003cimg id=\"thumbs-down\" data-repository=\"dsc-time-series-models-introduction\" title=\"Thumbs down!\" alt=\"thumbs down\"\u003e\n\u003c/div\u003e\n\u003ch5\u003eHave specific feedback? \u003ca href=\"https://github.com/learn-co-curriculum/dsc-time-series-models-introduction/issues/new\"\u003eTell us here!\u003c/a\u003e\n\u003c/h5\u003e\u003c/footer\u003e","exportId":"modeling-time-series-data-introduction"},{"id":197367,"title":"Basic Time Series Models","type":"Assignment","indent":0,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_mark_done","completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-basic-time-series-models\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-basic-time-series-models/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","exportId":"g747e08959248d6f56d169dbde42204c8"},{"id":197369,"title":"Basic Time Series Models - Lab","type":"Assignment","indent":1,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_mark_done","completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-basic-time-series-models-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-basic-time-series-models-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","exportId":"g95b09de1227a7a19b301fe2cefa6fad2"},{"id":197373,"title":"Correlation and Autocorrelation in Time Series","type":"Assignment","indent":0,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_mark_done","completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-corr-autocorr-in-time-series\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-corr-autocorr-in-time-series/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","exportId":"g13fd6669c9d9a7d16859808cedfc68d8"},{"id":197376,"title":"Correlation and Autocorrelation in Time Series - Lab","type":"Assignment","indent":1,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_mark_done","completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-corr-autocorr-in-time-series-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-corr-autocorr-in-time-series-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","exportId":"gfd8c77f82835e9df5254d856f144bea8"},{"id":197381,"title":"ARMA Models","type":"WikiPage","indent":0,"locked":false,"requirement":"must_mark_done","completed":false,"content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-arma-models\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-arma-models\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-arma-models/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eYou've seen two basic time series models now, the random walk and white noise models. In this lesson, you'll learn about two other very important time series models that are widely used to understand and predict future values in stochastic processes: the Autoregressive (AR) and Moving Average (MA) models.\u003c/p\u003e\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\u003cp\u003eYou will be able to:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eExplain what autoregressive means in an autoregressive model\u003c/li\u003e\n\u003cli\u003eExplain what a moving average model means\u003c/li\u003e\n\u003cli\u003eDescribe how AR and MA can be combined to form an ARMA model\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003eThe Autoregressive Model\u003c/h2\u003e\n\u003cp\u003eAn autoregressive (AR) model is when a value from a time series is regressed on previous values from the same time series.\u003c/p\u003e\n\u003cp\u003eIn words, the mathematical idea is the following:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Ctext%7BToday%20=%20constant%20%2b%20slope%7D%20%5Ctimes%20%5Ctext%7Byesterday%20%2b%20noise%7D\"\u003e\u003c/p\u003e\n\u003cp\u003eOr, mathematically:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Clarge%20Y_t%20=%20%5Cmu%20%2b%20%5Cphi%20*%20Y_%7Bt-1%7D%2b%5Cepsilon_t\"\u003e\u003c/p\u003e\n\u003cp\u003eSome notes based on this formula: - If the slope is 0, the time series is a white noise model with mean \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cmu\"\u003e - If the slope is not 0, the time series is autocorrelated - Bigger slope means bigger autocorrelation - When there is a negative slope, the time series follows an oscillatory process\u003c/p\u003e\n\u003cp\u003eWe simulated some time series below. Have a look at them, and make sure this follows your intuition looking at the formula.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-arma-models/master/images/AR_model.png\"\u003e\u003c/p\u003e\n\u003cp\u003eNote that simply having a value for \u003cem\u003ephi\u003c/em\u003e ( \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cphi\"\u003e ) slightly bigger than 1, the time series clearly goes in one direction. Note the scale of the y-axis, where the y-axis scale for all the other processes is between -10 and 10, the last time series goes down to values of -100.\u003c/p\u003e\n\u003cp\u003eLet's look at the autocorrelation plots as well.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-arma-models/master/images/AR_ACF.png\"\u003e\u003c/p\u003e\n\u003cp\u003eThe oscillatory process of the time series with \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cphi=0.9\"\u003e is reflected in the autocorrelation function, returning an oscillatory autocorrelation function as well. \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cphi=0.2\"\u003e leads to a very low, insignificant, autocorrelation. \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cphi=0.8\"\u003e leads to a strong autocorrelation for the first few lags and then incurs a steep decline. Having a \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cphi=1.02\"\u003e (just slightly bigger than 1) leads to strong and long-lasting autocorrelation.\u003c/p\u003e\n\u003cp\u003eNext, let's look at the partial autocorrelation plots.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-arma-models/master/images/AR_PACF.png\"\u003e\u003c/p\u003e\n\u003cp\u003eFor each of these PACFs, we notice a high value for 1 lag, then autocorrelations of 0, except for the second one. This is no big surprise, as the slope parameter is fairly small, so the relationship between a value and the next one is fairly limited.\u003c/p\u003e\n\u003ch2\u003eThe Moving Average Model\u003c/h2\u003e\n\u003cp\u003eThe Moving Average model can be described as the weighted sum of today's and yesterday's noise.\u003c/p\u003e\n\u003cp\u003eIn words, the mathematical idea is the following:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Ctext%7BToday%20=%20Mean%20%2b%20Noise%20%2b%20Slope%7D%20%5Ctimes%20%5Ctext%7Byesterday's%20noise%7D\"\u003e\u003c/p\u003e\n\u003cp\u003eOr, mathematically:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Clarge%20Y_t%20=%20%5Cmu%20%2b%5Cepsilon_t%20%2b%20%5Ctheta%20*%20%5Cepsilon_%7Bt-1%7D\"\u003e\u003c/p\u003e\n\u003cp\u003eSome notes based on this formula: - If the slope is 0, the time series is a white noise model with mean \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cmu\"\u003e - If the slope is not 0, the time series is autocorrelated and depends on the previous white noise process - Bigger slope means bigger autocorrelation - When there is a negative slope, the time series follow an oscillatory process\u003c/p\u003e\n\u003cp\u003eFor the Moving Average Model we also simulated some time series with varying parameters below.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-arma-models/master/images/MA_model.png\"\u003e\u003c/p\u003e\n\u003cp\u003eWhen there is a positive \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Ctheta\"\u003e there is a certain persistence in level, meaning that each observation is generally close to its neighbors. This is more pronounced for higher values of \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Ctheta\"\u003e . MA series with negative coefficients, however, show oscillatory patterns. Recall that when \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Ctheta=0\"\u003e , the process is a true white noise process!\u003c/p\u003e\n\u003cp\u003eLet's look at the ACF plots.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-arma-models/master/images/MA_ACF.png\"\u003e\u003c/p\u003e\n\u003cp\u003eRemember that MA processes have autocorrelations, but because of the structure of the MA formula (regressing it on the noise term of the previous observation) there is only a dependence for one period, and the autocorrelation is zero for lags 2 and higher.\u003c/p\u003e\n\u003cp\u003eIf \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Ctheta%20\u003e0%22\u003e%20the%20lag%20one%20autocorrelation%20is%20positive,%20if%20%20\u003cimg%20src=\"\u003e the lag one autocorrelation is negative.\u003c/p\u003e\n\u003cp\u003eNext, let's look at the partial autocorrelation plots.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-arma-models/master/images/MA_PACF.png\"\u003e\u003c/p\u003e\n\u003cp\u003eFor PACFs, a typical structure is that there is a strong correlation with the 1-period lag (strength depending on \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Ctheta\"\u003e ), and then the PACF gradually tails off. You can particularly observe this for \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Ctheta=0.9\"\u003e and \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Ctheta=-0.95\"\u003e .\u003c/p\u003e\n\u003ch2\u003eHigher-order AR and MA models\u003c/h2\u003e\n\u003cp\u003eLet's look at the formulas of AR and MA again:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eAR: \u003cimg src=\"https://render.githubusercontent.com/render/math?math=Y_t%20=%20%5Cmu%20%2b%20%5Cphi%20*%20Y_%7Bt-1%7D%2b%5Cepsilon_t\"\u003e\n\u003c/li\u003e\n\u003cli\u003eMA: \u003cimg src=\"https://render.githubusercontent.com/render/math?math=Y_t%20=%20%5Cmu%20%2b%5Cepsilon_t%20%2b%20%5Ctheta%20*%20%5Cepsilon_%7Bt-1%7D\"\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eNote that these models are constructed in a way that processes only depend directly on the previous observation in the process. These are known as \"1st order models\", and denoted by AR(1) and MA(1) processes respectively. Let's look at AR(2) and MA(2).\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eAR(2): \u003cimg src=\"https://render.githubusercontent.com/render/math?math=Y_t%20=%20%5Cmu%20%2b%20%5Cphi_1%20*%20Y_%7Bt-1%7D%2b%5Cphi_2%20*%20Y_%7Bt-2%7D%2b%5Cepsilon_t\"\u003e\n\u003c/li\u003e\n\u003cli\u003eMA(2): \u003cimg src=\"https://render.githubusercontent.com/render/math?math=Y_t%20=%20%5Cmu%20%2b%5Cepsilon_t%20%2b%20%5Ctheta_1%20*%20%5Cepsilon_%7Bt-1%7D%2b%20%5Ctheta_2%20*%20%5Cepsilon_%7Bt-2%7D\"\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eNeedless to say, this can be extended to higher-orders as well! Generally, the order of an AR model is denoted \u003cimg src=\"https://render.githubusercontent.com/render/math?math=p\"\u003e , and the order of an MA model is denoted \u003cimg src=\"https://render.githubusercontent.com/render/math?math=q\"\u003e .\u003c/p\u003e\n\u003ch2\u003eACF and PACF intuition for AR(p) and MA(q)\u003c/h2\u003e\n\u003cp\u003eA quick overview of how higher order models affect the ACF and PACF:\u003c/p\u003e\n\u003ch3\u003eAR(p)\u003c/h3\u003e\n\u003cp\u003eConsidering a time series that was generated by an autoregression (AR) process with an order of \u003cimg src=\"https://render.githubusercontent.com/render/math?math=p\"\u003e , we would expect the ACF plot for the AR(p) time series to be strong to a lag of \u003cimg src=\"https://render.githubusercontent.com/render/math?math=p\"\u003e and remain stagnant for subsequent lag values, trailing off at some point as the effect is weakened. The PACF, on the other hand, describes the direct relationship between an observation and its lag. This generally leads to no correlation for lag values beyond \u003cimg src=\"https://render.githubusercontent.com/render/math?math=p\"\u003e .\u003c/p\u003e\n\u003ch3\u003eMA(q)\u003c/h3\u003e\n\u003cp\u003eWith a time series generated by a moving average (MA) process with an order \u003cimg src=\"https://render.githubusercontent.com/render/math?math=q\"\u003e , we would expect the ACF for the MA(q) process to show a strong correlation with recent values up to the lag of \u003cimg src=\"https://render.githubusercontent.com/render/math?math=q\"\u003e , then an immediate decline to minimal or no correlation. For the PACF, we would expect the plot to show a strong relationship to the lag and then a tailing off to no correlation from the lag onwards.\u003c/p\u003e\n\u003ch2\u003eARMA models\u003c/h2\u003e\n\u003cp\u003eNow that we've seen AR and MA models, it is important to note that \u003cstrong\u003ethere is no reason why AR and MA models would not coexist\u003c/strong\u003e. That's where ARMA models come in, which basically means that in this model, a regression on past values takes place (AR part) and also that the error term is modeled as a linear combination of error terms of the recent past (MA part). Generally, one denotes ARMA as ARMA(p, q).\u003c/p\u003e\n\u003cp\u003eAn ARMA(2,1) model is given by:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=Y_t%20=%20%5Cmu%20%2b%20%5Cphi_1%20Y_%7Bt-1%7D%2b%5Cphi_2%20Y_%7Bt-2%7D%2b%20%5Ctheta%20%5Cepsilon_%7Bt-1%7D%2b%5Cepsilon_t\"\u003e\u003c/p\u003e\n\u003cp\u003eA short table to summarize ACF and PACF for AR(p), MA(q), and ARMA(p, q):\u003c/p\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003e\u003c/th\u003e\n\u003cth\u003eAR(p)\u003c/th\u003e\n\u003cth\u003eMA(q)\u003c/th\u003e\n\u003cth\u003eARMA(p, q)\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd\u003eACF\u003c/td\u003e\n\u003ctd\u003eTails off\u003c/td\u003e\n\u003ctd\u003eCuts off after lag q\u003c/td\u003e\n\u003ctd\u003eTails off\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003ePACF\u003c/td\u003e\n\u003ctd\u003eCuts off after lag p\u003c/td\u003e\n\u003ctd\u003eTails off\u003c/td\u003e\n\u003ctd\u003eTails off\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003ch2\u003eNote on modeling\u003c/h2\u003e\n\u003cp\u003eSeeing the table above, you might get an idea of why ACF and PACF are so useful when modeling! What you generally will try to do for any time series analysis is:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eDetrend your time series using differencing. ARMA models represent stationary processes, so we have to make sure there are no trends in our time series\u003c/li\u003e\n\u003cli\u003eLook at ACF and PACF of the time series\u003c/li\u003e\n\u003cli\u003eDecide on the AR, MA, and order of these models\u003c/li\u003e\n\u003cli\u003eFit the model to get the correct parameters and use for prediction\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003eAdditional resources\u003c/h2\u003e\n\u003cp\u003eTo learn more about AR, MA, and ARMA, have a look at lessons 1 and 2 \u003ca href=\"https://onlinecourses.science.psu.edu/stat510/node/41/\"\u003ehere\u003c/a\u003e.\u003c/p\u003e\n\u003ch2\u003eSummary\u003c/h2\u003e\n\u003cp\u003eGreat! Now that you have learned the basics of AR, MA, and ARMA models, let's look at some time series and how to model them in the next lesson!\u003c/p\u003e","exportId":"arma-models"},{"id":197386,"title":"ARMA Models in statsmodels","type":"Assignment","indent":0,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_mark_done","completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-arma-models-statsmodels\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-arma-models-statsmodels/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","exportId":"g550d57825b87cbd1a1ccfd722c39dfee"},{"id":197390,"title":"ARMA Models in statsmodels - Lab","type":"Assignment","indent":1,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_mark_done","completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-arma-models-statsmodels-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-arma-models-statsmodels-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","exportId":"gb8e9f73e2c5c06e41e700610fe3b55fc"},{"id":197394,"title":"Modeling Time Series Data - Recap","type":"WikiPage","indent":0,"locked":false,"requirement":"must_mark_done","completed":false,"content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-time-series-models-section-recap\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-time-series-models-section-recap\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-time-series-models-section-recap/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\u003ch2\u003eKey Takeaways\u003c/h2\u003e\n\u003cp\u003eThe key takeaways from this section include:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eA white noise model has a fixed and constant mean and variance, and no correlation over time\u003c/li\u003e\n\u003cli\u003eA random walk model has no specified mean or variance, but has a strong dependence over time\u003c/li\u003e\n\u003cli\u003eThe Pandas \u003ccode\u003e.corr()\u003c/code\u003e method can be used to return the correlation between various time series in the DataFrame\u003c/li\u003e\n\u003cli\u003eAutocorrelation allows us to identify how strongly each time series observation is related to previous observations\u003c/li\u003e\n\u003cli\u003eThe Autocorrelation Function (ACF) is a function that represents autocorrelation of a time series as a function of the time lag\u003c/li\u003e\n\u003cli\u003eThe Partial Autocorrelation Function (or PACF) gives the partial correlation of a time series with its own lagged values, controlling for the values of the time series at all shorter lags\u003c/li\u003e\n\u003cli\u003eARMA (Autoregressive and Moving Average) modeling is a tool for forecasting time series values by regressing the variable on its own lagged (past) values\u003c/li\u003e\n\u003cli\u003eARMA models assume that you've already detrended your data and that there is no seasonality\u003c/li\u003e\n\u003c/ul\u003e","exportId":"modeling-time-series-data-recap"}]},{"id":21124,"name":"APPENDIX: More Time Series","status":"completed","unlockDate":null,"prereqs":[],"requirement":null,"sequential":false,"exportId":"gdef2abc00f439dbd68296ba90ddf5ba2","items":[{"id":197609,"title":"Time Series: SARIMA Models - Lab","type":"Assignment","indent":1,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":null,"completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-sarima-models-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-sarima-models-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","exportId":"gc90a0be09b0a290f138aa62d4823d44e"},{"id":197613,"title":"Time Series: Facebook Prophet - Lab","type":"Assignment","indent":1,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":null,"completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-facebook-prophet-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-facebook-prophet-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","exportId":"ga70b5b86efba6490cfd5ac5183691cfc"}]},{"id":21098,"name":"Topic 39: Natural Language Processing","status":"unlocked","unlockDate":null,"prereqs":[],"requirement":"all","sequential":false,"exportId":"g5f9e63789902db391d77480b4e906874","items":[{"id":197399,"title":"Natural Language Processing - Introduction","type":"WikiPage","indent":0,"locked":false,"requirement":"must_mark_done","completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-nlp-section-intro\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-nlp-section-intro/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eThis lesson summarizes the topics we'll be covering in this section and why they'll be important to you as a data scientist.\u003c/p\u003e\n\n\u003ch2\u003eFoundations of Natural Language Processing (NLP)\u003c/h2\u003e\n\n\u003cp\u003eIn this section we will be covering Natural Language Processing (NLP), which refers to analytics tasks that deal with natural human language, in the form of text or speech.\u003c/p\u003e\n\n\u003ch3\u003eNatural Language Tool Kit (NLTK)\u003c/h3\u003e\n\n\u003cp\u003eWe'll start by providing more context on the Natural Language Tool Kit (NLTK), one of the most popular NLP libraries used in Python.  This library was developed by researchers at the University of Pennsylvania, and it has quickly become one of the most powerful and complete library of NLP tools available. \u003c/p\u003e\n\n\u003ch3\u003eRegular Expressions\u003c/h3\u003e\n\n\u003cp\u003eData preprocessing is an essential part of NLP, and that's why being very familiar with \u003cstrong\u003eregular expressions\u003c/strong\u003e is extremely important. Regular expressions, or \"Regex\" is extremely useful for NLP. We can use regex to quickly pattern match and filter through text documents. \u003c/p\u003e\n\n\u003ch3\u003eFeature Engineering for Text Data\u003c/h3\u003e\n\n\u003cp\u003eWorking with text data comes with a lot of ambiguity. Feature engineering for NLP is pretty specific, and in this section you'll learn some feature engineering techniques that are essential when working with text data. You'll learn how to remove stop words from your text, as well as how to create frequency distributions, representing histograms that give us an overview of the total number of times each word occurs in a given text corpus. \u003c/p\u003e\n\n\u003cp\u003eAdditionally, you'll learn about stemming and lemmatization, which is the technique of removing suffixes from our words (and can enhance our text insight by creating frequency histograms \u003cem\u003eafter\u003c/em\u003e having performed stemming or lemmatization!). You'll also learn how to create bigrams, which creates an insight on how often two words occur together!\u003c/p\u003e\n\n\u003ch3\u003eContext-Free Grammars and Part-of-Speech (POS) Tagging\u003c/h3\u003e\n\n\u003cp\u003eIn NLP, it is important to understand what context-free grammars and part-of-speech tagging are. Context-free grammars refer to bits of text that are grammatically correct, but feel like complete nonsense when considering the same bit of text on the semantic level. POS tagging refers to the act of helping a computer understand how to interpret a sentence. The context-free grammars (CFG) defines the rules of how sentences can exist. You'll see multiple examples on how to use both CFG and POS tagging, and why they are important!\u003c/p\u003e\n\n\u003ch3\u003eText Classification\u003c/h3\u003e\n\n\u003cp\u003eWe will finish off this section by explaining the general process to set text data up for classification problems.\u003c/p\u003e\n\n\u003ch2\u003eSummary\u003c/h2\u003e\n\n\u003cp\u003eIn this section, you'll learn the foundations of NLP and different techniques to make a computer understand text!\u003c/p\u003e\n\u003cfooter class=\"fis-footer\" style=\"visibility: hidden;\"\u003e\u003cdiv class=\"fis-feedback\"\u003e\n\u003ch5\u003eHow do you feel about this lesson?\u003c/h5\u003e\n\u003cimg id=\"thumbs-up\" data-repository=\"dsc-nlp-section-intro\" title=\"Thumbs up!\" alt=\"thumbs up\"\u003e\u003cimg id=\"thumbs-down\" data-repository=\"dsc-nlp-section-intro\" title=\"Thumbs down!\" alt=\"thumbs down\"\u003e\n\u003c/div\u003e\n\u003ch5\u003eHave specific feedback? \u003ca href=\"https://github.com/learn-co-curriculum/dsc-nlp-section-intro/issues/new\"\u003eTell us here!\u003c/a\u003e\n\u003c/h5\u003e\u003c/footer\u003e","exportId":"natural-language-processing-introduction"},{"id":197404,"title":"NLP and Word Vectorization","type":"WikiPage","indent":0,"locked":false,"requirement":"must_mark_done","completed":false,"content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-nlp-and-word-vectorization\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-nlp-and-word-vectorization\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-nlp-and-word-vectorization/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eIn this lesson, we'll learn about some foundational concepts in Natural Language Processing such as stemming and lemmatization, as well as various strategies for converting text data into word vectors!\u003c/p\u003e\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\u003cp\u003eYou will be able to:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eExplain stemming and lemmatization\u003c/li\u003e\n\u003cli\u003eExplain what stop words are and why they are frequently removed\u003c/li\u003e\n\u003cli\u003eDefine tokenization in the context of NLP\u003c/li\u003e\n\u003cli\u003eDefine TF-IDF vectorization and its components\u003c/li\u003e\n\u003cli\u003eDefine count vectorization and its relationship to bag of words\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003eWhat is Natural Language Processing?\u003c/h2\u003e\n\u003cp\u003e\u003cstrong\u003e\u003cem\u003eNatural Language Processing\u003c/em\u003e\u003c/strong\u003e, or \u003cstrong\u003e\u003cem\u003eNLP\u003c/em\u003e\u003c/strong\u003e, is the study of how computers can interact with humans through the use of human language. Although this is a field that is quite important to Data Scientists, it does not belong to Data Science alone. NLP has been around for quite a while, and sits at the intersection of \u003cem\u003eComputer Science\u003c/em\u003e, \u003cem\u003eArtificial Intelligence\u003c/em\u003e, \u003cem\u003eLinguistics\u003c/em\u003e, and \u003cem\u003eInformation Theory\u003c/em\u003e. In the early days of NLP, it mainly consisted of trying to program algorithms that contained many rules borrowed from the field of linguistics. However, in the 1980s, machine learning started to show great success with many NLP tasks, and many of these rule-based methods took a back seat to approaches involving machine learning and AI. Fast forward to now, and NLP has become an area of applied machine learning that Data Scientists all around the globe work in every day.\u003c/p\u003e\n\u003ch2\u003eNLP and Bayesian Statistics\u003c/h2\u003e\n\u003cp\u003eAs machine learning has come into its own, we've seen NLP products get better and better. For instance, in just a few decades, we've gone from rule-based chat bots with preprogrammed responses to things like Siri and \u003ca class=\"\" href=\"https://www.youtube.com/watch?v=D5VN56jQMWM\"\u003eGoogle Duplex\u003c/a\u003e (if you aren't familiar with Duplex, take a few minutes to follow that link and watch the demo on YouTube -- you won't be disappointed!). Much of the most exciting advancements currently happening in the field of NLP are due to Deep Learning. However, we can still do amazing things with machine learning and text data by making use of Bayesian methods. For instance, you may remember a time in the early 2000s when the problem of email spam was bad, and getting worse. This problem was eventually solved through the application of machine learning -- specifically, \u003cstrong\u003e\u003cem\u003eNaive Bayesian Classification\u003c/em\u003e\u003c/strong\u003e! For the remainder of this section, we'll focus on how we can apply our newfound knowledge of Bayesian methods to solve real-world NLP tasks such as \u003ca href=\"http://www.paulgraham.com/spam.html\"\u003espam filtering\u003c/a\u003e and text classification.\u003c/p\u003e\n\u003ch2\u003eWorking With Text Data\u003c/h2\u003e\n\u003cp\u003eWorking with text data comes with a unique set of problems and solutions that other types of datasets don't have. Often, text data requires more cleaning and preprocessing than normal data, in order to get it into a format where we can use statistical methods or machine learning to work with it. Let's explore some of the things we generally need to do to get text data into a form where we can work with it.\u003c/p\u003e\n\u003ch2\u003eCreating a Bag of Words\u003c/h2\u003e\n\u003cp\u003eThe most common approach to working with text is to vectorize it by creating a \u003cstrong\u003e\u003cem\u003eBag of Words\u003c/em\u003e\u003c/strong\u003e. In this case, the name \"Bag of Words\" is quite descriptive of the final product -- the bag contains information about all the important words in the text individually, but not in any particular order. It's as if we take every word in a \u003cstrong\u003e\u003cem\u003eCorpus\u003c/em\u003e\u003c/strong\u003e and throw them into a bag. With a large enough corpus, we'll often see certain patterns start to emerge -- for instance, a bag of words made out of Shakespeare's \u003cem\u003eHamlet\u003c/em\u003e is probably more similar to a bag of words made out of \u003cem\u003eMacbeth\u003c/em\u003e than it is to something like \u003cem\u003eThe Hunger Games\u003c/em\u003e. The simplest way to create a bag of words is to just count how many times each unique word is used in a given corpus. If we have a number for every word, then we have a way to treat each bag as a \u003cstrong\u003e\u003cem\u003evector\u003c/em\u003e\u003c/strong\u003e, which opens up all kinds of machine learning tools for use.\u003c/p\u003e\n\u003cp\u003eLet's explore some of the steps that must occur before we can fully vectorize a text and work with it.\u003c/p\u003e\n\u003ch3\u003eBasic Cleaning and Tokenization\u003c/h3\u003e\n\u003cp\u003eOne of the most basic problems seen when working with text data is things like punctuation and capitalization. Although counting how many times a word appears in a text sounds straightforward at first, it can actually be quite complicated at times, and will almost always require some decisions on our part. For instance, consider the following sentence:\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\"Apple shareholders have had a great year. Apple's stock price has gone steadily upwards -- Apple even broke a trillion-dollar valuation, continuing the dominance of this tech stock.\"\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eIf we were to count how many times each word appears in this sentence, we would likely say that \"Apple\" has a count of three. However, if we wrote a basic Python script to do this, our algorithm would tell us that the word \"Apple\" only appears twice! To a computer, \"Apple\" and \"Apple's\" are different words. Capitalization is also a problem -- \"apple\" would also be counted as a different word. Similarly, punctuation is also a problem. A basic counting algorithm would see \"stock\" and \"stock.\" as two completely different words.\u003c/p\u003e\n\u003cp\u003eFirst and foremost, cleaning a text dataset usually means removing punctuation, and lowercasing everything. However, this can be tricky, and require decisions on your part based on the text you're working with and your goals -- for instance, whether or not apostrophes should be removed.\u003c/p\u003e\n\u003cp\u003eThe goal of this step is to create word \u003cstrong\u003e\u003cem\u003etokens\u003c/em\u003e\u003c/strong\u003e. The sentence \"Where did you get those coconuts?\", when cleaned and tokenized, would probably look more like \u003ccode\u003e['where', 'did', 'you, 'get', 'those', 'coconuts']\u003c/code\u003e.\u003c/p\u003e\n\u003cp\u003eHowever, there are still other important decisions to make during the tokenization stage. For instance, should \"run\" and \"runs\" be counted as the same token, or as different tokens? How about \"ran\", or \"running\"?\u003c/p\u003e\n\u003ch3\u003eStemming, Lemmatization, and Stop Words\u003c/h3\u003e\n\u003cp\u003eSometimes, depending on the task, it may be best to leave \"run\" and \"runs\" as different tokens. However, this often is not the case -- especially with smaller datasets. NLP methods such as \u003cstrong\u003e\u003cem\u003eStemming\u003c/em\u003e\u003c/strong\u003e and \u003cstrong\u003e\u003cem\u003eLemmatization\u003c/em\u003e\u003c/strong\u003e help us deal with this problem, where we reduce each word token down to its root word. For cases such as \"run\", \"runs\", \"running\" and \"ran\", they are more similar than different -- we may want our algorithm to treat these as the same word, \"run\".\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e\u003cem\u003eStemming\u003c/em\u003e\u003c/strong\u003e accomplishes this by removing the ends of words where the end signals some sort of derivational change to the word. For instance, we know that adding an 's' to the end of a word makes it plural -- a stemming algorithm given the word \"cats\" would return \"cat\". Note that stems do not have to make sense as actual English words. For example, \"ponies\" would be reduced to \"poni\", not \"pony\". Stemming is a more crude, heuristic process that contains rule sets that tells the algorithm how to stem each word, and what it should be stemmed to. The process is more crude than lemmatization, but it's also easier to implement. For instance, take a look at this example subset of stemming rules from the \u003ca href=\"https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html\"\u003eStanford NLP Group\u003c/a\u003e:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-nlp-and-word-vectorization/master/images/new_stemming.png\" width=\"400\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e\u003cem\u003eLemmatization\u003c/em\u003e\u003c/strong\u003e accomplishes pretty much the same thing as stemming, but does it in a more complex way, by examining the \u003cstrong\u003e\u003cem\u003emorphology\u003c/em\u003e\u003c/strong\u003e of words and attempting to reduce each word to its most basic form, or \u003cstrong\u003e\u003cem\u003elemma\u003c/em\u003e\u003c/strong\u003e. Note that the results here often end up a bit different than stemming. See the following table for an example of the differences in results:\u003c/p\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth style=\"text-align: center;\"\u003eWord\u003c/th\u003e\n\u003cth style=\"text-align: center;\"\u003eStem\u003c/th\u003e\n\u003cth style=\"text-align: center;\"\u003eLemma\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd style=\"text-align: center;\"\u003eStudies\u003c/td\u003e\n\u003ctd style=\"text-align: center;\"\u003eStudi\u003c/td\u003e\n\u003ctd style=\"text-align: center;\"\u003eStudy\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd style=\"text-align: center;\"\u003eStudying\u003c/td\u003e\n\u003ctd style=\"text-align: center;\"\u003eStudy\u003c/td\u003e\n\u003ctd style=\"text-align: center;\"\u003eStudy\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003cp\u003eFinally, you may have intuited that many words in a text are pretty much useless and contain little to no actual information. For instance, words such as \"the\" and \"of\". These are called \u003cstrong\u003e\u003cem\u003eStop Words\u003c/em\u003e\u003c/strong\u003e, and are often removed after tokenization is complete in order to reduce the dimensionality of each corpus down to only the words that contain important information. Popular NLP frameworks and toolkits such as NLTK contain a list of stop words for most languages, which allow us to easily loop through our tokenized corpus and remove any stop words we find.\u003c/p\u003e\n\u003ch2\u003eVectorization Strategies\u003c/h2\u003e\n\u003cp\u003eOnce we cleaned and tokenized our text data, we can convert it to vectors. However, there are a few different ways we can do this. Depending on our goals and our dataset, some may be more useful than others.\u003c/p\u003e\n\u003ch3\u003eCount Vectorization\u003c/h3\u003e\n\u003cp\u003eOne of the most basic, but useful ways of vectorizing text data is to simply count the number of times each word appears in the corpus. If working with a single document, we just create a single vector, where each element in the vector corresponds to the count of a unique word in the document. If working with multiple documents, we would store everything in a DataFrame, with each column representing a unique word, while each row represents the count vector for a given document.\u003c/p\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth style=\"text-align: center;\"\u003eDocument\u003c/th\u003e\n\u003cth style=\"text-align: center;\"\u003eAardvark\u003c/th\u003e\n\u003cth style=\"text-align: center;\"\u003eApple\u003c/th\u003e\n\u003cth\u003e...\u003c/th\u003e\n\u003cth\u003eZebra\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd style=\"text-align: center;\"\u003e1\u003c/td\u003e\n\u003ctd style=\"text-align: center;\"\u003e0\u003c/td\u003e\n\u003ctd style=\"text-align: center;\"\u003e3\u003c/td\u003e\n\u003ctd\u003e...\u003c/td\u003e\n\u003ctd\u003e1\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd style=\"text-align: center;\"\u003e2\u003c/td\u003e\n\u003ctd style=\"text-align: center;\"\u003e1\u003c/td\u003e\n\u003ctd style=\"text-align: center;\"\u003e2\u003c/td\u003e\n\u003ctd\u003e...\u003c/td\u003e\n\u003ctd\u003e0\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003cp\u003eNote that we do not need to have a column for every word in the English language -- just a column for each word that shows up in the total vocabulary of our document or documents. If we have multiple documents, we just combine the unique words from each document to get the total dimensionality that allows us to represent each. If a word doesn't show up in a given document, that's fine -- that just means the count is 0 for that row and column.\u003c/p\u003e\n\u003ch3\u003eTF-IDF Vectorization\u003c/h3\u003e\n\u003cp\u003eTF-IDF stands for \u003cstrong\u003e\u003cem\u003eTerm Frequency-Inverse Document Frequency\u003c/em\u003e\u003c/strong\u003e. It is a combination of two individual metrics, which are the TF and IDF, respectively. TF-IDF is used when we have multiple documents. It is based on the idea that rare words contain more information about the content of a document than words that are used many times throughout all the documents. For instance, if we treated every article in a newspaper as a separate document, looking at the amount of times the word \"he\" or \"she\" is used probably doesn't tell us much about what that given article is about -- however, the amount of times \"touchdown\" is used can provide good signal that the article is probably about sports.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e\u003cem\u003eTerm Frequency\u003c/em\u003e\u003c/strong\u003e is calculated with the following formula:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Clarge%20%5Ctext%7BTerm%20Frequency%7D(t)%20=%20%5Cfrac%7B%5Ctext%7Bnumber%20of%20times%20t%20appears%20in%20a%20document%7D%7D%7B%5Ctext%7Btotal%20number%20of%20terms%20in%20the%20document%7D%7D\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e\u003cem\u003eInverse Document Frequency\u003c/em\u003e\u003c/strong\u003e is calculated with the following formula:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Clarge%20%5Ctext%7BIDF%7D(t)%20=%20log_e(%5Cfrac%7B%5Ctext%7BTotal%20Number%20of%20Documents%7D%7D%7B%5Ctext%7BNumber%20of%20Documents%20with%20t%20in%20it%7D%7D)\"\u003e\u003c/p\u003e\n\u003cp\u003eThe \u003cstrong\u003e\u003cem\u003eTF-IDF\u003c/em\u003e\u003c/strong\u003e value for a given word in a given document is just found by multiplying the two!\u003c/p\u003e\n\u003ch2\u003eSummary\u003c/h2\u003e\n\u003cp\u003eIn this lesson, you learned about some foundational concepts in Natural Language Processing such as stemming and lemmatization, as well as various strategies for converting text data into word vectors.\u003c/p\u003e","exportId":"nlp-and-word-vectorization"},{"id":197408,"title":"Word Vectorization - Lab","type":"Assignment","indent":1,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_mark_done","completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-word-vectorization-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-word-vectorization-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","exportId":"g4af22bf8d80e6133dd9c77767c2ff51b"},{"id":197412,"title":"Introduction to NLP with NLTK","type":"WikiPage","indent":0,"locked":false,"requirement":"must_mark_done","completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-introduction-to-nltk\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-introduction-to-nltk/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eIn this lesson, we'll discuss a general overview of Natural Language Processing, and the popular Python library for NLP, \u003cstrong\u003e\u003cem\u003eNatural Language Tool Kit\u003c/em\u003e\u003c/strong\u003e (NLTK).\u003c/p\u003e\n\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\n\u003cp\u003eYou will be able to:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eIdentify ways we can use NLTK to simplify and accelerate common preprocessing tasks for text data\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch2\u003eWhat is Natural Language Processing?\u003c/h2\u003e\n\n\u003cp\u003e\u003cstrong\u003e\u003cem\u003eNatural Language Processing\u003c/em\u003e\u003c/strong\u003e, or \u003cstrong\u003e\u003cem\u003eNLP\u003c/em\u003e\u003c/strong\u003e, refers to analytics tasks that deal with natural human language, in the form of text or speech. These tasks usually involve some sort of machine learning, whether for text classification or for feature generation, but NLP isn't just machine learning. Tasks such as text preprocessing and cleaning also fall under the NLP umbrella. \u003c/p\u003e\n\n\u003cp\u003eThe most common Python library used for NLP tasks is \u003cstrong\u003e\u003cem\u003eNatural Language Tool Kit\u003c/em\u003e\u003c/strong\u003e, or NLTK for short. This library was developed by researchers at the University of Pennsylvania, and quickly became the most powerful and complete library of NLP tools available. \u003c/p\u003e\n\n\u003ch2\u003eUsing NLTK\u003c/h2\u003e\n\n\u003cp\u003eNLTK is a sort of \"one-stop shop\" for all things NLP. It contains many sample corpora, with everything from full texts from Project Gutenberg to transcripts of State of the Union speeches from US Presidents. This library contains functions and tools for everything from data cleaning and preprocessing, to linguistic analysis, to feature generation and extraction. NLTK even contains its own Bayesian classifiers for quick testing (although realistically, you'll likely want to continue using scikit-learn for these sorts of tasks). \u003c/p\u003e\n\n\u003cp\u003eNLP is unique in that in addition to statistics and math, it also relies heavily on the field of \u003cstrong\u003e\u003cem\u003eLingustics\u003c/em\u003e\u003c/strong\u003e. Many of the concepts you'll run into will be grounded in linguistics. Some of them will seem a bit foreign to you if you haven't studied languages or grammar yet, but don't worry! The reality of it all is that you don't need deep expertise in linguistics to work with text data, because NLTK was built by professionals to make it easier for everyone to access the linguistic tools and methods needed for working with text data. Although a linguist knows how to manually generate something like a \u003cstrong\u003e\u003cem\u003eParse Tree\u003c/em\u003e\u003c/strong\u003e for a sentence, NLTK provides this functionality for you in just a few lines of code. \u003c/p\u003e\n\n\u003cp\u003e\u003cstrong\u003eA sample Parse Tree created with NLTK\u003c/strong\u003e\u003c/p\u003e\n\n\u003cp\u003e\u003c/p\u003e \u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-introduction-to-nltk/master/images/new_parse_tree.png\" width=\"750\"\u003e \n\n\u003ch2\u003eWorking With Text, Simplified\u003c/h2\u003e\n\n\u003cp\u003eGenerally, projects that work with text data follow the same overall pattern as any other projects. The main difference is that text projects usually require a bit more cleaning and preprocessing than regular data, in order to get the text into a format that's usable for modeling. \u003c/p\u003e\n\n\u003cp\u003eHere are some of the ways that NLTK can make our lives easier when working with text data:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003e\u003cp\u003e\u003cstrong\u003e\u003cem\u003eStop Word Removal\u003c/em\u003e\u003c/strong\u003e: NLTK contains a full library of stop words, making it easy to remove the words that don't matter from our data.    \u003c/p\u003e\u003c/li\u003e\n\u003cli\u003e\u003cp\u003e\u003cstrong\u003e\u003cem\u003eFiltering and Cleaning\u003c/em\u003e\u003c/strong\u003e: NLTK provides simple, easy ways to create and filter frequency distributions, as well providing multiple ways to clean, stem, lemmatize, or tokenize datasets.   \u003c/p\u003e\u003c/li\u003e\n\u003cli\u003e\u003cp\u003e\u003cstrong\u003e\u003cem\u003eFeature Selection and Feature Engineering\u003c/em\u003e\u003c/strong\u003e: NLTK contains tools to quickly generate features such as bigrams and n-grams. It also contains major libraries such as the \u003cstrong\u003e\u003cem\u003ePenn Tree Bank\u003c/em\u003e\u003c/strong\u003e to allow quick feature engineering, such as generating part-of-speech tags, or sentence polarity. \u003c/p\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003eAgain, don't worry if you're not sure what things like 'lemmatize' mean yet -- we'll cover all of that soon! With effective use of NLTK, we can quickly process and work with text data, allowing us to quickly get our data into the shape needed for tasks we're familiar with, such as classification!\u003c/p\u003e\n\n\u003cp\u003eFor the remainder of this section, we're going to spend some time getting comfortable with NLTK, while also learning about foundational concepts of linguistics that underpin many of the tasks in NLP. We'll learn to effectively use NLTK to clean and preprocess data in a variety of ways. We'll gain some practice filtering data with regular expressions, generate text statistics to compare text documents, and quickly engineer features to help us better train classifiers for text classification!\u003c/p\u003e\n\n\u003ch2\u003eSummary\u003c/h2\u003e\n\n\u003cp\u003eIn this lesson, we learned about what NLP is, and how the NLTK package can save us time and make us more effective when working with text data. \u003c/p\u003e\n\u003cfooter class=\"fis-footer\" style=\"visibility: hidden;\"\u003e\u003cdiv class=\"fis-feedback\"\u003e\n\u003ch5\u003eHow do you feel about this lesson?\u003c/h5\u003e\n\u003cimg id=\"thumbs-up\" data-repository=\"dsc-introduction-to-nltk\" title=\"Thumbs up!\" alt=\"thumbs up\"\u003e\u003cimg id=\"thumbs-down\" data-repository=\"dsc-introduction-to-nltk\" title=\"Thumbs down!\" alt=\"thumbs down\"\u003e\n\u003c/div\u003e\n\u003ch5\u003eHave specific feedback? \u003ca href=\"https://github.com/learn-co-curriculum/dsc-introduction-to-nltk/issues/new\"\u003eTell us here!\u003c/a\u003e\n\u003c/h5\u003e\u003c/footer\u003e","exportId":"introduction-to-nlp-with-nltk"},{"id":197417,"title":"Introduction to Regular Expressions","type":"WikiPage","indent":0,"locked":false,"requirement":"must_mark_done","completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-introduction-to-regular-expressions\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-introduction-to-regular-expressions/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eIn this lesson, we'll learn about how we can use \u003cstrong\u003e\u003cem\u003eRegular Expressions\u003c/em\u003e\u003c/strong\u003e for pattern matching and filtering when working with text data. \u003c/p\u003e\n\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\n\u003cp\u003eYou will be able to:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eIdentify common use cases where regular expressions are useful \u003c/li\u003e\n\u003cli\u003eCreate regex code to capture meaningful patterns found in text \u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch2\u003eWhat Are Regular Expressions?\u003c/h2\u003e\n\n\u003cp\u003e\u003cstrong\u003e\u003cem\u003eRegular Expressions\u003c/em\u003e\u003c/strong\u003e are a type of pattern that describe some text. We can use these regular expressions to quickly match patterns and filter through text documents. Regular Expressions (or regex, for short) are an important tool anytime we need to pull information from a larger text document without manually reading the entire thing. For data scientists, regex is extremely useful for data gathering. With regex, we can quickly scrape webpages by using regex to search through the html and find the info needed. \u003c/p\u003e\n\n\u003ch3\u003eUse Cases for NLP\u003c/h3\u003e\n\n\u003cp\u003eRegex is especially useful for Natural Language Processing. By definition, just about any text document you work with on an NLP task is going to be one that contains a large amount of text. One of the more common NLP-specific use cases for regex is to use regex during the tokenization stage to define the rules for where we should split strings into separate tokens. As an example, NLTK's basic \u003ccode\u003eword_tokenize()\u003c/code\u003e function would split a word that contains an apostrophe into 3 separate tokens -- \u003ccode\u003e'they're'\u003c/code\u003e gets broken into \u003ccode\u003e[\"they\", \"'\", \"re\"]\u003c/code\u003e. This is because the word tokenizer has instructions to just grab sequences of letters as the basic tokens, and an apostrophe isn't a letter. When preprocessing text data, it's quite common to use some small regex patterns to create a more intelligent tokenization scheme to avoid problems like this, so that our tokenizer treats words like \u003ccode\u003e'they're'\u003c/code\u003e as a single token. \u003c/p\u003e\n\n\u003ch2\u003eCreating Basic Patterns\u003c/h2\u003e\n\n\u003cp\u003eRegex is only as good as the \u003cstrong\u003e\u003cem\u003ePatterns\u003c/em\u003e\u003c/strong\u003e we create. We can use these patterns to find, or to replace text. There are many, many things we can do with regex, and covering them all is outside the scope of this lesson. Instead, we'll just focus on some of the more useful, basic patterns that allow us to begin using regex to work with text data. \u003c/p\u003e\n\n\u003cp\u003eLet's take a look at a basic regex pattern, to get a feel for what they look like. \u003c/p\u003e\n\n\u003cpre\u003e\u003ccode class=\"python\"\u003eimport re\nsentence = 'he said that she said \"hello\".'\npattern = 'he'\np = re.compile(sentence)\np.findall() # Output will be ['he', 'he, 'he']\n\u003c/code\u003e\u003c/pre\u003e\n\n\u003cp\u003eWe define a pattern by a Python string. We can then use the regular expressions library, \u003ccode\u003ere\u003c/code\u003e, to compile this pattern. Once we have a compiled pattern, we just need to pass in a string and the pattern will find every instance of that pattern in the string. \u003c/p\u003e\n\n\u003cp\u003eFor people new to regex, the results from the pattern above might be surprising at first. The pattern successfully matches the word 'he', but it also matches the letters 'he' that are found inside of the words 'she' and 'hello'.  Subsequences inside of larger sequences are fair game to regex. If we just wanted to match the word 'he', we would need to specify that the pattern needs to start and end with a space, or use of \u003cstrong\u003e\u003cem\u003eanchors\u003c/em\u003e\u003c/strong\u003e for things like word boundaries. \u003c/p\u003e\n\n\u003ch2\u003eRanges, Groups, and Quantifiers\u003c/h2\u003e\n\n\u003cp\u003eObviously, we don't want to have to explicitly type every valid match for any search into our pattern. That would defeat the purpose. Luckily, we don't have to type every possible uppercase letter to match on uppercase letters. Instead, we can use a \u003cstrong\u003e\u003cem\u003eRange\u003c/em\u003e\u003c/strong\u003e such as \u003ccode\u003e[A-Z]\u003c/code\u003e. This will match any uppercase letter. Ranges are always inside of square brackets. We can put many things inside of ranges at the same time, and regex will match on any of them. For instance, if we wanted to find any uppercase letter, lowercase letter, or digit, we could use \u003ccode\u003e[A-Za-z0-9]\u003c/code\u003e. \u003c/p\u003e\n\n\u003ch3\u003eCharacter Classes\u003c/h3\u003e\n\n\u003cp\u003eCharacter classes are a special case of ranges. Since it's quite a common task to use ranges to do things like match on words or numbers, regex actually includes character classes as a shortcut. For instance, we could use \u003ccode\u003e\\d\u003c/code\u003e to match any digit -- this is equivalent to using \u003ccode\u003e[0-9]\u003c/code\u003e. We could also use \u003ccode\u003e\\w\u003c/code\u003e to match on any word. In the same vein, we can use \u003ccode\u003e\\D\u003c/code\u003e to get anything that \u003cem\u003eisn't\u003c/em\u003e a digit, or \u003ccode\u003e\\W\u003c/code\u003e to match on everything that isn't a word. There are a few other types of character classes as well. For a full list, check out the cheat sheet below!\u003c/p\u003e\n\n\u003ch3\u003eGroups and Quantifiers\u003c/h3\u003e\n\n\u003cp\u003eGroups are kind of like ranges, but they specify an exact pattern to match on. Groups are denoted by parentheses. Whereas \u003ccode\u003e[A-Z0-9]\u003c/code\u003e matches on any uppercase letter or any digit, \u003ccode\u003e(A-Z0-9)\u003c/code\u003e will only match on the sequence \u003ccode\u003e'A-Z0-9'\u003c/code\u003e exactly. This becomes much more useful when paired with \u003cstrong\u003e\u003cem\u003eQuantifiers\u003c/em\u003e\u003c/strong\u003e, which allows us to specify how many times a group should happen in a row. If we want to specify an exact number of times, we can use curly braces. For instance, a group followed by \u003ccode\u003e{3}\u003c/code\u003e will only match on patterns that have that group repeated exactly 3 times. The most common quantifiers are usually:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003e\n\u003ccode\u003e*\u003c/code\u003e (0 or more times)\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003e+\u003c/code\u003e (1 or more times)\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003e?\u003c/code\u003e (0 or 1 times)\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003eIn this way, we can fill a grouping with any pattern, tell and specify the number of times we can expect to see that pattern. When we include things like ranges, groupings, and quantifiers together, it becomes easy to write a pattern that can match complex things, like email addresses -- take a look at the example provided below, and see if you can figure out how it works!\u003c/p\u003e\n\n\u003cp\u003e\u003ccode\u003e'([A-Za-z]+)@([A-Za-z]+)\\.com'\u003c/code\u003e \u003c/p\u003e\n\n\u003cp\u003eThis pattern matches basic email addresses like '\u003ca href=\"mailto:joe@gmail.com\"\u003ejoe@gmail.com\u003c/a\u003e', but not '\u003ca href=\"mailto:john.doe@gmail.com\"\u003ejohn.doe@gmail.com\u003c/a\u003e', or '\u003ca href=\"mailto:joe@stanford.edu\"\u003ejoe@stanford.edu\u003c/a\u003e'. Take a look at the pattern again -- how would you need to modify the pattern in order for it to match either of those, as well?\u003c/p\u003e\n\n\u003ch2\u003eAlways Keep A Cheat Sheet Handy\u003c/h2\u003e\n\n\u003cp\u003eRegex is confusing, but it gets easier. With that being said, don't worry about trying to memorize all of the different symbols and metacharacters. Instead, focus on how patterns work, and just look up the symbols when you need them. The internet is filled with great regex cheatsheets. Here's an easy one to keep on hand for future reference:\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-introduction-to-regular-expressions/master/images/regex_cheat_sheet.png\"\u003e\u003c/p\u003e\n\n\u003ch2\u003eSummary\u003c/h2\u003e\n\n\u003cp\u003eIn this lesson, we learned about what regular expressions are, how they are used in NLP for specific tasks, and some common patterns and tools in regex. \u003c/p\u003e\n\u003cfooter class=\"fis-footer\" style=\"visibility: hidden;\"\u003e\u003cdiv class=\"fis-feedback\"\u003e\n\u003ch5\u003eHow do you feel about this lesson?\u003c/h5\u003e\n\u003cimg id=\"thumbs-up\" data-repository=\"dsc-introduction-to-regular-expressions\" title=\"Thumbs up!\" alt=\"thumbs up\"\u003e\u003cimg id=\"thumbs-down\" data-repository=\"dsc-introduction-to-regular-expressions\" title=\"Thumbs down!\" alt=\"thumbs down\"\u003e\n\u003c/div\u003e\n\u003ch5\u003eHave specific feedback? \u003ca href=\"https://github.com/learn-co-curriculum/dsc-introduction-to-regular-expressions/issues/new\"\u003eTell us here!\u003c/a\u003e\n\u003c/h5\u003e\u003c/footer\u003e","exportId":"introduction-to-regular-expressions"},{"id":197423,"title":"Regular Expressions - Codealong","type":"Assignment","indent":1,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_mark_done","completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-regular-expressions-codealong\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-regular-expressions-codealong/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","exportId":"gcf7f63d29efacf29c4069fd830ae1ce3"},{"id":197426,"title":"Feature Engineering for Text Data","type":"WikiPage","indent":0,"locked":false,"requirement":"must_mark_done","completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-feature-engineering-for-text-data\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-feature-engineering-for-text-data/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eIn this lesson, we'll examine some common approaches to feature engineering for text data. \u003c/p\u003e\n\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\n\u003cp\u003eYou will be able to:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eExplain what stop words are and why they are frequently removed \u003c/li\u003e\n\u003cli\u003eExplain stemming and lemmatization\u003cbr\u003e\n\u003c/li\u003e\n\u003cli\u003eDefine bigrams and n-grams \u003c/li\u003e\n\u003cli\u003eDefine mutual information in the context of NLP \u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch2\u003eCommon Approaches to NLP Feature Engineering\u003c/h2\u003e\n\n\u003cp\u003eAs you've likely noticed by now, working with text data comes with \u003cstrong\u003e\u003cem\u003ea lot\u003c/em\u003e\u003c/strong\u003e of ambiguity. When all we start with is an arbitrarily-sized string of words, there's no clear answer as to what sorts of features we should engineer, or even where we should start! The goal of this lesson is to provide a framework for working with text data, and help us figure out exactly what sorts of features we should create when working with text data. \u003c/p\u003e\n\n\u003cp\u003eIn this lesson, we'll focus on the following topics:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eStopword Removal\u003c/li\u003e\n\u003cli\u003eFrequency Distributions\u003c/li\u003e\n\u003cli\u003eStemming and Lemmatization\u003c/li\u003e\n\u003cli\u003eBigrams, N-grams, and Mutual Information Score\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch2\u003eRemoving Stop Words\u003c/h2\u003e\n\n\u003cp\u003eWhen working with text data, one of the first steps to try is to remove the \u003cstrong\u003e\u003cem\u003eStop Words\u003c/em\u003e\u003c/strong\u003e from the text. One common feature of text data (regardless of language!) is the inclusion of stop words for grammatical structure. Words such as \"a\", \"and\", \"but\", and \"or\" are examples of stop words. While a sentence would be both grammatically incorrect and hard to understand without them, from a modeling standpoint, stop words provide little to no actual value. If we create a \u003cstrong\u003e\u003cem\u003eFrequency Distribution\u003c/em\u003e\u003c/strong\u003e to see the number of times each word is used in a corpus, we'll almost always find that the top spots are dominated by stop words, which tell us nothing about the actual content of the corpus. Removing stop words allows us to reduce the overall dimensionality of our dataset (which is always a good thing), while also distilling the overall vocabulary of our bag-of-words down only to the words that really matter. \u003c/p\u003e\n\n\u003cp\u003e\u003cem\u003eNLTK\u003c/em\u003e makes it extremely easy to remove stopwords. The library includes a full corpus of all stopwords for all the languages NLTK supports. Since we usually only want the stopwords relevant to the language our text data is in, NLTK even makes it easy to filter out the unneeded stop words and grab only the ones that pertain to our problem. \u003c/p\u003e\n\n\u003cp\u003eThe following example shows how we can get all the stopwords for English from NLTK:\u003c/p\u003e\n\n\u003cpre\u003e\u003ccode class=\"python\"\u003efrom nltk.corpus import stopwords\nimport string\n\nstopwords_list = stopwords.words('english')\n\nstopwords_list += list(string.punctuation)\n\u003c/code\u003e\u003c/pre\u003e\n\n\u003cp\u003eOnce we have a list of stopwords, we can easily remove them from our text data after we've tokenized our data. Recall that we can easily tokenize text data using NLTK's \u003ccode\u003eword_tokenize()\u003c/code\u003e function. Once we have a list of word tokens, all we need to do is use a list comprehension, and omit any tokens that can be found in our stopwords list.  For example:\u003c/p\u003e\n\n\u003cpre\u003e\u003ccode class=\"python\"\u003efrom nltk import word_tokenize\n\ntokens = word_tokenize(some_text_data)\n\nstopped_tokens = [w.lower() for w in tokens if w not in stopwords_list]\n\u003c/code\u003e\u003c/pre\u003e\n\n\u003ch2\u003eFrequency Distributions\u003c/h2\u003e\n\n\u003cp\u003eOnce we have tokenized our data and removed all the stop words, the next step is usually to explore our text data through a \u003cstrong\u003e\u003cem\u003eFrequency Distribution\u003c/em\u003e\u003c/strong\u003e. This is just a fancy way of saying that we create a histogram that tells us the total number of times each word is used in a given corpus. \u003c/p\u003e\n\n\u003cp\u003eOnce we have tokenized our text data, we can use NLTK to easily create a frequency distribution using \u003ccode\u003enltk.FreqDist()\u003c/code\u003e. A frequency distribution is analogous to a Python dictionary, with a few more bells and whistles attached to make it easier to use for NLP tasks. Each key is a word token, and each value is the corresponding number of times that token appeared in the tokenized corpus given to the \u003ccode\u003eFreqDist\u003c/code\u003e object at instantiation. \u003c/p\u003e\n\n\u003cp\u003eWe can easily filter a \u003ccode\u003eFreqDist()\u003c/code\u003e object to see the most common words by using the \u003ccode\u003e.most_common()\u003c/code\u003e built-in method, as seen below:\u003c/p\u003e\n\n\u003cpre\u003e\u003ccode class=\"python\"\u003efrom  nltk import FreqDist\nfreqdist = FreqDist(tokens)\n\nmost_common = freqdist.most_common(200)\n\u003c/code\u003e\u003c/pre\u003e\n\n\u003cp\u003eOnce we have the most common words, we can easily use this to filter out the text and reduce the dimensionality of particularly large datasets, as needed. \u003c/p\u003e\n\n\u003ch2\u003eStemming and Lemmatization\u003c/h2\u003e\n\n\u003cp\u003eConsider the words 'run', 'running', 'ran', and 'runs'. If we create a basic frequency distribution, each of these words will be treated as a separate token. After all, they are different words. However, we know that they pretty much mean the same thing. Counting these words as individual separate tokens can sometimes hurt our model by needlessly increasing dimensionality, and hiding important information from our model. Although we instinctively know that those four words are all talking about the same action, our model will default to thinking that they are four completely different concepts. The way we deal with this is to remove suffixes through techniques such as \u003cstrong\u003e\u003cem\u003eStemming\u003c/em\u003e\u003c/strong\u003e or \u003cstrong\u003e\u003cem\u003eLemmatization\u003c/em\u003e\u003c/strong\u003e.\u003c/p\u003e\n\n\u003cp\u003ePeople often get stemming and lemmatization confused, because they are extremely similar. They generally accomplish the same task, but they use different means to do so. \u003c/p\u003e\n\n\u003cp\u003e\u003cstrong\u003e\u003cem\u003eStemming\u003c/em\u003e\u003c/strong\u003e follows a predetermined set of rules to reduce a word to its \u003cem\u003estem\u003c/em\u003e.  Words like 'running' and 'runs' will be reduced down to 'run', because the stemmer contains rules that understands how to deal with suffixes such as '-ing' and '-s'. The best stemmer currently available is the \u003cstrong\u003e\u003cem\u003ePorter Stemmer\u003c/em\u003e\u003c/strong\u003e. For code samples demonstrating how to use it, check out NLTK's documentation for the \u003ca href=\"http://www.nltk.org/howto/stem.html\"\u003ePorter Stemmer\u003c/a\u003e.\u003c/p\u003e\n\n\u003cp\u003e\u003cstrong\u003e\u003cem\u003eLemmatization\u003c/em\u003e\u003c/strong\u003e differs from stemming in that it reduces each word down to a linguistically valid \u003cstrong\u003e\u003cem\u003elemma\u003c/em\u003e\u003c/strong\u003e, or root word. It does this through stored linguistic mappings. Lemmatization is generally more complex, but also more accurate. This is because the rules that guide things like the Porter Stemmer are good, but far from perfect. For example, stemmers commonly deal with the suffix \u003ccode\u003e-ed\u003c/code\u003e by just  dropping it from the word. This usually works, until it runs into an edge case like the word 'agreed'. When stemmed, 'agreed' becomes 'agre'. Lemmatization does not make this mistake, because it contains a mapping for the word that tells it what 'agreed' should be reduced down to. Generally, most lemmatizers make use of the famous \u003cstrong\u003e\u003cem\u003eWordNet\u003c/em\u003e\u003c/strong\u003e lexical database. \u003c/p\u003e\n\n\u003cp\u003eNLTK makes it quite easy to make use of lemmatization, as demonstrated below:\u003c/p\u003e\n\n\u003cpre\u003e\u003ccode class=\"python\"\u003efrom nltk.stem.wordnet import WordNetLemmatizer\n\nlemmatizer = WordNetLemmatizer()\n\nlemmatizer.lemmatize('feet') # foot\nlemmatizer.lemmatize('running') # run\n\u003c/code\u003e\u003c/pre\u003e\n\n\u003ch2\u003eBigrams and Mutual Information Score\u003c/h2\u003e\n\n\u003cp\u003eAnother alternative to tokenization is to instead create \u003cstrong\u003e\u003cem\u003eBigrams\u003c/em\u003e\u003c/strong\u003e out of the text. A bigram is just a pair of adjacent words, treated as a single unit. \u003c/p\u003e\n\n\u003cp\u003eConsider the sentence \"the dog played outside\". If we created bigrams out of this sentence, we would get \u003ccode\u003e('the', 'dog'), ('dog', 'played'), ('played', 'outside')\u003c/code\u003e. From a modeling perspective, this can be quite useful, because sometimes pairs of words are greater than the sum of their parts. Note that bigrams are just a special case of \u003cstrong\u003e\u003cem\u003en-grams\u003c/em\u003e\u003c/strong\u003e -- we can choose any number of words for a sequence. Alternatively, it's quite common to create n-grams at the character level, rather than the word level. \u003c/p\u003e\n\n\u003cp\u003eOne handy feature of bigrams is that we can apply a frequency filter to only keep bigrams that show up more than a set number of times. In this way, we can get rid of all bigrams that only occur because of random chance, and keep the bigrams that must mean something, because they occur together multiple times. How strict your frequency filter should be depends on a number of factors, and generally, it's something you'll have to experiment with to get right. However, most experts tend to apply a minimum frequency filter of 5. \u003c/p\u003e\n\n\u003cp\u003eAnother way we can make use of bigrams is to calculate their \u003cstrong\u003e\u003cem\u003ePointwise Mutual Information Score\u003c/em\u003e\u003c/strong\u003e. This is a statistical measure from information theory that generally measures the mutual dependence between two words. In plain english, this measures how much information the bigram itself contains by computing the dependence between the two words in the bigram. For instance, the bigram \u003ccode\u003e('San', 'Francisco')\u003c/code\u003e would likely have a high mutual information score, because when these tokens appear in the text, it is highly likely that they appear together, and unlikely that they appear next other words. \u003c/p\u003e\n\n\u003cp\u003eIn practice, you don't need to worry too much about how to calculate mutual information, because NLTK provides an easy way to do this for us. We'll explore this in detail in the next lab. Instead, your main takeaway on this topic should be that mutual information scores are a type of feature that you can engineer for text data that may provide good information for you when it comes to exploring the text data or fitting a model to it. \u003c/p\u003e\n\n\u003ch2\u003eSummary\u003c/h2\u003e\n\n\u003cp\u003eIn this lesson, we learned about various types of feature engineering we can perform on text data, and what each one means!\u003c/p\u003e\n\u003cfooter class=\"fis-footer\" style=\"visibility: hidden;\"\u003e\u003cdiv class=\"fis-feedback\"\u003e\n\u003ch5\u003eHow do you feel about this lesson?\u003c/h5\u003e\n\u003cimg id=\"thumbs-up\" data-repository=\"dsc-feature-engineering-for-text-data\" title=\"Thumbs up!\" alt=\"thumbs up\"\u003e\u003cimg id=\"thumbs-down\" data-repository=\"dsc-feature-engineering-for-text-data\" title=\"Thumbs down!\" alt=\"thumbs down\"\u003e\n\u003c/div\u003e\n\u003ch5\u003eHave specific feedback? \u003ca href=\"https://github.com/learn-co-curriculum/dsc-feature-engineering-for-text-data/issues/new\"\u003eTell us here!\u003c/a\u003e\n\u003c/h5\u003e\u003c/footer\u003e","exportId":"feature-engineering-for-text-data"},{"id":197430,"title":"Corpus Statistics - Lab","type":"Assignment","indent":1,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_mark_done","completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-corpus-statistics-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-corpus-statistics-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","exportId":"gd9ca4d7e43d0dd71897c824e143e4b8f"},{"id":197434,"title":"Context-Free Grammars and POS Tagging","type":"WikiPage","indent":0,"locked":false,"requirement":"must_mark_done","completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-context-free-grammars-and-POS-tagging\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-context-free-grammars-and-POS-tagging/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eIn this lesson, we'll explore the concept of context-free grammars, and the role they play in linguistics and NLP, particularly in relation to part-of-speech tagging.\u003c/p\u003e\n\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\n\u003cp\u003eYou will be able to:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eDescribe how you might manually create rules related to context-free grammar \u003c/li\u003e\n\u003cli\u003eDefine context-free grammars \u003c/li\u003e\n\u003cli\u003eExplain parts of speech (POS) tagging, and why it is important in NLP \u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch2\u003eWhat is a Context-Free Grammar?\u003c/h2\u003e\n\n\u003cp\u003eConsider the following sentence: \u003c/p\u003e\n\n\u003cblockquote\u003e\n\u003cp\u003e\"Colorless green ideas sleep furiously.\"\u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003cp\u003eThis is a sentence dreamed up by the famous linguist \u003ca href=\"https://en.wikipedia.org/wiki/Noam_Chomsky\"\u003eNoam Chomsky\u003c/a\u003e. This sentence, while correct at the \u003cstrong\u003e\u003cem\u003egrammatical\u003c/em\u003e\u003c/strong\u003e or \u003cstrong\u003e\u003cem\u003esyntactic\u003c/em\u003e\u003c/strong\u003e level, is just a bunch of nonsense when we consider it at the \u003cstrong\u003e\u003cem\u003esemantic\u003c/em\u003e\u003c/strong\u003e level. The sentence follows all the proper rules for a sentence in English, although in reality, it's complete nonsense. This was one of Chomsky's big ideas -- that speech contains an underlying \"deep structure\" that we recognize, regardless of the actual content of the sentence. We don't need any context about what the sentence is actually about to determine if the grammar is correct -- hence the name, \u003cstrong\u003e\u003cem\u003eContext-Free Grammar\u003c/em\u003e\u003c/strong\u003e, which we'll refer to as 'CFG' for short, for the remainder of this lesson. \u003c/p\u003e\n\n\u003cp\u003eIn order to understand CFGs, we first need to back up and gain a little background knowledge about linguistics. According to linguistics, there are five different levels of language:\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-context-free-grammars-and-POS-tagging/master/images/new_LevelsOfLanguage-Graph.png\"\u003e\u003c/p\u003e\n\n\u003cp\u003eWhen talking about CFGs, we're focusing on the \u003cstrong\u003e\u003cem\u003esyntactic level\u003c/em\u003e\u003c/strong\u003e. This level worries only about the structure of the sentence, not the informational content. \u003c/p\u003e\n\n\u003cp\u003eSo why do CFGs matter to us? For starters, they are an important part of computer science as a whole, as any code we write gets fed through a parser to determine what we want the computer to actually do. For NLP specifically, they are important because they describe a way that we can write a grammar to interpret sentences at the syntactic level. This is an approach that can be used when we want to generate \u003cstrong\u003e\u003cem\u003ePart-Of-Speech (POS) Tags\u003c/em\u003e\u003c/strong\u003e. Consider the word \"run\". This word can be interpreted as either a noun or a verb. As a noun, we may be talking about the concept of going for a jog, or a run scored in a baseball game. As a verb, we may be talking about the action of running. On its own, we don't know this. Part of the way we know which meaning to interpret for the word is our understanding of where the word fits into the sentence, and the part of speech it occupies in that sentence -- we implicitly recognize that the sentence \"I run in the mornings\" uses run as a verb, while the sentence \"The Yankees scored a run\" uses it as a noun, all based on it's placement in the sentence. \u003c/p\u003e\n\n\u003cp\u003eThis brings us to the concept of \u003cstrong\u003e\u003cem\u003eParse Trees\u003c/em\u003e\u003c/strong\u003e. \u003c/p\u003e\n\n\u003ch2\u003eParse Trees and Sentence Structure\u003c/h2\u003e\n\n\u003cp\u003eIn English, sentences consist of a \u003cstrong\u003e\u003cem\u003eNoun Phrase\u003c/em\u003e\u003c/strong\u003e followed by a \u003cstrong\u003e\u003cem\u003eVerb Phrase\u003c/em\u003e\u003c/strong\u003e, which may optionally be followed by a \u003cstrong\u003e\u003cem\u003ePrepositional Phrase\u003c/em\u003e\u003c/strong\u003e. This seems simple, but it gets more tricky when we realize that there is a recursive structure to these phrases. A noun phrase may consist of multiple smaller noun phrases, and in some cases, even a verb phrase. Similarly, a verb phrase can consist of multiple smaller verb phrases and noun phrases, which can themselves be made up of smaller noun phrases and verb phrases. \u003c/p\u003e\n\n\u003cp\u003eThis leads levels of \u003cstrong\u003e\u003cem\u003eambiguity\u003c/em\u003e\u003c/strong\u003e that can be troublesome for computers. NLTK's documentation explains this by examining the classic Groucho Marx joke:\u003c/p\u003e\n\n\u003cblockquote\u003e\n\u003cp\u003e\"While hunting in Africa, I shot an elephant in my pajamas. How he got into my pajamas, I don't know.\"\u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003cp\u003eThere are two different ways that we can interpret the first sentence. The common way that we interpret it is that a person shot an elephant while wearing pajamas. However, the alternative interpretation that is still correct (and the source of Marx's timeless punchline) is that Marx shot an elephant that was actually \u003cem\u003ein\u003c/em\u003e his pajamas. While we humans immediately understand the correct interpretation of the sentence (and hopefully get the joke), a computer has no way of knowing which of the two is the correct interpretation. \u003c/p\u003e\n\n\u003cp\u003eThe difference between the two interpretations can be most easily understood by comparing the \u003cstrong\u003e\u003cem\u003eParse Tree\u003c/em\u003e\u003c/strong\u003e for each. Take a look at this diagram from the \u003ca href=\"https://www.nltk.org/book/ch08.html\"\u003eNLTK Book's chapter on analyzing sentence structure\u003c/a\u003e:\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-context-free-grammars-and-POS-tagging/master/images/parse_tree.png\"\u003e\u003c/p\u003e\n\n\u003cp\u003eLet's break these diagrams down piece by piece. The first, most natural interpretation of the phrase \"I shot an elephant in my pajamas\" breaks down the sentence as such:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eNoun phrase: \u003ccode\u003e['I']\u003c/code\u003e\n\u003c/li\u003e\n\u003cli\u003eVerb phrase: \u003ccode\u003e['shot', 'an', 'elephant']\u003c/code\u003e\n\u003c/li\u003e\n\u003cli\u003ePrepositional phrase: \u003ccode\u003e['in', 'my', 'pajamas']\u003c/code\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003eThis is the basic sentence structure that we are used to hearing. The noun phrase tells us the subject of the sentence, the verb phrase tells us what the subject did, and the prepositional phrase offers more information about the circumstances of the action, e.g. where, when, how, etc. Note that the verb phrase here is made up of a verb ('shot'), followed by a noun phrase ('an elephant'), much in the same way that the prepositional phrase consists of a preposition ('in'), followed by a noun phrase ('my pajamas'). This nested structure is \u003cstrong\u003e\u003cem\u003erecursive\u003c/em\u003e\u003c/strong\u003e.\u003c/p\u003e\n\n\u003cp\u003eHowever, the ambiguity that Marx plays off of uses the second parse tree's structure:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eNoun phrase: \u003ccode\u003e['I']\u003c/code\u003e\n\u003c/li\u003e\n\u003cli\u003eVerb phrase: \u003ccode\u003e['shot', 'an', 'elephant', 'in', 'my', 'pajamas']\u003c/code\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003eIf we compare the two parse trees visually, the difference becomes clear. Whereas in the first interpretation, the verb phrase consists of a verb phrase and a prepositional phrase, the second interpretation is different, treating the prepositional phrase as part of a noun phrase, which is, in turn, part of the noun phrase contained within that verb phrase. If all the grammar terms are making your head spin a little, don't worry, that's normal! The simple explanation here is that the first interpretation treats 'elephant' and 'in my pajamas' as belonging to different things, while the second treats 'elephant in my pajamas' as a single phrase. \u003c/p\u003e\n\n\u003ch2\u003eWhy Does This Matter?\u003c/h2\u003e\n\n\u003cp\u003eYou may be wondering why any of this actually matters to a Data Scientist. At a glance, it mostly just seems like a rehashing of a bunch of grade-school grammar rules. The answer is that using parse trees to understand sentence structure can help us determine meaning when working with human speech. It also helps highlight why this is such a complicated task -- computers do not have the ability to judge the meaning of a sentence based on things like semantic context like we do. Put simply, we know what an elephant is, what pajamas are, and understand that it's highly unlikely that an elephant could fit in pajamas. This helps us determine how we understand that sentence on the fly -- computers don't have this luxury, so they don't know which to choose!\u003c/p\u003e\n\n\u003ch2\u003ePOS Tagging and CFGs\u003c/h2\u003e\n\n\u003cp\u003eThis brings us to part of speech tagging. One way that we can help a computer understand how to interpret a sentence is to create a CFG for it to use when parsing. The CFG defines the rules of how sentences can exist. We do this by labeling different word tokens as their grammatical types, and then defining which combinations of grammatical types are valid examples of verb phrases, noun phrases, etc. \u003c/p\u003e\n\n\u003cp\u003eLet's take a look at the example CFG from the NLTK link provided above:\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-context-free-grammars-and-POS-tagging/master/images/cfg.png\"\u003e\u003c/p\u003e\n\n\u003cp\u003eLet's break down this CFG, and see if we can understand it a bit better. \u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003e\n\u003ccode\u003eS -\u0026gt; NP VP\u003c/code\u003e A sentence (S) consists of a Noun Phrase (NP) followed by a Verb Phrase (VP).\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003ePP -\u0026gt; P NP\u003c/code\u003e A Prepositional Phrase (PP) consists of a Preposition (P) followed by a Noun Phrase (NP)\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003eNP -\u0026gt; Det N | Det N PP | 'I'\u003c/code\u003e A Noun Phrase (NP) can consist of:\n\n\u003cul\u003e\n\u003cli\u003ea Determiner (Det) followed by a Noun (N), or (as denoted by \u003ccode\u003e|\u003c/code\u003e) \u003c/li\u003e\n\u003cli\u003ea Determiner (Det) followed by a Noun (N), followed by a Prepositional Phrase (PP), or\u003c/li\u003e\n\u003cli\u003eThe token \u003ccode\u003e'I'\u003c/code\u003e.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003eVP -\u0026gt; V NP | VP PP\u003c/code\u003e A Verb Phrase can consist of:\n\n\u003cul\u003e\n\u003cli\u003ea Verb (V) followed by a Noun Phrase (NP) or\u003c/li\u003e\n\u003cli\u003ea Verb Phrase (VP) followed by a Prepositional Phrase (PP)\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003eDet -\u0026gt; 'an' | 'my'\u003c/code\u003e Determiners are the tokens 'an' or 'my'\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003eN -\u0026gt; 'elephant' | 'pajamas'\u003c/code\u003e Nouns are the tokens 'elephant' or 'pajamas'\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003eV -\u0026gt; 'shot'\u003c/code\u003e Verbs are the token 'shot'\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003eP -\u0026gt; 'in'\u003c/code\u003e Prepositions are the token 'in'\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003eAs we can see, the CFG provides explicit rules as to both:\n1. How sentences, noun phrases, verb phrases, and prepositional phrases may be structured\n2. What parts of speech each token belongs to \u003c/p\u003e\n\n\u003cp\u003eThis defines a very small CFG that allows the parser to successfully generate parse trees for the Groucho Marx's sentence. Note that both the parse trees seen above are valid, according to the rules defined in this grammar. Even though this grammar is quite explicit, both of them work. \u003c/p\u003e\n\n\u003cp\u003eSo what happens if this CFG runs across a sentence structure it doesn't understand, or a token that it doesn't have a POS label for? It fails! True CFGs are quite complex. This was a toy example. \u003c/p\u003e\n\n\u003cp\u003eIn the next lab, we'll gain some practice writing some toy CFGs for a few target sentences. We'll also learn how we can skip all this fun stuff and get existing POS tags for our tokens straight from NLTK whenever we need them, thanks to databases such as the \u003cstrong\u003e\u003cem\u003ePenn Tree Bank\u003c/em\u003e\u003c/strong\u003e!\u003c/p\u003e\n\n\u003ch2\u003eSummary\u003c/h2\u003e\n\n\u003cp\u003eIn this lesson, we dove into linguistics to understand the concept of a \u003cstrong\u003e\u003cem\u003eContext-Free Grammar\u003c/em\u003e\u003c/strong\u003e, and explored how they can be used to create parse trees for sentences.\u003c/p\u003e\n\u003cfooter class=\"fis-footer\" style=\"visibility: hidden;\"\u003e\u003cdiv class=\"fis-feedback\"\u003e\n\u003ch5\u003eHow do you feel about this lesson?\u003c/h5\u003e\n\u003cimg id=\"thumbs-up\" data-repository=\"dsc-context-free-grammars-and-POS-tagging\" title=\"Thumbs up!\" alt=\"thumbs up\"\u003e\u003cimg id=\"thumbs-down\" data-repository=\"dsc-context-free-grammars-and-POS-tagging\" title=\"Thumbs down!\" alt=\"thumbs down\"\u003e\n\u003c/div\u003e\n\u003ch5\u003eHave specific feedback? \u003ca href=\"https://github.com/learn-co-curriculum/dsc-context-free-grammars-and-POS-tagging/issues/new\"\u003eTell us here!\u003c/a\u003e\n\u003c/h5\u003e\u003c/footer\u003e","exportId":"context-free-grammars-and-pos-tagging"},{"id":197437,"title":"Context-Free Grammars - Codealong","type":"Assignment","indent":0,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_mark_done","completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-context-free-grammars-codealong\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-context-free-grammars-codealong/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","exportId":"g4bd186e43e87c61c4f6d438f7ac7fbb5"},{"id":197440,"title":"Text Classification","type":"WikiPage","indent":0,"locked":false,"requirement":"must_mark_done","completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-text-classification\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-text-classification/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eIn this lesson, we'll discuss the general process for setting up text datasets for classification problems.\u003c/p\u003e\n\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\n\u003cp\u003eYou will be able to:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eList the steps for classifying text data \u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch2\u003eClassification for Text Data\u003c/h2\u003e\n\n\u003cp\u003eFor the final lab of this section, we'll use everything we've learned so far to build a classifier that works well with text data. As you've probably guessed, text data is significantly harder to work with than most traditional datasets, because of the sheer amount of preprocessing needed to get the data into a format acceptable to a machine learning algorithm. \u003c/p\u003e\n\n\u003cp\u003eThe main challenge in working with text data isn't just the preprocessing -- its the number of decisions you have to make about how you'll clean and structure the data. In a traditional dataset full of numerical and categorical features, the preprocessing steps are fairly straightforward. Generally, we normalize the numeric data, check for and deal with multicollinearity, convert categorical data to numerical format through one-hot encoding, and so forth. Although the steps themselves may not be easy, there's generally little ambiguity about \u003cem\u003ewhat needs to be done\u003c/em\u003e. Text data is a bit more ambiguous. Let's examine some of the decisions we generally need to make when working with text data.\u003c/p\u003e\n\n\u003ch2\u003eCleaning and Preprocessing Text Data\u003c/h2\u003e\n\n\u003cp\u003eOnce we have our data, the fun part begins. We'll need to begin by preprocessing and cleaning our text data. As you've seen throughout this section, preprocessing text data is a bit more challenging than working with more traditional data types because there's no clear-cut answer for exactly what sort of preprocessing and cleaning we need to do. When working with traditional datasets, our goals are generally pretty clear for this stage -- normalize and clean our numerical data, convert categorical data to a numeric format, check for and deal with multicollinearity, etc. The steps we take are largely dependent on what the data already looks like when we get a hold of it. Text data is different -- if we inspect a raw text dataset, we'll generally see that it only has one dimension -- the actual text, in the form of a string. This could be anything from a tweet to a full novel. This means that we need to make some decisions about how to preprocess our data. Before we can begin cleaning and preprocessing our text data, we need to make some decisions about things such as:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eDo we remove stop words or not?\u003cbr\u003e\n\u003c/li\u003e\n\u003cli\u003eDo we stem or lemmatize our text data, or leave the words as is?\u003cbr\u003e\n\u003c/li\u003e\n\u003cli\u003eIs basic tokenization enough, or do we need to support special edge cases through the use of regex?\u003cbr\u003e\n\u003c/li\u003e\n\u003cli\u003eDo we use the entire vocabulary, or just limit the model to a subset of the most frequently used words? If so, how many?\u003cbr\u003e\n\u003c/li\u003e\n\u003cli\u003eDo we engineer other features, such as bigrams, or POS tags, or Mutual Information Scores?\u003cbr\u003e\n\u003c/li\u003e\n\u003cli\u003eWhat sort of vectorization should we use in our model? Boolean Vectorization? Count Vectorization? TF-IDF? More advanced vectorization strategies such as Word2Vec?\u003cbr\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003eThese are all questions that we'll need to think about pretty much anytime we begin working with text data.\u003c/p\u003e\n\n\u003ch2\u003eFeature Engineering\u003c/h2\u003e\n\n\u003cp\u003eAnother common decision point when working with text data is exactly what features to include in the dataset. As we saw in a previous lab, NLTK makes it quite easy to do things like generate part-of-speech tags for words, or create word or character-level n-grams. In general, there's no great answer for exactly which features will improve the performance of your model, and which won't. This means that your best bet is to experiment, and treat the entire project as an iterative process! When working with text data, don't be afraid to try modeling on alternative forms of the text data, such as bigrams or n-grams. Similarly, we encourage you to explore how adding in additional features such as POS tags or mutual information scores affect the overall model performance. Sometimes, it has a great effect on performance. Other times, not much. Either way, you won't know until you try!\u003c/p\u003e\n\n\u003ch2\u003eSummary\u003c/h2\u003e\n\n\u003cp\u003eIn this lesson, we discussed the challenges that come with working with text data for classification, and the types of decisions we should be ready to make when cleaning and preprocessing a dataset!\u003c/p\u003e\n\u003cfooter class=\"fis-footer\" style=\"visibility: hidden;\"\u003e\u003cdiv class=\"fis-feedback\"\u003e\n\u003ch5\u003eHow do you feel about this lesson?\u003c/h5\u003e\n\u003cimg id=\"thumbs-up\" data-repository=\"dsc-text-classification\" title=\"Thumbs up!\" alt=\"thumbs up\"\u003e\u003cimg id=\"thumbs-down\" data-repository=\"dsc-text-classification\" title=\"Thumbs down!\" alt=\"thumbs down\"\u003e\n\u003c/div\u003e\n\u003ch5\u003eHave specific feedback? \u003ca href=\"https://github.com/learn-co-curriculum/dsc-text-classification/issues/new\"\u003eTell us here!\u003c/a\u003e\n\u003c/h5\u003e\u003c/footer\u003e","exportId":"text-classification"},{"id":197445,"title":"Text Classification - Lab","type":"Assignment","indent":1,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_mark_done","completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-text-classification-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-text-classification-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","exportId":"g581a73de14060f438fe098fbf0dd1bec"},{"id":197450,"title":"Natural Language Processing - Recap","type":"WikiPage","indent":0,"locked":false,"requirement":"must_mark_done","completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-nlp-section-recap\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-nlp-section-recap/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\u003ch2\u003eKey Takeaways\u003c/h2\u003e\n\n\u003cp\u003eThe key takeaways from this section include:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eNLP has become increasingly popular over the past few years, and NLP researchers have achieved very insightful insights\u003cbr\u003e\n\u003c/li\u003e\n\u003cli\u003eThe Natural Language Tool Kit (NLTK) is one of the most popular Python libraries for NLP\u003c/li\u003e\n\u003cli\u003eRegular Expressions are an important part of NLP, which can be used for pattern matching and filtering\u003c/li\u003e\n\u003cli\u003eRegular Expressions can become confusing, so make sure to use our provided cheat sheet the first few times you work with regex\u003c/li\u003e\n\u003cli\u003eIt is strongly recommended you take some time to use regex tester websites to ensure you understand how changing your regex pattern affects your results when working towards a correct answer!\u003c/li\u003e\n\u003cli\u003eFeature Engineering is essential when working with text data, and to understand the dynamics of your text\u003c/li\u003e\n\u003cli\u003eCommon feature engineering techniques are removing stop words, stemming, lemmatization, and n-grams\u003c/li\u003e\n\u003cli\u003eWhen diving deeper into grammar and linguistics, context-free grammars and part-of-speech tagging is important\u003c/li\u003e\n\u003cli\u003eIn this context, parse trees can help computers when dealing with ambiguous words \u003c/li\u003e\n\u003cli\u003e\n\u003cem\u003eHow\u003c/em\u003e you clean and preprocess your data will have a major effect on the conclusions you'll be able to draw in your NLP classification problems \u003c/li\u003e\n\u003c/ul\u003e\n\u003cfooter class=\"fis-footer\" style=\"visibility: hidden;\"\u003e\u003cdiv class=\"fis-feedback\"\u003e\n\u003ch5\u003eHow do you feel about this lesson?\u003c/h5\u003e\n\u003cimg id=\"thumbs-up\" data-repository=\"dsc-nlp-section-recap\" title=\"Thumbs up!\" alt=\"thumbs up\"\u003e\u003cimg id=\"thumbs-down\" data-repository=\"dsc-nlp-section-recap\" title=\"Thumbs down!\" alt=\"thumbs down\"\u003e\n\u003c/div\u003e\n\u003ch5\u003eHave specific feedback? \u003ca href=\"https://github.com/learn-co-curriculum/dsc-nlp-section-recap/issues/new\"\u003eTell us here!\u003c/a\u003e\n\u003c/h5\u003e\u003c/footer\u003e","exportId":"natural-language-processing-recap"}]},{"id":21107,"name":"Topic 40: Neural Networks","status":"unlocked","unlockDate":null,"prereqs":[],"requirement":"all","sequential":false,"exportId":"gec64cb3d0e40942f77b789169b8ddacf","items":[{"id":197459,"title":"Neural Networks - Introduction","type":"WikiPage","indent":0,"locked":false,"requirement":"must_mark_done","completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-neural-networks-section-intro\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-neural-networks-section-intro/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eIn this section, you'll be introduced to one of the most advanced machine learning algorithms currently in the world -- neural networks! \u003c/p\u003e\n\n\u003ch2\u003eDeep Learning\u003c/h2\u003e\n\n\u003cp\u003eThe time has come to learn about one of the most exciting and fast-growing areas of data science: Deep Learning! When we talk about deep learning, we are talking about (deep) neural networks. You'll learn all about them in this section. You'll also use Python to build (basic) neural networks from scratch.\u003c/p\u003e\n\n\u003ch3\u003eNeural Networks\u003c/h3\u003e\n\n\u003cp\u003eIn this section, you'll learn what it means when we talk about neural networks. You'll learn about the essential building blocks like \"layers\", \"nodes\", \"arrows\", \"weights\", \"loss\", \"cost function\", etc. You'll learn that a neural network generally consists of several layers, and how a logistic regression model can be represented as a neural network with just one layer. You'll be able to explain what the advantages and disadvantages of using neural networks are, and get an insight of how forward and backward propagation are used in neural networks to minimize the loss and \"optimize\" your neural network.\u003c/p\u003e\n\n\u003ch3\u003eKeras\u003c/h3\u003e\n\n\u003cp\u003eYou'll be introduced to Keras, a leading open source neural network library in Python, which makes building neural networks surprisingly easy. Before building your first neural network model in Keras, you'll learn about tensors and why they are important when building deep learning models. \u003c/p\u003e\n\n\u003ch2\u003eSummary\u003c/h2\u003e\n\n\u003cp\u003eIn this section, you'll learn the basics of neural networks and how to implement them in Keras!\u003c/p\u003e\n\u003cfooter class=\"fis-footer\" style=\"visibility: hidden;\"\u003e\u003cdiv class=\"fis-feedback\"\u003e\n\u003ch5\u003eHow do you feel about this lesson?\u003c/h5\u003e\n\u003cimg id=\"thumbs-up\" data-repository=\"dsc-neural-networks-section-intro\" title=\"Thumbs up!\" alt=\"thumbs up\"\u003e\u003cimg id=\"thumbs-down\" data-repository=\"dsc-neural-networks-section-intro\" title=\"Thumbs down!\" alt=\"thumbs down\"\u003e\n\u003c/div\u003e\n\u003ch5\u003eHave specific feedback? \u003ca href=\"https://github.com/learn-co-curriculum/dsc-neural-networks-section-intro/issues/new\"\u003eTell us here!\u003c/a\u003e\n\u003c/h5\u003e\u003c/footer\u003e","exportId":"neural-networks-introduction"},{"id":197464,"title":"Introduction to Neural Networks ","type":"Assignment","indent":0,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_mark_done","completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-introduction-to-neural-networks\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-introduction-to-neural-networks/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","exportId":"gffa036196b934e18fcee36cfbcf6fd48"},{"id":197470,"title":"Introduction to Neural Networks - Lab","type":"Assignment","indent":1,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_mark_done","completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-introduction-to-neural-networks-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-introduction-to-neural-networks-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","exportId":"g49d3c582a310e0e78243e0f475f7a4f0"},{"id":197477,"title":"Introduction to Keras","type":"Assignment","indent":0,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_mark_done","completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-introduction-to-keras\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-introduction-to-keras/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","exportId":"g021829d58a0c8e3807c9059bf191da7f"},{"id":197481,"title":"Keras - Lab","type":"Assignment","indent":1,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_mark_done","completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-introduction-to-keras-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-introduction-to-keras-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","exportId":"g3502c431022e3820cf9cf756efed0090"},{"id":197486,"title":"Neural Networks - Recap","type":"WikiPage","indent":0,"locked":false,"requirement":"must_mark_done","completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-neural-networks-section-recap\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-neural-networks-section-recap/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\u003ch2\u003eKey Takeaways\u003c/h2\u003e\n\n\u003cp\u003eThe key takeaways from this section include:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eNeural networks are powerful models that can be customized and tweaked using various amounts of nodes, layers, ...\u003c/li\u003e\n\u003cli\u003eThe most basic neural networks are single-layer densely connected neural networks, which have very similar properties as logistic regression models\u003c/li\u003e\n\u003cli\u003eCompared to more traditional statistics and ML techniques, neural networks perform particularly well when using unstructured data\u003c/li\u003e\n\u003cli\u003eApart from densely connected networks, other types of neural networks include convolutional neural networks, recurrent neural networks, and generative adversarial neural networks \u003c/li\u003e\n\u003cli\u003eWhen working with image data, it's important to understand how image data is stored when working with them in Python\u003c/li\u003e\n\u003cli\u003eLogistic regression can be seen as a single-layer neural network with a sigmoid activation function\u003c/li\u003e\n\u003cli\u003eNeural networks use loss and cost functions to minimize the \"loss\", which is a function that summarizes the difference between the actual outcome (eg. pictures contain santa or not) and the model prediction (whether the model correctly identifies pictures with santas)\u003c/li\u003e\n\u003cli\u003eBackward and forward propagation are used to estimate the so-called \"model weights\"\u003c/li\u003e\n\u003cli\u003eAdding more layers to neural networks can substantially increase model performance\u003c/li\u003e\n\u003cli\u003eSeveral activations can be used in model nodes, you can explore with different types and evaluate how it affects performance\u003c/li\u003e\n\u003c/ul\u003e\n\u003cfooter class=\"fis-footer\" style=\"visibility: hidden;\"\u003e\u003cdiv class=\"fis-feedback\"\u003e\n\u003ch5\u003eHow do you feel about this lesson?\u003c/h5\u003e\n\u003cimg id=\"thumbs-up\" data-repository=\"dsc-neural-networks-section-recap\" title=\"Thumbs up!\" alt=\"thumbs up\"\u003e\u003cimg id=\"thumbs-down\" data-repository=\"dsc-neural-networks-section-recap\" title=\"Thumbs down!\" alt=\"thumbs down\"\u003e\n\u003c/div\u003e\n\u003ch5\u003eHave specific feedback? \u003ca href=\"https://github.com/learn-co-curriculum/dsc-neural-networks-section-recap/issues/new\"\u003eTell us here!\u003c/a\u003e\n\u003c/h5\u003e\u003c/footer\u003e","exportId":"neural-networks-recap"}]},{"id":21109,"name":"Topic 41: Deep Neural Networks","status":"unlocked","unlockDate":null,"prereqs":[],"requirement":"all","sequential":false,"exportId":"ga0833e15bc16ce26ac3417623b45aa11","items":[{"id":197500,"title":"Deep Neural Networks - Introduction","type":"WikiPage","indent":0,"locked":false,"requirement":"must_mark_done","completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-deep-learning-section-intro\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-deep-learning-section-intro/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eThis lesson summarizes the topics we'll be covering in this section and why they'll be important to you as a data scientist.\u003c/p\u003e\n\n\u003ch2\u003eMulti-Layer Perceptrons\u003c/h2\u003e\n\n\u003cp\u003eIn the previous section you learned a lot about how neural networks work. In this section, you'll learn why deeper networks sometimes lead to better results, and we'll generalize what you have learned before to get your matrix dimensions right for deep networks. You'll build deeper neural networks from scratch, and also learn how to build these using Keras.\u003c/p\u003e\n\n\u003ch2\u003eDeep Networks\u003c/h2\u003e\n\n\u003cp\u003eYou'll learn that deep representations are really good at automating what used to be a tedious and time-consuming process of feature engineering. In this section, you'll see that you can actually build a smaller but deeper neural network with exponentially less hidden units which performs even better than a network with more hidden units. The reason for this is that learning happens in each layer, and adding more layers (even with fewer limits) can lead to very powerful predictions! You'll learn about matrix notation for these deep networks and how to build a network like that from scratch.\u003c/p\u003e\n\n\u003ch2\u003eSummary\u003c/h2\u003e\n\n\u003cp\u003eIn this section, you'll extend your deep learning knowledge by learning about deeper neural networks. You'll also learn how to use Keras to build deep learning models!\u003c/p\u003e\n\u003cfooter class=\"fis-footer\" style=\"visibility: hidden;\"\u003e\u003cdiv class=\"fis-feedback\"\u003e\n\u003ch5\u003eHow do you feel about this lesson?\u003c/h5\u003e\n\u003cimg id=\"thumbs-up\" data-repository=\"dsc-deep-learning-section-intro\" title=\"Thumbs up!\" alt=\"thumbs up\"\u003e\u003cimg id=\"thumbs-down\" data-repository=\"dsc-deep-learning-section-intro\" title=\"Thumbs down!\" alt=\"thumbs down\"\u003e\n\u003c/div\u003e\n\u003ch5\u003eHave specific feedback? \u003ca href=\"https://github.com/learn-co-curriculum/dsc-deep-learning-section-intro/issues/new\"\u003eTell us here!\u003c/a\u003e\n\u003c/h5\u003e\u003c/footer\u003e","exportId":"deep-neural-networks-introduction"},{"id":197505,"title":"Deeper Neural Networks","type":"Assignment","indent":0,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_mark_done","completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-deeper-neural-networks\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-deeper-neural-networks/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","exportId":"g17c9d8480ba5bdd1a2170d37c73d0873"},{"id":197509,"title":"Deeper Neural Networks - Lab","type":"Assignment","indent":1,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_mark_done","completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-deeper-neural-networks-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-deeper-neural-networks-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","exportId":"g0677b15f508cb160637b9b55f9a38db2"},{"id":197513,"title":"Image Classification with Multi-Layer Perceptrons","type":"WikiPage","indent":0,"locked":false,"requirement":"must_mark_done","completed":false,"content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-image-classification-with-mlps\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-image-classification-with-mlps\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-image-classification-with-mlps/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eIn this lesson, you'll learn why deeper networks sometimes lead to better results, and we'll generalize what you have learned before to get your matrix dimensions right in deep networks.\u003c/p\u003e\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\u003cp\u003eYou will be able to:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eExplain what is meant by \"deep representations\" of images\u003c/li\u003e\n\u003cli\u003eMathematically represent forward and back propagation in a deep neural network\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003eWhy deep representations?\u003c/h2\u003e\n\u003cp\u003eDeep representations are really good at automating what used to be a tedious process of feature engineering. Not only would modelers need to have complex programming and analytical skills, they would also often require domain knowledge in order to manually build features that would then be passed on to a regression or classification algorithm. With deep representations, this time consuming process is often severely diminished.\u003c/p\u003e\n\u003cp\u003eFor example, the deep layers of a neural network for computer might look like this:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003efirst layer detects edges in pictures\u003c/li\u003e\n\u003cli\u003esecond layer groups edges together and starts to detect different parts\u003c/li\u003e\n\u003cli\u003emore layers: group even bigger parts together, etc.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eor in the case of audio:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003efirst layer: low lever wave features\u003c/li\u003e\n\u003cli\u003esecond layer: basic units of sounds, \"phonemes\"\u003c/li\u003e\n\u003cli\u003ethird: word recognition\u003c/li\u003e\n\u003cli\u003efourth: sentence recognition\u003c/li\u003e\n\u003cli\u003e...\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThe general idea is shallow networks detect \"simple\" things, and the deeper you go, the more complex things can be detected.\u003c/p\u003e\n\u003cp\u003eYou can build a smaller but deeper neural network that needs exponentially less hidden units but performs better, because learning happens in each layer!\u003c/p\u003e\n\u003ch2\u003eDeep Network Architecture and Notation\u003c/h2\u003e\n\u003cp\u003eLet's try to generalize all the notation to get things straight and know the dimensions of all matrices we'll be working with. Let's have a look at this 3-layer network:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-image-classification-with-mlps/master/images/new_classwmips.png\" width=\"800\"\u003e\u003c/p\u003e\n\u003cp\u003eImagine that there are 300 cases, or observations (m = 300). What do our matrices look like?\u003c/p\u003e\n\u003cp\u003eLet's start with \u003cimg src=\"https://render.githubusercontent.com/render/math?math=Z%5E%7B%5B1%5D%7D%20=%20W%5E%7B%5B1%5D%7D%20X%20%2bb%5E%7B%5B1%5D%7D\"\u003e .\u003c/p\u003e\n\u003cp\u003eWhile not shown above in the diagram, Z is the output of the linear part of one of our hidden layers.\u003c/p\u003e\n\u003cp\u003eBreaking this down, we have:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cimg src=\"https://render.githubusercontent.com/render/math?math=W%5E%7B%5B1%5D%7D\"\u003e is the weights matrix with dimensions (4 x 2)\u003c/li\u003e\n\u003cli\u003eIf we look at all our samples, \u003cimg src=\"https://render.githubusercontent.com/render/math?math=x\"\u003e is a (2 x 300)-matrix\u003c/li\u003e\n\u003cli\u003e\n\u003cimg src=\"https://render.githubusercontent.com/render/math?math=Z%5E%7B%5B1%5D%7D\"\u003e is a (4 x 300)-matrix\u003c/li\u003e\n\u003cli\u003e\n\u003cimg src=\"https://render.githubusercontent.com/render/math?math=b%5E%7B%5B1%5D%7D\"\u003e is a (4 x 1)-matrix. Due to broadcasting in Python, this matrix will be duplicated into a (4 x 300)-matrix\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eSimilarly, the second hidden layer also has a linear function attached.\u003c/p\u003e\n\u003cp\u003eIn \u003cimg src=\"https://render.githubusercontent.com/render/math?math=Z%5E%7B%5B2%5D%7D%20=%20W%5E%7B%5B2%5D%7D%20A%5E%7B%5B1%5D%7D%20%2bb%5E%7B%5B2%5D%7D\"\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eThe dimension of \u003cimg src=\"https://render.githubusercontent.com/render/math?math=A%5E%7B%5B1%5D%7D\"\u003e is the same as the dimension of \u003cimg src=\"https://render.githubusercontent.com/render/math?math=Z%5E%7B%5B1%5D%7D\"\u003e : (4 x 300)\u003c/li\u003e\n\u003cli\u003e\n\u003cimg src=\"https://render.githubusercontent.com/render/math?math=W%5E%7B%5B2%5D%7D\"\u003e is the weights matrix with dimensions (3 x 4)\u003c/li\u003e\n\u003cli\u003e\n\u003cimg src=\"https://render.githubusercontent.com/render/math?math=Z%5E%7B%5B2%5D%7D\"\u003e is a (3 x 300)-matrices\u003c/li\u003e\n\u003cli\u003e\n\u003cimg src=\"https://render.githubusercontent.com/render/math?math=b%5E%7B%5B2%5D%7D\"\u003e is a (3 x 1)-matrix. Due to broadcasting in Python, this matrix will be duplicated into a (3 x 300)-matrix\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003eGeneralizing Notation\u003c/h2\u003e\n\u003cp\u003eFrom here, we wish to generalize our notation to a deep network with \u003cimg src=\"https://render.githubusercontent.com/render/math?math=L\"\u003e layers as opposed to 2. For each of these layers, we have parameters associated with the linear transformation of the layer, and parameters associated with the activation function applied to the output of this linear transformation.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eParameters for the linear transformation:\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=W%5E%7B%5Bl%5D%7D:%20(n%5E%7B%5Bl%5D%7D,%20n%5E%7B%5Bl-1%5D%7D)\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=b%5E%7B%5Bl%5D%7D:%20(n%5E%7B%5Bl%5D%7D,%201)\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=dW%5E%7B%5Bl%5D%7D:%20(n%5E%7B%5Bl%5D%7D,%20n%5E%7B%5Bl-1%5D%7D)\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=db%5E%7B%5Bl%5D%7D:%20(n%5E%7B%5Bl%5D%7D,%201)\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eParameters for the activation function:\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=a%5E%7B%5Bl%5D%7D,%20z%5E%7B%5Bl%5D%7D:%20(n%5E%7B%5Bl%5D%7D,%201)\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=Z%5E%7B%5Bl%5D%7D,%20A%5E%7B%5Bl%5D%7D:%20(n%5E%7B%5Bl%5D%7D,%20m)\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=dZ%5E%7B%5Bl%5D%7D,%20dA%5E%7B%5Bl%5D%7D:%20(n%5E%7B%5Bl%5D%7D,%20m)\"\u003e\u003c/p\u003e\n\u003ch2\u003eForward Propagation\u003c/h2\u003e\n\u003cp\u003eRecall that deep networks work by performing forward propagation; evaluating a cost function associated with the output of the neural network by successively calculating the output of each layer given initial parameter values, and passing this output on to the next layer until a finalized output has been calculated and the cost function can then be evaluated.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eInput is \u003cimg src=\"https://render.githubusercontent.com/render/math?math=a%5E%7B%5Bl-1%5D%7D\"\u003e\n\u003c/li\u003e\n\u003cli\u003eOutput \u003cimg src=\"https://render.githubusercontent.com/render/math?math=a%5E%7B%5Bl%5D%7D\"\u003e , save \u003cimg src=\"https://render.githubusercontent.com/render/math?math=z%5E%7B%5Bl%5D%7D,%20w%5E%7B%5Bl%5D%7D,%20b%5E%7B%5Bl%5D%7D,%20a%5E%7B%5Bl-1%5D%7D\"\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eHere's some more details about how the forward propagation calculation is performed:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=Z%5E1\"\u003e is the output of the linear transformation of the initial input \u003cimg src=\"https://render.githubusercontent.com/render/math?math=A%5E1\"\u003e (the observations). In successive layers, \u003cimg src=\"https://render.githubusercontent.com/render/math?math=A%5El\"\u003e is the output from the previous hidden layer. In all of these cases, \u003cimg src=\"https://render.githubusercontent.com/render/math?math=W%5El\"\u003e is a matrix of weights to be optimized to minimize the cost function. \u003cimg src=\"https://render.githubusercontent.com/render/math?math=b%5El\"\u003e is also optimized but is a vector as opposed to a matrix.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=g%5El\"\u003e is the activation function which takes the output of this linear transformation and yields the input to the next hidden layer.\u003c/p\u003e\n\u003cp\u003eMathematically we have:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=Z%5E%7B%5Bl%5D%7D=%20W%5E%7B%5Bl%5D%7D%20A%5E%7B%5Bl-1%5D%7D%20%2b%20b%5E%7B%5Bl%5D%7D\"\u003e \u003cbr\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=A%5E%7B%5Bl%5D%7D=%20g%5E%7B%5Bl%5D%7D%20(%20Z%5E%7B%5Bl%5D%7D)\"\u003e\u003c/p\u003e\n\u003cp\u003eHere, \u003cimg src=\"https://render.githubusercontent.com/render/math?math=Z%5E%7B%5Bl%5D%7D,%20A%5E%7B%5Bl%5D%7D\"\u003e both have a shape of \u003cimg src=\"https://render.githubusercontent.com/render/math?math=(n%5E%7B%5Bl%5D%7D,%20m)\"\u003e\u003c/p\u003e\n\u003ch2\u003eBackward Propagation\u003c/h2\u003e\n\u003cp\u003eOnce an output for the neural network given the current parameter weights has been calculated, we must back propagate to calculate the gradients of layer parameters with respect to the cost function. This will allow us to apply an optimization algorithm such as gradient descent in order to make small adjustments to the parameters in order to minimize our cost (and improve our predictions).\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eInput: \u003cimg src=\"https://render.githubusercontent.com/render/math?math=da%20%5E%7B%5Bl%5D%7D\"\u003e\n\u003c/li\u003e\n\u003cli\u003eOutput: \u003cimg src=\"https://render.githubusercontent.com/render/math?math=da%5E%7B%5Bl-1%5D%7D\"\u003e , \u003cimg src=\"https://render.githubusercontent.com/render/math?math=dW%5E%7B%5Bl%5D%7D,%20db%5E%7B%5Bl%5D%7D\"\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eIn terms of formulas, the gradients for our respective parameters in each activation layer are given by:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=dZ%5E%7B%5Bl%5D%7D=%20dA%20%5E%7B%5Bl%5D%7D%20*%20g%5E%7B%5Bl%5D'%7D%20(Z%5E%7B%5Bl%5D%7D)\"\u003e \u003cbr\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=dW%5E%7B%5Bl%5D%7D%20=%20%5Cdfrac%7B1%7D%7Bm%7D%20dZ%5E%7B%5Bl%5D%7D*%20A%5E%7B%5Bl-1%5DT%7D\"\u003e \u003cbr\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=db%5E%7B%5Bl%5D%7D%20=%20%5Cdfrac%7B1%7D%7Bm%7D%20np.sum(dZ%5E%7B%5Bl%5D%7D,%20axis=1,%20keepdims=True)\"\u003e \u003cbr\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=dA%5E%7B%5Bl-1%5D%7D%20=%20W%5E%7B%5Bl%5DT%7D*dZ%5E%7B%5Bl%5D%7D\"\u003e\u003c/p\u003e\n\u003ch2\u003eProcess Overview\u003c/h2\u003e\n\u003cp\u003eTo summarize the process once more, we begin by defining a model architecture which includes the number of hidden layers, activation functions, and the number of units in each of these.\u003c/p\u003e\n\u003cp\u003eWe then initialize parameters for each of these layers (typically randomly). After the initial parameters are set, forward propagation evaluates the model giving a prediction, which is then used to evaluate a cost function. Forward propagation involves evaluating each layer and then piping this output into the next layer.\u003c/p\u003e\n\u003cp\u003eEach layer consists of a linear transformation and an activation function. The parameters for the linear transformation in \u003cstrong\u003eeach\u003c/strong\u003e layer include \u003cimg src=\"https://render.githubusercontent.com/render/math?math=W%5El\"\u003e and \u003cimg src=\"https://render.githubusercontent.com/render/math?math=b%5El\"\u003e . The output of this linear transformation is represented by \u003cimg src=\"https://render.githubusercontent.com/render/math?math=Z%5El\"\u003e . This is then fed through the activation function (again, for each layer) giving us an output \u003cimg src=\"https://render.githubusercontent.com/render/math?math=A%5El\"\u003e which is the input for the next layer of the model.\u003c/p\u003e\n\u003cp\u003eAfter forward propagation is completed and the cost function is evaluated, back propogation is used to calculate gradients of the initial parameters with respect to this cost function. Finally, these gradients are then used in an optimization algorithm, such as gradient descent, to make small adjustments to the parameters and the entire process of forward propagation, back propagation, and parameter adjustments is repeated until the modeller is satisfied with the results.\u003c/p\u003e\n\u003ch2\u003eAdditional Resources\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"https://www.coursera.org/learn/neural-networks-deep-learning/lecture/rz9xJ/why-deep-representations\"\u003ehttps://www.coursera.org/learn/neural-networks-deep-learning/lecture/rz9xJ/why-deep-representations\u003c/a\u003e\u003c/p\u003e\n\u003ch2\u003eSummary\u003c/h2\u003e\n\u003cp\u003eIn this brief lesson, we gave an intuitive justification behind using deep network structures and reviewed the architecture for neural nets in general. In upcoming lessons, we will begin to extend our previous work in creating a single layer neural network in order to build a deeper more powerful model.\u003c/p\u003e","exportId":"image-classification-with-multi-layer-perceptrons"},{"id":197514,"title":"Image Classification with MLPs - Lab","type":"Assignment","indent":1,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_mark_done","completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-image-classification-with-mlps-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-image-classification-with-mlps-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","exportId":"gde61e951e7ac6dd47355b675556c49d7"},{"id":197519,"title":"Deep Neural Networks - Recap","type":"WikiPage","indent":0,"locked":false,"requirement":"must_mark_done","completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-deep-learning-section-recap\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-deep-learning-section-recap/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\u003ch2\u003eKey Takeaways\u003c/h2\u003e\n\n\u003cp\u003eThe key takeaways from this section include:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eDeep neural network representations can lighten the burden and automate certain tasks of heavy data preprocessing\u003c/li\u003e\n\u003cli\u003eDeep representations need exponentially fewer hidden units than shallow networks, to obtain the same performance\u003c/li\u003e\n\u003cli\u003eParameter initialization, forward propagation, cost function evaluation, and backward propagation are again the cornerstones of deep networks\u003c/li\u003e\n\u003cli\u003eTensors are the building blocks of neural networks and a good understanding of them and how to use them in Python is crucial\u003c/li\u003e\n\u003cli\u003eScalars can be seen as 0-D tensors. Vectors can be seen as 1-D tensors, and matrices as 2-D tensors\u003c/li\u003e\n\u003cli\u003eThe usage of tensors reaches beyond matrices: tensors can have N dimensions\u003c/li\u003e\n\u003cli\u003eTensors can be created and manipulated using Numpy\u003c/li\u003e\n\u003cli\u003eKeras makes building neural networks in Python easy, and you learned how to do that in this section\u003c/li\u003e\n\u003cli\u003eYou can use Keras to do some NLP as well, e.g. for tokenization \u003c/li\u003e\n\u003c/ul\u003e\n\u003cfooter class=\"fis-footer\" style=\"visibility: hidden;\"\u003e\u003cdiv class=\"fis-feedback\"\u003e\n\u003ch5\u003eHow do you feel about this lesson?\u003c/h5\u003e\n\u003cimg id=\"thumbs-up\" data-repository=\"dsc-deep-learning-section-recap\" title=\"Thumbs up!\" alt=\"thumbs up\"\u003e\u003cimg id=\"thumbs-down\" data-repository=\"dsc-deep-learning-section-recap\" title=\"Thumbs down!\" alt=\"thumbs down\"\u003e\n\u003c/div\u003e\n\u003ch5\u003eHave specific feedback? \u003ca href=\"https://github.com/learn-co-curriculum/dsc-deep-learning-section-recap/issues/new\"\u003eTell us here!\u003c/a\u003e\n\u003c/h5\u003e\u003c/footer\u003e","exportId":"deep-neural-networks-recap"}]},{"id":21112,"name":"Topic 42: Tuning Neural Networks","status":"unlocked","unlockDate":null,"prereqs":[],"requirement":"all","sequential":false,"exportId":"gf1159e033ba6b238f2f604049da21ac3","items":[{"id":197531,"title":"Tuning Neural Networks - Introduction","type":"WikiPage","indent":0,"locked":false,"requirement":"must_mark_done","completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-tuning-neural-networks-intro\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-tuning-neural-networks-intro/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eNow that you have a general sense of the architecture of neural networks and some of their underlying concepts, its time to further investigate how to properly tune a model for optimal performance. Specifically, you'll take a look at two main techniques: regularization and normalization. \u003c/p\u003e\n\n\u003ch2\u003eRegularization\u003c/h2\u003e\n\n\u003cp\u003eYou've seen regularization before in many other models including linear regression. For example, recall the L1 and L2 penalties which modify ordinary linear regression. These updated loss functions can help tune models so they do not overfit to the training data. For neural networks, you'll use a surprisingly similar process in order to achieve well trained models that are neither overfit nor underfit.\u003c/p\u003e\n\n\u003ch2\u003eNormalization and Tuning Neural Networks\u003c/h2\u003e\n\n\u003cp\u003eAnother modeling problem occurs when one gets trapped into a local minimum when searching for an optimal solution using an iterative approach such as gradient descent. One technique for counteracting this scenario is normalizing features. Normalization in deep learning models can drastically decrease computation time, mitigate common issues such as vanishing or exploding gradients, and increase model performance.\u003c/p\u003e\n\n\u003ch3\u003eOptimization\u003c/h3\u003e\n\n\u003cp\u003eFinally, you'll look at alternative optimization algorithms. These are of primary interest when one encounters local minimum. Knowing when one has hit such a pitfall can be challenging and typically requires experimenting with different optimization approaches and learning rates.\u003c/p\u003e\n\n\u003ch2\u003eSummary\u003c/h2\u003e\n\n\u003cp\u003eIn this section, you'll extend your deep learning knowledge by learning about regularization and optimizing your neural network models. \u003c/p\u003e\n\u003cfooter class=\"fis-footer\" style=\"visibility: hidden;\"\u003e\u003cdiv class=\"fis-feedback\"\u003e\n\u003ch5\u003eHow do you feel about this lesson?\u003c/h5\u003e\n\u003cimg id=\"thumbs-up\" data-repository=\"dsc-tuning-neural-networks-intro\" title=\"Thumbs up!\" alt=\"thumbs up\"\u003e\u003cimg id=\"thumbs-down\" data-repository=\"dsc-tuning-neural-networks-intro\" title=\"Thumbs down!\" alt=\"thumbs down\"\u003e\n\u003c/div\u003e\n\u003ch5\u003eHave specific feedback? \u003ca href=\"https://github.com/learn-co-curriculum/dsc-tuning-neural-networks-intro/issues/new\"\u003eTell us here!\u003c/a\u003e\n\u003c/h5\u003e\u003c/footer\u003e","exportId":"tuning-neural-networks-introduction"},{"id":197542,"title":"Tuning Neural Networks with Regularization","type":"WikiPage","indent":0,"locked":false,"requirement":"must_mark_done","completed":false,"content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-tuning-neural-networks-with-regularization\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-tuning-neural-networks-with-regularization\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-tuning-neural-networks-with-regularization/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eNow that you've learned about neural networks and some streamlined methods for building such models, it's time to further explore how to tune and optimize the performance of these networks. One important aspect is reducing the time and resources needed to train these models. In previous lessons, when importing the Santa images, you immediately reduced each image to an extremely pixelated 64x64 representation. On top of that, you further down-sampled the dataset to reduce the number of observations. This was because training neural networks is resource intensive and is often a time consuming process as a result. Typically you also want to improve the accuracy and performance of these models. In this lesson, you will begin to examine various techniques related to these goals, beginning with the discussion of validation sets.\u003c/p\u003e\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\u003cp\u003eYou will be able to:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eExplain the relationship between bias and variance in neural networks\u003c/li\u003e\n\u003cli\u003eExplain how regularization affects the nodes of a neural network\u003c/li\u003e\n\u003cli\u003eExplain L1, L2, and dropout regularization in a neural network\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003eHyperparameters and iterative deep learning\u003c/h2\u003e\n\u003cp\u003eFirst, there are many hyperparameters you can tune. These include:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003enumber of hidden units\u003c/li\u003e\n\u003cli\u003enumber of layers\u003c/li\u003e\n\u003cli\u003elearning rate ( \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Calpha\"\u003e )\u003c/li\u003e\n\u003cli\u003eactivation function\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThe question then becomes, how do you choose these parameters? One primary method is to develop validation sets to strike a balance between specificity and generalization.\u003c/p\u003e\n\u003ch2\u003eTraining, Validation, and Test Sets\u003c/h2\u003e\n\u003cp\u003eWhen tuning neural networks it typically helps to split the data into three distinct partitions as follows: - You train algorithms on the training set - You'll use a validation set to decide which one will be your final model after parameter tuning - After having chosen the final model (and having evaluated long enough), you'll use the test set to get an unbiased estimate of the classification performance (or whatever your evaluation metric will be)\u003c/p\u003e\n\u003cp\u003eRemeber that it is \u003cstrong\u003eVERY IMPORTANT\u003c/strong\u003e to make sure that the holdout (validation) and test samples come from the same distribution: eg. same resolution of Santa pictures.\u003c/p\u003e\n\u003ch2\u003eBias and Variance in Deep Learning\u003c/h2\u003e\n\u003cp\u003eFinding a balance between generalization and specificity is at the heart of the bias-variance trade off. To further examine this process for tuning neural networks, let's return to a simple example you've seen before.\u003c/p\u003e\n\u003ch3\u003eThe Circles Example\u003c/h3\u003e\n\u003cp\u003eIn classical machine learning, you often need to consider \"bias-variance trade-off\". You'll investigate these concepts here, and see how deep learning is slightly different and a trade-off isn't always present!\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eBias = underfitting\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eHigh variance = overfitting\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eGood fit --\u0026gt; somewhere in between\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eTo start, take another look at the two circles data, the data looked like this:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-tuning-neural-networks-with-regularization/canvas/images/example.png\" alt=\"title\"\u003e\u003c/p\u003e\n\u003cp\u003eRecall that you fit a logistic regression model to the data here. You got something that looked like the picture below. The model didn't do a particularly good job at discriminating between the yellow and purple dots. You could say this is a model with a \u003cstrong\u003ehigh bias\u003c/strong\u003e, the model is \u003cstrong\u003eunderfitting\u003c/strong\u003e.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-tuning-neural-networks-with-regularization/canvas/images/underfitting.png\" alt=\"title\"\u003e\u003c/p\u003e\n\u003cp\u003eWhen using a neural network, what you reached in the end was a pretty good decision boundary, a circle discriminating between the yellow and purple dots:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-tuning-neural-networks-with-regularization/canvas/images/good.png\" alt=\"title\"\u003e\u003c/p\u003e\n\u003cp\u003eAt the other end of the spectrum, you might experience \u003cstrong\u003eoverfitting\u003c/strong\u003e, where you create a circle which is super sensitive to small deviations of the colored dots, like the example below. You can also call this a model with \u003cstrong\u003ehigh variance\u003c/strong\u003e.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-tuning-neural-networks-with-regularization/canvas/images/overfitting.png\" alt=\"title\"\u003e\u003c/p\u003e\n\u003ch2\u003eThe Santa Example\u003c/h2\u003e\n\u003cp\u003e\u0026nbsp;\u003c/p\u003e\n\u003cp\u003e\u003cimg style=\"height: 220px;\" src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-tuning-neural-networks-with-regularization/master/images/S_4.jpg\" alt=\"Santa\"\u003e \u003cimg style=\"height: 220px;\" src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-tuning-neural-networks-with-regularization/master/images/NS_1.jpg\" alt=\"CD\"\u003e\u003c/p\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003e\u003c/th\u003e\n\u003cth\u003eHigh variance\u003c/th\u003e\n\u003cth\u003eHigh bias\u003c/th\u003e\n\u003cth\u003eHigh variance \u0026amp; bias\u003c/th\u003e\n\u003cth\u003eLow variance and bias\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd\u003etrain set error\u003c/td\u003e\n\u003ctd\u003e12%\u003c/td\u003e\n\u003ctd\u003e26%\u003c/td\u003e\n\u003ctd\u003e26%\u003c/td\u003e\n\u003ctd\u003e12%\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003evalidation set error\u003c/td\u003e\n\u003ctd\u003e25%\u003c/td\u003e\n\u003ctd\u003e28%\u003c/td\u003e\n\u003ctd\u003e40%\u003c/td\u003e\n\u003ctd\u003e13%\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003cp\u003eAssume that our best model can get to a validation set accuracy of 87%. Note that \"high\" and \"low\" are relative! Also, in deep learning there is less of a bias variance trade-off!\u003c/p\u003e\n\u003ch2\u003eRules of Thumb Regarding Bias / Variance\u003c/h2\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003eHigh Bias? (training performance)\u003c/th\u003e\n\u003cth\u003eHigh variance? (validation performance)\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd\u003eUse a bigger network\u003c/td\u003e\n\u003ctd\u003eMore data\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eTrain longer\u003c/td\u003e\n\u003ctd\u003eRegularization\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eLook for other existing NN architextures\u003c/td\u003e\n\u003ctd\u003eLook for other existing NN architextures\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003ch2\u003eRegularization\u003c/h2\u003e\n\u003cp\u003eUse regularization when the model overfits to the data.\u003c/p\u003e\n\u003ch3\u003eL1 and L2 regularization\u003c/h3\u003e\n\u003ch4\u003e\u003cstrong\u003eIn Logistic Regression\u003c/strong\u003e\u003c/h4\u003e\n\u003cp\u003eLet's look back at the logistic regression example with lambda, a regularization parameter (another hyperparameter you have to tune).\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=J%20(w,b)%20=%20%5Cdfrac%7B1%7D%7Bm%7D%20%5Cdisplaystyle%5Csum%5Em_%7Bi=1%7D%5Cmathcal%7BL%7D(%5Chat%20y%5E%7B(i)%7D,%20y%5E%7B(i)%7D)%2b%20%5Cdfrac%7B%5Clambda%7D%7B2m%7D%7C%7Cw%7C%7C_2%5E2\"\u003e \u003cbr\u003e\u003cbr\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%7C%7Cw%7C%7C_2%5E2%20=%20%5Cdisplaystyle%5Csum%5E%7Bn_x%7D_%7Bj=1%7Dw_j%5E2=%20w%5ETw\"\u003e\u003c/p\u003e\n\u003cp\u003eThis is called L2-regularization. You can also add a regularization term for \u003cimg src=\"https://render.githubusercontent.com/render/math?math=b\"\u003e , but \u003cimg src=\"https://render.githubusercontent.com/render/math?math=b\"\u003e is just one parameter. L2-regularization is the most common type of regularization.\u003c/p\u003e\n\u003cp\u003eL1-regularization is where you just add a term:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cdfrac%7B%5Clambda%7D%7Bm%7D%7C%7Cw%7C%7C_1\"\u003e (could also be 2 in the denominator)\u003c/p\u003e\n\u003ch4\u003e\u003cstrong\u003eIn A Neural Network\u003c/strong\u003e\u003c/h4\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=J%20(w%5E%7B%5B1%5D%7D,b%5E%7B%5B1%5D%7D,...,w%5E%7B%5BL%5D%7D,b%5E%7B%5BL%5D%7D)%20=%20%5Cdfrac%7B1%7D%7Bm%7D%20%20%5Cdisplaystyle%5Csum%5Em_%7Bi=1%7D%5Cmathcal%7BL%7D(%5Chat%20y%5E%7B(i)%7D,%20y%5E%7B(i)%7D)%2b%20%5Cdfrac%7B%5Clambda%7D%7B2m%7D%20%5Cdisplaystyle%5Csum%5EL_%7Bl=1%7D%7C%7Cw%5E%7B%5Bl%5D%7D%7C%7C%5E2\"\u003e \u003cbr\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%7C%7Cw%5E%7B%5Bl%5D%7D%7C%7C%5E2%20=%20%20%5Cdisplaystyle%5Csum%5E%7Bn%5E%7B%5Bl-1%5D%7D%7D_%7Bi=1%7D%20%20%5Cdisplaystyle%5Csum%5E%7Bn%5E%7B%5Bl%5D%7D%7D_%7Bj=1%7D%20(w_%7Bij%7D%5E%7B%5Bl%5D%7D)%5E2\"\u003e\u003c/p\u003e\n\u003cp\u003eThis matrix norm is called the \"Frobenius norm\", also referred to as \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%7C%7Cw%5E%7B%5Bl%5D%7D%7C%7C%5E2%20_F\"\u003e\u003c/p\u003e\n\u003cp\u003eHow does backpropagation change now?\u003c/p\u003e\n\u003cp\u003eWhichever expression you have from the backpropagation, and add \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cdfrac%7B%5Clambda%7D%7Bm%7D%20w%5E%7B%5Bl%5D%7D\"\u003e .\u003c/p\u003e\n\u003cp\u003eSo,\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=dw%5E%7B%5Bl%5D%7D%20=%20%5Ctext%7B%5Bbackpropagation%20derivatives%5D%20%7D%2b%20%5Cdfrac%7B%5Clambda%7D%7Bm%7D%20w%5E%7B%5Bl%5D%7D\"\u003e\u003c/p\u003e\n\u003cp\u003eAfterwards, \u003cimg src=\"https://render.githubusercontent.com/render/math?math=w%5E%7B%5Bl%5D%7D\"\u003e is updated again as \u003cimg src=\"https://render.githubusercontent.com/render/math?math=w%5E%7B%5Bl%5D%7D%20:=%20w%5E%7B%5Bl%5D%7D%20-%20%5Calpha%20dw%5E%7B%5Bl%5D%7D\"\u003e\u003c/p\u003e\n\u003cp\u003eL2-regularization is called weight decay, because regularization will make your load smaller:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=w%5E%7B%5Bl%5D%7D:=%20w%5E%7B%5Bl%5D%7D%20-%20%5Calpha%20%5Cbigr(%20%5Ctext%7B%5Bbackpropagation%20derivatives%5D%20%7D%2b%20%5Cdfrac%7B%5Clambda%7D%7Bm%7D%20w%5E%7B%5Bl%5D%7D%5Cbigr)\"\u003e \u003cbr\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=w%5E%7B%5Bl%5D%7D:=%20w%5E%7B%5Bl%5D%7D%20-%20%5Cdfrac%7B%5Calpha%5Clambda%7D%7Bm%7Dw%5E%7B%5Bl%5D%7D%20-%20%5Calpha%20%5Ctext%7B%5Bbackpropagation%20derivatives%5D%7D\"\u003e\u003c/p\u003e\n\u003cp\u003ehence your weights will become smaller by a factor \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cbigr(1-%20%5Cdfrac%7B%5Calpha%5Clambda%7D%7Bm%7D%5Cbigr)\"\u003e.\u003c/p\u003e\n\u003cp\u003eIntuition for regularization: the weight matrices will be penalized from being too large. Actually, the network will be forced to almost be simplified.\u003c/p\u003e\n\u003cp\u003eAlso: e.g., \u003cem\u003etanh\u003c/em\u003e function, if \u003cimg src=\"https://render.githubusercontent.com/render/math?math=w\"\u003e is small, the activation function will be mostly operating in the linear region and not \"explode\" as easily.\u003c/p\u003e\n\u003ch2\u003eDropout Regularization\u003c/h2\u003e\n\u003cp\u003eWhen you apply the Dropout technique, a random subset of nodes (also called the units) in a layer are ignored (their weights set to zero) during each phase of training. Below is an image from the \u003ca href=\"http://www.jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf\"\u003eoriginal paper\u003c/a\u003e that introduced this technique.\u003c/p\u003e\n\u003cp\u003eOn the left you can see a standard neural network with four layers (one input layer, two hidden layers, and an output layer). On the right, you can see the network after Dropout is applied during one step of training. This technique is very effective because it allows us to train neural networks on different parts of the data, thus ensuring that our model is not overly sensitive noise in the data.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-tuning-neural-networks-with-regularization/canvas/images/dropout.png\" alt=\"title\"\u003e\u003c/p\u003e\n\u003cp\u003eIn Keras, you specify \u003cem\u003eDropout\u003c/em\u003e using the \u003ccode\u003eDropout\u003c/code\u003e layer, which is applied to input and hidden layers. The \u003ccode\u003eDropout\u003c/code\u003e layers requires one argument, \u003ccode\u003erate\u003c/code\u003e, which specifies the fraction of units to drop, usually between 0.2 and 0.5.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"python\"\u003emodel = models.Sequential()\nmodel.add(layers.Dense(5, activation='relu', input_shape=(500,)))\nmodel.add(layers.Dropout(0.3))\nmodel.add(layers.Dense(5, activation='relu'))\nmodel.add(layers.Dropout(0.3))\nmodel.add(layers.Dense(1, activation='sigmoid'))\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eIn different iterations through the training set, different nodes will be zeroed out!\u003c/p\u003e\n\u003ch2\u003eSummary\u003c/h2\u003e\n\u003cp\u003eIn this lesson you began to explore how to further tune and optimize out of the box neural networks built with Keras. This included regularization analogous to previous machine learning work you've seen, as well dropout regularization, which can be used to further prune your networks. In the upcoming lab you'll get a chance to experiment with these concepts in practice and observe their effect on your models outputs.\u003c/p\u003e","exportId":"tuning-neural-networks-with-regularization"},{"id":197546,"title":"Tuning Neural Networks with Regularization - Lab","type":"Assignment","indent":1,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_mark_done","completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-tuning-neural-networks-with-regularization-lab-v2-1\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-tuning-neural-networks-with-regularization-lab-v2-1/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","exportId":"ga9f534dba2b781f0f5c2ce6bf283b0e4"},{"id":197551,"title":"Tuning Neural Networks with Normalization","type":"WikiPage","indent":0,"locked":false,"requirement":"must_mark_done","completed":false,"content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-tuning-neural-networks-with-normalization\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-tuning-neural-networks-with-normalization\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-tuning-neural-networks-with-normalization/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eNow that we've investigated some methods for tuning our networks, we will investigate some further methods and concepts regarding reducing training time. These concepts will begin to form a more cohesive framework for choices along the modelling process.\u003c/p\u003e\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\u003cp\u003eYou will be able to:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eExplain what normalization does to training time with neural networks and why\u003c/li\u003e\n\u003cli\u003eExplain what a vanishing or exploding gradient is, and how it is related to model convergence\u003c/li\u003e\n\u003cli\u003eCompare the different optimizer strategies for neural networks\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003eNormalized Inputs: Speed up Training\u003c/h2\u003e\n\u003cp\u003eOne way to speed up training of your neural networks is to normalize the input. In fact, even if training time were not a concern, normalization to a consistent scale (typically 0 to 1) across features should be used to ensure that the process converges to a stable solution. Similar to some of our previous work in training models, one general process for standardizing our data is subtracting the mean and dividing by the standard deviation.\u003c/p\u003e\n\u003ch2\u003eVanishing or Exploding Gradients\u003c/h2\u003e\n\u003cp\u003eNot only will normalizing your inputs speed up training, it can also mitigate other risks inherent in training neural networks. For example, in a neural network, having input of various ranges can lead to difficult numerical problems when the algorithm goes to compute gradients during forward and back propogation. This can lead to untenable solutions and will prevent the algorithm from converging to a solution. In short, make sure you normalize your data! Here's a little more mathematical background:\u003c/p\u003e\n\u003cp\u003eTo demonstrate, imagine a very deep neural network. Assume \u003cimg src=\"https://render.githubusercontent.com/render/math?math=g(z)=z\"\u003e (so no transformation, just a linear activation function), and biases equal to 0.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Chat%20y%20=%20w%5E%7B%5BL%5D%7Dw%5E%7B%5BL-1%5D%7Dw%5E%7B%5BL-2%5D%7D...%20w%5E%7B%5B3%5D%7Dw%5E%7B%5B2%5D%7Dw%5E%7B%5B1%5D%7Dx\"\u003e\u003c/p\u003e\n\u003cp\u003eRecall that \u003cimg src=\"https://render.githubusercontent.com/render/math?math=z%5E%7B%5B1%5D%7D%20=w%5E%7B%5B1%5D%7Dx\"\u003e , and that \u003cimg src=\"https://render.githubusercontent.com/render/math?math=a%5E%7B%5B1%5D%7D=g(z%5E%7B%5B1%5D%7D)=z%5E%7B%5B1%5D%7D\"\u003e\u003c/p\u003e\n\u003cp\u003eSimilarly, \u003cimg src=\"https://render.githubusercontent.com/render/math?math=a%5E%7B%5B2%5D%7D=g(z%5E%7B%5B2%5D%7D)=g(w%5E%7B%5B2%5D%7Da%5E%7B%5B1%5D%7D)\"\u003e\u003c/p\u003e\n\u003cp\u003eImagine two nodes in each layer, and w = \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cbig%5B%20%7B1.3%20%5Cquad%200%7D%5Catop%7B0%20%5Cquad%201.3%7D%20%5Cbig%5D\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Chat%20y%20=%20w%5E%7B%5BL%5D%7D%20%5Cbig%5B%7B1.3%20%5Cquad%200%7D%5Catop%7B0%20%5Cquad%201.3%7D%20%5Cbig%5D%20%5E%7BL-1%7D%20%20%20x\"\u003e\u003c/p\u003e\n\u003cp\u003eEven if the \u003cimg src=\"https://render.githubusercontent.com/render/math?math=w\"\u003e 's are slightly smaller than 1 or slightly larger, the activations will explode when there are many layers in the network!\u003c/p\u003e\n\u003ch2\u003eOther Solutions to Vanishing and Exploding Gradients\u003c/h2\u003e\n\u003cp\u003eAside from normalizing the data, you can also investigate the impact of changing the initialization parameters when you first launch the gradient descent algorithm.\u003c/p\u003e\n\u003cp\u003eFor initialization, the more input features feeding into layer l, the smaller you want each \u003cimg src=\"https://render.githubusercontent.com/render/math?math=w_i\"\u003e to be.\u003c/p\u003e\n\u003cp\u003eA common rule of thumb is:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=Var(w_i)%20=%201/n\"\u003e\u003c/p\u003e\n\u003cp\u003eor\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=Var(w_i)%20=%202/n\"\u003e\u003c/p\u003e\n\u003cp\u003eOne common initialization strategy for the relu activation function is:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ew^{[l]} = np.random.randn(shape)*np.sqrt(2/n_(l-1)) \n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eLater, we'll discuss other initialization strategies pertinent to other activation fuctions.\u003c/p\u003e\n\u003ch2\u003eOptimization\u003c/h2\u003e\n\u003cp\u003eIn addition, you could even use an alternative convergence algorithm instead of gradient descent. One issue with gradient descent is that it oscillates to a fairly big extent, because the derivative is bigger in the vertical direction.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-tuning-neural-networks-with-normalization/master/images/new_optimizer.png\" width=\"600\"\u003e\u003c/p\u003e\n\u003cp\u003eWith that, here are some optimization algorithms that work faster than gradient descent:\u003c/p\u003e\n\u003ch3\u003eGradient Descent with Momentum\u003c/h3\u003e\n\u003cp\u003eCompute an exponentially weighthed average of the gradients and use that gradient instead. The intuitive interpretation is that this will successively dampen oscillations, improving convergence.\u003c/p\u003e\n\u003cp\u003eMomentum:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003ecompute \u003cimg src=\"https://render.githubusercontent.com/render/math?math=dW\"\u003e and \u003cimg src=\"https://render.githubusercontent.com/render/math?math=db\"\u003e on the current minibatch\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003ecompute \u003cimg src=\"https://render.githubusercontent.com/render/math?math=V_%7Bdw%7D%20=%20%5Cbeta%20V_%7Bdw%7D%20%2b%20(1-%5Cbeta)dW\"\u003e and\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003ecompute \u003cimg src=\"https://render.githubusercontent.com/render/math?math=V_%7Bdb%7D%20=%20%5Cbeta%20V_%7Bdb%7D%20%2b%20(1-%5Cbeta)db\"\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cblockquote\u003e\n\u003cp\u003eThese are the moving averages for the derivatives of \u003cimg src=\"https://render.githubusercontent.com/render/math?math=W\"\u003e and \u003cimg src=\"https://render.githubusercontent.com/render/math?math=b\"\u003e\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=W:=%20W-%20%5Calpha%20Vdw\"\u003e \u003cbr\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=b:=%20b-%20%5Calpha%20Vdb\"\u003e\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eThis averages out gradient descent, and will \"dampen\" oscillations. Generally, \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cbeta=0.9\"\u003e is a good hyperparameter value.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch3\u003eRMSprop\u003c/h3\u003e\n\u003cp\u003eRMSprop stands for \"root mean square\" prop. It slows down learning in one direction and speed up in another one. On each iteration, it uses exponentially weighted average of the squares of the derivatives.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=S_%7Bdw%7D%20=%20%5Cbeta%20S_%7Bdw%7D%20%2b%20(1-%5Cbeta)dW%5E2\"\u003e \u003cbr\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=S_%7Bdb%7D%20=%20%5Cbeta%20S_%7Bdw%7D%20%2b%20(1-%5Cbeta)db%5E2\"\u003e \u003cbr\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=W:=%20W-%20%5Calpha%20%5Cdfrac%7Bdw%7D%7B%5Csqrt%7BS_%7Bdw%7D%7D%7D\"\u003e and \u003cbr\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=b:=%20b-%20%5Calpha%20%5Cdfrac%7Bdb%7D%7B%5Csqrt%7BS_%7Bdb%7D%7D%7D\"\u003e\u003c/p\u003e\n\u003cp\u003eIn the direction where we want to learn fast, the corresponding \u003cimg src=\"https://render.githubusercontent.com/render/math?math=S\"\u003e will be small, so dividing by a small number. On the other hand, in the direction where we will want to learn slow, the corresponding \u003cimg src=\"https://render.githubusercontent.com/render/math?math=S\"\u003e will be relatively large, and updates will be smaller.\u003c/p\u003e\n\u003cp\u003eOften, add small \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cepsilon\"\u003e in the denominator to make sure that you don't end up dividing by 0.\u003c/p\u003e\n\u003ch3\u003eAdam Optimization Algorithm\u003c/h3\u003e\n\u003cp\u003e\"Adaptive Moment Estimation\", basically using the first and second moment estimations. Works very well in many situations! It takes momentum and RMSprop to put it together!\u003c/p\u003e\n\u003cp\u003eInitialize:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=V_%7Bdw%7D=0,%20S_%7Bdw%7D=0,%20V_%7Bdb%7D=0,%20S_%7Bdb%7D=0\"\u003e\u003c/p\u003e\n\u003cp\u003eFor each iteration:\u003c/p\u003e\n\u003cp\u003eCompute \u003cimg src=\"https://render.githubusercontent.com/render/math?math=dW,%20db\"\u003e using the current mini-batch:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=V_%7Bdw%7D%20=%20%5Cbeta_1%20V_%7Bdw%7D%20%2b%20(1-%5Cbeta_1)dW\"\u003e , \u003cimg src=\"https://render.githubusercontent.com/render/math?math=V_%7Bdb%7D%20=%20%5Cbeta_1%20V_%7Bdb%7D%20%2b%20(1-%5Cbeta_1)db\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=S_%7Bdw%7D%20=%20%5Cbeta_2%20S_%7Bdw%7D%20%2b%20(1-%5Cbeta_2)dW%5E2\"\u003e , \u003cimg src=\"https://render.githubusercontent.com/render/math?math=S_%7Bdb%7D%20=%20%5Cbeta_2%20S_%7Bdb%7D%20%2b%20(1-%5Cbeta_2)db%5E2\"\u003e\u003c/p\u003e\n\u003cp\u003eIs like momentum and then RMSprop. We need to perform a correction! This is sometimes also done in RSMprop, but definitely here too.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=V%5E%7Bcorr%7D_%7Bdw%7D=%20%5Cdfrac%7BV_%7Bdw%7D%7D%7B1-%5Cbeta_1%5Et%7D\"\u003e , \u003cimg src=\"https://render.githubusercontent.com/render/math?math=V%5E%7Bcorr%7D_%7Bdb%7D=%20%5Cdfrac%7BV_%7Bdb%7D%7D%7B1-%5Cbeta_1%5Et%7D\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=S%5E%7Bcorr%7D_%7Bdw%7D=%20%5Cdfrac%7BS_%7Bdw%7D%7D%7B1-%5Cbeta_2%5Et%7D\"\u003e , \u003cimg src=\"https://render.githubusercontent.com/render/math?math=S%5E%7Bcorr%7D_%7Bdb%7D=%20%5Cdfrac%7BS_%7Bdb%7D%7D%7B1-%5Cbeta_2%5Et%7D\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=W:=%20W-%20%5Calpha%20%5Cdfrac%7BV%5E%7Bcorr%7D_%7Bdw%7D%7D%7B%5Csqrt%7BS%5E%7Bcorr%7D_%7Bdw%7D%2b%5Cepsilon%7D%7D\"\u003e and\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=b:=%20b-%20%5Calpha%20%5Cdfrac%7BV%5E%7Bcorr%7D_%7Bdb%7D%7D%7B%5Csqrt%7BS%5E%7Bcorr%7D_%7Bdb%7D%2b%5Cepsilon%7D%7D\"\u003e\u003c/p\u003e\n\u003cp\u003eHyperparameters:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Calpha\"\u003e\u003c/li\u003e\n\u003cli\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cbeta_1%20=%200.9\"\u003e\u003c/li\u003e\n\u003cli\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cbeta_2%20=%200.999\"\u003e\u003c/li\u003e\n\u003cli\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cepsilon%20=%2010%5E%7B-8%7D\"\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eGenerally, only \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Calpha\"\u003e gets tuned.\u003c/p\u003e\n\u003ch3\u003eLearning Rate Decay\u003c/h3\u003e\n\u003cp\u003eLearning rate decreases across epochs.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Calpha%20=%20%5Cdfrac%7B1%7D%7B1%2b%5Ctext%7Bdecay_rate%20*%20epoch_nb%7D%7D*%20%5Calpha_0\"\u003e\u003c/p\u003e\n\u003cp\u003eother methods:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Calpha%20=%200.97%20%5E%7B%5Ctext%7Bepoch_nb%7D%7D*%20%5Calpha_0\"\u003e (or exponential decay)\u003c/p\u003e\n\u003cp\u003eor\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Calpha%20=%20%5Cdfrac%7Bk%7D%7B%5Csqrt%7B%5Ctext%7Bepoch_nb%7D%7D%7D*%20%5Calpha_0\"\u003e\u003c/p\u003e\n\u003cp\u003eor\u003c/p\u003e\n\u003cp\u003eManual decay!\u003c/p\u003e\n\u003ch2\u003eHyperparameter Tuning\u003c/h2\u003e\n\u003cp\u003eNow that you've seen some optimization algorithms, take another look at all the hyperparameters that need tuning:\u003c/p\u003e\n\u003cp\u003eMost important: - \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Calpha\"\u003e\u003c/p\u003e\n\u003cp\u003eNext: - \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cbeta\"\u003e (momentum) - Number of hidden units - mini-batch-size\u003c/p\u003e\n\u003cp\u003eFinally: - Number of layers - Learning rate decay\u003c/p\u003e\n\u003cp\u003eAlmost never tuned: - \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cbeta_1\"\u003e , \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cbeta_2\"\u003e , \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cepsilon\"\u003e (Adam)\u003c/p\u003e\n\u003cp\u003eThings to do:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eDon't use a grid, because hard to say in advance which hyperparameters will be important\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003eAdditional Resources\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://www.coursera.org/learn/deep-neural-network/lecture/lXv6U/normalizing-inputs\"\u003ehttps://www.coursera.org/learn/deep-neural-network/lecture/lXv6U/normalizing-inputs\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://www.coursera.org/learn/deep-neural-network/lecture/y0m1f/gradient-descent-with-momentum\"\u003ehttps://www.coursera.org/learn/deep-neural-network/lecture/y0m1f/gradient-descent-with-momentum\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003eSummary\u003c/h2\u003e\n\u003cp\u003eIn this lesson you began learning about issues regarding the convergence of neural networks training. This included the need for normalization as well as initialization parameters and some optimization algorithms. In the upcoming lab, you'll further investigate these ideas in practice and observe their impacts from various perspectives.\u003c/p\u003e","exportId":"tuning-neural-networks-with-normalization"},{"id":197556,"title":"Tuning Neural Networks with Normalization - Lab","type":"Assignment","indent":1,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_mark_done","completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-tuning-neural-networks-with-normalization-lab-v2-1\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-tuning-neural-networks-with-normalization-lab-v2-1/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","exportId":"g59df043892200b17758cb18112232fb3"},{"id":197561,"title":"Tuning Neural Networks from Start to Finish - Lab","type":"Assignment","indent":1,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_mark_done","completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-tuning-neural-networks-from-start-to-finish-lab-v2-1\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-tuning-neural-networks-from-start-to-finish-lab-v2-1/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","exportId":"g9e2060c34e3aeb853796494094b66f02"},{"id":197565,"title":"Tuning Neural Networks - Recap","type":"WikiPage","indent":0,"locked":false,"requirement":"must_mark_done","completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-tuning-neural-networks-recap\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-tuning-neural-networks-recap/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\u003ch2\u003eKey Takeaways\u003c/h2\u003e\n\n\u003cp\u003eThe key takeaways from this section include: \u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eValidation and test sets are used when iteratively building deep neural networks\u003c/li\u003e\n\u003cli\u003eLike traditional machine learning models, we need to watch out for the bias variance trade-off when building deep learning models\u003c/li\u003e\n\u003cli\u003eSeveral regularization techniques can help us limit overfitting: L1 Regularization, L2 Regularization, Dropout Regularization, etc ...\u003c/li\u003e\n\u003cli\u003eTraining of deep neural networks can be sped up by using normalized inputs\u003c/li\u003e\n\u003cli\u003eNormalized inputs can also help mitigate a common issue of vanishing or exploding gradients \u003c/li\u003e\n\u003cli\u003eExamples of alternatives for gradient descent are: RMSprop, Adam, Gradient Descent with Momentum, etc. \u003c/li\u003e\n\u003cli\u003eHyperparameter tuning is of crucial importance when working with deep learning models, as setting the parameters right can lead to great improvements in model performance \u003c/li\u003e\n\u003c/ul\u003e\n\u003cfooter class=\"fis-footer\" style=\"visibility: hidden;\"\u003e\u003cdiv class=\"fis-feedback\"\u003e\n\u003ch5\u003eHow do you feel about this lesson?\u003c/h5\u003e\n\u003cimg id=\"thumbs-up\" data-repository=\"dsc-tuning-neural-networks-recap\" title=\"Thumbs up!\" alt=\"thumbs up\"\u003e\u003cimg id=\"thumbs-down\" data-repository=\"dsc-tuning-neural-networks-recap\" title=\"Thumbs down!\" alt=\"thumbs down\"\u003e\n\u003c/div\u003e\n\u003ch5\u003eHave specific feedback? \u003ca href=\"https://github.com/learn-co-curriculum/dsc-tuning-neural-networks-recap/issues/new\"\u003eTell us here!\u003c/a\u003e\n\u003c/h5\u003e\u003c/footer\u003e","exportId":"tuning-neural-networks-recap"}]},{"id":21134,"name":"APPENDIX: Convolutional Neural Networks ","status":"completed","unlockDate":null,"prereqs":[],"requirement":null,"sequential":false,"exportId":"g919680e127e8fc0bf1981893bb0f2e5f","items":[{"id":197706,"title":"Convolutional Neural Networks - Introduction","type":"WikiPage","indent":0,"locked":false,"requirement":null,"completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-cnn-intro\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-cnn-intro/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eUntil now, you've learned about densely connected networks and how they can be super powerful for classification problems when you have unstructured data such as text or images. In one of the previous labs, you analyzed images and used densely connected neural networks to classify images according to whether they contained Santa or not. In this section you'll learn about another type of neural networks that work particularly well on image data: Convolutional Neural Networks.\u003c/p\u003e\n\n\u003ch3\u003eConvolutional Neural Networks\u003c/h3\u003e\n\n\u003cp\u003eThere are several issues when using densely connected neural networks on image data. Firstly, dense layers learn global patterns rather than local patterns, and densely connected networks can really grow very big if we have high resolution images. In this section, you'll see why Convolutional Neural Networks are often preferred over densely connected networks for image processing. Additionally, you'll learn what a convolution operation is, the different building blocks of convolutional neural networks (including filters, padding schemes, strided convolutions, etc.), and the types of network layers that are part of your convolutional neural networks.\u003c/p\u003e\n\n\u003ch3\u003eBuilding a CNN from Scratch\u003c/h3\u003e\n\n\u003cp\u003eOnce you understand how CNNs work, you'll practice building one from scratch. You'll learn how to preprocess your image data so your model can be trained using Keras. Just like with densely connected networks, Keras provides an extremely user-friendly tool to build CNNs.\u003c/p\u003e\n\n\u003ch3\u003eVisualizing Intermediate Activations\u003c/h3\u003e\n\n\u003cp\u003eAs with densely connected networks, CNNs are complicated networks that are considered a \"black box\" tool with little insight in what's happening in the network layers. However, when using CNNs, you're essentially changing your image through filters in every layer. You'll learn to get some insight in your black box models by visualizing the intermediate layers in your CNNs!\u003c/p\u003e\n\n\u003ch2\u003eSummary\u003c/h2\u003e\n\n\u003cp\u003eIn this section, you'll extend your deep learning knowledge by learning about convolutional neural networks. \u003c/p\u003e\n\u003cfooter class=\"fis-footer\" style=\"visibility: hidden;\"\u003e\u003cdiv class=\"fis-feedback\"\u003e\n\u003ch5\u003eHow do you feel about this lesson?\u003c/h5\u003e\n\u003cimg id=\"thumbs-up\" data-repository=\"dsc-cnn-intro\" title=\"Thumbs up!\" alt=\"thumbs up\"\u003e\u003cimg id=\"thumbs-down\" data-repository=\"dsc-cnn-intro\" title=\"Thumbs down!\" alt=\"thumbs down\"\u003e\n\u003c/div\u003e\n\u003ch5\u003eHave specific feedback? \u003ca href=\"https://github.com/learn-co-curriculum/dsc-cnn-intro/issues/new\"\u003eTell us here!\u003c/a\u003e\n\u003c/h5\u003e\u003c/footer\u003e","exportId":"convolutional-neural-networks-introduction"},{"id":197711,"title":"Convolutional Neural Networks","type":"WikiPage","indent":0,"locked":false,"requirement":null,"completed":false,"content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-convolutional-neural-networks\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-convolutional-neural-networks\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-convolutional-neural-networks/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eConvolutional Neural Networks (CNNs), build upon the fully connected neural networks you've seen to date. Since detailed images can have incredibly high dimensions based on the number of pixels, CNNs provide an alternative formulation for analyzing groups of pixels. Without the convolutional operation, fitting neural networks to medium to large images would be infeasible for all but the most powerful computers. For example, given a color image with 500 x 500 pixels, you would have 500 x 500 x 3 = 750,000 input features, \u003cimg src=\"https://render.githubusercontent.com/render/math?math=(x_1,...,x_%7B750,000%7D)\"\u003e . From there, even having 2000 hidden units (3% of the input), in the first hidden layer, would result in roughly 1.5 billion parameters!\u003c/p\u003e\n\u003cp\u003eCNNs have certain features that identify patterns in images because of \"convolution operation\" including:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eDense layers learn global patterns in their input feature space\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eConvolution layers learn local patterns, and this leads to the following interesting features:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eUnlike with densely connected networks, when a convolutional neural network recognizes a pattern in one region, these insights can be shared and applied to other regions.\u003c/li\u003e\n\u003cli\u003eDeeper convolutional neural networks can learn spatial hierarchies. A first layer will learn small local patterns, a second layer will learn larger patterns using features of the first layer patterns, etc.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eBecause of these properties, CNNs are great for tasks like:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eImage classification\u003c/li\u003e\n\u003cli\u003eObject detection in images\u003c/li\u003e\n\u003cli\u003ePicture neural style transfer\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\u003cp\u003eYou will be able to:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eDefine what a convolution is, as it relates to CNNs\u003c/li\u003e\n\u003cli\u003eExplain how convolutions work using RGB images\u003c/li\u003e\n\u003cli\u003eDescribe what a pooling layer is in a neural network\u003c/li\u003e\n\u003cli\u003eExplain how padding works with convolution layers of a neural network\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003eBuilding CNNs in Keras\u003c/h2\u003e\n\u003cp\u003eBuilding a CNN in Keras is very similar to the previous neural networks that you've built to date. To start, you will initialize a sequential model as before and go on adding layers. However, rather then simply adding additional dense layers or dropouts between them, we will now start to investigate other potential layer architectures including convolutional layers.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-convolutional-neural-networks/master/images/Image_158CNN.png\"\u003e\u003c/p\u003e\n\u003ch2\u003eThe Convolution Operation\u003c/h2\u003e\n\u003cp\u003eThe idea behind the convolutional operation is to detect complex building blocks, or features, that can aid in the larger task such as image recognition. For example, we'll detect vertical or horizontal edges present in the image. Let's look at what horizontal edge detection would look like:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-convolutional-neural-networks/master/images/conv.png\"\u003e\u003c/p\u003e\n\u003cp\u003eThis is a simplified 5 x 5 pixel image (greyscale!). You use a so-called \"filter\" (denoted on the right) to perform a convolution operation. This particular filter operation will detect horizontal edges. The matrix in the left should have number in it (from 1-255, or let's assume we rescaled it to number 1-10). The output is a 3 x 3 matrix. (\u003cem\u003eThis example is for computational clarity, no clear edges\u003c/em\u003e)\u003c/p\u003e\n\u003cp\u003eIn Keras, function for the convolution step is \u003ccode\u003eConv2D\u003c/code\u003e.\u003c/p\u003e\n\u003cp\u003eThe convolutional operation applies this filter (typically 3x3 or 5x5) to each possible 3x3 or 5x5 region of the original image. The graphic below demonstrates this process.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-convolutional-neural-networks/master/images/convolution-layer-a.png\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://stanford.edu/%7Eshervine/teaching/cs-230/cheatsheet-convolutional-neural-networks\"\u003egif courtesy of Stanford University\u003c/a\u003e\u003c/p\u003e\n\u003ch2\u003ePadding\u003c/h2\u003e\n\u003cp\u003eThere are some issues with using filters on images including:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eThe image shrinks with each convolution layer: you're throwing away information in each layer! For example:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eStarting from a 5 x 5 matrix, and using a 3 x 3 matrix, you end up with a 3 x 3 image\u003c/li\u003e\n\u003cli\u003eStarting from a 10 x 10 matrix, and using a 3 x 3 matrix, you end up with a 8 x 8 image, etc.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eThe pixels around the edges are used much less in the outputs due to the filter\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eFor example, if you apply 3x3 filters to a 5x5 image, the original 5x5 image contains 25 pixels, but tiling the 3x3 filter only has 9 possible locations. Here's the 4 of the 9 possible locations for the 3x3 filter on a 5x5 image:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-convolutional-neural-networks/master/images/5by5_3by3_1.jpeg\" width=\"200\"\u003e \u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-convolutional-neural-networks/master/images/5by5_3by3_2.jpeg\" width=\"200\"\u003e \u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-convolutional-neural-networks/master/images/5by5_3by3_3.jpeg\" width=\"200\"\u003e \u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-convolutional-neural-networks/master/images/5by5_3by3_4.jpeg\" width=\"200\"\u003e\u003c/p\u003e\n\u003cp\u003eFortunately, padding solves both of these problems! Just one layer of pixels around the edges preserves the image size when having a 3 x 3 filter. We can also use bigger filters, but generally the dimensions are odd!\u003c/p\u003e\n\u003cp\u003eSome further terminology regarding padding that you should be aware of includes:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\"Valid\" - no padding\u003c/li\u003e\n\u003cli\u003e\"Same\" - padding such that output is same as the input size\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eBy adding padding to our 5x5 image, (now a 6x6 image by adding a border of pixels) we can add padding so that each pixel of our original 5x5 image can be the center of a 3x3 convolution window filter.\u003c/p\u003e\n\u003ch2\u003eStrided convolutions\u003c/h2\u003e\n\u003cp\u003eAnother method to change the output of your convolutions is to change the stride. The stride is how the convolution filter is moved over the original image. In our above example, we moved the filter one pixel to the right starting from the upper left hand corner, and then began to do this again after moving the filter one pixel down. Alternatively, by changing the stride, we could move our filter by 2 pixels each time, resulting in a smaller number of possible locations for the filter.\u003c/p\u003e\n\u003cp\u003eStrided convolutions are rarely used in practice but a good feature to be aware of for some models.\u003c/p\u003e\n\u003ch2\u003eConvolutions on RGB images\u003c/h2\u003e\n\u003cp\u003eInstead of 5 x 5 grayscale, imagine a 7 x 7 RGB image, which boils down to having a 7 x 7 x 3 tensor. (The image itself is compromised by a 7 by 7 matrix of pixels, each with 3 numerical values for the RGB values.) From there, you will need to use a filter that has the third dimension equal to 3 as well, let's say, 3 x 3 x 3 (a 3D \"cube\").\u003c/p\u003e\n\u003cp\u003eThis allows you to detect horizontal edges in the blue channel.\u003c/p\u003e\n\u003cp\u003eThen, in each layer, you can convolve with several 3D filters. Afterwards, you stack every output of the result together, giving you a matrix of shape 5 x 5 x \u003ccode\u003enumber_of_filters\u003c/code\u003e.\u003c/p\u003e\n\u003cp\u003eIf you think of it, the filter plays the same role as the w^{[1]} in our densely connected networks.\u003c/p\u003e\n\u003cp\u003eThe advantage is, while your image may be huge, the amount of parameters you have still only depends on how many filters you're using!\u003c/p\u003e\n\u003cp\u003eImagine 20 (3 x 3 x 3) --\u0026gt; 20 * 27 + a bias for each filter (1* 20) = 560 parameters.\u003c/p\u003e\n\u003cp\u003eNotation:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cimg src=\"https://render.githubusercontent.com/render/math?math=f%5E%7B%5Bl%5D%7D\"\u003e = size of the filter\u003c/li\u003e\n\u003cli\u003e\n\u003cimg src=\"https://render.githubusercontent.com/render/math?math=p%5E%7B%5Bl%5D%7D\"\u003e = padding\u003c/li\u003e\n\u003cli\u003e\n\u003cimg src=\"https://render.githubusercontent.com/render/math?math=s%5E%7B%5Bl%5D%7D\"\u003e = amount of stride\u003c/li\u003e\n\u003cli\u003e\n\u003cimg src=\"https://render.githubusercontent.com/render/math?math=n_c%5E%7B%5Bl%5D%7D\"\u003e = number of filters\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003efilter: \u003cimg src=\"https://render.githubusercontent.com/render/math?math=f%5E%7B%5Bl%5D%7D\"\u003e x \u003cimg src=\"https://render.githubusercontent.com/render/math?math=f%5E%7B%5Bl%5D%7D\"\u003e x \u003cimg src=\"https://render.githubusercontent.com/render/math?math=n_c%5E%7B%5Bl-1%5D%7D\"\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eInput = \u003cimg src=\"https://render.githubusercontent.com/render/math?math=n_h%5E%7B%5Bl-1%5D%7D%20*%20n_w%5E%7B%5Bl-1%5D%7D%20*%20n_c%5E%7B%5Bl-1%5D%7D\"\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eOutput = \u003cimg src=\"https://render.githubusercontent.com/render/math?math=n_h%5E%7B%5Bl%5D%7D%20*%20n_w%5E%7B%5Bl%5D%7D%20*%20n_c%5E%7B%5Bl%5D%7D\"\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eHeight and width are given by:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=n_h%5E%7B%5Bl%5D%7D=%20%5CBigr%5Clfloor%5Cdfrac%7Bn_h%5E%7B%5Bl-1%5D%7D%2b2p%5E%7B%5Bl%5D%7D-f%5E%7B%5Bl%5D%7D%7D%7Bs%5E%7B%5Bl%5D%7D%7D%2b1%5CBigr%5Crfloor\"\u003e \u003cbr\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=n_w%5E%7B%5Bl%5D%7D=%20%5CBigr%5Clfloor%5Cdfrac%7Bn_w%5E%7B%5Bl-1%5D%7D%2b2p%5E%7B%5Bl%5D%7D-f%5E%7B%5Bl%5D%7D%7D%7Bs%5E%7B%5Bl%5D%7D%7D%2b1%5CBigr%5Crfloor\"\u003e\u003c/p\u003e\n\u003cp\u003eActivations: \u003cimg src=\"https://render.githubusercontent.com/render/math?math=a%5E%7B%5Bl%5D%7D\"\u003e is of dimension \u003cimg src=\"https://render.githubusercontent.com/render/math?math=n_h%5E%7B%5Bl%5D%7D%20*%20n_w%5E%7B%5Bl%5D%7D%20*%20n_c%5E%7B%5Bl%5D%7D\"\u003e\u003c/p\u003e\n\u003ch2\u003ePooling layer\u003c/h2\u003e\n\u003cp\u003eThe last element in a CNN architecture (before fully connected layers as we have previously discussed in other neural networks) is the pooling layer. This layer is meant to substantially downsample the previous convolutional layers. The idea behind this is that the previous convolutional layers will find patterns such as edges or other basic shapes present in the pictures. From there, pooling layers such as Max pooling (the most common) will take a summary of the convolutions from a larger section. In practice, Max pooling (taking the max of all convolutions from a larger area of the original image) works better than average pooling as we are typically looking to detect whether a feature is present in that region. Downsampling is essential in order to produce viable execution times in the model training.\u003c/p\u003e\n\u003cp\u003eMax pooling has some important hyperparameters: - \u003cimg src=\"https://render.githubusercontent.com/render/math?math=f\"\u003e (filter size) - \u003cimg src=\"https://render.githubusercontent.com/render/math?math=S\"\u003e (stride)\u003c/p\u003e\n\u003cp\u003eCommon hyperparameters include: \u003ccode\u003ef=2\u003c/code\u003e, \u003ccode\u003es=2\u003c/code\u003e and \u003ccode\u003ef=3\u003c/code\u003e, \u003ccode\u003es=2\u003c/code\u003e, this shrinks the size of the representations. If a feature is detected anywhere in the quadrants, a high number will appear, so max pooling preserves this feature.\u003c/p\u003e\n\u003ch2\u003eFully Connected Layers in CNN\u003c/h2\u003e\n\u003cp\u003eOnce you have added a number of convolutional layers and pooling layers, you then will add fully connected (dense) layers as we did before in previous neural network models. This now allows the network to learn a final decision function based on these transformed informative inputs generating from the convolutional and pooling layers.\u003c/p\u003e\n\u003ch2\u003eSummary\u003c/h2\u003e\n\u003cp\u003eIn this lesson, you learned about the basic concepts behind CNNs including their use cases and general architecture. In the upcoming lab, you'll begin to look at how you can build these models in Python using Keras.\u003c/p\u003e","exportId":"convolutional-neural-networks"},{"id":197717,"title":"Convolutional Neural Networks - Codealong","type":"Assignment","indent":0,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":null,"completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-convolutional-neural-networks-codealong\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-convolutional-neural-networks-codealong/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","exportId":"gd919e29f628ca3cb56a0eaa1b92c9823"},{"id":197723,"title":"Building a CNN from Scratch","type":"Assignment","indent":0,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":null,"completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-building-a-cnn-from-scratch\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-building-a-cnn-from-scratch/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","exportId":"gda1cffb40f24b5c7208326f1c703f24a"},{"id":197727,"title":"Visualizing Intermediate Activations","type":"Assignment","indent":0,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":null,"completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-visualizing-intermediate-activations\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-visualizing-intermediate-activations/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","exportId":"gf24089e714a5280c53ada52bbe6085e5"},{"id":197730,"title":"Visualizing Activation Functions - Lab","type":"Assignment","indent":1,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":null,"completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-visualizing-activation-functions-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-visualizing-activation-functions-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","exportId":"gbe8fd1ecc88b063ae494c9fba34c5e21"},{"id":197735,"title":"Convolutional Neural Networks - Recap","type":"WikiPage","indent":0,"locked":false,"requirement":null,"completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-cnn-recap\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-cnn-recap/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eWell done! In this section you learned all about convolutional neural networks! You should now have enough of an introductory primer to be able to do some image recognition tasks on your own! \u003c/p\u003e\n\n\u003ch2\u003eKey Takeaways\u003c/h2\u003e\n\n\u003cp\u003eRemember that the essence of a CNN is the convolutional operation. A window is slided across the image based on a stride size. Padding can be used to prevent shrinkage and to make sure pixels at the edge of an image deserve the necessary attention. Each convolution then works to adjust the weights of the kernel through backpropagation  during training. Going back to the general architecture, max pooling is typically used between convolutional layers to reduce the dimensionality. \u003c/p\u003e\n\n\u003cp\u003eOverall, CNNs are a useful model for image recognition due to their ability to recognize visual patterns at varying scales. After developing the convolutional and pooling layers to form a base, the end of the network architecture still connects back to a densely connected network to perform classification.\u003c/p\u003e\n\n\u003ch2\u003eSummary\u003c/h2\u003e\n\n\u003cp\u003eIn this section you learned all about convolutional neural networks! From here, you'll learn more about tuning neural networks and other neural network architectures!\u003c/p\u003e\n\u003cfooter class=\"fis-footer\" style=\"visibility: hidden;\"\u003e\u003cdiv class=\"fis-feedback\"\u003e\n\u003ch5\u003eHow do you feel about this lesson?\u003c/h5\u003e\n\u003cimg id=\"thumbs-up\" data-repository=\"dsc-cnn-recap\" title=\"Thumbs up!\" alt=\"thumbs up\"\u003e\u003cimg id=\"thumbs-down\" data-repository=\"dsc-cnn-recap\" title=\"Thumbs down!\" alt=\"thumbs down\"\u003e\n\u003c/div\u003e\n\u003ch5\u003eHave specific feedback? \u003ca href=\"https://github.com/learn-co-curriculum/dsc-cnn-recap/issues/new\"\u003eTell us here!\u003c/a\u003e\n\u003c/h5\u003e\u003c/footer\u003e","exportId":"convolutional-neural-networks-recap"}]},{"id":21142,"name":"APPENDIX: Deep NLP","status":"completed","unlockDate":null,"prereqs":[],"requirement":null,"sequential":false,"exportId":"g536a96d569359cfae3eb2c7b09a35d39","items":[{"id":197783,"title":"Deep NLP with Word Embeddings - Introduction","type":"WikiPage","indent":0,"locked":false,"requirement":null,"completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-deep-nlp-intro\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-deep-nlp-intro/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eIn this section you'll strengthen your deep learning and natural language processing skills by learning about word embeddings! Word embeddings are a unique coding schema for text corpora that preserve many underlying features, allowing for interesting geometric relations in this hyperspace. Specifically, you'll look at how similarity metrics can represent how two words relate to each other, and these transformations can be applied to multiple word pairs. For example, a similarity metric could encapsulate analogies like \"man is to woman as king is to ____\". \u003c/p\u003e\n\n\u003ch3\u003eWord Embeddings\u003c/h3\u003e\n\n\u003cp\u003eIn this section, you'll learn about the concept of word embeddings, and how you can use them to model the semantic meanings of words in a high-dimensional embedding space! Word embeddings use similarity metrics to represent how two words relate to each other. This way, we can understand the words in our corpus to a bigger extent. A typical example is the example of \"Man\" vs \"woman\" and \"king\" vs \"queen\": word embeddings can capture that the word \"man\" relates to the word \"woman\" the same way the word \"king\" relates to \"queen\"!\u003c/p\u003e\n\n\u003ch3\u003eUsing Word2Vec\u003c/h3\u003e\n\n\u003cp\u003eCreating word embeddings is not an easy task. Word embeddings can be created using so-called \"Word2Vec\" models that are  given enough training data. At its core, Word2Vec is just another deep neural network, that looks at sequences of words and words that are often used in similar contexts (or \u003cem\u003eclose\u003c/em\u003e to each other in sentences). In this section you'll learn how to train a Word2Vec model, and you'll explore the embedding space.\u003c/p\u003e\n\n\u003ch3\u003eClassification with Word Embeddings\u003c/h3\u003e\n\n\u003cp\u003eTo wrap up this section, we'll focus on the practical aspects of how Word2Vec and word embeddings can be used to improve our text classification models. We'll start by learning how transfer learning can be used by loading pre-trained word vectors into our Word2Vec model. Then, we'll learn about how we can get the word vectors we need and combine them into mean word vectors, and how we can streamline this process by writing our own vectorizer class that is compatible with scikit-learn pipelines. Next, we'll see how deep neural networks with their own embedding layers can be trained, and how Keras preprocesses the text data to make everything run smoothly!\u003c/p\u003e\n\n\u003ch2\u003eSummary\u003c/h2\u003e\n\n\u003cp\u003eIn this section, you'll dive deeper into NLP and get better classification results using word embeddings!\u003c/p\u003e\n\u003cfooter class=\"fis-footer\" style=\"visibility: hidden;\"\u003e\u003cdiv class=\"fis-feedback\"\u003e\n\u003ch5\u003eHow do you feel about this lesson?\u003c/h5\u003e\n\u003cimg id=\"thumbs-up\" data-repository=\"dsc-deep-nlp-intro\" title=\"Thumbs up!\" alt=\"thumbs up\"\u003e\u003cimg id=\"thumbs-down\" data-repository=\"dsc-deep-nlp-intro\" title=\"Thumbs down!\" alt=\"thumbs down\"\u003e\n\u003c/div\u003e\n\u003ch5\u003eHave specific feedback? \u003ca href=\"https://github.com/learn-co-curriculum/dsc-deep-nlp-intro/issues/new\"\u003eTell us here!\u003c/a\u003e\n\u003c/h5\u003e\u003c/footer\u003e","exportId":"deep-nlp-with-word-embeddings-introduction"},{"id":197787,"title":"Word Embeddings","type":"WikiPage","indent":0,"locked":false,"requirement":null,"completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-word-embeddings\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-word-embeddings/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eIn this lesson, you'll learn about the concept of \u003cstrong\u003e\u003cem\u003eWord Embeddings\u003c/em\u003e\u003c/strong\u003e, and how you can use them to model the semantic meanings of words in a high-dimensional embedding space!\u003c/p\u003e\n\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\n\u003cp\u003eYou will be able to:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eDemonstrate how word vectors are structured \u003c/li\u003e\n\u003cli\u003eCompare and contrast word vector embeddings with other text vectorization strategies \u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch2\u003eWhat Are Word Embeddings?\u003c/h2\u003e\n\n\u003cp\u003e\u003cstrong\u003e\u003cem\u003eWord Embeddings\u003c/em\u003e\u003c/strong\u003e are a type of vectorization strategy that computes word vectors from a text corpus by training a neural network, which results in a high-dimensional embedding space, where each word in the corpus is a unique vector in that space. In this embedding space, the position of the vector relative to the other vectors captures semantic meaning. This method of creating distributed representations of words in a high-dimensional embedding space was first introduced in a landmark paper from members of the Google Brain team in 2013 at the Neural Information Processing Systems (NeurIPS, for short). You can read the full paper from Mikolov et al by following \u003ca href=\"https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf\"\u003ethis link\u003c/a\u003e.\u003c/p\u003e\n\n\u003ch3\u003eCapturing Semantic Relationships\u003c/h3\u003e\n\n\u003cp\u003eSo far, the vectorization strategies you've learned have focused only on how often a word appears in a given text, but they don't focus at all on capturing the semantic meaning. This is one area where using the Word2Vec model to create \u003cstrong\u003e\u003cem\u003eWord Vector Embeddings\u003c/em\u003e\u003c/strong\u003e really shines, because it will capture those semantic relationships between words, for instance, a Word2Vec model that is given enough data and training will learn that there is a semantic relationship between the word 'person' and 'people'. Furthermore, vector one would need to travel to get from the singular 'person' to the plural 'people' will be the same vector that will get you from the singular version of a word to it's plural - meaning that our model will 'learn' how to model the relationship between singular and plural versions of the same word. Take a look at the examples below:\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-word-embeddings/master/images/embeddings.png\"\u003e\u003c/p\u003e\n\n\u003cp\u003eAs you can see in the diagram above, the embedding space shows that the model has positioned the words 'king' and 'queen' in the same relationship that the vector 'man' has to 'woman'. The vector that gets you from 'king' to 'queen' or from 'man' to 'woman' is the vector for gender! You can see other examples show that the model also learns representations for verb tense, or even for countries and their capitals. This is more impressive when you realize that the model learns these relationships from reading a large enough corpus of text, without being given an explicit direction or instruction - that is, the researchers did not expressly feed the model sentences like \"Madrid is the capital of Spain\".  \u003c/p\u003e\n\n\u003cp\u003eSince the words are all embedded in the same high-dimensional space, you can use the same similarity metrics you've used before, such as things like \u003cem\u003eCosine Similarity\u003c/em\u003e or even \u003cem\u003eEuclidean Distance\u003c/em\u003e. In a future lab, you'll experiment with using a trained Word2Vec model for tasks like finding the most similar word(s) to a given word. Trained Word2Vec models also excel at things like the analogies questions that were made famous by the SAT test.\u003c/p\u003e\n\n\u003cp\u003eLet's end this lesson by taking a look at how the word vectors are actually structured. \u003c/p\u003e\n\n\u003ch2\u003eA Small Example\u003c/h2\u003e\n\n\u003cp\u003eSo far, you've learned vectorization strategies such as \u003cem\u003eCount Vectorization\u003c/em\u003e and \u003cem\u003eTF-IDF Vectorization\u003c/em\u003e. Recall that the vectors created by these algorithms are \u003cstrong\u003e\u003cem\u003eSparse Vectors\u003c/em\u003e\u003c/strong\u003e. The length of a vector created by TF-IDF or Count Vectorization is the length of the total vocabulary of the text corpus. In these vectors, the vast majority of elements in the vector are 0, which is a massive waste of space, and a ton of extra dimensionality that can hurt our model's performance (recall the \u003cstrong\u003e\u003cem\u003eCurse of Dimensionality\u003c/em\u003e\u003c/strong\u003e)! If you were to use TF-IDF vectorization to turn the word 'apple' into a vector representation with a text corpus containing 100,000 words, then our word vector would contain a value at the element that corresponds to the word 'apple', and then 99,999 \u003cem\u003e0\u003c/em\u003es!\u003c/p\u003e\n\n\u003cp\u003eVectors created through word embeddings are different - the size of the vector is a tunable parameter you can set. \u003c/p\u003e\n\n\u003cp\u003eLet's look at a toy example. Consider the diagram below. First, pay attention to what each of the columns mean. Let's assume that you built a model to 'rate' each of the animals across each of these four categories, relative to one another.  \u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-word-embeddings/master/images/vectors.png\" width=\"800\"\u003e\u003c/p\u003e\n\n\u003cp\u003eIn this embedding space, the vectorized representation of the word 'dog' would be \u003ccode\u003e[-0.4, 0.37, 0.02, -0.34]\u003c/code\u003e. As you'll see when you study the actual Word2Vec model, you can use some nifty tricks to train a neural network to act as a sort of 'lookup table', where you can get the vector out for any given word. In the next lesson, you'll spend a bit more time understanding exactly how the model learns the correct values for each word. \u003c/p\u003e\n\n\u003ch2\u003eSummary\u003c/h2\u003e\n\n\u003cp\u003eIn this lesson, you learned about the concept of \u003cstrong\u003e\u003cem\u003eWord Embeddings\u003c/em\u003e\u003c/strong\u003e, and explored how they work. \u003c/p\u003e\n\u003cfooter class=\"fis-footer\" style=\"visibility: hidden;\"\u003e\u003cdiv class=\"fis-feedback\"\u003e\n\u003ch5\u003eHow do you feel about this lesson?\u003c/h5\u003e\n\u003cimg id=\"thumbs-up\" data-repository=\"dsc-word-embeddings\" title=\"Thumbs up!\" alt=\"thumbs up\"\u003e\u003cimg id=\"thumbs-down\" data-repository=\"dsc-word-embeddings\" title=\"Thumbs down!\" alt=\"thumbs down\"\u003e\n\u003c/div\u003e\n\u003ch5\u003eHave specific feedback? \u003ca href=\"https://github.com/learn-co-curriculum/dsc-word-embeddings/issues/new\"\u003eTell us here!\u003c/a\u003e\n\u003c/h5\u003e\u003c/footer\u003e","exportId":"word-embeddings"},{"id":197791,"title":"Using Word2Vec","type":"WikiPage","indent":0,"locked":false,"requirement":null,"completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-using-word2vec\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-using-word2vec/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eIn this lesson, you'll take a look at how the \u003cstrong\u003e\u003cem\u003eWord2Vec\u003c/em\u003e\u003c/strong\u003e model actually works, and then learn how you can make use of Word2Vec using the open-source \u003ccode\u003egensim\u003c/code\u003e library!\u003c/p\u003e\n\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\n\u003cp\u003eYou will be able to:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eDescribe the tunable parameters of a Word2Vec model \u003c/li\u003e\n\u003cli\u003eDescribe the architecture of the Word2Vec model \u003c/li\u003e\n\u003cli\u003eTrain a Word2Vec model and transform words into vectors \u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch2\u003eHow Word2Vec Works\u003c/h2\u003e\n\n\u003cp\u003eBy now, you've gained an understanding of what a word embedding space is, and you've learned a little bit about how the words are represented as Dense vectors. However, we haven't touched on how the model actually learns the correct values for all the word vectors in the embedding space. To put it another way, how does the Word2Vec model learn exactly \u003cem\u003ewhere\u003c/em\u003e to embed each word vector inside the high dimensional embedding space?\u003c/p\u003e\n\n\u003cp\u003eNote that this explanation will stay fairly high-level, since you don't actually need to understand every part of how the Word2Vec model works in order to use it effectively for Data Science tasks. If you'd like to dig deeper in to how the model actually works, we recommend you start by reading this tutorial series from Chris McCormick (\u003ca href=\"http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/\"\u003epart 1\u003c/a\u003e and \u003ca href=\"http://mccormickml.com/2017/01/11/word2vec-tutorial-part-2-negative-sampling/\"\u003epart 2\u003c/a\u003e), and then moving onto the actual \u003ca href=\"https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf\"\u003eWord2Vec White Paper by Mikolov et al\u003c/a\u003e. The graphics used in this lesson are actually from Chris McCormick's excellent blog posts explaining how Word2Vec actually works.\u003c/p\u003e\n\n\u003ch3\u003eWindow Size and Training Data\u003c/h3\u003e\n\n\u003cp\u003eAt its core, Word2Vec is just another deep neural network. It's not even a particularly complex neural network -- the model contains an input layer, a single hidden layer, and and an output layer that uses the softmax activation function, meaning that the model is meant for multiclass classification. The model examines a \u003cstrong\u003e\u003cem\u003ewindow\u003c/em\u003e\u003c/strong\u003e of words, which is a tunable parameter that you can set when working with the model. Let's take a look at a graphic that explains how this all actually looks on a real example of data:\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-using-word2vec/master/images/training_data.png\"\u003e\u003c/p\u003e\n\n\u003cp\u003eIn the example above, the model has a window size of 5, meaning that the model considers a word, and the two words to the left and right of this word.  \u003c/p\u003e\n\n\u003ch3\u003eThe Skip-Gram Architecture\u003c/h3\u003e\n\n\u003cp\u003eSo what exactly is this deep neural network predicting?\u003c/p\u003e\n\n\u003cp\u003eThe most clever thing about the Word2Vec model is the type of problem it trains the network to solve, which creates the dense vectors for every word as a side effect! A typical task for a neural network is sentence completion. A trained model should be able to take in a sentence like \"the cat sat on the\" and output the most likely next word in the sentence, which should be something like \"mat\", or \"floor\". This is a form of \u003cstrong\u003e\u003cem\u003eSequence Generation\u003c/em\u003e\u003c/strong\u003e. Given a certain context (the words that came before), the model should be able to generate the next most plausible word (or words) in the sequence. \u003c/p\u003e\n\n\u003cp\u003eWord2Vec takes this idea, and flips it on its head. Instead of predicting the next word given a context, the model trains to predict the context surrounding a given word! This means that given the example word \"fox\" from above, the model should learn to predict the words \"quick\", \"brown\", \"jumps\", and \"over\", although crucially, not in any particular order. You're likely asking yourself why a model like this would be useful -- there are a massive amount of correct contexts that can surround a given word, which means that the output trained model itself likely isn't very useful to us. This intuition is correct -- the \u003cem\u003eoutput\u003c/em\u003e of the model is pretty useless to us.  However, in the case of Word2Vec, it's not the model that we're interested in. It turns out that by training to predict the context window for a given word, the neurons in the hidden layer end up learning the embedding space!  This is the reason why the size of the word vectors output by a Word2Vec model are a parameter that you can set ourselves. If you want word vectors of size 300, then you just include 300 neurons in our hidden layer. If you want vectors of size 100, then you include 100 neurons, and so on. Take a look at the following diagram of the Word2Vec model's architecture:\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-using-word2vec/master/images/new_skip_gram_net_arch.png\" width=\"800\"\u003e\u003c/p\u003e\n\n\u003ch3\u003eHidden Layers as a \"Lookup Table\"\u003c/h3\u003e\n\n\u003cp\u003eTo recap, the Word2Vec model learns to solve a \"fake\" problem, which you don't actually care about. The input layer of the network contains one neuron for every word in the vocabulary. If there are 10,000 words, then there are 10,000 input neurons, with each one corresponding to a unique word in the vocabulary. Since these input neurons feed into a dense hidden layer, this means that each neuron will have a unique weight for each of the 10,000 words in the vocabulary. If there are 10,000 words and you want vectors of size 300, then this means the hidden layer will be of shape \u003ccode\u003e[10000, 300]\u003c/code\u003e. To put it another way -- each of the 10,000 words will have it's own unique vector of weights, which will be of size 300, since there are 300 neurons.  \u003c/p\u003e\n\n\u003cp\u003eOnce you've trained the model, you don't actually need the output layer anymore -- all that matters is the hidden layer, which will now act as a \"Lookup Table\" that allows us to quickly get the vector for any given word in the vocabulary. \u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-using-word2vec/master/images/new_word2vec_weight_matrix_lookup_table.png\" width=\"600\"\u003e\u003c/p\u003e\n\n\u003cp\u003eHere's the beautiful thing about this lookup table -- when you input a given word, it is passed into the model in a one-hot encoded format. This means that in a vocabulary of 10,000 words, you'll have a \u003ccode\u003e1\u003c/code\u003e at the element that corresponds to the word that we're looking up the word vector for, and \u003ccode\u003e0\u003c/code\u003e for every other element in the vector. If you multiply this one-hot encoded vector by the weight matrix that is our hidden layer, then the vector for every word will be zeroed out, except for the vector that corresponds to the word that you are most interested in!\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-using-word2vec/master/images/matrix_mult_w_one_hot.png\"\u003e\u003c/p\u003e\n\n\u003ch3\u003eUnderstanding the Intuition Behind Word2Vec\u003c/h3\u003e\n\n\u003cp\u003eSo how does the model actually learn the correct weights for each word in a way that captures their semantic context and meaning? The intuition behind Word2Vec is actually quite simple, when you think about the idea of the context window that it's learning to predict. Recall the following quote, which you've seen before:\u003c/p\u003e\n\n\u003cblockquote\u003e\n\u003cp\u003e\"You shall know a word by the company it keeps.\"  -- J.R. Firth, Linguist\u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003cp\u003eIn the case of the Word2Vec model, the \"company\" a word keeps are the words surrounding it, and the model is learning to predict these companions! By exploring many different contexts, the model attempts to decipher which words are appropriate in which contexts. For example, consider the sentence \"we have two cats as pets\". You could easily substitute the word \"cats\" for \"dogs\" and the entire sentence would still make perfect sense. While the meaning of the sentence is undoubtedly changed, there is also a lesson regarding the fact that both are nouns and pets. Without even worrying about the embedding space, you can easily understand that words that have similar meanings will likely also be used in many of the same kinds of sentences. The more similar words are, the more sentences in which they are likely to share context windows! This is exactly what the model is learning, and this is why words that are similar end up near each other inside the embedding space. The ways that they are \u003cem\u003enot\u003c/em\u003e similar also helps the model learn to differentiate between them, since there will be patterns here as well. For instance, consider \"ran\" and \"run\", and \"walk\" and \"walked\". They differ only in tense. From the perspective of the sentences present in a large text corpus (models are commonly trained on all of Wikipedia, to give you an idea of the sheer size and scale of most datasets), the model will see numerous examples of how \"ran\" is similar to \"walked\", as well as examples of how the context windows for \"ran\" are different from \"run\" in the same ways that the context windows for \"walked\" are different from \"walk\"! \u003c/p\u003e\n\n\u003ch2\u003eTraining A Word2Vec Model with \u003ccode\u003egensim\u003c/code\u003e\n\u003c/h2\u003e\n\n\u003cp\u003eNow, take look at how you can apply the Word2Vec model using the \u003ccode\u003egensim\u003c/code\u003e library!\u003c/p\u003e\n\n\u003cp\u003eTo train a Word2Vec model, you first need to import the model from the \u003ccode\u003egensim\u003c/code\u003e library and instantiate it. Upon instantiation, you'll need to provide the model with certain parameters including:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003ethe dataset you'll be training on\u003c/li\u003e\n\u003cli\u003ethe \u003ccode\u003esize\u003c/code\u003e of the word vectors you want to learn \u003c/li\u003e\n\u003cli\u003ethe \u003ccode\u003ewindow\u003c/code\u003e size to use when training the model\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003emin_count\u003c/code\u003e, which corresponds to the minimum number of times a word must be used in the corpus in order to be included in the training (for instance, \u003ccode\u003emin_count=5\u003c/code\u003e would only learn word embeddings for words that appear 5 or more times throughout the entire training set)\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003eworkers\u003c/code\u003e, the number of threads to use for training, which can speed up processing (\u003ccode\u003e4\u003c/code\u003e is typically used, since most processors nowadays have at least 4 cores). \u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003eOnce you've instantiated the model, you'll still need to call the model's \u003ccode\u003e.train()\u003c/code\u003e method, and pass in the following parameters:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eThe same dataset that you passed in at instantiation\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003etotal_examples\u003c/code\u003e, which is the number of words in the model. You don't need to calculate this manually -- instead, you can just pass in the instantiated model's \u003ccode\u003e.corpus_count\u003c/code\u003e attribute for this parameter.\u003c/li\u003e\n\u003cli\u003eThe number of \u003ccode\u003eepochs\u003c/code\u003e to train the model for. \u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003eThe following example demonstrates how to instantiate and train a Word2Vec model:\u003c/p\u003e\n\n\u003cpre\u003e\u003ccode class=\"python\"\u003efrom gensim.models import Word2Vec\n\nmodel = Word2Vec(data, size=100, window=5, min_count=1, workers=4)\n\nmodel.train(data, total_examples=model.corpus_count)\n\u003c/code\u003e\u003c/pre\u003e\n\n\u003ch3\u003eExploring the Embedding Space\u003c/h3\u003e\n\n\u003cp\u003eOnce you have trained the model, you can easily explore the embedding space using the built-in methods and functionality provided by gensim's \u003ccode\u003eWord2Vec\u003c/code\u003e class. \u003c/p\u003e\n\n\u003cp\u003eThe actual Word2Vec model itself is quite large. Normally, you only need the actual vectors and the words that correspond to them, which are stored inside of \u003ccode\u003emodel.wv\u003c/code\u003e as a \u003ccode\u003eWord2VecKeyedVectors\u003c/code\u003e object. To save time and space, it's usually easiest to just store the \u003ccode\u003emodel.wv\u003c/code\u003e inside it's own variable, and then work directly with that. You can then use this model for various sorts of functionality, which you'll demonstrate below!\u003c/p\u003e\n\n\u003cpre\u003e\u003ccode class=\"python\"\u003ewv = model.wv\n\nwv.most_similar('Cat')\n\nwv.most_similar(negative='Cat')\n\nwv['Cat']\n\nwv.vectors\n\nwv.most_similar(positive=['king', 'woman'], negative=['man'])\n\u003c/code\u003e\u003c/pre\u003e\n\n\u003cp\u003eIn the next lab, you'll train a Word2Vec model, and then explore the embedding space it has learned. \u003c/p\u003e\n\n\u003ch2\u003eSummary\u003c/h2\u003e\n\n\u003cp\u003eIn this lesson, you learned about how the Word2Vec model actually works, and how you can train and use a Word2Vec model using the \u003ccode\u003egensim\u003c/code\u003e library!\u003c/p\u003e\n\u003cfooter class=\"fis-footer\" style=\"visibility: hidden;\"\u003e\u003cdiv class=\"fis-feedback\"\u003e\n\u003ch5\u003eHow do you feel about this lesson?\u003c/h5\u003e\n\u003cimg id=\"thumbs-up\" data-repository=\"dsc-using-word2vec\" title=\"Thumbs up!\" alt=\"thumbs up\"\u003e\u003cimg id=\"thumbs-down\" data-repository=\"dsc-using-word2vec\" title=\"Thumbs down!\" alt=\"thumbs down\"\u003e\n\u003c/div\u003e\n\u003ch5\u003eHave specific feedback? \u003ca href=\"https://github.com/learn-co-curriculum/dsc-using-word2vec/issues/new\"\u003eTell us here!\u003c/a\u003e\n\u003c/h5\u003e\u003c/footer\u003e","exportId":"using-word2vec"},{"id":197795,"title":"Generating Word Embeddings - Lab","type":"Assignment","indent":1,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":null,"completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-generating-word-embeddings-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-generating-word-embeddings-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","exportId":"gfec2e4016c3b1477ed76b57a2fef86d9"},{"id":197801,"title":"Classification with Word Embeddings","type":"WikiPage","indent":0,"locked":false,"requirement":null,"completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-classification-with-word-embeddings\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-classification-with-word-embeddings/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eIn this lesson, you'll look at the practical aspects of how you can use word embeddings and Word2Vec models for text classification!\u003c/p\u003e\n\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\n\u003cp\u003eYou will be able to:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eDescribe what an embedding layer is in a neural network \u003c/li\u003e\n\u003cli\u003eUse pretrained word embeddings from popular pretrained models such as GloVe \u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch2\u003eGetting Started\u003c/h2\u003e\n\n\u003cp\u003eIn this lesson, you'll start by reviewing \u003cstrong\u003e\u003cem\u003eTransfer Learning\u003c/em\u003e\u003c/strong\u003e and loading pre-trained word vectors. Then, you'll learn about how to get important word vectors, combine them into \u003cstrong\u003e\u003cem\u003eMean Word Vectors\u003c/em\u003e\u003c/strong\u003e, and streamline this process by writing a custom vectorizer class compatible with scikit-learn pipelines. Finally, you'll end the lesson by examining how to train deep neural networks that include their own word embedding layers, and how you can use Keras to preprocess text data conveniently!\u003c/p\u003e\n\n\u003ch2\u003eUsing Pretrained Word Vectors With GloVe\u003c/h2\u003e\n\n\u003cp\u003ePerhaps the single best way to improve performance for text classification is to make use of weights from a Word2Vec model that has been trained for a very long time on a massive amount of text data. With deep learning, more data is almost always the single best thing that can improve model performance, and the embedded word vectors created by a Word2Vec model are no exception. For this reason, it's almost always a good idea to load one of the top-tier, industry-standard models that been open sourced for this exact purpose. The most common model to use for this is the \u003cstrong\u003e\u003cem\u003eGloVe\u003c/em\u003e\u003c/strong\u003e (short for \u003cstrong\u003e\u003cem\u003eGlobal Vectors for Word Representation\u003c/em\u003e\u003c/strong\u003e) model by the Stanford NLP Group. This model is trained on massive datasets, such as the entirety of Wikipedia, for a very long time on server clusters with multiple GPUs. It would be absolutely impossible for us to train a model of similar quality on our own machines. However, because the model weights are open-source, you don't need to! Instead, you'll simply download the weights and go from there. \u003c/p\u003e\n\n\u003cp\u003eFor text classification purposes, loading the weights precludes the need for us to instantiate or train a Word2Vec model entirely -- instead, you just:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eGet the total vocabulary in our dataset\u003c/li\u003e\n\u003cli\u003eDownload and unzip the GloVe file needed from the Stanford NLP Group's website\u003c/li\u003e\n\u003cli\u003eRead the GloVe file, and save only the vectors that correspond to the words that appear in the vocabulary of our dataset \u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003eThis can be a fairly involved process, so the code for this is provided for you in the next lab. That said, it's important to take some time and examine this code until you have at least general idea of what's going on!\u003c/p\u003e\n\n\u003ch2\u003eMean Word Embeddings\u003c/h2\u003e\n\n\u003cp\u003eLoading a pretrained model like GloVe may provide you with the most accurate word vectors we could possibly hope, but each vector is still just a single word. This isn't very conducive to classification as is at this stage, because it's highly likely that any text classification will be focused on arbitrarily-sized blobs of text, such as sentences or paragraphs. With that, the question is how to get these sentences and paragraphs into a format that can be used for classification, while making use of the word vectors from GloVe?\u003c/p\u003e\n\n\u003cp\u003eThe answer is to compute a \u003cstrong\u003e\u003cem\u003eMean Word Embedding\u003c/em\u003e\u003c/strong\u003e. The idea behind this is simple. To get the vector representation for any arbitrarily-sized block of text, all you need to do is get the vector for every individual word that appears in that block of text, and average them together! The benefit of this is that no matter how big or small that block of text is, the mean word embedding of that sentence will be the same size as all of the others, because the vectors you're averaging together all have the exact same dimensionality! This makes it a simple matter to get a block of text into a format that we can use with traditional supervised learning models such as Support Vector Machines or Gradient Boosted Trees. \u003c/p\u003e\n\n\u003ch3\u003eWorking With scikit-learn pipelines\u003c/h3\u003e\n\n\u003cp\u003eAs you'll see in the next lab, it's worth the extra bit of work to build a class that works with the requirements of a scikit-learn \u003ccode\u003ePipeline()\u003c/code\u003e class, so that you can pass the data straight in and generate the mean word embeddings on the fly. This way, you don't need to write the same set of code twice to generate mean word embeddings for both the training and test set. This is also important if the dataset is too large to fit into your computer's memory, as it will allow you to partially train models and load in different chunks of the dataset. By building a vectorizer class that handles creating the mean word embeddings rather than just writing the code procedurally, you'll save yourself a lot of work in the long run!\u003c/p\u003e\n\n\u003cp\u003eThe code for the mean embedding vectorizer class is also provided for you in the next lab. As you'll see, the class requires both \u003ccode\u003e.fit()\u003c/code\u003e and \u003ccode\u003e.transform()\u003c/code\u003e methods to be compliant with scikit-learn's \u003ccode\u003ePipeline()\u003c/code\u003e class. Take some time to study this code until you understand what it's doing -- it isn't complex, and understanding how to do this yourself will pay dividends in the long run. After all, writing clean, reusable code always does!\u003c/p\u003e\n\n\u003ch2\u003eDeep Learning \u0026amp; Embedding Layers\u003c/h2\u003e\n\n\u003cp\u003eOne problem you may have noticed with the mean word embedding strategy is that by combining all the words, you lose some information that is contained in the sequence of the words. In natural language, the position and phrasing of words in a sentence can often contain information that we pick up on. This is a downside to this approach, and one of the reasons why \u003cstrong\u003e\u003cem\u003eSequence Models\u003c/em\u003e\u003c/strong\u003e tend to outperform all of the 'shallow' algorithms (note: this term just refers to any machine learning algorithms that do not fall under the umbrella of deep learning -- it doesn't make any judgments about whether they are better or worse, as that is almost always dependent on the situation!). In the next lesson, you'll learn about sequence models including \u003cstrong\u003e\u003cem\u003eRecurrent Neural Networks\u003c/em\u003e\u003c/strong\u003e and \u003cstrong\u003e\u003cem\u003eLong Short Term Memory Cells\u003c/em\u003e\u003c/strong\u003e. Moreover, in the next lab, you'll also see a preview example of these, so that you can see how to use \u003cstrong\u003e\u003cem\u003eEmbedding Layers\u003c/em\u003e\u003c/strong\u003e directly within neural networks!\u003c/p\u003e\n\n\u003cp\u003eAn \u003cstrong\u003e\u003cem\u003eEmbedding Layer\u003c/em\u003e\u003c/strong\u003e is just a layer that learns the word embeddings for our dataset on the fly, right there inside the neural network. Essentially, its a way to make use of all the benefits of Word2Vec, without worrying about finding a way to include a separately trained Word2Vec model's output into our neural networks (which are probably already complicated enough!). You'll see an example of an \u003cstrong\u003e\u003cem\u003eEmbedding Layer\u003c/em\u003e\u003c/strong\u003e in the next lab. You should make note of a couple caveats that come with using embedding layers in your neural network -- namely:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eThe embedding layer must always be the first layer of the network, meaning that it should immediately follow the \u003ccode\u003eInput()\u003c/code\u003e layer \u003c/li\u003e\n\u003cli\u003eAll words in the text should be integer-encoded, with each unique word encoded as it's own unique integer\u003cbr\u003e\n\u003c/li\u003e\n\u003cli\u003eThe size of the embedding layer must always be greater than the total vocabulary size of the dataset! The first parameter denotes the vocabulary size, while the second denotes the size of the actual word vectors\u003c/li\u003e\n\u003cli\u003eThe size of the sequences passed in as data must be set when creating the layer (all data will be converted to padded sequences of the same size during the preprocessing step)\u003cbr\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003eIn the next lab, you'll make use of Keras' text preprocessing tools to convert the data from text to a tokenized format. Then, you'll convert the tokenized sentences to sequences. Finally, you'll pad the sequences, so that they're all the same length. During this step, you'll exclusively make use of the preprocessing tools provided by Keras. Don't worry if this all seems a bit complex right now, as you'll soon see, this is actually the most straightforward part of the next lab!\u003c/p\u003e\n\n\u003cp\u003eFor a full rundown of how to use embedding layers in Keras, see the \u003ca href=\"https://keras.io/layers/embeddings/\"\u003eKeras Documentation for Embedding Layers\u003c/a\u003e.\u003c/p\u003e\n\n\u003ch2\u003eSummary\u003c/h2\u003e\n\n\u003cp\u003eIn this lesson, you focused on the practical and pragmatic elements of using Word2Vec and word embeddings for text classification. You learned about how to load professional-quality pretrained word vectors with the Stanford NLP Group's open source GloVe data, as well as how to generate mean word embeddings that work with scikit-learn pipelines, and how to add embedding layers into neural networks with Keras!\u003c/p\u003e\n\u003cfooter class=\"fis-footer\" style=\"visibility: hidden;\"\u003e\u003cdiv class=\"fis-feedback\"\u003e\n\u003ch5\u003eHow do you feel about this lesson?\u003c/h5\u003e\n\u003cimg id=\"thumbs-up\" data-repository=\"dsc-classification-with-word-embeddings\" title=\"Thumbs up!\" alt=\"thumbs up\"\u003e\u003cimg id=\"thumbs-down\" data-repository=\"dsc-classification-with-word-embeddings\" title=\"Thumbs down!\" alt=\"thumbs down\"\u003e\n\u003c/div\u003e\n\u003ch5\u003eHave specific feedback? \u003ca href=\"https://github.com/learn-co-curriculum/dsc-classification-with-word-embeddings/issues/new\"\u003eTell us here!\u003c/a\u003e\n\u003c/h5\u003e\u003c/footer\u003e","exportId":"classification-with-word-embeddings"},{"id":197806,"title":"Classification with Word Embeddings - Codealong","type":"Assignment","indent":0,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":null,"completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-classification-with-word-embeddings-codealong\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-classification-with-word-embeddings-codealong/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","exportId":"g121f22a4b44ed598b79a72372b7b38a9"},{"id":197813,"title":"Sequence Model Use Cases","type":"WikiPage","indent":0,"locked":false,"requirement":null,"completed":false,"content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-sequence-model-use-cases\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-sequence-model-use-cases\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-sequence-model-use-cases/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eIn this lesson, you'll learn about \u003cstrong\u003e\u003cem\u003eSequence Models\u003c/em\u003e\u003c/strong\u003e, and what makes them different from traditional multi-layer perceptrons. You'll also examine some of the common things sequence models can be used for!\u003c/p\u003e\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\u003cp\u003eYou will be able to:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eDefine a Sequence Model\u003c/li\u003e\n\u003cli\u003eList some of the use cases for Sequence Models\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003eWhat is a Sequence Model?\u003c/h2\u003e\n\u003cp\u003eA \u003cstrong\u003e\u003cem\u003eSequence Model\u003c/em\u003e\u003c/strong\u003e is a general term for a special class of deep neural networks that work with time series of data as input (or any data where you want the model to consider the data one point at a time, in order). This means that they are great for problems where the order of the data matters - for instance, stock price data or text. In both cases, the data only makes sense in order. For instance, scrambling the words in a sentence destroys the meaning of the sentence, and it's impossible to predict if a stock price is going to go up or down if you don't see the prices in sequential order. In both cases, the sequence of the data matters.\u003c/p\u003e\n\u003cp\u003eConsider the following problem: you are given the sentence \"you are going to\" and asked to complete the sentence by generating at least 5 more words. The second word that you choose will depend heavily on the first word that you choose. The third word that you choose will depend heavily on the first and second words that you choose, and so on. Because of this, it is crucial that the models \u003cem\u003eremember\u003c/em\u003e the previous words that they generated. In computer science, you call this being \u003cstrong\u003e\u003cem\u003estateful\u003c/em\u003e\u003c/strong\u003e. This means that when the model is generating the second word, it needs to know what it generated as the first word! To do this, \u003cstrong\u003e\u003cem\u003eRecurrent Neural Networks\u003c/em\u003e\u003c/strong\u003e feed their output for timestep \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Clarge%20x_t\"\u003e back into the model as input for timestep \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Clarge%20x_%7Bt%20%2b%201%7D\"\u003e !\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-sequence-model-use-cases/master/images/rnn.gif\"\u003e\u003c/p\u003e\n\u003cp\u003eThere are many different kinds of sequence models, and they are most generally referred to as \u003cstrong\u003e\u003cem\u003eRecurrent Neural Networks\u003c/em\u003e\u003c/strong\u003e, or \u003cstrong\u003e\u003cem\u003eRNNs\u003c/em\u003e\u003c/strong\u003e. In the next lesson, you'll dig into how they work. Let's examine some of the things that RNNs can do!\u003c/p\u003e\n\u003ch2\u003eSequence Model Use Cases\u003c/h2\u003e\n\u003ch3\u003eText Classification\u003c/h3\u003e\n\u003cp\u003eOne of the most common applications of RNNs is for plain old text classification. Recall that all the models that you've used so far for text generation have been incapable of focusing on the order of the words, which means that they're likely to miss out on more advanced pieces of information such as connotation, context, sarcasm, etc. However, since RNNs examine the words one at a time and remember what they've seen at each time step, they're able to capture this information quite effectively in most cases! As the final part of this section, we'll actually build one of these models which will be able to detect toxic comments from real-world Wikipedia comments!\u003c/p\u003e\n\u003ch3\u003eSequence Generation\u003c/h3\u003e\n\u003cp\u003eSequence generation is probably some of the most incredible things you can do with neural networks, because they excel at coming up with wacky, almost-human sounding names for things when fed the right data. For instance, all of the following cookie names were generated by feeding a dataset of actual cookie names from recipes. The model was built to generate it's own cookie names letter by letter, based on what it saw in the recipe names. Since the model is responsible for generating its own output letter by letter, one at a time, this makes it a prime example of \u003cstrong\u003e\u003cem\u003eSequence Generation\u003c/em\u003e\u003c/strong\u003e!\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-sequence-model-use-cases/master/images/rnn_cookie_names.png\"\u003e\u003c/p\u003e\n\u003ch3\u003eSequence-to-Sequence Models\u003c/h3\u003e\n\u003cp\u003eIf you've ever used Google Translate before, then you've already interacted with a \u003cstrong\u003e\u003cem\u003eSequence to Sequence Model\u003c/em\u003e\u003c/strong\u003e. These models learn to map an input sequence to an output sequence, usually through an \u003cstrong\u003e\u003cem\u003eEncoder-Decoder\u003c/em\u003e\u003c/strong\u003e architecture. Note that although going from a sequence of English words to the corresponding sequence of French words is probably the basic example of Sequence to Sequence models, there are many other kinds of problems that are Sequence to Sequence that aren't immediately obvious. For instance, check out this example of a neural network that completes drawings of a mosquito based on how you start drawing the bug!\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-sequence-model-use-cases/master/images/multi_sketch_mosquito.gif\"\u003e\u003c/p\u003e\n\u003cp\u003eHere's another example from \u003ca href=\"https://phillipi.github.io/pix2pix/\"\u003epix2pix\u003c/a\u003e. Now, stop what you're doing, follow that link, and take a few minutes to play around with pix2pix -- watching it generate photos from your own drawings is really cool!\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-sequence-model-use-cases/master/images/pix2pix.gif\"\u003e\u003c/p\u003e\n\u003ch2\u003eSummary\u003c/h2\u003e\n\u003cp\u003eIn this lesson, you learned about sequence models, and some of their more common use cases.\u003c/p\u003e","exportId":"sequence-model-use-cases"},{"id":197818,"title":"Understanding Recurrent Neural Networks","type":"WikiPage","indent":0,"locked":false,"requirement":null,"completed":false,"content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-understanding-recurrent-neural-networks\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-understanding-recurrent-neural-networks\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-understanding-recurrent-neural-networks/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eIn this lesson, you'll learn about a new type of model architecture you haven't seen yet  \u003cstrong\u003e\u003cem\u003eRecurrent Neural Networks\u003c/em\u003e\u003c/strong\u003e!\u003c/p\u003e\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\u003cp\u003eYou will be able to:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eExplain the role time steps play in RNN models\u003c/li\u003e\n\u003cli\u003eExplain back propagation through time\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003eData as Time Sequences\u003c/h2\u003e\n\u003cp\u003eThe hallmark of Recurrent Neural Networks is that they are used to evaluate \u003cstrong\u003e\u003cem\u003eSequences\u003c/em\u003e\u003c/strong\u003e of data, rather than just individual data points. So what is sequence data, and how do you distinguish it from other kinds of data, so that you know when to use an RNN?\u003c/p\u003e\n\u003cp\u003eTime series data is a classic example of sequence data. You care about the value over time, and any given point in time can really only be examined relative to the other points of time in that sequence. For instance, knowing the price of Google stock today doesn't provide enough information for us to classify it as a something you should or shouldn't buy  for that, you would need to examine today's price relative to the previous day(s) price to see if it's going up or down.\u003c/p\u003e\n\u003cp\u003eAnother great example of sequence data is text. All text data is sequence data by default  a letter only makes sense when it's words are in the proper order. You would lose all information if you made a \"Bag of Letters\". Words themselves are sequence data, and can be used for all kinds of novel sequence generation tasks. You've probably seen articles in popular culture about people using neural networks to generate novel band names, cookie names, Pokemon names, etc. These are always done with Recurrent Neural Networks, because they are a perfect fit for sequence data. For this reason, RNNs excel at NLP tasks, because they can take in text as full sequences of words, from a single sentence up to an entire document or book! Because of this, they do not suffer the same loss of information that comes from a traditional Bag-of-Words vectorization approach.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-understanding-recurrent-neural-networks/master/images/unrolled.gif\"\u003e\u003c/p\u003e\n\u003cp\u003eLet's take a look at the overall structure of an RNN to see how it interacts with this sequence data!\u003c/p\u003e\n\u003ch2\u003eBasic RNN Architecture\u003c/h2\u003e\n\u003cp\u003eA basic Recurrent Neural Network is just a neural network that passes it's output from a given example back into itself as input for the next example. Intuitively, this approach makes sense. If you want to predict what Google's stock price is going to be two days from now, the most important input you can give it is what you think the price will be one day from now!\u003c/p\u003e\n\u003cp\u003eWhen drawn as a diagram, RNNs are usually represented in an \u003cstrong\u003e\u003cem\u003eUnrolled\u003c/em\u003e\u003c/strong\u003e representation, which shows the components at each given timestep. The image on the left is a how an RNN is denoted in a diagram \"rolled up\", while the image on the right is \"unrolled\". The current timestep is denoted with the input node \u003cimg src=\"https://render.githubusercontent.com/render/math?math=X_t\"\u003e , which makes the previous timestep \u003cimg src=\"https://render.githubusercontent.com/render/math?math=X_%7Bt-1%7D\"\u003e and the next timestep \u003cimg src=\"https://render.githubusercontent.com/render/math?math=X_%7Bt%2b1%7D\"\u003e . \u003cimg src=\"https://render.githubusercontent.com/render/math?math=H_0\"\u003e represents the model's output for timestep 0, which will then be passed back into the model in \u003cimg src=\"https://render.githubusercontent.com/render/math?math=X_1\"\u003e .\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-understanding-recurrent-neural-networks/master/images/new-RNN-unrolled.png\"\u003e\u003c/p\u003e\n\u003ch2\u003eBackpropagation Through Time\u003c/h2\u003e\n\u003cp\u003eOne interesting aspect of working with RNNs is that they use a modified form of back propagation called \u003cstrong\u003e\u003cem\u003eBack Propagation Through Time (BPTT)\u003c/em\u003e\u003c/strong\u003e. Because the model is trained on sequence data, it has the potential to be right or wrong at every point in that sequence. This means that you need to adjust the model's weights at each time point to effectively learn from sequence data. Because the model starts at the most recent output, and then works backwards to calculate the loss and update the weights at each time step, the model is said to be going \"back in time\" to learn. Since you have to update every single weight at every single time step, that means that BPTT is much more computationally expensive than traditional back propagation. For instance, if a single data point is a sequence with 1000 time steps, then the model will perform a full round of back propagation for each of the 1000 points in that single sequence.\u003c/p\u003e\n\u003ch3\u003eTruncated Back Prop Through Time\u003c/h3\u003e\n\u003cp\u003eThis was a major hurdle for traditional RNN architectures, but a solution exists in the form of the \u003cstrong\u003e\u003cem\u003eTruncated Back Propagation Through Time (TBPTT)\u003c/em\u003e\u003c/strong\u003e algorithm! We won't go deep into the specifics, but essentially, this algorithm increases performance by breaking a big sequence of 1000 points into 50 sequences of 20. This significantly improves training time over regular BPTT, but is still significantly slower than vanilla back propagation.\u003c/p\u003e\n\u003cp\u003eFun Fact: Truncated Back Prop Through Time was invented in the dissertation of Ilya Sutskever, one of the lead researchers at Open AI!\u003c/p\u003e\n\u003ch2\u003eSummary\u003c/h2\u003e\n\u003cp\u003eIn this lesson, you learned about the sequence data. You also learned about the architecture of RNNs, and the modified back prop algorithm they use for training!\u003c/p\u003e","exportId":"understanding-recurrent-neural-networks"},{"id":197823,"title":"LSTMs and GRUs","type":"WikiPage","indent":0,"locked":false,"requirement":null,"completed":false,"content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-lstms-and-grus\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-lstms-and-grus\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-lstms-and-grus/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eIn this lesson, you'll learn about two advanced types of neurons that typically outperform basic RNNs, \u003cstrong\u003e\u003cem\u003eLong Short Term Memory Cells\u003c/em\u003e\u003c/strong\u003e and \u003cstrong\u003e\u003cem\u003eGated Recurrent Units\u003c/em\u003e\u003c/strong\u003e! You'll explore the problems they solve that increase their effectiveness compared to traditional vanilla RNNs, and compare and contrast the two neurons types to get a feel for what exactly they do and how they do it!\u003c/p\u003e\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\u003cp\u003eYou will be able to:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eExplain why vanishing and exploding gradients exist when training RNNs\u003c/li\u003e\n\u003cli\u003eDescribe the basic architecture and function of a GRU\u003c/li\u003e\n\u003cli\u003eDescribe the architecture and function of an LSTM cell\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003eRNNs and Gradient Problems\u003c/h2\u003e\n\u003cp\u003eOne of the biggest problems with standard Recurrent Neural Networks is that they get \u003cstrong\u003e\u003cem\u003eSaturated\u003c/em\u003e\u003c/strong\u003e. The problem with this it that they use a sigmoid or tanh activation function, and there are large areas of each function where the derivative is very, very close to 0. When the derivatives are low, this means the weight updates are small, which means that the \"learning\" of the model slows to a crawl! This happens because after many, many weight updates, many weights will have been pushed into an extremely positive or extremely negative value. All you have to do is get past -5 or +5 to get to very small values.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-lstms-and-grus/master/images/new_vanishing_gradient.png\"\u003e\u003c/p\u003e\n\u003cp\u003eWhen gradients are close to 0 because the values are extremely low, this is called \u003cstrong\u003e\u003cem\u003eVanishing Gradient\u003c/em\u003e\u003c/strong\u003e. Similarly, networks can also get to the point where the gradients are much, much large, resulting in massive weight updates that cause the model to thrash between 1 extremely wrong answer and another. When this happens, it is called \u003cstrong\u003e\u003cem\u003eExploding Gradient\u003c/em\u003e\u003c/strong\u003e. In practice, you can easily solve exploding gradients by just \"clipping\" the weight updates by bounding them at a maximum value. However, there's no good solution for vanishing gradients!\u003c/p\u003e\n\u003cp\u003eAn intuitive way to think of this in terms of Information Theory -- the network is trying to encapsulate too much information from all of the time steps. Take a look at the following diagram, which you saw in the previous lesson. Pay attention to the colors that represent each word:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-lstms-and-grus/master/images/unrolled.gif\"\u003e\u003c/p\u003e\n\u003cp\u003eNotice how the further along the sequence goes, the less overall area the navy blue color (for the first word, \"What\") gets. As each new word in the sequence gets processed, the amount of \"room\" the RNN has to remember things gets saturated. It turns out, remembering too many things is a pretty surefire way to get your model to crash and burn. This makes it hard for dealing with long-term dependencies in the data. For instance, consider the following sentence:\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\"Marilyn studied in France during the summer and fall semesters of college in 2016. As a result, she speaks fluent {_}\"\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eIf you were to use a traditional RNN to predict the next word in this sentence, it would likely have trouble figuring out the answer because of the number of time steps between the word to predict and the word that contains the information necessary to make a prediction, \"France\".\u003c/p\u003e\n\u003ch2\u003eRemembering and Forgetting\u003c/h2\u003e\n\u003cp\u003eThis is where the modern versions of RNNs come in. In practice, when building models for sequence data, people rarely use traditional RNN architectures anymore. Instead they make use of \u003cstrong\u003e\u003cem\u003eLSTMs\u003c/em\u003e\u003c/strong\u003e and \u003cstrong\u003e\u003cem\u003eGRUs\u003c/em\u003e\u003c/strong\u003e. Both of these models can be thought of as special types of neurons that can be used in an RNN. Although they work a little differently, they have the same strength -- the ability to \u003cstrong\u003e\u003cem\u003eforget information\u003c/em\u003e\u003c/strong\u003e! By constantly updating their internal state, they can learn what is important to remember, and when it is okay to forget it.\u003c/p\u003e\n\u003cp\u003eConsider the word prediction example you just looked at. You clearly need to remember the word \"France\", but there are plenty of words in between France and the word you need to predict that aren't that important, and you can safely ignore, such as \"during the\", \"and\", \"of\", etc. Furthermore, let's assume that the model learns enough to answer this question, but the next thousand words in the sequence is about something completely different. Do you really still need to hold on to the information about where Marilyn studied? How can you tell when you need to remember something and when you need to forget something? This is where GRUs and LSTMs have different approaches. Let's take a quick look at how they both work.\u003c/p\u003e\n\u003ch2\u003eGated Recurrent Units (GRUs)\u003c/h2\u003e\n\u003cp\u003e\u003cstrong\u003e\u003cem\u003eGated Recurrent Units\u003c/em\u003e\u003c/strong\u003e, or \u003cstrong\u003e\u003cem\u003eGRUs\u003c/em\u003e\u003c/strong\u003e, are a special type of cell that passes along it's internal state at each time step. However, not every part of the internal state is passed along, but only the important stuff! GRUs make use of two \"gate\" functions: a \u003cstrong\u003e\u003cem\u003eReset Gate\u003c/em\u003e\u003c/strong\u003e, which determines what should be removed from the cell's internal state before passing itself along to the next time step, and an \u003cstrong\u003e\u003cem\u003eUpdate Gate\u003c/em\u003e\u003c/strong\u003e, which determines how much of the state from the previous time step should be used in the current time step.\u003c/p\u003e\n\u003cp\u003eThe following technical diagram shows the internal operations of how a GRU cell works. Don't worry about trying to understand what every part of this diagram means. Internally, its just some equations for the update and reset operations, coupled with matrix multiplication and sigmoid functions. Instead, focus on the the \u003cimg src=\"https://render.githubusercontent.com/render/math?math=S_t\"\u003e line, which moves from left to right and denotes the state being updated and passed onto the next layer.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-lstms-and-grus/master/images/new_gru.png\" width=\"400\"\u003e\u003c/p\u003e\n\u003ch2\u003eLong Short Term Memory Cells (LSTMs)\u003c/h2\u003e\n\u003cp\u003e\u003cstrong\u003e\u003cem\u003eLong Short Term Memory Cells\u003c/em\u003e\u003c/strong\u003e, or \u003cstrong\u003e\u003cem\u003eLSTMs\u003c/em\u003e\u003c/strong\u003e, are another sort of specialized neurons for use in RNNs that are able to effectively learn what to remember and what to forget in sequence models.\u003c/p\u003e\n\u003cp\u003eLSTMs are generally like GRUs, except that they use three gates instead of two. LSTMs have:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003ean \u003cstrong\u003e\u003cem\u003eInput Gate\u003c/em\u003e\u003c/strong\u003e, which determines how much of the cell state that was passed along should be kept\u003c/li\u003e\n\u003cli\u003ea \u003cstrong\u003e\u003cem\u003eForget Gate\u003c/em\u003e\u003c/strong\u003e, which determines how much of the current state should be forgotten\u003c/li\u003e\n\u003cli\u003ean \u003cstrong\u003e\u003cem\u003eOutput Gate\u003c/em\u003e\u003c/strong\u003e, which determines how much of the current state should be exposed to the next layers in the network\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eAs you can see, they essentially accomplish the same thing as GRUs, but they do it in a slightly different way. Both models do a great job learning patterns from sequences, even when they are long and extremely complex! You'll find a diagram of a LSTM cell below. Just like with GRUs, don't worry about what the symbols mean or the math behind it. You can always pick that up later if you're curious. Instead, try to focus on how the information flows through this diagram from left to right, and where the various gates are for each function performed!\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-lstms-and-grus/master/images/new_LSTM3_chain.png\" width=\"800\"\u003e\u003c/p\u003e\n\u003cp\u003eThere's no good answer yet as to whether GRUs or LSTMs are superior to one another. In practice, GRUs tend to have a slight advantage in many use cases, but this is far from guaranteed. The best thing to do is to build a model with each and see which one does better.\u003c/p\u003e\n\u003ch2\u003eSummary\u003c/h2\u003e\n\u003cp\u003eIn this lesson, you learned about how LSTMs and GRUs can help the models avoid problems such as vanishing and exploding gradients when working with large sequences of data. You also learned about the structure of LSTMs and GRUs, and how they are able to \"forget\" information!\u003c/p\u003e","exportId":"lstms-and-grus"},{"id":197827,"title":"Deep NLP with Word Embeddings - Recap","type":"WikiPage","indent":0,"locked":false,"requirement":null,"completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-deep-nlp-recap\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-deep-nlp-recap/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eCongratulations! You know have a myriad of powerful NLP tools that you can begin to tap into and further explore. With that, take a minute to review some of the key concepts you were exposed to in this section. \u003c/p\u003e\n\n\u003ch2\u003eRNNs and Word Embeddings\u003c/h2\u003e\n\n\u003cp\u003eRemember that word embeddings are a type of vectorization strategy that computes word vectors from a text corpus. They use similarity metrics, which can reveal how certain words relate to each other, or \"semantic relationships\".\u003c/p\u003e\n\n\u003cp\u003eUnlike TF-IDF vectorization, the size of word embeddings is a tunable parameter, which can help overcoming the curse of dimensionality. Word embeddings can be created using Word2Vec models -- given enough training data. \u003c/p\u003e\n\n\u003cp\u003eSince deep learning is used to create Word2Vec models, training word embeddings can be really time consuming, and when building a predictive model you'd want to avoid spending a lot of time here. Pretrained word vectors are very useful here, and GLoVe is the most commonly used model. When using GLoVe, and moving towards a vector representation for any arbitrarily-sized block of text, mean word embeddings can be used.\u003c/p\u003e\n\n\u003ch2\u003eGRUs and LSTMs\u003c/h2\u003e\n\n\u003cp\u003eBuilding on this, you then took a look at some new architectures for neural nets. Aside from having a temporal aspect as with RNNs, GRUs (Gated Recurrent Units) and LSTMs (Long Short Term Memory Cells) have capabilities for both summarizing important information seen prior and forgetting needless details to free up memory. This acts as an analogy to human memory and allows for improved performance in many tasks. \u003c/p\u003e\n\n\u003ch2\u003eSummary\u003c/h2\u003e\n\n\u003cp\u003eIn this section you learned about advanced deep network architectures including RNNs, GRUs, and LSTMs. You also saw how to create word embeddings, an alternative methodology for encoding textual data into numerical spaces. With that, you also saw how to use transfer learning to apply Word2Vec models and improve NLP classification algorithms.\u003c/p\u003e\n\u003cfooter class=\"fis-footer\" style=\"visibility: hidden;\"\u003e\u003cdiv class=\"fis-feedback\"\u003e\n\u003ch5\u003eHow do you feel about this lesson?\u003c/h5\u003e\n\u003cimg id=\"thumbs-up\" data-repository=\"dsc-deep-nlp-recap\" title=\"Thumbs up!\" alt=\"thumbs up\"\u003e\u003cimg id=\"thumbs-down\" data-repository=\"dsc-deep-nlp-recap\" title=\"Thumbs down!\" alt=\"thumbs down\"\u003e\n\u003c/div\u003e\n\u003ch5\u003eHave specific feedback? \u003ca href=\"https://github.com/learn-co-curriculum/dsc-deep-nlp-recap/issues/new\"\u003eTell us here!\u003c/a\u003e\n\u003c/h5\u003e\u003c/footer\u003e","exportId":"deep-nlp-with-word-embeddings-recap"}]},{"id":21117,"name":"Topic 43: Operationalizing Code and AWS","status":"unlocked","unlockDate":null,"prereqs":[],"requirement":"all","sequential":false,"exportId":"g276a82d33778ab6f907554563246c5a6","items":[{"id":197573,"title":"Amazon Web Services - Introduction","type":"WikiPage","indent":0,"locked":false,"requirement":"must_mark_done","completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-productionizing-machine-learning-models-section-intro\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-productionizing-machine-learning-models-section-intro/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eIn this section, you'll be introduced to Amazon Web Services (AWS) - the most popular cloud service. \u003c/p\u003e\n\n\u003ch2\u003eMachine Learning and the Cloud\u003c/h2\u003e\n\n\u003cp\u003eWe'll begin this section by learning about all the ways that cloud computing services such as \u003cstrong\u003e\u003cem\u003eAmazon Web Services (AWS)\u003c/em\u003e\u003c/strong\u003e have made things better and easier for data scientists. We'll also explore why being able to productionize the machine learning models you create so that other people can use them is one of the most valuable skills you can have as a data scientist. \u003c/p\u003e\n\n\u003ch2\u003eAmazon Web Services (AWS)\u003c/h2\u003e\n\n\u003cp\u003eOne we understand the importance of cloud services and how they fit into the picture for data scientists, we'll jump right in to the most popular cloud service, AWS. We'll learn about what AWS ecosystem contains and how we can use it. We'll also create an account and learn our way around the AWS dashboard. \u003c/p\u003e\n\n\u003ch2\u003eAWS SageMaker\u003c/h2\u003e\n\n\u003cp\u003eOnce we know the basics of AWS, we'll learn how we can make use of the most important tool for Data Scientists, \u003cstrong\u003e\u003cem\u003eAWS SageMaker\u003c/em\u003e\u003c/strong\u003e! We'll see how we can incorporate AWS SageMaker into our workflow to simplify things like distributed training or model productionization! \u003c/p\u003e\n\n\u003ch2\u003eHands-On Practice Shipping Models\u003c/h2\u003e\n\n\u003cp\u003eFinally, we will train and ship real-world models using AWS SageMaker. We'll start by training and productionizing some classical machine learning models with scikit-learn and then set up endpoints with AWS SageMaker so that we can make them available for inference. Then, we'll move onto training a Deep Learning model with SageMaker, so that we can make use of distributed training to speed things up, and then ship the model to production. Finally, we'll use SageMaker to train and productionize a more advanced Convolutional Neural Network for image classification. \u003c/p\u003e\n\n\u003ch2\u003eSummary\u003c/h2\u003e\n\n\u003cp\u003eBy the end of this section, you'll know the basics of how to use AWS for Data Science projects, and you'll have hands-on experience training and productionizing three different machine learning models. This will set you up for success with your capstone project!\u003c/p\u003e\n\u003cfooter class=\"fis-footer\" style=\"visibility: hidden;\"\u003e\u003cdiv class=\"fis-feedback\"\u003e\n\u003ch5\u003eHow do you feel about this lesson?\u003c/h5\u003e\n\u003cimg id=\"thumbs-up\" data-repository=\"dsc-productionizing-machine-learning-models-section-intro\" title=\"Thumbs up!\" alt=\"thumbs up\"\u003e\u003cimg id=\"thumbs-down\" data-repository=\"dsc-productionizing-machine-learning-models-section-intro\" title=\"Thumbs down!\" alt=\"thumbs down\"\u003e\n\u003c/div\u003e\n\u003ch5\u003eHave specific feedback? \u003ca href=\"https://github.com/learn-co-curriculum/dsc-productionizing-machine-learning-models-section-intro/issues/new\"\u003eTell us here!\u003c/a\u003e\n\u003c/h5\u003e\u003c/footer\u003e","exportId":"amazon-web-services-introduction"},{"id":197577,"title":"Data Science and Machine Learning Engineering","type":"WikiPage","indent":0,"locked":false,"requirement":"must_mark_done","completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-data-science-and-machine-learning-engineering\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-data-science-and-machine-learning-engineering/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eIn this lesson, we'll learn about the importance of being able to write production-quality code to make the models you've trained usable, and the leading cloud environments that make this possible!\u003c/p\u003e\n\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\n\u003cul\u003e\n\u003cli\u003eExplain why cloud computing and putting models into production is important to data scientists \u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch2\u003eData Science Skills and the Job Market\u003c/h2\u003e\n\n\u003cp\u003eYou're almost done with your studies, and will soon be ready to begin the job hunt. Since this is one of the most important topics a Data Scientist can know, we've elected to leave it until the very end, so that it'll be fresh in your mind going into your capstone project and the start of your career -- \u003cstrong\u003e\u003cem\u003eputting models into production\u003c/em\u003e\u003c/strong\u003e. \u003c/p\u003e\n\n\u003cp\u003eAs you start to look at job postings, you probably notice that a \"Data Scientist\" job can mean many, many different things. In some companies, it means a data analyst or a DBA focused on databases or data pipelines. In others, it means someone with a scientific mindset skilled with running A/B tests. Yet others may be highly specialized machine learning roles in areas like NLP, Computer Vision, or Deep Learning -- and these roles may break down further into specializations focused on either research or implementation. As a Junior Data Scientist entering the workforce, it's most likely that you'll land in a generalist role, spending the first few years of your career working on various tasks that focus on all of these areas at least a little bit. Specialization happens later in your career. Out of the gate, the best thing that you can be is a strong generalist, with the demonstrated ability to contribute to many different sorts of projects that might be expected of a Data Science team. \u003c/p\u003e\n\n\u003cp\u003eOver the course of your studies, you've picked up many different skills in many different domain areas that will allow you to contribute to data science projects in a professional environment. However, when it comes to the job market, not all Data Science skills are created equal. While different data scientists or recruiters may rank these skills differently based on their own experiences or needs, one thing most agree on is that the ability to \u003cstrong\u003e\u003cem\u003eproductionize a model\u003c/em\u003e\u003c/strong\u003e is both very valuable and very rare when it comes to Junior Data Scientists. This presents a massive opportunity for you -- if you can become proficient in productionizing the models you've created (and demonstrate this proficiency in your portfolio of projects!), you become a much more interesting candidate for any role.  \u003c/p\u003e\n\n\u003ch2\u003eProductionizing Models as a Career Skill\u003c/h2\u003e\n\n\u003cp\u003eAt large companies such as Google and Facebook, Data Scientists typically run experiments and train models until they have found a solution that works. Once they have trained a validated the model, they typically then hand off productionization of the model to \u003cstrong\u003e\u003cem\u003eMachine Learning Engineers\u003c/em\u003e\u003c/strong\u003e. Whereas the Data Scientist creates the basic prototype, the Machine Learning Engineer's job is to put that model into a production system in a performant and maintainable manner. Whereas Data Scientists at large companies focus on the \"big picture\" by finding solutions to business problems, Machine Learning Engineers focus on the details, implementing the solutions created by the data scientists in the best way possible. Data Scientists focus more on analytics and statistics, whereas Machine Learning Engineers will have a stronger command of backend engineering, data structures and algorithms, and software engineering overall. The following diagram lays out the relationship between different technical roles well:\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-data-science-and-machine-learning-engineering/master/images/new-venn-diagram.png\" height=\"80%\" width=\"80%\"\u003e\u003c/p\u003e\n\n\u003cp\u003eAs you can see from the overlap between \u003cem\u003eData Scientist\u003c/em\u003e and \u003cem\u003eMachine Learning Engineer\u003c/em\u003e , there is still significant overlap between the two -- a Data Scientist should be able to productionize a basic machine learning model, just as a Machine Learning Engineer should able to train a model, validate results, and deal with overfitting. \u003c/p\u003e\n\n\u003ch3\u003eBeing a 'Scrappy' Data Scientist\u003c/h3\u003e\n\n\u003cp\u003eMany junior data scientists have at least one or two areas where they have significant holes in their knowledge. There are many paths into data science, and many junior data scientists are from backgrounds that have overlap with certain parts of data science. For instance, it's not uncommon for bioinformatics professionals or statisticians to rebrand themselves as a data scientist to take advantage of the higher salary in this field. While their backgrounds may give them a very strong understanding of analytics, scientific experimentation, or understanding how machine learning models work, these professionals often have little exposure to engineering, and thus can create and train models, but aren't able to put them into production so that the company can actually use them. Similarly, many junior data scientists on the job market have nothing more than a bachelor's degree in computer science and a passing understanding of machine learning -- in this case, productionizing a model is easy, but they may lack depth of understanding when it comes to the model itself. This isn't an insurmountable problem -- large companies almost always have some role where a candidate's skills can be useful, and they can invest in training the employee and skilling them up in areas where they're a bit weak. \u003c/p\u003e\n\n\u003cp\u003eWith small and medium-sized companies, this is a much bigger problem. Data Scientists in smaller organizations are expected to be a bit more independent, and will likely have to \"wear more hats\". For a data science role at a startup, it's a common expectation for their data scientists to handle every part of a data science project. This means starting by interviewing key stakeholders and identifying the problem to be solved, followed by rapidly prototyping a solution until you've trained/tuned/validated a model that meets your standards, followed by actually putting that model in production!  This means that it's very important to be 'scrappy', and be able to handle anything that's thrown at you as a data scientist. Smaller companies often don't have the funds or the infrastructure for a separate Machine Learning Engineering team to handle the details of implementation. In this respect, being able to productionize a machine learning model is the most practical, useful skill you can have in your Data Science toolbox. For all but the largest companies, it doesn't matter how great you are at training models -- until you put that model into production so that the rest of the organization can actually \u003cem\u003euse\u003c/em\u003e it, it might as well not exist! \u003c/p\u003e\n\n\u003cp\u003eThe TL;DR here is quite simple:\u003c/p\u003e\n\n\u003col\u003e\n\u003cli\u003eMany data scientists don't know how to put machine learning models into production.\u003cbr\u003e\n\u003c/li\u003e\n\u003cli\u003ePutting a model into production is a mandatory skill for data scientists at most small to medium-sized companies.\u003c/li\u003e\n\u003cli\u003eBeing able to productionize models will make you a much more attractive candidate to employers, and give you a competitive advantage!\u003c/li\u003e\n\u003c/ol\u003e\n\n\u003ch2\u003eData Science and Cloud Computing\u003c/h2\u003e\n\n\u003cp\u003eWe've established that being able to productionize machine learning models is a valuable skill -- so how do we do it? This answer has changed in recent years thanks to cloud computing platforms such as \u003cstrong\u003e\u003cem\u003eAmazon Web Services\u003c/em\u003e\u003c/strong\u003e. A decade ago, productionizing a machine learning model would have meant building your own web server with something like \u003ca href=\"http://flask.pocoo.org/\"\u003eFlask\u003c/a\u003e or \u003ca href=\"https://www.djangoproject.com/\"\u003eDjango\u003c/a\u003e and hosting somewhere, just like you would with any web app. However, the creation of cloud computing platforms changed things in a big way, and data science is no exception. Now, we don't even need to worry about things like server code -- instead, we can use preexisting services from AWS that were created specifically to simplify the process of productionizing machine learning solutions!\u003c/p\u003e\n\n\u003cp\u003eFor the remainder of this section, we're going to dig deep into all the amazing tools AWS provides, and learn how we can use them to make you more effective data scientists! \u003c/p\u003e\n\n\u003ch2\u003eSummary\u003c/h2\u003e\n\n\u003cp\u003eIn this lesson, we learned about the similarities and differences between Data Scientists and Machine Learning Engineers. We also learned why the ability to productionize a machine learning model is a crucial skill for data scientists, as well as how this skill can provide a great competitive advantage when applying for jobs!\u003c/p\u003e\n\u003cfooter class=\"fis-footer\" style=\"visibility: hidden;\"\u003e\u003cdiv class=\"fis-feedback\"\u003e\n\u003ch5\u003eHow do you feel about this lesson?\u003c/h5\u003e\n\u003cimg id=\"thumbs-up\" data-repository=\"dsc-data-science-and-machine-learning-engineering\" title=\"Thumbs up!\" alt=\"thumbs up\"\u003e\u003cimg id=\"thumbs-down\" data-repository=\"dsc-data-science-and-machine-learning-engineering\" title=\"Thumbs down!\" alt=\"thumbs down\"\u003e\n\u003c/div\u003e\n\u003ch5\u003eHave specific feedback? \u003ca href=\"https://github.com/learn-co-curriculum/dsc-data-science-and-machine-learning-engineering/issues/new\"\u003eTell us here!\u003c/a\u003e\n\u003c/h5\u003e\u003c/footer\u003e","exportId":"data-science-and-machine-learning-engineering"},{"id":197581,"title":"The AWS Ecosystem","type":"WikiPage","indent":0,"locked":false,"requirement":"must_mark_done","completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-the-aws-ecosystem\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-the-aws-ecosystem/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eIn this lesson, we'll get set up to use \u003cstrong\u003e\u003cem\u003eAmazon Web Services\u003c/em\u003e\u003c/strong\u003e, and then get to know our way around the platform before digging into AWS SageMaker in the next lesson. \u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-the-aws-ecosystem/master/images/awscloud.svg\"\u003e\u003c/p\u003e\n\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\n\u003cul\u003e\n\u003cli\u003eSet up an AWS account and explore the Amazon Resource Center \u003c/li\u003e\n\u003cli\u003eExplain what the \"regions\" are in AWS and why it is important to choose the right one \u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch2\u003eGetting Started\u003c/h2\u003e\n\n\u003cp\u003eBefore we can begin exploring everything AWS has to offer, we'll need to create an account on the platform. To do this, start by following this link to \u003ca href=\"https://aws.amazon.com/\"\u003eAmazon Web Services\u003c/a\u003e. While you're there, you may want to take the time to bookmark it -- chances are this is a website you'll use frequently in your career as a Data Scientist!\u003c/p\u003e\n\n\u003ch3\u003eWill This Cost Money?\u003c/h3\u003e\n\n\u003cp\u003eAlthough you will need a credit card to register for AWS, working through this section will not cost any money. AWS provides a free tier for learning and prototyping on the platform -- this is the tier we'll use for everything going forward. As long as you correctly register for the free tier, this will not cost you any money. \u003c/p\u003e\n\n\u003ch3\u003eRegister Your Email\u003c/h3\u003e\n\n\u003cp\u003eBegin by clicking the \"Sign Up\" button in the top right-hand corner of the page. \u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-the-aws-ecosystem/master/images/aws-1.png\"\u003e\u003c/p\u003e\n\n\u003cp\u003eNext, create an account by adding your email and password. You'll also need to set an \u003cstrong\u003e\u003cem\u003eAWS Account Name\u003c/em\u003e\u003c/strong\u003e. \u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-the-aws-ecosystem/master/images/aws-2.png\"\u003e\u003c/p\u003e\n\n\u003cp\u003eOn the next screen, enter your contact information. \u003cstrong\u003e\u003cem\u003eMake sure you set your account type to 'Personal'!\u003c/em\u003e\u003c/strong\u003e \u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-the-aws-ecosystem/master/images/aws-3.png\"\u003e\u003c/p\u003e\n\n\u003cp\u003eThis next page is especially important -- be sure to select the \u003cstrong\u003e\u003cem\u003eBasic Plan\u003c/em\u003e\u003c/strong\u003e! As a reminder, you will be asked to enter a credit card number during the next few steps. Although we will only be making use of the free tier of services for AWS, be aware that you will still need to enter a credit card number in order to complete the registration process. \u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-the-aws-ecosystem/master/images/aws-4.png\"\u003e\u003c/p\u003e\n\n\u003cp\u003eNow that you're all signed up, click the \"Sign in to the Console\" button to actually enter the AWS Console. \u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-the-aws-ecosystem/master/images/aws-5.png\"\u003e\u003c/p\u003e\n\n\u003cp\u003eAlright, you've now created an AWS Account! Let's take a look around. \u003c/p\u003e\n\n\u003ch2\u003eThe AWS Console\u003c/h2\u003e\n\n\u003cp\u003eNow that you're signed in, you'll see the \u003cstrong\u003e\u003cem\u003eAWS Console\u003c/em\u003e\u003c/strong\u003e. This is your \"home screen\" for AWS -- it allows you to quickly navigate through the thousands of services offered on AWS to find what you need. The easiest way to find what you need is the \"Find Services\" search bar at the top of the body of the page. \u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-the-aws-ecosystem/master/images/aws-6.png\"\u003e\u003c/p\u003e\n\n\u003cp\u003eYou can also click the \"See All Services\" dropdown to see a full list of services you can use in AWS. There are \u003cstrong\u003ea ton\u003c/strong\u003e of services, but don't let yourself get overwhelmed -- you'll probably never end up using the vast majority of these, as only a few apply to the work of a data scientist. \u003c/p\u003e\n\n\u003ch2\u003eUse Cases for Data Scientists\u003c/h2\u003e\n\n\u003cp\u003eWe've now created an account for AWS, so that we can take advantage of the Cloud. As data scientists, we'll find that a cloud computing service like AWS is very helpful in a number of ways. Aside from productionizing the model as a whole, the most important thing the cloud enables data scientists to do is to train much, much larger models by distributing training across entire clusters of servers. Without cloud computing, it would be impossible to train some of the larger deep learning models that exist today. The ability to distribute training of a neural network across a GPU allowed AI researchers to create massive models in a reasonable amount of time by creating a server cluster full of hundreds of GPUs. While this works, building a server like this is cost prohibitive to all but major companies and universities. Thankfully, services like AWS allow us to rent time on these servers per minute, making distributed training available for anybody at extremely cheap prices, paying only for what we use. AWS provides other great uses for data scientists beyond speedy training times -- it also plays a major part with databases. In your job as a data scientist, the databases you connect to in order to get your data will almost certainly be stored on AWS, or a competitor cloud platform. AWS servers also allow for companies to make use of big data frameworks such as Hadoop or Spark across a cluster of servers. \u003c/p\u003e\n\n\u003ch2\u003eUsing the Amazon Resource Center\u003c/h2\u003e\n\n\u003cp\u003eAs platforms go, you won't find many with more options than AWS. It has an amazing amount of offerings, with more getting added all the time. While AWS is great for basic use cases like hosting a server or a website, it also has all kinds of different offerings in areas such as Databases, Machine Learning, Data Analytics and other areas useful to Data Scientists. It's not possible for us to cover how to use every service in AWS in this section -- but luckily, we don't need to, because Amazon already has! The \u003ca href=\"https://aws.amazon.com/getting-started/\"\u003eGetting Started Resource Center\u003c/a\u003e contains a ton of awesome tutorials, demonstrations, and sample projects for just about everything you would ever want to know about any service on AWS. We \u003cstrong\u003e\u003cem\u003estrongly recommend\u003c/em\u003e\u003c/strong\u003e bookmarking this page, as the tutorials they offer are very high quality, and free!\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-the-aws-ecosystem/master/images/aws-7.png\"\u003e\u003c/p\u003e\n\n\u003ch2\u003eA Note On Regions\u003c/h2\u003e\n\n\u003cp\u003eBefore we move onto digging into \u003cstrong\u003e\u003cem\u003eAWS SageMaker\u003c/em\u003e\u003c/strong\u003e in the next lesson, it's worth taking a moment to explain \"Regions\" and what they have to do with AWS. AWS has data centers all over the world, and they are \u003cstrong\u003enot\u003c/strong\u003e interchangeable when it comes to your projects. Click on the \"Region\" tab in the top right corner of the navigation bar, and you should see a dropdown of all the different data centers you can choose from. It is \u003cstrong\u003e\u003cem\u003every important\u003c/em\u003e\u003c/strong\u003e that you always choose the same region to connect to with your projects. Each region is its own unique data center, and anything you do on in that region is only in that region. One of the most common mistakes newcomers to AWS make is thinking they've lost their project because they are connected to a different data center and don't realize it. We'll remind you of this again later, but it can't hurt to say it twice: always make sure you're connected to the correct data center! This goes doubly for when you're creating a new project. \u003c/p\u003e\n\n\u003ch2\u003eSummary\u003c/h2\u003e\n\n\u003cp\u003eIn this lesson, we signed up for Amazon Web Services and explored some of the different options on the platform. We also learned about where AWS fits in the data science process. \u003c/p\u003e\n\u003cfooter class=\"fis-footer\" style=\"visibility: hidden;\"\u003e\u003cdiv class=\"fis-feedback\"\u003e\n\u003ch5\u003eHow do you feel about this lesson?\u003c/h5\u003e\n\u003cimg id=\"thumbs-up\" data-repository=\"dsc-the-aws-ecosystem\" title=\"Thumbs up!\" alt=\"thumbs up\"\u003e\u003cimg id=\"thumbs-down\" data-repository=\"dsc-the-aws-ecosystem\" title=\"Thumbs down!\" alt=\"thumbs down\"\u003e\n\u003c/div\u003e\n\u003ch5\u003eHave specific feedback? \u003ca href=\"https://github.com/learn-co-curriculum/dsc-the-aws-ecosystem/issues/new\"\u003eTell us here!\u003c/a\u003e\n\u003c/h5\u003e\u003c/footer\u003e","exportId":"the-aws-ecosystem"},{"id":197586,"title":"Introduction to Amazon SageMaker","type":"WikiPage","indent":0,"locked":false,"requirement":"must_mark_done","completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-introduction-to-aws-sagemaker\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-introduction-to-aws-sagemaker/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eIn this lesson, we'll learn about \u003cstrong\u003e\u003cem\u003eAmazon SageMaker\u003c/em\u003e\u003c/strong\u003e, and explore some of the common use cases it covers for data scientists. \u003c/p\u003e\n\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\n\u003cul\u003e\n\u003cli\u003eList the use cases of Amazon SageMaker \u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch2\u003eWhat is SageMaker?\u003c/h2\u003e\n\n\u003cp\u003eSageMaker is a platform created by Amazon to centralize all the various services related to Data Science and Machine Learning. If you're a data scientist working on AWS, chances are that you'll be spending most (if not all) of your time in SageMaker getting things done. You can get to SageMaker by just searching for \"SageMaker\" inside the spotlight search bar in the AWS Console. \u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-introduction-to-aws-sagemaker/master/images/sagemaker.png\"\u003e\u003c/p\u003e\n\n\u003ch2\u003eSageMaker Use Cases\u003c/h2\u003e\n\n\u003cp\u003eWhen you visit the page for SageMaker, you'll notice that the following graphic highlighting the various use cases SageMaker can help with:\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-introduction-to-aws-sagemaker/master/images/use_cases.png\"\u003e\u003c/p\u003e\n\n\u003cp\u003eYou'll also notice these same categories on the sidebar on the left side of the screen, with more detailed links to services that fall under each category:\n\u003cbr\u003e\n\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-introduction-to-aws-sagemaker/master/images/sidebar.png\"\u003e\u003c/p\u003e\n\n\u003cp\u003eHere's a brief explanation of what each of these service areas are used for in a professional data science setting.\u003c/p\u003e\n\n\u003ch3\u003eGround Truth\u003c/h3\u003e\n\n\u003cp\u003eOne of the hardest, most expensive, and most tedious parts of data science is getting the labels needed for supervised learning projects. For projects inside companies, it's quite common to start by gathering the proprietary data needed in order to train a model that can answer the business question and/or provide the service your company needs. One of the major use cases SageMaker provides is a well-structured way to manage data labeling projects. \u003cstrong\u003e\u003cem\u003eSageMaker GroundTruth\u003c/em\u003e\u003c/strong\u003e allows you to manage private teams, in case your information is sensitive, or to manage public teams by leveraging \u003cstrong\u003e\u003cem\u003eAWS Mechanical Turk\u003c/em\u003e\u003c/strong\u003e, which crowdsources labels from an army of public contractors that have signed up and are paid by the label. \u003c/p\u003e\n\n\u003cp\u003eRecently, Amazon launched an automated labeling service that makes use of machine learning models to generate labels in a human-in-the-loop format, where only labels that are above a particular confidence threshold (which you set yourself) are auto-generated by the model. This allows your contractors to focus only on the tough examples, and saves you from having to pay as much for labels for the easy examples which a model can handle. \u003c/p\u003e\n\n\u003ch3\u003eNotebooks\u003c/h3\u003e\n\n\u003cp\u003eThese are exactly what they sound like -- cloud-based jupyter notebooks, a data scientist's 'bread and butter'!  SageMaker notebooks are just like regular jupyter notebooks, with a bit more added functionality. For instance, it's quite easy to choose from a bunch of pre-configured kernels to select which version of Python/TensorFlow/etc. you want to use. You can start a notebook from scratch inside SageMaker and do all of your work in the cloud, or you can upload preexisting notebooks into SageMaker, allowing you to do you work on a local machine and move it over to the cloud when you're ready for training!\u003c/p\u003e\n\n\u003cp\u003eWe strongly recommend you take a minute to poke around inside a SageMaker notebook to get a feel for what it looks like and what it can do. They're pretty amazing!\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-introduction-to-aws-sagemaker/master/images/notebook.png\"\u003e\u003c/p\u003e\n\n\u003ch3\u003eTraining\u003c/h3\u003e\n\n\u003cp\u003eSageMaker's training services allow you to easily leverage cloud computing with AWS's specialized GPU and TPU servers, allowing you to train massive models that simply wouldn't be possible on a local machine. There are a ton of configuration options, and you can easily set budgets, limits, training times, and even auto-tune your hyperparameters! Although this is outside the scope of our lessons on AWS, Amazon provides some pretty amazing (and fast!) tutorials about how to use more specific services like cloud training or \u003ca href=\"https://aws.amazon.com/blogs/aws/sagemaker-automatic-model-tuning/\"\u003emodel tuning\u003c/a\u003e once you've completed this section! \u003c/p\u003e\n\n\u003ch3\u003eInference\u003c/h3\u003e\n\n\u003cp\u003eArguably the most important part of the data science pipeline, \u003cstrong\u003e\u003cem\u003eInference\u003c/em\u003e\u003c/strong\u003e services focus on allowing you to create endpoints so that people can consume your models over the internet! One of the most handy parts of SageMaker's approach to inference is the fact that you can productionize your own model, or just use one of theirs! While there are certainly times where you'll need to create, train, and host your own model, AWS has made things simple by allowing you to use their own models and charging you on a per-use basis. For instance, let's say that you needed to make some time series forecasts. While you could go down the very complicated route of training your own model, you could also just make use of AWS SageMaker's \u003cem\u003eDeepAR\u003c/em\u003e model, which uses the most cutting-edge time series model available to make forecasts on your data. \u003c/p\u003e\n\n\u003ch2\u003eSummary\u003c/h2\u003e\n\n\u003cp\u003eIn this lesson, we learned about Amazon SageMaker, and explored some of the common use cases it covers for data scientists!\u003c/p\u003e\n\u003cfooter class=\"fis-footer\" style=\"visibility: hidden;\"\u003e\u003cdiv class=\"fis-feedback\"\u003e\n\u003ch5\u003eHow do you feel about this lesson?\u003c/h5\u003e\n\u003cimg id=\"thumbs-up\" data-repository=\"dsc-introduction-to-aws-sagemaker\" title=\"Thumbs up!\" alt=\"thumbs up\"\u003e\u003cimg id=\"thumbs-down\" data-repository=\"dsc-introduction-to-aws-sagemaker\" title=\"Thumbs down!\" alt=\"thumbs down\"\u003e\n\u003c/div\u003e\n\u003ch5\u003eHave specific feedback? \u003ca href=\"https://github.com/learn-co-curriculum/dsc-introduction-to-aws-sagemaker/issues/new\"\u003eTell us here!\u003c/a\u003e\n\u003c/h5\u003e\u003c/footer\u003e","exportId":"introduction-to-amazon-sagemaker"},{"id":197591,"title":"Productionizing a Model with Docker and SageMaker","type":"Assignment","indent":0,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_mark_done","completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-productionizing-models-with-sagemaker\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-productionizing-models-with-sagemaker/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","exportId":"g0f0233ecfbd7bc2cfc18149458f8aec7"},{"id":197596,"title":"Amazon Web Services - Recap","type":"WikiPage","indent":0,"locked":false,"requirement":"must_mark_done","completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-productionizing-machine-learning-models-section-recap\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-productionizing-machine-learning-models-section-recap/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\u003ch2\u003eKey Takeaways\u003c/h2\u003e\n\n\u003cp\u003eThe key takeaways from this section include:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eAWS is a \u003cstrong\u003e\u003cem\u003eCloud-Computing Platform\u003c/em\u003e\u003c/strong\u003e which we can use for a variety of use cases in data science.\u003c/li\u003e\n\u003cli\u003eIn this section, we learned about how to sign up for AWS, and how to make sure that we have the right region selected when working in AWS.\u003c/li\u003e\n\u003cli\u003eAmazon has centralized all of the major data science services inside \u003cstrong\u003e\u003cem\u003eAmazon SageMaker\u003c/em\u003e\u003c/strong\u003e. SageMaker provides numerous services for things such as:\n\n\u003cul\u003e\n\u003cli\u003eData Labeling\u003c/li\u003e\n\u003cli\u003eCloud-based Notebooks\u003c/li\u003e\n\u003cli\u003eTraining and Model Tuning\u003c/li\u003e\n\u003cli\u003eInference\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eWe can set up our own models, or use the preexisting models provided by AWS. Similarly, we can set up our own inference endpoints, or make use of preexisting endpoints created by AWS. \u003c/li\u003e\n\u003cli\u003eCreating our own endpoint requires us to use a Docker instance, as we saw in the previous codealong. Much of the work required to create an endpoint for our own model is boilerplate, and we can use it again and again across multiple projects. \u003c/li\u003e\n\u003c/ul\u003e\n\u003cfooter class=\"fis-footer\" style=\"visibility: hidden;\"\u003e\u003cdiv class=\"fis-feedback\"\u003e\n\u003ch5\u003eHow do you feel about this lesson?\u003c/h5\u003e\n\u003cimg id=\"thumbs-up\" data-repository=\"dsc-productionizing-machine-learning-models-section-recap\" title=\"Thumbs up!\" alt=\"thumbs up\"\u003e\u003cimg id=\"thumbs-down\" data-repository=\"dsc-productionizing-machine-learning-models-section-recap\" title=\"Thumbs down!\" alt=\"thumbs down\"\u003e\n\u003c/div\u003e\n\u003ch5\u003eHave specific feedback? \u003ca href=\"https://github.com/learn-co-curriculum/dsc-productionizing-machine-learning-models-section-recap/issues/new\"\u003eTell us here!\u003c/a\u003e\n\u003c/h5\u003e\u003c/footer\u003e","exportId":"amazon-web-services-recap"}]},{"id":21125,"name":"APPENDIX: Graph Theory","status":"completed","unlockDate":null,"prereqs":[],"requirement":null,"sequential":false,"exportId":"g8b281238363fa2c1f167f093d9fd1b04","items":[{"id":197622,"title":"Graph Theory - Introduction","type":"WikiPage","indent":0,"locked":false,"requirement":null,"completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-network-introduction\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-network-introduction/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eIn this section, you'll investigate a new data structure: networks! Networks are a useful data structure to map a range of applications from driving directions to social networks.\u003c/p\u003e\n\n\u003ch2\u003eNetwork Graphs\u003c/h2\u003e\n\n\u003cp\u003eNetworks are another way of representing data that you have yet to fully investigate. In their most simple case, a network contains \u003cstrong\u003enodes\u003c/strong\u003e connected by \u003cstrong\u003eedges\u003c/strong\u003e like this:\n\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-network-introduction/master/images/graph.png\"\u003e\u003c/p\u003e\n\n\u003cp\u003eNodes represent some object such as people, languages, countries, or tags, to name a few. The relationships between these objects are the edges between them. For example, later in this section you'll investigate the relationship of various technology tags on the popular website \u003ca href=\"stackoverflow.com\"\u003eStackOverflow\u003c/a\u003e. One potential network visualization of this data looks like this:\n\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-network-introduction/master/images/stackoverflow_clusters.png\"\u003e\u003c/p\u003e\n\n\u003ch2\u003ePath Searching\u003c/h2\u003e\n\n\u003cp\u003eAn important concept in network analysis are path searching algorithms. Finding the shortest path between two nodes is a foundational concept for creating a distance metric which can then be used to conduct more advanced analyses. Mapping applications such as Google Maps, Apple Maps, Waze, or Uber are also natural applications for path searching algorithms. In this section, you'll investigate Dijkstra's algorithm for finding the shortest path between two points, coding it from scratch using Python.\u003c/p\u003e\n\n\u003ch2\u003eCentrality\u003c/h2\u003e\n\n\u003cp\u003eOnce you've familiar with the concept of path searching, you'll then go on to investigate properties of nodes and edges. Centrality is a key concept in this, helping to determine which nodes are most influential in a network, or hold pivotal positions in connecting the network.\u003c/p\u003e\n\n\u003ch2\u003eCliques and Clustering\u003c/h2\u003e\n\n\u003cp\u003eMoving from the study of single objects nodes and edges within the network, you'll then start to investigate larger structures. With this, you'll investigate the concept of cliques and clusters in order to subdivide a network into smaller groups. Natural applications of this include sub-setting social networks into groups or categorizing items such as books or languages.\u003c/p\u003e\n\n\u003ch2\u003eRecommendation Systems\u003c/h2\u003e\n\n\u003cp\u003eTo round out this section, you'll investigate how networks can be used to fuel recommendation systems, a popular and exciting topic. With this, you'll work on recommending amazon products to customers.\u003c/p\u003e\n\n\u003ch2\u003eSummary\u003c/h2\u003e\n\n\u003cp\u003eGet ready to dive into the exciting realm of networks! In this section, you'll get to play around with a range of datasets from Twitter, Game of Thrones, and the Amazon Marketplace!\u003c/p\u003e\n\u003cfooter class=\"fis-footer\" style=\"visibility: hidden;\"\u003e\u003cdiv class=\"fis-feedback\"\u003e\n\u003ch5\u003eHow do you feel about this lesson?\u003c/h5\u003e\n\u003cimg id=\"thumbs-up\" data-repository=\"dsc-network-introduction\" title=\"Thumbs up!\" alt=\"thumbs up\"\u003e\u003cimg id=\"thumbs-down\" data-repository=\"dsc-network-introduction\" title=\"Thumbs down!\" alt=\"thumbs down\"\u003e\n\u003c/div\u003e\n\u003ch5\u003eHave specific feedback? \u003ca href=\"https://github.com/learn-co-curriculum/dsc-network-introduction/issues/new\"\u003eTell us here!\u003c/a\u003e\n\u003c/h5\u003e\u003c/footer\u003e","exportId":"graph-theory-introduction"},{"id":197627,"title":"Introduction to Graph Theory","type":"WikiPage","indent":0,"locked":false,"requirement":null,"completed":false,"content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-intro-graph-theory\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-intro-graph-theory\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-intro-graph-theory/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eIn this lesson, you'll get an introduction to some basic terminology regarding graphs and graph theory. To start, here's a graph!\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-intro-graph-theory/master/images/graph1.png\"\u003e\u003c/p\u003e\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\u003cp\u003eYou will be able to:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eExplain what nodes and edges are in graph theory\u003c/li\u003e\n\u003cli\u003eExplain the difference between directed and undirected graphs\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003eNodes and Edges: The Building Blocks of Graphs\u003c/h2\u003e\n\u003cp\u003eTo start, graphs are composed of two primary objects: \u003cstrong\u003enodes\u003c/strong\u003e and \u003cstrong\u003eedges\u003c/strong\u003e. In the picture above, the nodes are the circles, while the lines that connect them are edges. Typically, nodes represent some entity such as a person, businesses, places, or webpages. In turn, edges then represent the relationships between these entities. For example, you might have a graph of a social network in which each node represents a person, and each edge represents whether those two individuals are connected or friends within the network.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-intro-graph-theory/master/images/graph2.png\"\u003e\u003c/p\u003e\n\u003cp\u003eAs you can see, Jen is a well connected character in this scenario: she has a connecting edge with literally every other node in the graph! On the other hand, Jake is the least connected. He has no other connections other than Jen.\u003c/p\u003e\n\u003ch2\u003eDirected vs Undirected Graphs\u003c/h2\u003e\n\u003cp\u003eAnother important concept in graph theory is the difference between directed and undirected graphs. The previous two examples have demonstrated undirected graphs. As the name implies, the edges in an undirected graph represent a mutual connection between two nodes. For example, the previous undirected graph could represent a mutual relationship such as \"Friends\" on Facebook or \"Connections\" on LinkedIn. In contrast, a direct graph looks like this:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-intro-graph-theory/master/images/graph3.png\"\u003e\u003c/p\u003e\n\u003cp\u003eAs you can see, each of the edges now has an arrow indicating a direction. This scenario could represent an alternative type of social network such as Twitter in which individual's relationships are not necessarily mutual. Instead, Twitter users can follow other users to stay up to date with their activity. In the graph depicted above, Sally isn't following anyone. However, both Bob and Jen are following Sally. There is also one mutual relationship depicted: Jake is following Jen and she is also following him.\u003c/p\u003e\n\u003ch2\u003eConnectedness\u003c/h2\u003e\n\u003cp\u003eConnectedness aims to quantify the number of edges attached to a node. In the graphs above, Jen is undoubtedly the most connected of the individuals depicted. In the undirected graph, she was connected to everyone. Similarly, if your goal is to become an \u003cem\u003einfluencer\u003c/em\u003e, you're going to need to develop quite the following and become a very connected node. You'll explore more details in how connectedness is quantified in the upcoming lessons. For now, take some time to think about other implications of connectedness. For example, how might you be able to use connectedness to determine friend circles or cliques in social networks?\u003c/p\u003e\n\u003ch2\u003ePath Searching\u003c/h2\u003e\n\u003cp\u003ePath searching algorithms aim to find the shortest distance between any two nodes. This can then be used as a distance metric between nodes. Additionally, this can have interesting implications. For example, in a graph network of a website, a path searching algorithm might outline how many steps are required for a customer to move from the homepage, to browsing for an item, all the way through completing their purchase at checkout. You've actually already seen some basic examples of path searching algorithms in your work with traversing JSON files. There, you took a look at developing breadth-first versus depth-first recursive procedures to create an outline of the structure of an arbitrary JSON file.\u003c/p\u003e\n\u003ch2\u003eSummary\u003c/h2\u003e\n\u003cp\u003eIn this lesson, you got a brief introduction to graph theory, including some basic definitions and foundational concepts. Remember that graphs are composed of primary objects called nodes and the relationships between those objects, known as edges. Additionally, graphs can be directed or undirected depending on the nature of the edges and the relationships between nodes.\u003c/p\u003e","exportId":"introduction-to-graph-theory"},{"id":197635,"title":"Introduction to NetworkX","type":"Assignment","indent":0,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":null,"completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-networkX-intro\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-networkX-intro/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","exportId":"gd5546ee7f0ca0758abec95363ffaf8c5"},{"id":197640,"title":"Introduction to NetworkX - Lab","type":"Assignment","indent":1,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":null,"completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-networkX-intro-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-networkX-intro-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","exportId":"ga1447a0475f0838fa82e90775a779485"},{"id":197642,"title":"Simple and Shortest Paths","type":"Assignment","indent":0,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":null,"completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-graph-theory-shortest-path\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-graph-theory-shortest-path/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","exportId":"gf057ff7aa80100653161461b977f43f1"},{"id":197646,"title":"Simple and Shortest Paths - Lab","type":"Assignment","indent":1,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":null,"completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-graph-theory-shortest-path-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-graph-theory-shortest-path-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","exportId":"g40b6c15fc2920faf3ff41c3e7cb089af"},{"id":197652,"title":"Node Centrality","type":"Assignment","indent":0,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":null,"completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-node-centrality\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-node-centrality/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","exportId":"gf6b89c6ad7f9e67e3acea8ee704d8571"},{"id":197657,"title":"Node Centrality - Lab","type":"Assignment","indent":1,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":null,"completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-node-centrality-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-node-centrality-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","exportId":"g4a334319ab37f18d3ef05fb1d1986116"},{"id":197662,"title":"Network Clustering","type":"Assignment","indent":0,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":null,"completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-network-clustering\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-network-clustering/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","exportId":"g7060fdce18ee0fb08e702f7658f24fea"},{"id":197668,"title":"Network Clustering - Lab","type":"Assignment","indent":1,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":null,"completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-network-clustering-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-network-clustering-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","exportId":"g23001e4161fe27e0d6bb3b07a3e574ab"},{"id":197673,"title":"Network Connectivity:  Community Detection -Lab","type":"Assignment","indent":1,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":null,"completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-network-community-detection-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-network-community-detection-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","exportId":"ga652e513c124ee3685343bdf8b88938e"},{"id":197680,"title":"Recommendation Systems","type":"Assignment","indent":0,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":null,"completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-network-recommendation-systems\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-network-recommendation-systems/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","exportId":"g2cc004165ab91dcd7c9452c6b2b0050c"},{"id":197686,"title":"Amazon Recommendation System - Lab","type":"Assignment","indent":1,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":null,"completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-network-recomendation-systems-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-network-recomendation-systems-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","exportId":"gca1eaecb686e0a3170787fc030994922"},{"id":197693,"title":"Graph Theory - Recap","type":"WikiPage","indent":0,"locked":false,"requirement":null,"completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-networks-recap\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-networks-recap/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eIn this section you explored a new data structure: networks! While network analysis is a deep topic with many additional topics to explore, you should have a good initial introduction and enough to conduct some preliminary analyses for social networks and building recommendation systems.\u003c/p\u003e\n\n\u003ch2\u003eNetworks\u003c/h2\u003e\n\n\u003cp\u003eYou've seen that networks can represent a range of different underlying data. From directions, social networks, and customer databases, networks are a wonderful way to represent the relationships between individuals. They also make for some snazzy visuals!\u003c/p\u003e\n\n\u003ch2\u003ePaths\u003c/h2\u003e\n\n\u003cp\u003eThe first stop along your journey was paths! Here, you investigated Dijkstra's algorithm to find the shortest path between nodes. This harked back to some of your experience scraping the web when you used recursive functions to perform breadth and depth based search techniques to transverse a json file. While you didn't directly explore this application, networks are also a natural representation for exploring internet traffic and web page structures.\u003c/p\u003e\n\n\u003ch2\u003eCentrality\u003c/h2\u003e\n\n\u003cp\u003eOnce you had a metric to calculate the distance between nodes, you then started to investigate other important concepts of networks such as which nodes were most influential or connected within a graph. You saw how alternative metrics can provide different insights on node structure. As a quick recap:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003e\n\u003cstrong\u003eDegree-centrality\u003c/strong\u003e: The number of edges attached to a node\u003c/li\u003e\n\u003cli\u003e\n\u003cstrong\u003eCloseness-centrality\u003c/strong\u003e: The reciprocal of the sum of the distances to all other nodes in the network \u003c/li\u003e\n\u003cli\u003e\n\u003cstrong\u003eBetweeness-centrality\u003c/strong\u003e: The number of shortest paths between all node pairs the node lies on divided by the maximum number of shortests-paths any one node in the network lies on \u003c/li\u003e\n\u003cli\u003e\n\u003cstrong\u003eEigenvalue-centrality\u003c/strong\u003e: An iterative algorithm which assigns relative influence to a node based on the number and importance of connected nodes. Can be very computationally expensive to compute for large networks. Google's PageRank algorithm is a variation of eigenvalue-centrality \u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch2\u003eClustering\u003c/h2\u003e\n\n\u003cp\u003eAfter discussing centrality, you then focused on larger structures within a network, breaking apart nodes into clusters to examine subgroups. While this is a common and useful application, it is an ill-defined problem mathematically, often making it difficult to definitively determine an optimal clustering schema. \u003c/p\u003e\n\n\u003ch2\u003eRecommendations\u003c/h2\u003e\n\n\u003cp\u003eFinally, you rounded out the section by investigating how networks can be used to provide recommendations to users. To do this, you investigated a preliminary approach known as collaborative filtering, specifically exploring user-based collaborative filtering in which similar users are identified and their preferences are used to generate recommendations to the user in question. There are many alternative approaches to recommendations systems such as using Singular Value Decomposition. \u003c/p\u003e\n\n\u003ch2\u003eSummary\u003c/h2\u003e\n\n\u003cp\u003eA lot was covered in this section! From this, you should have a solid introduction to networks, and some of their applications. Going forward, continue to explore ongoing developments in clustering social networks, and generating recommendations from these fascinating data structures.\u003c/p\u003e\n\u003cfooter class=\"fis-footer\" style=\"visibility: hidden;\"\u003e\u003cdiv class=\"fis-feedback\"\u003e\n\u003ch5\u003eHow do you feel about this lesson?\u003c/h5\u003e\n\u003cimg id=\"thumbs-up\" data-repository=\"dsc-networks-recap\" title=\"Thumbs up!\" alt=\"thumbs up\"\u003e\u003cimg id=\"thumbs-down\" data-repository=\"dsc-networks-recap\" title=\"Thumbs down!\" alt=\"thumbs down\"\u003e\n\u003c/div\u003e\n\u003ch5\u003eHave specific feedback? \u003ca href=\"https://github.com/learn-co-curriculum/dsc-networks-recap/issues/new\"\u003eTell us here!\u003c/a\u003e\n\u003c/h5\u003e\u003c/footer\u003e","exportId":"graph-theory-recap"}]},{"id":21140,"name":"APPENDIX: Transfer Learning","status":"completed","unlockDate":null,"prereqs":[],"requirement":null,"sequential":false,"exportId":"g43bb3a4afafe8c644e77311cf1616ede","items":[{"id":197751,"title":"Transfer Learning - Introduction","type":"WikiPage","indent":0,"locked":false,"requirement":null,"completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-transfer-learning-intro\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-transfer-learning-intro/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eIn this section you'll learn all about transfer learning and how it could be specifically applied to convolutional neural networks. There are also other applications of transfer learning like NLP.\u003c/p\u003e\n\n\u003ch2\u003eConvolutional Neural Networks (Continued)\u003c/h2\u003e\n\n\u003cp\u003eIn an earlier section, you learned about the fundamentals of convolutional neural networks and how to use them. In this section, you'll deepen your CNN knowledge and learn about concepts that will allow you to reuse pretrained models from other image recognition tasks. This will help you solve problems where only limited data is available.\u003c/p\u003e\n\n\u003ch3\u003eUsing Pretrained Networks\u003c/h3\u003e\n\n\u003cp\u003eYou will learn about the concept of \"convolutional bases\" and why they are useful. The use of a convolutional base, or a \"pretrained network\" has the advantage that hierarchical features that already have been \"pre-learned\" by this network can act as a generic model. Because of that reason, these networks can be used for a wide variety of computer vision tasks, even if your new problem involves completely different classes of images. You'll learn about the pretrained networks that are available in Keras, the use of pretrained networks through feature extraction (meaning that you run your new data through the pretrained network and training a new classifier on top of the pretrained network), and the use of pretrained networks through finetuning.\u003c/p\u003e\n\n\u003ch3\u003eImage Classification\u003c/h3\u003e\n\n\u003cp\u003eAt the end of this section, you'll work through a lab that combines the knowledge you gained in this section and the previous one. You'll work on a dog breed classification problem, a dataset used in a Kaggle competition, and build both a convolutional neural network from scratch, and a CNN using a pretrained network.\u003c/p\u003e\n\n\u003ch2\u003eSummary\u003c/h2\u003e\n\n\u003cp\u003eIn this section, you'll extend your deep learning knowledge by learning about transfer learning. \u003c/p\u003e\n\u003cfooter class=\"fis-footer\" style=\"visibility: hidden;\"\u003e\u003cdiv class=\"fis-feedback\"\u003e\n\u003ch5\u003eHow do you feel about this lesson?\u003c/h5\u003e\n\u003cimg id=\"thumbs-up\" data-repository=\"dsc-transfer-learning-intro\" title=\"Thumbs up!\" alt=\"thumbs up\"\u003e\u003cimg id=\"thumbs-down\" data-repository=\"dsc-transfer-learning-intro\" title=\"Thumbs down!\" alt=\"thumbs down\"\u003e\n\u003c/div\u003e\n\u003ch5\u003eHave specific feedback? \u003ca href=\"https://github.com/learn-co-curriculum/dsc-transfer-learning-intro/issues/new\"\u003eTell us here!\u003c/a\u003e\n\u003c/h5\u003e\u003c/footer\u003e","exportId":"transfer-learning-introduction"},{"id":197756,"title":"Using Pretrained Networks","type":"Assignment","indent":0,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":null,"completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-using-pretrained-networks\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-using-pretrained-networks/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","exportId":"gf6786f1c116d1462286609041ecf2943"},{"id":197763,"title":"Using Pretrained Networks - Codealong","type":"Assignment","indent":0,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":null,"completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-using-pretrained-networks-codealong\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-using-pretrained-networks-codealong/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","exportId":"g8dcb0ed9dda1678da4c9568f5a9024ae"},{"id":197767,"title":"Image Classification - Lab","type":"Assignment","indent":1,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":null,"completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-image-classification-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-image-classification-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","exportId":"g7f795263473e749df9e93f3c2adf4c5b"},{"id":197772,"title":"Transfer Learning - Recap","type":"WikiPage","indent":0,"locked":false,"requirement":null,"completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-transfer-learning-recap\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-transfer-learning-recap/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eIn this section, you learned how you can adapt pretrained models to improve the performance of neural networks when limited training data is available. While you specifically investigated CNNs and the VGG-19 model, these concepts are also applicable to other domains as well. For example, GloVe (Global Vectors for Word Representation) is a pretrained model that can be useful in a variety of natural language processing tasks.\u003c/p\u003e\n\n\u003cp\u003eRemember that the general process for transfer learning begins by taking a pretrained model like VGG-19 and freezing the weights so that they remain constant. From there, you can then append a standard densely connected classifier to perform the task at hand. In essence, the pretrained model acts as a form of feature engineering applied to the underlying dataset. \u003c/p\u003e\n\n\u003cp\u003eAfter the classifier is trained with the frozen pretrained model, a few of the top layers from the pretrained model can be unfrozen for fine tuning. Remember that you should only do this after training the classifier on top of the fully frozen model. Unfreezing parts of the pretrained model earlier is prone to overwriting any useful feature weights encoded in the pretrained model as there will be large gradients in forward and backward propagation passes until the densely connected layers converge to a stable solution. Also, remember that little is to be gained by unfreezing more than a few of the top layers from a pretrained model. Base layers of models such as VGG-19 will pick up very granular features such as colors or edges in image recognition. As such, these base layers are typically well formulated features across many domains. Unfreezing top layers has far more impact on tuning as these final layers often pick up domain specific features, so when adopting a model to a new problem domain such as predicting flower species instead of predicting animal kingdoms, these top layers can often be more impactful if retrained to the specific application.\u003c/p\u003e\n\n\u003ch2\u003eSummary\u003c/h2\u003e\n\n\u003cp\u003eIn this section, you got an overview of transfer learning and how to adapt pretrained models. From here, you'll continue to learn about other neural network architectures and build upon your growing deep learning knowledge.\u003c/p\u003e\n\u003cfooter class=\"fis-footer\" style=\"visibility: hidden;\"\u003e\u003cdiv class=\"fis-feedback\"\u003e\n\u003ch5\u003eHow do you feel about this lesson?\u003c/h5\u003e\n\u003cimg id=\"thumbs-up\" data-repository=\"dsc-transfer-learning-recap\" title=\"Thumbs up!\" alt=\"thumbs up\"\u003e\u003cimg id=\"thumbs-down\" data-repository=\"dsc-transfer-learning-recap\" title=\"Thumbs down!\" alt=\"thumbs down\"\u003e\n\u003c/div\u003e\n\u003ch5\u003eHave specific feedback? \u003ca href=\"https://github.com/learn-co-curriculum/dsc-transfer-learning-recap/issues/new\"\u003eTell us here!\u003c/a\u003e\n\u003c/h5\u003e\u003c/footer\u003e","exportId":"transfer-learning-recap"}]},{"id":21152,"name":" Campus Milestones Instructions","status":"completed","unlockDate":null,"prereqs":[],"requirement":null,"sequential":false,"exportId":"gcffff5f518709ac96557c858bedb2837","items":[{"id":197841,"title":"Blogging Overview","type":"WikiPage","indent":0,"locked":false,"requirement":null,"completed":false,"content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-blogging-overview\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-blogging-overview\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-blogging-overview/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e  \u003ch2\u003eIntroduction\u003c/h2\u003e  \u003cp\u003eIn this lesson, we discuss how to write good blog posts that meet Flatiron School's requirements.\u003c/p\u003e  \u003ch2\u003eObjectives\u003c/h2\u003e  \u003cp\u003eThis lesson covers...\u003c/p\u003e  \u003cul\u003e \u003cli\u003eWhy blogging is valuable\u003c/li\u003e \u003cli\u003eTopics to blog about\u003c/li\u003e \u003cli\u003eWhat makes for a good blog post\u003c/li\u003e \u003cli\u003eHow to start your blog\u003c/li\u003e \u003cli\u003eFlatiron School blog requirements \u003c/li\u003e \u003c/ul\u003e  \u003ch2\u003eWhy Should I Blog?\u003c/h2\u003e  \u003cp\u003eBlogging has many benefits:\u003c/p\u003e  \u003cul\u003e \u003cli\u003e\u003cp\u003e\u003cstrong\u003eDevelop your written communication skills.\u003c/strong\u003e Your writing ability will be critical to your success when completing job applications and presenting your work to colleagues. Blogging is great practice for identifying and clearly communicating the most important points of any subject.\u003c/p\u003e\u003c/li\u003e \u003cli\u003e\u003cp\u003e\u003cstrong\u003eDemonstrate your talent to employers.\u003c/strong\u003e Potential employers will review your blog to determine whether to offer you an interview or a job. Some students have even been invited to interview or exempted from technical interviews based on their blogs.\u003c/p\u003e\u003c/li\u003e \u003cli\u003e\u003cp\u003e\u003cstrong\u003eStrengthen your knowledge.\u003c/strong\u003e Blogging helps you explore new topics, deepen your understanding, and crystallize what you've learned.\u003c/p\u003e\u003c/li\u003e \u003cli\u003e\u003cp\u003e\u003cstrong\u003eHelp your peers and the broader community.\u003c/strong\u003e Have you ever Googled a question you had and found the answer on a blog? Writing blog posts helps others who are following in your footsteps!\u003c/p\u003e\u003c/li\u003e \u003c/ul\u003e  \u003ch2\u003eWhat Should I Blog About?\u003c/h2\u003e  \u003cp\u003eHere are some blog topic ideas:\u003c/p\u003e  \u003cul\u003e \u003cli\u003e\u003cp\u003eWhy did you decide to learn data science?\u003c/p\u003e\u003c/li\u003e \u003cli\u003e\u003cp\u003eDescribe how a DS technique works, when you might use it, and its strengths/weaknesses.\u003c/p\u003e\u003c/li\u003e \u003cli\u003e\u003cp\u003eSummarize an End of Phase Project by explaining your problem, the dataset, your methodology, and your results.\u003c/p\u003e\u003c/li\u003e \u003cli\u003e\u003cp\u003eDive into something that you want to learn more about, maybe because you find it challenging or it wasn't covered in the course.\u003c/p\u003e\u003c/li\u003e \u003cli\u003e\u003cp\u003eWrite a tutorial to help aspiring data scientists to implement a tool or method.\u003c/p\u003e\u003c/li\u003e \u003cli\u003e\u003cp\u003eFind an interesting data science paper and summarize why it is important. This can be a new paper from the past few months, or you can refer to \u003ca href=\"https://docs.google.com/spreadsheets/d/1UYmAT13AAknrOatzLeeAsN4tS7ENjn2fpJNGzOZ67rQ/edit?usp=sharing\"\u003ethis spreadsheet\u003c/a\u003e.\u003c/p\u003e\u003c/li\u003e \u003c/ul\u003e  \u003ch2\u003eWhat Does A Good Blog Post Look Like?\u003c/h2\u003e  \u003cp\u003eWe recommend you take a look at our \u003ca href=\"https://drive.google.com/drive/folders/1UBiRCRLzVP5CHU3PJNwoMZAe3ajUBm2a?usp=sharing\"\u003eblog templates\u003c/a\u003e and \u003ca href=\"https://docs.google.com/document/d/1eqL8Dsj7dH7s_MRnf_4-3kCiSz72POHTfb-sBRN5Zhs/edit?usp=sharing\"\u003eexamples\u003c/a\u003e to get an idea for what makes a blog post good.\u003c/p\u003e  \u003cul\u003e \u003cli\u003e\u003cp\u003eStrike a balance between providing a meaningful investigation of your topic and being concise. Constrain the scope so it will be interesting and digestible in about 1000-3000 words (this is not a firm limit).\u003c/p\u003e\u003c/li\u003e \u003cli\u003e\n\u003cp\u003eUse clear and consistent formatting to make your content accessible and professional-looking.\u003c/p\u003e  \u003cul\u003e \u003cli\u003eWhen presenting code, use code snippets instead of screenshots.\u003c/li\u003e \u003cli\u003eMake URLs into hyperlinks that are easy for readers to click into.\u003c/li\u003e \u003cli\u003eUse headings to provide structure and flow to your post.\u003c/li\u003e \u003c/ul\u003e\n\u003c/li\u003e \u003cli\u003e\u003cp\u003eCite and link to resources you used to write your post.\u003c/p\u003e\u003c/li\u003e \u003c/ul\u003e  \u003ch2\u003eHow Do I Start My Blog?\u003c/h2\u003e  \u003cp\u003eIf you already have a professional blog that you'd like to use for your data science content, you can add your posts to that. Otherwise, you will need to start a new blog. If you have a personal blog, you should avoid using it for this purpose so that you can continue using it for personal content without worrying about how it might be perceived by potential employers.\u003c/p\u003e  \u003cp\u003eThere are multiple blogging platforms to choose from that make it easy to start a blog, here are some of our favorites:\u003c/p\u003e  \u003cul\u003e \u003cli\u003e\u003ca href=\"https://www.blogger.com/\"\u003eBlogger\u003c/a\u003e\u003c/li\u003e \u003cli\u003e\u003ca href=\"https://dev.to/\"\u003edev.to\u003c/a\u003e\u003c/li\u003e \u003cli\u003e\u003ca href=\"https://pages.github.com/\"\u003eGitHub Pages\u003c/a\u003e\u003c/li\u003e \u003cli\u003e\u003ca href=\"https://medium.com/\"\u003eMedium\u003c/a\u003e\u003c/li\u003e \u003cli\u003e\u003ca href=\"https://wordpress.com/\"\u003eWordpress\u003c/a\u003e\u003c/li\u003e \u003c/ul\u003e  \u003cp\u003eDifferent platforms have different pros and cons, so do a little research to decide what is best for you.\u003c/p\u003e  \u003ch2\u003eBlog Requirements\u003c/h2\u003e  \u003cp\u003eTo succeed in your career transition and graduate from Flatiron School, you must complete the following activities. These requirements are designed to give you the best opportunity to deepen your knowledge, practice communication skills, and showcase yourself to potential employers.\u003c/p\u003e  \u003cul\u003e \u003cli\u003e\u003cp\u003eSet up a publicly accessible blog \u003c/p\u003e\u003c/li\u003e \u003cli\u003e\u003cp\u003ePublish at least four blog posts on it, including \u003cstrong\u003eone per Phase for Phases 1-4\u003c/strong\u003e\u003c/p\u003e\u003c/li\u003e \u003cli\u003e\n\u003cp\u003eSubmit URLs to your posts \u003cstrong\u003eby the end of each Phase\u003c/strong\u003e in the Blog Post assignments\u003c/p\u003e  \u003cul\u003e \u003cli\u003eThese assignments are located in the Milestones topics of the Phase 1-4 Canvas courses\u003c/li\u003e \u003c/ul\u003e\n\u003c/li\u003e \u003cli\u003e\n\u003cp\u003eWrite blog posts that...\u003c/p\u003e  \u003cul\u003e \u003cli\u003eDiscuss data science topics\u003c/li\u003e \u003cli\u003eAre composed primarily of original material you wrote\u003c/li\u003e \u003cli\u003eInclude proper attribution\u003c/li\u003e \u003cli\u003eHave high-quality content and formatting\u003c/li\u003e \u003cli\u003eAre something you would proudly show to a potential employer\u003c/li\u003e \u003c/ul\u003e\n\u003c/li\u003e \u003c/ul\u003e  \u003cp\u003eAfter you submit your blog posts, your teacher will grade them as Complete or Incomplete. Your blogs must all be submitted on time and receive Complete grades in order to continue through your program.\u003c/p\u003e  \u003cp\u003eHave fun and happy blogging!\u003c/p\u003e","exportId":"blogging-overview"},{"id":197848,"title":"Project Submission \u0026 Review (Campus)","type":"WikiPage","indent":0,"locked":false,"requirement":null,"completed":false,"content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-project-submissions-campus\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-project-submissions-campus\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-project-submissions-campus/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e  \u003ch2\u003eIntroduction\u003c/h2\u003e  \u003cp\u003eIn this lesson, we review the requirements, submission, and review process for the Phase Projects.\u003c/p\u003e  \u003ch2\u003eObjectives\u003c/h2\u003e  \u003cp\u003eYou will be able to:\u003c/p\u003e  \u003cul\u003e \u003cli\u003eCreate project deliverables that meet Flatiron School requirements\u003c/li\u003e \u003cli\u003eSubmit your project deliverables in Canvas\u003c/li\u003e \u003cli\u003ePrepare for your project review\u003c/li\u003e \u003c/ul\u003e  \u003ch2\u003eCreate Your Project Deliverables\u003c/h2\u003e  \u003cp\u003eComplete the deliverables for your project, guided by the rubric at the bottom of the main project assignment. Keep in mind that the audience for these deliverables is not only your teacher, but also potential employers. Employers will look at your project deliverables to evaluate multiple skills, including coding, modeling, communication, and domain knowledge. You will want to polish these as much as you can, both during the course and afterwards.\u003c/p\u003e  \u003ch3\u003eGitHub Repository\u003c/h3\u003e  \u003cp\u003eYour GitHub repository is the public-facing version of your project that your instructors and potential employers will see - make it as accessible as you can. At a minimum, it should contain all your project files and a README.md file that summarizes your project and helps visitors navigate the repository.\u003c/p\u003e  \u003ch3\u003eJupyter Notebook\u003c/h3\u003e  \u003cp\u003eYour Jupyter Notebook is the primary source of information about your analysis. At a minimum, it should contain or import all of the code used in your project and walk the reader through your project from start to finish. You may choose to use multiple Jupyter Notebooks in your project, but you should have one that provides a full project overview as a point of entry for visitors.\u003c/p\u003e  \u003ch3\u003eNon-Technical Presentation\u003c/h3\u003e  \u003cp\u003eYour non-technical presentation is your opportunity to communicate clearly and concisely about your project and it's real-world relevance. The target audience should be people with limited technical knowledge who may be interested in leveraging your project. We recommend using Google Slides, PowerPoint or Keynote to create your presentation slides. You will then present your project to your cohort.\u003c/p\u003e  \u003ch2\u003eSubmit Your Project\u003c/h2\u003e  \u003cp\u003eTo submit your project in Canvas, you will create and upload PDF versions of three project deliverables. You will also submit the URL to your GitHub repository in a separate assignment.\u003c/p\u003e  \u003ch3\u003ePresentation Slides PDF Creation\u003c/h3\u003e  \u003col\u003e \u003cli\u003eExport your presentation as a PDF from the program in which you created it.\u003c/li\u003e \u003cli\u003eGive it a short descriptive file name (e.g. \u003ccode\u003epresentation.pdf\u003c/code\u003e).\u003c/li\u003e \u003cli\u003ePlace a copy of the PDF in your GitHub repository.\u003c/li\u003e \u003c/ol\u003e  \u003ch3\u003eGitHub Repository PDF Creation\u003c/h3\u003e  \u003col\u003e \u003cli\u003eNavigate to the root directory of your project repository on GitHub, using your browser (we recommend Google Chrome).\u003c/li\u003e \u003cli\u003eSave the webpage as a PDF using the browser's Print functionality (\u003ca href=\"https://www.wikihow.com/Save-a-Web-Page-as-a-PDF-in-Google-Chrome\"\u003eGoogle Chrome Save to PDF instructions\u003c/a\u003e)\u003c/li\u003e \u003cli\u003eGive it a short descriptive file name (e.g. \u003ccode\u003egithub.pdf\u003c/code\u003e).\u003c/li\u003e \u003c/ol\u003e  \u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-project-submissions-campus/master/repo_pdf.gif\" alt=\"Repository PDF Creation\"\u003e\u003c/p\u003e  \u003ch3\u003eJupyter Notebook PDF Creation\u003c/h3\u003e  \u003col\u003e \u003cli\u003eOpen your Notebook in your browser (we recommend Google Chrome).\u003c/li\u003e \u003cli\u003e\n\u003cstrong\u003eRun the Notebook from start to finish\u003c/strong\u003e so that your output is visible.\u003c/li\u003e \u003cli\u003eSave the page as a PDF using the browser's Print functionality (\u003ca href=\"https://www.wikihow.com/Save-a-Web-Page-as-a-PDF-in-Google-Chrome\"\u003eGoogle Chrome Save to PDF instructions\u003c/a\u003e)\u003c/li\u003e \u003cli\u003eGive it a short descriptive file name (e.g. \u003ccode\u003enotebook.pdf\u003c/code\u003e).\u003c/li\u003e \u003c/ol\u003e  \u003cp\u003eIf you have difficulty creating a PDF version of your notebook, you can use \u003ca href=\"https://htmtopdf.herokuapp.com/ipynbviewer/\"\u003ethis tool\u003c/a\u003e instead. Set the Results Format to HTML + PDF. Then click View and Convert. Once its done, you should see links to .html and .pdf versions above the View and Convert button.\u003c/p\u003e  \u003ch3\u003ePDF Submission in Canvas\u003c/h3\u003e  \u003cp\u003eYou will need to submit all three PDF files as a single submission:\u003c/p\u003e  \u003col\u003e \u003cli\u003eClick \"Submit Assignment\" at the top of the \"Phase X Project\" assignment in the \"Milestones\" topic.\u003c/li\u003e \u003cli\u003eIn the \"File Upload\" box, click \"Choose File\" button to upload a single file.\u003c/li\u003e \u003cli\u003eClick the \"Add Another File\" link to upload an additional file.\u003c/li\u003e \u003cli\u003eRepeat Step 3 to upload one more file. After this is done, all three files should be uploaded.\u003c/li\u003e \u003cli\u003eHit the blue \"Submit Assignment\" button.\u003c/li\u003e \u003c/ol\u003e  \u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-project-submissions-campus/master/project_3pdf_submission.gif\" alt=\"Project PDF Submission\"\u003e\u003c/p\u003e  \u003ch3\u003eURL Submission in Canvas\u003c/h3\u003e  \u003cp\u003eThere is an additional Canvas assignment where you will just enter the URL for your project's GitHub repository. This is located in the \"Milestones\" topic in this course as \"Phase X Project - GitHub Repository URL.\"\u003c/p\u003e  \u003ch2\u003eProject Review\u003c/h2\u003e  \u003cp\u003eYou have until the end of the week to present your project and submit it for review. You will receive a grade of P (Pass) or NP (No Pass) - you must pass in order to move to the next phase with your cohort. Your teacher will grade your submission and give you feedback, typically early in the week after you submit.\u003c/p\u003e  \u003cp\u003eWhich review process you receive will depend on whether you passed the code challenge, as described below.\u003c/p\u003e  \u003ch3\u003ePassed Code Challenge: Check for Completion\u003c/h3\u003e  \u003cp\u003eIf you passed the code challenge, your teacher will review your project to check that it is complete.\u0026nbsp;Your project will pass if you have completed and submitted all project deliverables. Your project will not be graded using the project rubric at the bottom of the project assignment, although you may find it helpful for guiding your project.\u003c/p\u003e  \u003ch3\u003eDid Not Pass Code Challenge: Rubric Grading\u003c/h3\u003e  \u003cp\u003eIf you did not pass the code challenge, your teacher will use the rubric at the bottom of the project assignment to grade your submission. You can earn 0, 1, or 2 points on each rubric element. Your project will pass if it earns at least 50% of the points available.\u003c/p\u003e  \u003ch2\u003eConclusion\u003c/h2\u003e  \u003cp\u003eThank you for your hard work on this project - you're going to do great! Remember that future employers will also look at your projects when deciding whether to hire you, so having complete, polished projects will help you tremendously not only to pass this assignment, but also to get the job you want after you graduate.\u003c/p\u003e  \u003cp\u003eIf you have any questions about the project submission or review process, don't hesitate to ask your teacher.\u003c/p\u003e","exportId":"project-submission-and-review-campus"},{"id":197854,"title":"Phase 4 Project Checklist and Guidance","type":"ExternalUrl","indent":0,"locked":false,"requirement":null,"completed":false,"content":"https://docs.google.com/document/d/1VxrxqqIABf5_-LvZWYRZtSRzl0SWY5oVDqockeWJHIQ/edit?usp=sharing"}]},{"id":21153,"name":" Online Milestones Instructions","status":"completed","unlockDate":null,"prereqs":[],"requirement":null,"sequential":false,"exportId":"g510265fa7e63337d69c30fcd365c4ac1","items":[{"id":197864,"title":"Blogging Overview","type":"WikiPage","indent":0,"locked":false,"requirement":null,"completed":false,"content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-blogging-overview\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-blogging-overview\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-blogging-overview/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e  \u003ch2\u003eIntroduction\u003c/h2\u003e  \u003cp\u003eIn this lesson, we discuss how to write good blog posts that meet Flatiron School's requirements.\u003c/p\u003e  \u003ch2\u003eObjectives\u003c/h2\u003e  \u003cp\u003eThis lesson covers...\u003c/p\u003e  \u003cul\u003e \u003cli\u003eWhy blogging is valuable\u003c/li\u003e \u003cli\u003eTopics to blog about\u003c/li\u003e \u003cli\u003eWhat makes for a good blog post\u003c/li\u003e \u003cli\u003eHow to start your blog\u003c/li\u003e \u003cli\u003eFlatiron School blog requirements \u003c/li\u003e \u003c/ul\u003e  \u003ch2\u003eWhy Should I Blog?\u003c/h2\u003e  \u003cp\u003eBlogging has many benefits:\u003c/p\u003e  \u003cul\u003e \u003cli\u003e\u003cp\u003e\u003cstrong\u003eDevelop your written communication skills.\u003c/strong\u003e Your writing ability will be critical to your success when completing job applications and presenting your work to colleagues. Blogging is great practice for identifying and clearly communicating the most important points of any subject.\u003c/p\u003e\u003c/li\u003e \u003cli\u003e\u003cp\u003e\u003cstrong\u003eDemonstrate your talent to employers.\u003c/strong\u003e Potential employers will review your blog to determine whether to offer you an interview or a job. Some students have even been invited to interview or exempted from technical interviews based on their blogs.\u003c/p\u003e\u003c/li\u003e \u003cli\u003e\u003cp\u003e\u003cstrong\u003eStrengthen your knowledge.\u003c/strong\u003e Blogging helps you explore new topics, deepen your understanding, and crystallize what you've learned.\u003c/p\u003e\u003c/li\u003e \u003cli\u003e\u003cp\u003e\u003cstrong\u003eHelp your peers and the broader community.\u003c/strong\u003e Have you ever Googled a question you had and found the answer on a blog? Writing blog posts helps others who are following in your footsteps!\u003c/p\u003e\u003c/li\u003e \u003c/ul\u003e  \u003ch2\u003eWhat Should I Blog About?\u003c/h2\u003e  \u003cp\u003eHere are some blog topic ideas:\u003c/p\u003e  \u003cul\u003e \u003cli\u003e\u003cp\u003eWhy did you decide to learn data science?\u003c/p\u003e\u003c/li\u003e \u003cli\u003e\u003cp\u003eDescribe how a DS technique works, when you might use it, and its strengths/weaknesses.\u003c/p\u003e\u003c/li\u003e \u003cli\u003e\u003cp\u003eSummarize an End of Phase Project by explaining your problem, the dataset, your methodology, and your results.\u003c/p\u003e\u003c/li\u003e \u003cli\u003e\u003cp\u003eDive into something that you want to learn more about, maybe because you find it challenging or it wasn't covered in the course.\u003c/p\u003e\u003c/li\u003e \u003cli\u003e\u003cp\u003eWrite a tutorial to help aspiring data scientists to implement a tool or method.\u003c/p\u003e\u003c/li\u003e \u003cli\u003e\u003cp\u003eFind an interesting data science paper and summarize why it is important. This can be a new paper from the past few months, or you can refer to \u003ca href=\"https://docs.google.com/spreadsheets/d/1UYmAT13AAknrOatzLeeAsN4tS7ENjn2fpJNGzOZ67rQ/edit?usp=sharing\"\u003ethis spreadsheet\u003c/a\u003e.\u003c/p\u003e\u003c/li\u003e \u003c/ul\u003e  \u003ch2\u003eWhat Does A Good Blog Post Look Like?\u003c/h2\u003e  \u003cp\u003eWe recommend you take a look at our \u003ca href=\"https://drive.google.com/drive/folders/1UBiRCRLzVP5CHU3PJNwoMZAe3ajUBm2a?usp=sharing\"\u003eblog templates\u003c/a\u003e and \u003ca href=\"https://docs.google.com/document/d/1eqL8Dsj7dH7s_MRnf_4-3kCiSz72POHTfb-sBRN5Zhs/edit?usp=sharing\"\u003eexamples\u003c/a\u003e to get an idea for what makes a blog post good.\u003c/p\u003e  \u003cul\u003e \u003cli\u003e\u003cp\u003eStrike a balance between providing a meaningful investigation of your topic and being concise. Constrain the scope so it will be interesting and digestible in about 1000-3000 words (this is not a firm limit).\u003c/p\u003e\u003c/li\u003e \u003cli\u003e\n\u003cp\u003eUse clear and consistent formatting to make your content accessible and professional-looking.\u003c/p\u003e  \u003cul\u003e \u003cli\u003eWhen presenting code, use code snippets instead of screenshots.\u003c/li\u003e \u003cli\u003eMake URLs into hyperlinks that are easy for readers to click into.\u003c/li\u003e \u003cli\u003eUse headings to provide structure and flow to your post.\u003c/li\u003e \u003c/ul\u003e\n\u003c/li\u003e \u003cli\u003e\u003cp\u003eCite and link to resources you used to write your post.\u003c/p\u003e\u003c/li\u003e \u003c/ul\u003e  \u003ch2\u003eHow Do I Start My Blog?\u003c/h2\u003e  \u003cp\u003eIf you already have a professional blog that you'd like to use for your data science content, you can add your posts to that. Otherwise, you will need to start a new blog. If you have a personal blog, you should avoid using it for this purpose so that you can continue using it for personal content without worrying about how it might be perceived by potential employers.\u003c/p\u003e  \u003cp\u003eThere are multiple blogging platforms to choose from that make it easy to start a blog, here are some of our favorites:\u003c/p\u003e  \u003cul\u003e \u003cli\u003e\u003ca href=\"https://www.blogger.com/\"\u003eBlogger\u003c/a\u003e\u003c/li\u003e \u003cli\u003e\u003ca href=\"https://dev.to/\"\u003edev.to\u003c/a\u003e\u003c/li\u003e \u003cli\u003e\u003ca href=\"https://pages.github.com/\"\u003eGitHub Pages\u003c/a\u003e\u003c/li\u003e \u003cli\u003e\u003ca href=\"https://medium.com/\"\u003eMedium\u003c/a\u003e\u003c/li\u003e \u003cli\u003e\u003ca href=\"https://wordpress.com/\"\u003eWordpress\u003c/a\u003e\u003c/li\u003e \u003c/ul\u003e  \u003cp\u003eDifferent platforms have different pros and cons, so do a little research to decide what is best for you.\u003c/p\u003e  \u003ch2\u003eBlog Requirements\u003c/h2\u003e  \u003cp\u003eTo succeed in your career transition and graduate from Flatiron School, you must complete the following activities. These requirements are designed to give you the best opportunity to deepen your knowledge, practice communication skills, and showcase yourself to potential employers.\u003c/p\u003e  \u003cul\u003e \u003cli\u003e\u003cp\u003eSet up a publicly accessible blog \u003c/p\u003e\u003c/li\u003e \u003cli\u003e\u003cp\u003ePublish at least four blog posts on it, including \u003cstrong\u003eone per Phase for Phases 1-4\u003c/strong\u003e\u003c/p\u003e\u003c/li\u003e \u003cli\u003e\n\u003cp\u003eSubmit URLs to your posts \u003cstrong\u003eby the end of each Phase\u003c/strong\u003e in the Blog Post assignments\u003c/p\u003e  \u003cul\u003e \u003cli\u003eThese assignments are located in the Milestones topics of the Phase 1-4 Canvas courses\u003c/li\u003e \u003c/ul\u003e\n\u003c/li\u003e \u003cli\u003e\n\u003cp\u003eWrite blog posts that...\u003c/p\u003e  \u003cul\u003e \u003cli\u003eDiscuss data science topics\u003c/li\u003e \u003cli\u003eAre composed primarily of original material you wrote\u003c/li\u003e \u003cli\u003eInclude proper attribution\u003c/li\u003e \u003cli\u003eHave high-quality content and formatting\u003c/li\u003e \u003cli\u003eAre something you would proudly show to a potential employer\u003c/li\u003e \u003c/ul\u003e\n\u003c/li\u003e \u003c/ul\u003e  \u003cp\u003eAfter you submit your blog posts, your teacher will grade them as Complete or Incomplete. Your blogs must all be submitted on time and receive Complete grades in order to continue through your program.\u003c/p\u003e  \u003cp\u003eHave fun and happy blogging!\u003c/p\u003e","exportId":"blogging-overview"},{"id":197870,"title":"Project Submission \u0026 Review (Online)","type":"WikiPage","indent":0,"locked":false,"requirement":null,"completed":false,"content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-project-submissions-online\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-project-submissions-online\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-project-submissions-online/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e  \u003ch2\u003eIntroduction\u003c/h2\u003e  \u003cp\u003eIn this lesson, we review the requirements, submission, and review process for the Phase Projects.\u003c/p\u003e  \u003ch2\u003eObjectives\u003c/h2\u003e  \u003cp\u003eYou will be able to:\u003c/p\u003e  \u003cul\u003e \u003cli\u003eCreate project deliverables that meet Flatiron School requirements\u003c/li\u003e \u003cli\u003eSubmit your project deliverables in Canvas\u003c/li\u003e \u003cli\u003ePrepare for your project review\u003c/li\u003e \u003c/ul\u003e  \u003ch2\u003eSchedule Your Review ASAP\u003c/h2\u003e  \u003cp\u003e\u003cstrong\u003eReach out to an instructor immediately via Slack to let them know you've started your project and schedule your project review.\u003c/strong\u003e If you're not sure who to schedule with, please ask in your cohort channel in Slack.\u003c/p\u003e  \u003ch2\u003eCreate Your Project Deliverables\u003c/h2\u003e  \u003cp\u003eComplete the deliverables for your project, guided by the rubric at the bottom of the main project assignment. Keep in mind that the audience for these deliverables is not only your teacher, but also potential employers. Employers will look at your project deliverables to evaluate multiple skills, including coding, modeling, communication, and domain knowledge. You will want to polish these as much as you can, both during the course and afterwards.\u003c/p\u003e  \u003ch3\u003eGitHub Repository\u003c/h3\u003e  \u003cp\u003eYour GitHub repository is the public-facing version of your project that your instructors and potential employers will see - make it as accessible as you can. At a minimum, it should contain all your project files and a README.md file that summarizes your project and helps visitors navigate the repository.\u003c/p\u003e  \u003ch3\u003eJupyter Notebook\u003c/h3\u003e  \u003cp\u003eYour Jupyter Notebook is the primary source of information about your analysis. At a minimum, it should contain or import all of the code used in your project and walk the reader through your project from start to finish. You may choose to use multiple Jupyter Notebooks in your project, but you should have one that provides a full project overview as a point of entry for visitors.\u003c/p\u003e  \u003ch3\u003eNon-Technical Presentation\u003c/h3\u003e  \u003cp\u003eYour non-technical presentation is your opportunity to communicate clearly and concisely about your project and it's real-world relevance. The target audience should be people with limited technical knowledge who may be interested in leveraging your project. We recommend using Google Slides, PowerPoint or Keynote to create your presentation slides. You will then record yourself delivering the presentation.\u003c/p\u003e  \u003ch2\u003eSubmit Your Project\u003c/h2\u003e  \u003cp\u003eTo submit your project in Canvas, you will create and upload PDF versions of three project deliverables, then upload a recording of your video presentation. You will also submit the URL to your GitHub repository in a separate assignment.\u003c/p\u003e  \u003ch3\u003ePresentation Slides PDF Creation\u003c/h3\u003e  \u003col\u003e \u003cli\u003eExport your presentation as a PDF from the program in which you created it.\u003c/li\u003e \u003cli\u003eGive it a short descriptive file name (e.g. \u003ccode\u003epresentation.pdf\u003c/code\u003e).\u003c/li\u003e \u003cli\u003ePlace a copy of the PDF in your GitHub repository.\u003c/li\u003e \u003c/ol\u003e  \u003ch3\u003eGitHub Repository PDF Creation\u003c/h3\u003e  \u003col\u003e \u003cli\u003eNavigate to the root directory of your project repository on GitHub, using your browser (we recommend Google Chrome).\u003c/li\u003e \u003cli\u003eSave the webpage as a PDF using the browser's Print functionality (\u003ca href=\"https://www.wikihow.com/Save-a-Web-Page-as-a-PDF-in-Google-Chrome\"\u003eGoogle Chrome Save to PDF instructions\u003c/a\u003e)\u003c/li\u003e \u003cli\u003eGive it a short descriptive file name (e.g. \u003ccode\u003egithub.pdf\u003c/code\u003e).\u003c/li\u003e \u003c/ol\u003e  \u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-project-submissions-online/master/repo_pdf.gif\" alt=\"Repository PDF Creation\"\u003e\u003c/p\u003e  \u003ch3\u003eJupyter Notebook PDF Creation\u003c/h3\u003e  \u003col\u003e \u003cli\u003eOpen your Notebook in your browser (we recommend Google Chrome).\u003c/li\u003e \u003cli\u003e\n\u003cstrong\u003eRun the Notebook from start to finish\u003c/strong\u003e so that your output is visible.\u003c/li\u003e \u003cli\u003eSave the page as a PDF using the browser's Print functionality (\u003ca href=\"https://www.wikihow.com/Save-a-Web-Page-as-a-PDF-in-Google-Chrome\"\u003eGoogle Chrome Save to PDF instructions\u003c/a\u003e)\u003c/li\u003e \u003cli\u003eGive it a short descriptive file name (e.g. \u003ccode\u003enotebook.pdf\u003c/code\u003e).\u003c/li\u003e \u003c/ol\u003e  \u003cp\u003eIf you have difficulty creating a PDF version of your notebook, you can use \u003ca href=\"https://htmtopdf.herokuapp.com/ipynbviewer/\"\u003ethis tool\u003c/a\u003e instead. Set the Results Format to HTML + PDF. Then click View and Convert. Once its done, you should see links to .html and .pdf versions above the View and Convert button.\u003c/p\u003e  \u003ch3\u003ePDF Submission in Canvas\u003c/h3\u003e  \u003cp\u003eYou will need to submit all three PDF files as a single submission:\u003c/p\u003e  \u003col\u003e \u003cli\u003eClick \"Submit Assignment\" at the top of the \"Phase X Project\" assignment in the \"Milestones\" topic.\u003c/li\u003e \u003cli\u003eIn the \"File Upload\" box, click \"Choose File\" button to upload a single file.\u003c/li\u003e \u003cli\u003eClick the \"Add Another File\" link to upload an additional file.\u003c/li\u003e \u003cli\u003eRepeat Step 3 to upload one more file. After this is done, all three files should be uploaded.\u003c/li\u003e \u003cli\u003eHit the blue \"Submit Assignment\" button.\u003c/li\u003e \u003c/ol\u003e  \u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-project-submissions-online/master/project_3pdf_submission.gif\" alt=\"Project PDF Submission\"\u003e\u003c/p\u003e  \u003ch3\u003ePresentation Recording and Submission\u003c/h3\u003e  \u003cp\u003eAfter you've submitted the PDF files for the project assignment, you will upload a recording of your presentation as a media comment on your submission:\u003c/p\u003e  \u003col\u003e \u003cli\u003eRecord your live presentation to a video file on your computer. We recommend using Zoom to record your live presentation to a local video file (\u003ca href=\"https://support.zoom.us/hc/en-us/articles/201362473-Local-recording\"\u003einstructions here\u003c/a\u003e). Video files must be under 500 MB and formatted as 3GP, ASF, AVI, FLV, M4V, MOV, MP4, MPEG, QT, or WMV.\u003c/li\u003e \u003cli\u003eClick \"Submission Details\" on the top right of the \"Phase X Project\" assignment in the \"Milestones\" topic.\u003c/li\u003e \u003cli\u003eClick \"Media Comment\" beneath the \"Add a Comment\" box on the right of the page.\u003c/li\u003e \u003cli\u003eClick \"Upload Media\" and \"Select Video File\" to upload your file.\u003c/li\u003e \u003cli\u003eThe thumbnail for your video will appear as a blue rectangle while Zoom processes your file - return to this page later to confirm that your recording uploaded successfully.\u003c/li\u003e \u003c/ol\u003e  \u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-project-submissions-online/master/video_recording_submission.gif\" alt=\"Video Recording Submission\"\u003e\u003c/p\u003e  \u003ch3\u003eURL Submission in Canvas\u003c/h3\u003e  \u003cp\u003eThere is an additional Canvas assignment where you will just enter the URL for your project's GitHub repository. This is located in the \"Milestones\" topic in this course as \"Phase X Project - GitHub Repository URL.\"\u003c/p\u003e  \u003ch2\u003ePrepare For Project Review\u003c/h2\u003e  \u003cp\u003eProject reviews are focused on preparing you for technical interviews. Treat project reviews as if they were technical interviews, in both attitude and technical presentation \u003cem\u003e(sometimes technical interviews will feel arbitrary or unfair - if you want to get the job, commenting on that is seldom a good choice)\u003c/em\u003e.\u003c/p\u003e  \u003cp\u003eThe project review is comprised of a 45 minute 1:1 session with one of the instructors. During your project review, be prepared to:\u003c/p\u003e  \u003ch3\u003e1. Deliver your PDF presentation to a non-technical stakeholder.\u003c/h3\u003e  \u003cp\u003eIn this phase of the review (~10 mins) your instructor will play the part of a non-technical stakeholder that you are presenting your findings to. The presentation  should not exceed 5 minutes, giving the \"stakeholder\" 5 minutes to ask questions.\u003c/p\u003e  \u003cp\u003eIn the first half of the presentation (2-3 mins), you should summarize your methodology in a way that will be comprehensible to someone with no background in data science and that will increase their confidence in you and your findings. In the second half (the remaining 2-3 mins) you should summarize your findings and be ready to answer a couple of non-technical questions from the audience. The questions might relate to technical topics (sampling bias, confidence, etc) but will be asked in a non-technical way and need to be answered in a way that does not assume a background in statistics or machine learning. You can assume a smart, business stakeholder, with a non-quantitative college degree.\u003c/p\u003e  \u003ch3\u003e2. Go through the Jupyter Notebook, answering questions about how you made certain decisions. Be ready to explain things like:\u003c/h3\u003e \u003cpre\u003e\u003ccode\u003e* \"How did you pick the question(s) that you did?\"\u003cbr\u003e* \"Why are these questions important from a business perspective?\"\u003cbr\u003e* \"How did you decide on the data cleaning options you performed?\"\u003cbr\u003e* \"Why did you choose a given method or library?\"\u003cbr\u003e* \"Why did you select those visualizations and what did you learn from each of them?\"\u003cbr\u003e* \"Why did you pick those features as predictors?\"\u003cbr\u003e* \"How would you interpret the results?\"\u003cbr\u003e* \"How confident are you in the predictive quality of the results?\"\u003cbr\u003e* \"What are some of the things that could cause the results to be wrong?\" \u003c/code\u003e\u003c/pre\u003e \u003cp\u003eThink of the first phase of the review (~30 mins) as a technical boss reviewing your work and asking questions about it before green-lighting you to present to the business team. You should practice using the appropriate technical vocabulary to explain yourself. Don't be surprised if the instructor jumps around or sometimes cuts you off - there is a lot of ground to cover, so that may happen.\u003c/p\u003e  \u003cp\u003eIf any requirements are missing or if significant gaps in understanding are uncovered, be prepared to do one or all of the following: * Perform additional data cleanup, visualization, feature selection, modeling and/or model validation * Submit an improved version * Meet again for another Project Review\u003c/p\u003e  \u003cp\u003eWhat won't happen: * You won't be yelled at, belittled, or scolded * You won't be put on the spot without support * There's nothing you can do to instantly fail or blow it\u003c/p\u003e  \u003ch2\u003eGrading\u003c/h2\u003e  \u003cp\u003eYour teacher will use the rubric at the bottom of the main project assignment to grade your project. In order to pass, you must properly submit your project and score \"Accomplished\" or \"Exemplary\" on nearly all rubric elements. You will receive a score of P (Pass) or NP (No Pass) - you must pass in order to move to the next phase with your cohort. Your teacher will grade your submission sometime after your review.\u003c/p\u003e  \u003ch2\u003eConclusion\u003c/h2\u003e  \u003cp\u003eThank you for your hard work on this project - you're going to do great! Remember that future employers will also look at your projects when deciding whether to hire you, so having complete, polished projects will help you tremendously not only to pass this assignment, but also to get the job you want after you graduate.\u003c/p\u003e  \u003cp\u003eIf you have any questions about the project submission or review process, don't hesitate to ask your teacher.\u003c/p\u003e","exportId":"project-submission-and-review-online"}]},{"id":21157,"name":" Milestones","status":"completed","unlockDate":null,"prereqs":[],"requirement":null,"sequential":false,"exportId":"ge400cfd64b4a09a65ba6eb7b72e4325c","items":[{"id":197888,"title":"Phase 4 Project","type":"Assignment","indent":0,"locked":false,"submissionTypes":"a file upload","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":null,"completed":false,"content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-phase-4-project\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-phase-4-project\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-phase-4-project/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\u003cp\u003eFinal phase down -- you're absolutely crushing it! You've made it all the way through one of the toughest phase of this course. You must have an amazing brain in your head!\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-phase-4-project/main/images/brain.gif\"\u003e\u003c/p\u003e\n\u003ch2\u003eProject Overview\u003c/h2\u003e\n\u003cp\u003eFor this phase, you will choose a project that requires building one of these four models:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eTime Series Modeling\u003c/li\u003e\n\u003cli\u003eRecommendation System\u003c/li\u003e\n\u003cli\u003eImage Classification with Deep Learning\u003c/li\u003e\n\u003cli\u003eNatural Language Processing\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003eThe Data\u003c/h3\u003e\n\u003cp\u003eWe have provided a dataset suitable to each model, but you are also encouraged to source your own dataset. If you choose your own dataset, \u003cstrong\u003erun the dataset and business problem by your instructor for approval\u003c/strong\u003e before starting your project.\u003c/p\u003e\n\u003ch3\u003eHow to Choose a Project\u003c/h3\u003e\n\u003cp\u003eWhen choosing a project, consider:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eDepth:\u003c/strong\u003e Choose a project that similar to what you want to do for your capstone project (Phase 5). This will allow you to practice those methods in a group setting before needing to use it independently. This will help you build a better Capstone project and a portfolio that demonstrates the ability to deeply learn and implement one modeling approach.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eBreadth:\u003c/strong\u003e Choose a problem that you don't necessarily plan to use in your capstone project. This will allow you to develop applied experience with multiple modeling approaches. This will help you refine your areas of interest and build a portfolio that demonstrates the ability to learn and implement multiple modeling approaches.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eIf you are feeling overwhelmed or behind, we recommend you choose Problem #3: Image Classification with Deep Learning.\u003c/p\u003e\n\u003ch3\u003eProblem 1: Time Series Modeling\u003c/h3\u003e\n\u003cp\u003eIf you choose the Time Series option, you will be forecasting real estate prices of various zip codes using data from \u003ca href=\"https://www.zillow.com/research/data/\"\u003eZillow Research\u003c/a\u003e. For this project, you will be acting as a consultant for a fictional real-estate investment firm. The firm has asked you what seems like a simple question:\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eWhat are the top 5 best zip codes for us to invest in?\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eThis may seem like a simple question at first glance, but there's more than a little ambiguity here that you'll have to think through in order to provide a solid recommendation. Should your recommendation be focused on profit margins only? What about risk? What sort of time horizon are you predicting against? Your recommendation will need to detail your rationale and answer any sort of lingering questions like these in order to demonstrate how you define \"best\".\u003c/p\u003e\n\u003cp\u003eThere are many datasets on the \u003ca href=\"https://www.zillow.com/research/data/\"\u003eZillow Research Page\u003c/a\u003e, and making sure you have exactly what you need can be a bit confusing. For simplicity's sake, we have already provided the dataset for you in this repo -- you will find it in the file \u003ccode\u003etime-series/zillow_data.csv\u003c/code\u003e.\u003c/p\u003e\n\u003cp\u003eThe goal of this project is to have you complete a very common real-world task in regard to time series modeling. However, real world problems often come with a significant degree of ambiguity, which requires you to use your knowledge of statistics and data science to think critically about and answer. While the main task in this project is time series modeling, that isn't the overall goal -- it is important to understand that time series modeling is a tool in your toolbox, and the forecasts it provides you are what you'll use to answer important questions.\u003c/p\u003e\n\u003cp\u003eIn short, to pass this project, demonstrating the quality and thoughtfulness of your overall recommendation is at least as important as successfully building a time series model!\u003c/p\u003e\n\u003ch4\u003eStarter Jupyter Notebook\u003c/h4\u003e\n\u003cp\u003eFor this project, you will be provided with a Jupyter notebook, \u003ccode\u003etime-series/starter_notebook.ipynb\u003c/code\u003e, containing some starter code. If you inspect the Zillow dataset file, you'll notice that the datetimes for each sale are the actual column names -- this is a format you probably haven't seen before. To ensure that you're not blocked by preprocessing, we've provided some helper functions to help simplify getting the data into the correct format. You're not required to use this notebook or keep it in its current format, but we strongly recommend you consider making use of the helper functions so you can spend your time working on the parts of the project that matter.\u003c/p\u003e\n\u003ch4\u003eEvaluation\u003c/h4\u003e\n\u003cp\u003eIn addition to deciding which quantitative metric(s) you want to target (e.g. minimizing mean squared error), you need to start with a definition of \"best investment\". Consider additional metrics like risk vs. profitability, or ROI yield.\u003c/p\u003e\n\u003ch3\u003eProblem 2: Recommendation System\u003c/h3\u003e\n\u003cp\u003eIf you choose the Recommendation System option, you will be making movie recommendations based on the \u003ca href=\"https://grouplens.org/datasets/movielens/latest/\"\u003eMovieLens\u003c/a\u003e dataset from the GroupLens research lab at the University of Minnesota. Unless you are planning to run your analysis on a paid cloud platform, we recommend that you use the \"small\" dataset containing 100,000 user ratings (and potentially, only a particular subset of that dataset).\u003c/p\u003e\n\u003cp\u003eYour task is to:\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eBuild a model that provides top 5 movie recommendations to a user, based on their ratings of other movies.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eThe MovieLens dataset is a \"classic\" recommendation system dataset, that is used in numerous academic papers and machine learning proofs-of-concept. You will need to create the specific details about how the user will provide their ratings of other movies, in addition to formulating a more specific business problem within the general context of \"recommending movies\".\u003c/p\u003e\n\u003ch4\u003eCollaborative Filtering\u003c/h4\u003e\n\u003cp\u003eAt minimum, your recommendation system must use collaborative filtering. If you have time, consider implementing a hybrid approach, e.g. using collaborative filtering as the primary mechanism, but using content-based filtering to address the \u003ca href=\"https://en.wikipedia.org/wiki/Cold_start_(computing)\"\u003ecold start problem\u003c/a\u003e.\u003c/p\u003e\n\u003ch4\u003eEvaluation\u003c/h4\u003e\n\u003cp\u003eThe MovieLens dataset has explicit ratings, so achieving some sort of evaluation of your model is simple enough. But you should give some thought to the question of metrics. Since the rankings are ordinal, we know we can treat this like a regression problem. But when it comes to regression metrics there are several choices: RMSE, MAE, etc. \u003ca href=\"http://fastml.com/evaluating-recommender-systems/\"\u003eHere\u003c/a\u003e are some further ideas.\u003c/p\u003e\n\u003ch3\u003eProblem 3: Image Classification with Deep Learning\u003c/h3\u003e\n\u003cp\u003eIf you choose this option, you'll put everything you've learned together to build a deep neural network that trains on a large dataset for classification on a non-trivial task. In this case, using x-ray images of pediatric patients to identify whether or not they have pneumonia. The dataset comes from Kermany et al. on \u003ca href=\"https://data.mendeley.com/datasets/rscbjbr9sj/3\"\u003eMendeley\u003c/a\u003e, although there is also a version on \u003ca href=\"https://www.kaggle.com/paultimothymooney/chest-xray-pneumonia\"\u003eKaggle\u003c/a\u003e that may be easier to use.\u003c/p\u003e\n\u003cp\u003eYour task is to:\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eBuild a model that can classify whether a given patient has pneumonia, given a chest x-ray image.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch4\u003eAim for a Proof of Concept\u003c/h4\u003e\n\u003cp\u003eWith Deep Learning, data is king -- the more of it, the better. However, the goal of this project isn't to build the best model possible -- it's to demonstrate your understanding by building a model that works. You should try to avoid datasets and model architectures that won't run in reasonable time on your own machine. For many problems, this means downsampling your dataset and only training on a portion of it. Once you're absolutely sure that you've found the best possible architecture and other hyperparameters for your model, then consider training your model on your entire dataset overnight (or, as larger portion of the dataset that will still run in a feasible amount of time).\u003c/p\u003e\n\u003cp\u003eAt the end of the day, we want to see your thought process as you iterate and improve on a model. A project that achieves a lower level of accuracy but has clearly iterated on the model and the problem until it found the best possible approach is more impressive than a model with high accuracy that did no iteration. We're not just interested in seeing you finish a model -- we want to see that you understand it, and can use this knowledge to try and make it even better!\u003c/p\u003e\n\u003ch4\u003eEvaluation\u003c/h4\u003e\n\u003cp\u003eEvaluation is fairly straightforward for this project. But you'll still need to think about which metric to use and about how best to cross-validate your results.\u003c/p\u003e\n\u003ch3\u003eProblem 4: Natural Language Processing (NLP)\u003c/h3\u003e\n\u003cp\u003eIf you choose this option, you'll build an NLP model to analyze Twitter sentiment about Apple and Google products. The dataset comes from CrowdFlower via \u003ca href=\"https://data.world/crowdflower/brands-and-product-emotions\"\u003edata.world\u003c/a\u003e. Human raters rated the sentiment in over 9,000 Tweets as positive, negative, or neither.\u003c/p\u003e\n\u003cp\u003eYour task is to:\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eBuild a model that can rate the sentiment of a Tweet based on its content.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch4\u003eAim for a Proof of Concept\u003c/h4\u003e\n\u003cp\u003eThere are many approaches to NLP problems - start with something simple and iterate from there. For example, you could start by limiting your analysis to positive and negative Tweets only, allowing you to build a binary classifier. Then you could add in the neutral Tweets to build out a multiclass classifier. You may also consider using some of the more advanced NLP methods in the Mod 4 Appendix.\u003c/p\u003e\n\u003ch4\u003eEvaluation\u003c/h4\u003e\n\u003cp\u003eEvaluating multiclass classifiers can be trickier than binary classifiers because there are multiple ways to mis-classify an observation, and some errors are more problematic than others. Use the business problem that your NLP project sets out to solve to inform your choice of evaluation metrics.\u003c/p\u003e\n\u003ch3\u003eSourcing Your Own Data\u003c/h3\u003e\n\u003cp\u003eSourcing new data is a valuable skill for data scientists, but it requires a great deal of care. An inappropriate dataset or an unclear business problem can lead you spend a lot of time on a project that delivers underwhelming results. The guidelines below will help you complete a project that demonstrates your ability to engage in the full data science process.\u003c/p\u003e\n\u003cp\u003eYour dataset must be...\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eAppropriate for one of this project's models.\u003c/strong\u003e These are time series, recommendation systems, deep learning, or natural language processing.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eUsable to solve a specific business problem.\u003c/strong\u003e This solution must rely on your model.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eSomewhat complex.\u003c/strong\u003e It should contain thousands of rows and features that require creativity to use.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eUnfamiliar.\u003c/strong\u003e It can't be one we've already worked with during the course or that is commonly used for demonstration purposes (e.g. MNIST).\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eManageable.\u003c/strong\u003e Stick to datasets that you can model using the techniques introduced in Phase 4.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eOnce you've sourced your own dataset and identified the business problem you want to solve with it, you must to \u003cstrong\u003erun them by your instructor for approval\u003c/strong\u003e.\u003c/p\u003e\n\u003ch4\u003eProblem First, or Data First?\u003c/h4\u003e\n\u003cp\u003eThere are two ways that you can source your own dataset: \u003cstrong\u003e\u003cem\u003eProblem First\u003c/em\u003e\u003c/strong\u003e or \u003cstrong\u003e\u003cem\u003eData First\u003c/em\u003e\u003c/strong\u003e. The less time you have to complete the project, the more strongly we recommend a Data First approach to this project.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e\u003cem\u003eProblem First\u003c/em\u003e\u003c/strong\u003e: Start with a problem that you are interested in that you could potentially solve using one of the four project models. Then look for data that you could use to solve that problem. This approach is high-risk, high-reward: Very rewarding if you are able to solve a problem you are invested in, but frustrating if you end up sinking lots of time in without finding appropriate data. To mitigate the risk, set a firm limit for the amount of time you will allow yourself to look for data before moving on to the Data First approach.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e\u003cem\u003eData First\u003c/em\u003e\u003c/strong\u003e: Take a look at some of the most popular internet repositories of cool data sets we've listed below. If you find a data set that's particularly interesting for you, then it's totally okay to build your problem around that data set.\u003c/p\u003e\n\u003cp\u003eThere are plenty of amazing places that you can get your data from. We recommend you start looking at data sets in some of these resources first:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://archive.ics.uci.edu/ml/datasets.php\" target=\"_blank\"\u003eUCI Machine Learning Datasets Repository\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://www.kaggle.com/datasets\"\u003eKaggle Datasets\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/awesomedata/awesome-public-datasets\"\u003eAwesome Datasets Repo on Github\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003eThe Deliverables\u003c/h2\u003e\n\u003cp\u003eThere are three deliverables for this project:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eA \u003cstrong\u003eGitHub repository\u003c/strong\u003e\n\u003c/li\u003e\n\u003cli\u003eA \u003cstrong\u003eJupyter Notebook\u003c/strong\u003e\n\u003c/li\u003e\n\u003cli\u003eA \u003cstrong\u003enon-technical presentation\u003c/strong\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eReview the \"Project Submission \u0026amp; Review\" page in the \"Milestones Instructions\" topic for instructions on creating and submitting your deliverables. Refer to the rubric associated with this assignment for specifications describing high-quality deliverables.\u003c/p\u003e\n\u003ch3\u003eKey Points\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eChoose your project quickly.\u003c/strong\u003e We've given you a lot of choices - don't get stuck spending too much time choosing which project to do. Give yourself a firm time limit for picking a project (e.g. 2 hours) so you can get on with making something great. Don't worry about picking the perfect project - remember that you will get to do a new, larger Capstone project very soon!\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eYour Jupyter Notebook should demonstrate an iterative approach to modeling.\u003c/strong\u003e This means that you begin with a basic model, evaluate it, and then provide justification for and proceed to a new model. This is a great way to add narrative structure to your notebook, especially if you compare model performance across each iteration.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eYou must choose and implement an appropriate validation strategy.\u003c/strong\u003e This is one of the trickiest parts of machine learning models, especially for models that don't lend themselves easily to traditional cross-validation (e.g. time series \u0026amp; recommendation systems).\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003eGetting Started\u003c/h2\u003e\n\u003cp\u003eCreate a new repository for your project to get started. We recommend structuring your project repository similar to the structure in \u003ca href=\"https://github.com/learn-co-curriculum/dsc-project-template\"\u003ethe Phase 1 Project Template\u003c/a\u003e. You can do this either by creating a new fork of that repository to work in or by building a new repository from scratch that mimics that structure.\u003c/p\u003e\n\u003ch2\u003eProject Submission and Review\u003c/h2\u003e\n\u003cp\u003eReview the \"Project Submission \u0026amp; Review\" page in the \"Milestones Instructions\" topic to learn how to submit your project and how it will be reviewed. Your project must pass review for you to progress to the next Phase.\u003c/p\u003e\n\u003ch2\u003eSummary\u003c/h2\u003e\n\u003cp\u003eThis project is your chance to show off your data science prowess with some advanced machine learning algorithms. Now that you've gone through all of the core course content, we're excited to see what you are able to do!\u003c/p\u003e","exportId":"ga5b9102687ae238d813078a99550dec8"},{"id":197894,"title":"Phase 4 Project - GitHub Repository URL","type":"Assignment","indent":1,"locked":false,"submissionTypes":"a website url","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":null,"completed":false,"content":"\u003cp\u003e\u003cspan\u003ePlease put the URL to your Phase 4 Project GitHub Repository here.\u0026nbsp;\u003c/span\u003e\u003c/p\u003e","exportId":"ge3a2332be06fb7ebdca5e17879290a6a"},{"id":197907,"title":"Phase 4 Blog Post","type":"Assignment","indent":0,"locked":false,"submissionTypes":"a website url","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":null,"completed":false,"content":"\u003cp\u003e\u003cspan\u003ePlease put the URL to your Phase 4 Blog Post here. \u003c/span\u003e\u003cspan\u003eRefer to the \u003c/span\u003e\u003ca title=\"Blogging Overview\" href=\"pages/blogging-overview\" data-api-endpoint=\"https://learning.flatironschool.com/api/v1/courses/347/pages/blogging-overview\" data-api-returntype=\"Page\"\u003eBlogging Overview\u003c/a\u003e\u003cspan\u003e to learn about how to write good blog posts that\u003c/span\u003e\u003cspan style=\"font-family: inherit; font-size: 1rem;\"\u003e meet Flatiron Schools requirements.\u003c/span\u003e\u003c/p\u003e","exportId":"gc9968eeea195184c8693bc492d2e8946"}]}],"pages":[{"exportId":"project-submission-and-review-campus","title":"Project Submission \u0026 Review (Campus)","type":"WikiPage","content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-project-submissions-campus\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-project-submissions-campus\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-project-submissions-campus/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e  \u003ch2\u003eIntroduction\u003c/h2\u003e  \u003cp\u003eIn this lesson, we review the requirements, submission, and review process for the Phase Projects.\u003c/p\u003e  \u003ch2\u003eObjectives\u003c/h2\u003e  \u003cp\u003eYou will be able to:\u003c/p\u003e  \u003cul\u003e \u003cli\u003eCreate project deliverables that meet Flatiron School requirements\u003c/li\u003e \u003cli\u003eSubmit your project deliverables in Canvas\u003c/li\u003e \u003cli\u003ePrepare for your project review\u003c/li\u003e \u003c/ul\u003e  \u003ch2\u003eCreate Your Project Deliverables\u003c/h2\u003e  \u003cp\u003eComplete the deliverables for your project, guided by the rubric at the bottom of the main project assignment. Keep in mind that the audience for these deliverables is not only your teacher, but also potential employers. Employers will look at your project deliverables to evaluate multiple skills, including coding, modeling, communication, and domain knowledge. You will want to polish these as much as you can, both during the course and afterwards.\u003c/p\u003e  \u003ch3\u003eGitHub Repository\u003c/h3\u003e  \u003cp\u003eYour GitHub repository is the public-facing version of your project that your instructors and potential employers will see - make it as accessible as you can. At a minimum, it should contain all your project files and a README.md file that summarizes your project and helps visitors navigate the repository.\u003c/p\u003e  \u003ch3\u003eJupyter Notebook\u003c/h3\u003e  \u003cp\u003eYour Jupyter Notebook is the primary source of information about your analysis. At a minimum, it should contain or import all of the code used in your project and walk the reader through your project from start to finish. You may choose to use multiple Jupyter Notebooks in your project, but you should have one that provides a full project overview as a point of entry for visitors.\u003c/p\u003e  \u003ch3\u003eNon-Technical Presentation\u003c/h3\u003e  \u003cp\u003eYour non-technical presentation is your opportunity to communicate clearly and concisely about your project and it's real-world relevance. The target audience should be people with limited technical knowledge who may be interested in leveraging your project. We recommend using Google Slides, PowerPoint or Keynote to create your presentation slides. You will then present your project to your cohort.\u003c/p\u003e  \u003ch2\u003eSubmit Your Project\u003c/h2\u003e  \u003cp\u003eTo submit your project in Canvas, you will create and upload PDF versions of three project deliverables. You will also submit the URL to your GitHub repository in a separate assignment.\u003c/p\u003e  \u003ch3\u003ePresentation Slides PDF Creation\u003c/h3\u003e  \u003col\u003e \u003cli\u003eExport your presentation as a PDF from the program in which you created it.\u003c/li\u003e \u003cli\u003eGive it a short descriptive file name (e.g. \u003ccode\u003epresentation.pdf\u003c/code\u003e).\u003c/li\u003e \u003cli\u003ePlace a copy of the PDF in your GitHub repository.\u003c/li\u003e \u003c/ol\u003e  \u003ch3\u003eGitHub Repository PDF Creation\u003c/h3\u003e  \u003col\u003e \u003cli\u003eNavigate to the root directory of your project repository on GitHub, using your browser (we recommend Google Chrome).\u003c/li\u003e \u003cli\u003eSave the webpage as a PDF using the browser's Print functionality (\u003ca href=\"https://www.wikihow.com/Save-a-Web-Page-as-a-PDF-in-Google-Chrome\"\u003eGoogle Chrome Save to PDF instructions\u003c/a\u003e)\u003c/li\u003e \u003cli\u003eGive it a short descriptive file name (e.g. \u003ccode\u003egithub.pdf\u003c/code\u003e).\u003c/li\u003e \u003c/ol\u003e  \u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-project-submissions-campus/master/repo_pdf.gif\" alt=\"Repository PDF Creation\"\u003e\u003c/p\u003e  \u003ch3\u003eJupyter Notebook PDF Creation\u003c/h3\u003e  \u003col\u003e \u003cli\u003eOpen your Notebook in your browser (we recommend Google Chrome).\u003c/li\u003e \u003cli\u003e\n\u003cstrong\u003eRun the Notebook from start to finish\u003c/strong\u003e so that your output is visible.\u003c/li\u003e \u003cli\u003eSave the page as a PDF using the browser's Print functionality (\u003ca href=\"https://www.wikihow.com/Save-a-Web-Page-as-a-PDF-in-Google-Chrome\"\u003eGoogle Chrome Save to PDF instructions\u003c/a\u003e)\u003c/li\u003e \u003cli\u003eGive it a short descriptive file name (e.g. \u003ccode\u003enotebook.pdf\u003c/code\u003e).\u003c/li\u003e \u003c/ol\u003e  \u003cp\u003eIf you have difficulty creating a PDF version of your notebook, you can use \u003ca href=\"https://htmtopdf.herokuapp.com/ipynbviewer/\"\u003ethis tool\u003c/a\u003e instead. Set the Results Format to HTML + PDF. Then click View and Convert. Once its done, you should see links to .html and .pdf versions above the View and Convert button.\u003c/p\u003e  \u003ch3\u003ePDF Submission in Canvas\u003c/h3\u003e  \u003cp\u003eYou will need to submit all three PDF files as a single submission:\u003c/p\u003e  \u003col\u003e \u003cli\u003eClick \"Submit Assignment\" at the top of the \"Phase X Project\" assignment in the \"Milestones\" topic.\u003c/li\u003e \u003cli\u003eIn the \"File Upload\" box, click \"Choose File\" button to upload a single file.\u003c/li\u003e \u003cli\u003eClick the \"Add Another File\" link to upload an additional file.\u003c/li\u003e \u003cli\u003eRepeat Step 3 to upload one more file. After this is done, all three files should be uploaded.\u003c/li\u003e \u003cli\u003eHit the blue \"Submit Assignment\" button.\u003c/li\u003e \u003c/ol\u003e  \u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-project-submissions-campus/master/project_3pdf_submission.gif\" alt=\"Project PDF Submission\"\u003e\u003c/p\u003e  \u003ch3\u003eURL Submission in Canvas\u003c/h3\u003e  \u003cp\u003eThere is an additional Canvas assignment where you will just enter the URL for your project's GitHub repository. This is located in the \"Milestones\" topic in this course as \"Phase X Project - GitHub Repository URL.\"\u003c/p\u003e  \u003ch2\u003eProject Review\u003c/h2\u003e  \u003cp\u003eYou have until the end of the week to present your project and submit it for review. You will receive a grade of P (Pass) or NP (No Pass) - you must pass in order to move to the next phase with your cohort. Your teacher will grade your submission and give you feedback, typically early in the week after you submit.\u003c/p\u003e  \u003cp\u003eWhich review process you receive will depend on whether you passed the code challenge, as described below.\u003c/p\u003e  \u003ch3\u003ePassed Code Challenge: Check for Completion\u003c/h3\u003e  \u003cp\u003eIf you passed the code challenge, your teacher will review your project to check that it is complete.\u0026nbsp;Your project will pass if you have completed and submitted all project deliverables. Your project will not be graded using the project rubric at the bottom of the project assignment, although you may find it helpful for guiding your project.\u003c/p\u003e  \u003ch3\u003eDid Not Pass Code Challenge: Rubric Grading\u003c/h3\u003e  \u003cp\u003eIf you did not pass the code challenge, your teacher will use the rubric at the bottom of the project assignment to grade your submission. You can earn 0, 1, or 2 points on each rubric element. Your project will pass if it earns at least 50% of the points available.\u003c/p\u003e  \u003ch2\u003eConclusion\u003c/h2\u003e  \u003cp\u003eThank you for your hard work on this project - you're going to do great! Remember that future employers will also look at your projects when deciding whether to hire you, so having complete, polished projects will help you tremendously not only to pass this assignment, but also to get the job you want after you graduate.\u003c/p\u003e  \u003cp\u003eIf you have any questions about the project submission or review process, don't hesitate to ask your teacher.\u003c/p\u003e","frontPage":false},{"exportId":"clustering-recap","title":"Clustering - Recap","type":"WikiPage","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-clustering-recap\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-clustering-recap/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\u003ch2\u003eKey Takeaways\u003c/h2\u003e\n\u003cp\u003eThe key takeaways from this section include:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eThere are two main types of clustering algorithms: non-hierarchical clustering (k-means) and hierarchical agglomerative clustering\u003c/li\u003e\n\u003cli\u003eYou can quantify the performance of a clustering algorithm using metrics such as variance ratios\u003c/li\u003e\n\u003cli\u003eWhen working with the k-means clustering algorithm, it is useful to create elbow plots to find an optimal value for \u003cimg src=\"https://render.githubusercontent.com/render/math?math=k\"\u003e\n\u003c/li\u003e\n\u003cli\u003eWhen using hierarchical agglomerative clustering, different linkage criteria can be used to determine which clusters should be merged and at what point\u003c/li\u003e\n\u003cli\u003eDendrograms and clustergrams are very useful visual tools in hierarchical agglomerative clustering\u003c/li\u003e\n\u003cli\u003eAdvantages of k-means clustering include easy implementation and speed, whereas the main disadvantage is that it isn't always straightforward how to pick the \"right\" value for \u003cimg src=\"https://render.githubusercontent.com/render/math?math=k\"\u003e\n\u003c/li\u003e\n\u003cli\u003eAdvantages of hierarchical agglomerative clustering include easy visualization and intuitiveness, whereas the main disadvantage is that the result is very distance-metric-dependent\u003c/li\u003e\n\u003cli\u003eYou can use supervised and unsupervised learning together in a few different ways. Applications of this are look-alike models in market segmentation and semi-supervised learning\u003c/li\u003e\n\u003c/ul\u003e","frontPage":false},{"exportId":"neural-networks-introduction","title":"Neural Networks - Introduction","type":"WikiPage","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-neural-networks-section-intro\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-neural-networks-section-intro/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eIn this section, you'll be introduced to one of the most advanced machine learning algorithms currently in the world -- neural networks! \u003c/p\u003e\n\n\u003ch2\u003eDeep Learning\u003c/h2\u003e\n\n\u003cp\u003eThe time has come to learn about one of the most exciting and fast-growing areas of data science: Deep Learning! When we talk about deep learning, we are talking about (deep) neural networks. You'll learn all about them in this section. You'll also use Python to build (basic) neural networks from scratch.\u003c/p\u003e\n\n\u003ch3\u003eNeural Networks\u003c/h3\u003e\n\n\u003cp\u003eIn this section, you'll learn what it means when we talk about neural networks. You'll learn about the essential building blocks like \"layers\", \"nodes\", \"arrows\", \"weights\", \"loss\", \"cost function\", etc. You'll learn that a neural network generally consists of several layers, and how a logistic regression model can be represented as a neural network with just one layer. You'll be able to explain what the advantages and disadvantages of using neural networks are, and get an insight of how forward and backward propagation are used in neural networks to minimize the loss and \"optimize\" your neural network.\u003c/p\u003e\n\n\u003ch3\u003eKeras\u003c/h3\u003e\n\n\u003cp\u003eYou'll be introduced to Keras, a leading open source neural network library in Python, which makes building neural networks surprisingly easy. Before building your first neural network model in Keras, you'll learn about tensors and why they are important when building deep learning models. \u003c/p\u003e\n\n\u003ch2\u003eSummary\u003c/h2\u003e\n\n\u003cp\u003eIn this section, you'll learn the basics of neural networks and how to implement them in Keras!\u003c/p\u003e\n\u003cfooter class=\"fis-footer\" style=\"visibility: hidden;\"\u003e\u003cdiv class=\"fis-feedback\"\u003e\n\u003ch5\u003eHow do you feel about this lesson?\u003c/h5\u003e\n\u003cimg id=\"thumbs-up\" data-repository=\"dsc-neural-networks-section-intro\" title=\"Thumbs up!\" alt=\"thumbs up\"\u003e\u003cimg id=\"thumbs-down\" data-repository=\"dsc-neural-networks-section-intro\" title=\"Thumbs down!\" alt=\"thumbs down\"\u003e\n\u003c/div\u003e\n\u003ch5\u003eHave specific feedback? \u003ca href=\"https://github.com/learn-co-curriculum/dsc-neural-networks-section-intro/issues/new\"\u003eTell us here!\u003c/a\u003e\n\u003c/h5\u003e\u003c/footer\u003e","frontPage":false},{"exportId":"blogging-overview","title":"Blogging Overview","type":"WikiPage","content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-blogging-overview\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-blogging-overview\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-blogging-overview/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e  \u003ch2\u003eIntroduction\u003c/h2\u003e  \u003cp\u003eIn this lesson, we discuss how to write good blog posts that meet Flatiron School's requirements.\u003c/p\u003e  \u003ch2\u003eObjectives\u003c/h2\u003e  \u003cp\u003eThis lesson covers...\u003c/p\u003e  \u003cul\u003e \u003cli\u003eWhy blogging is valuable\u003c/li\u003e \u003cli\u003eTopics to blog about\u003c/li\u003e \u003cli\u003eWhat makes for a good blog post\u003c/li\u003e \u003cli\u003eHow to start your blog\u003c/li\u003e \u003cli\u003eFlatiron School blog requirements \u003c/li\u003e \u003c/ul\u003e  \u003ch2\u003eWhy Should I Blog?\u003c/h2\u003e  \u003cp\u003eBlogging has many benefits:\u003c/p\u003e  \u003cul\u003e \u003cli\u003e\u003cp\u003e\u003cstrong\u003eDevelop your written communication skills.\u003c/strong\u003e Your writing ability will be critical to your success when completing job applications and presenting your work to colleagues. Blogging is great practice for identifying and clearly communicating the most important points of any subject.\u003c/p\u003e\u003c/li\u003e \u003cli\u003e\u003cp\u003e\u003cstrong\u003eDemonstrate your talent to employers.\u003c/strong\u003e Potential employers will review your blog to determine whether to offer you an interview or a job. Some students have even been invited to interview or exempted from technical interviews based on their blogs.\u003c/p\u003e\u003c/li\u003e \u003cli\u003e\u003cp\u003e\u003cstrong\u003eStrengthen your knowledge.\u003c/strong\u003e Blogging helps you explore new topics, deepen your understanding, and crystallize what you've learned.\u003c/p\u003e\u003c/li\u003e \u003cli\u003e\u003cp\u003e\u003cstrong\u003eHelp your peers and the broader community.\u003c/strong\u003e Have you ever Googled a question you had and found the answer on a blog? Writing blog posts helps others who are following in your footsteps!\u003c/p\u003e\u003c/li\u003e \u003c/ul\u003e  \u003ch2\u003eWhat Should I Blog About?\u003c/h2\u003e  \u003cp\u003eHere are some blog topic ideas:\u003c/p\u003e  \u003cul\u003e \u003cli\u003e\u003cp\u003eWhy did you decide to learn data science?\u003c/p\u003e\u003c/li\u003e \u003cli\u003e\u003cp\u003eDescribe how a DS technique works, when you might use it, and its strengths/weaknesses.\u003c/p\u003e\u003c/li\u003e \u003cli\u003e\u003cp\u003eSummarize an End of Phase Project by explaining your problem, the dataset, your methodology, and your results.\u003c/p\u003e\u003c/li\u003e \u003cli\u003e\u003cp\u003eDive into something that you want to learn more about, maybe because you find it challenging or it wasn't covered in the course.\u003c/p\u003e\u003c/li\u003e \u003cli\u003e\u003cp\u003eWrite a tutorial to help aspiring data scientists to implement a tool or method.\u003c/p\u003e\u003c/li\u003e \u003cli\u003e\u003cp\u003eFind an interesting data science paper and summarize why it is important. This can be a new paper from the past few months, or you can refer to \u003ca href=\"https://docs.google.com/spreadsheets/d/1UYmAT13AAknrOatzLeeAsN4tS7ENjn2fpJNGzOZ67rQ/edit?usp=sharing\"\u003ethis spreadsheet\u003c/a\u003e.\u003c/p\u003e\u003c/li\u003e \u003c/ul\u003e  \u003ch2\u003eWhat Does A Good Blog Post Look Like?\u003c/h2\u003e  \u003cp\u003eWe recommend you take a look at our \u003ca href=\"https://drive.google.com/drive/folders/1UBiRCRLzVP5CHU3PJNwoMZAe3ajUBm2a?usp=sharing\"\u003eblog templates\u003c/a\u003e and \u003ca href=\"https://docs.google.com/document/d/1eqL8Dsj7dH7s_MRnf_4-3kCiSz72POHTfb-sBRN5Zhs/edit?usp=sharing\"\u003eexamples\u003c/a\u003e to get an idea for what makes a blog post good.\u003c/p\u003e  \u003cul\u003e \u003cli\u003e\u003cp\u003eStrike a balance between providing a meaningful investigation of your topic and being concise. Constrain the scope so it will be interesting and digestible in about 1000-3000 words (this is not a firm limit).\u003c/p\u003e\u003c/li\u003e \u003cli\u003e\n\u003cp\u003eUse clear and consistent formatting to make your content accessible and professional-looking.\u003c/p\u003e  \u003cul\u003e \u003cli\u003eWhen presenting code, use code snippets instead of screenshots.\u003c/li\u003e \u003cli\u003eMake URLs into hyperlinks that are easy for readers to click into.\u003c/li\u003e \u003cli\u003eUse headings to provide structure and flow to your post.\u003c/li\u003e \u003c/ul\u003e\n\u003c/li\u003e \u003cli\u003e\u003cp\u003eCite and link to resources you used to write your post.\u003c/p\u003e\u003c/li\u003e \u003c/ul\u003e  \u003ch2\u003eHow Do I Start My Blog?\u003c/h2\u003e  \u003cp\u003eIf you already have a professional blog that you'd like to use for your data science content, you can add your posts to that. Otherwise, you will need to start a new blog. If you have a personal blog, you should avoid using it for this purpose so that you can continue using it for personal content without worrying about how it might be perceived by potential employers.\u003c/p\u003e  \u003cp\u003eThere are multiple blogging platforms to choose from that make it easy to start a blog, here are some of our favorites:\u003c/p\u003e  \u003cul\u003e \u003cli\u003e\u003ca href=\"https://www.blogger.com/\"\u003eBlogger\u003c/a\u003e\u003c/li\u003e \u003cli\u003e\u003ca href=\"https://dev.to/\"\u003edev.to\u003c/a\u003e\u003c/li\u003e \u003cli\u003e\u003ca href=\"https://pages.github.com/\"\u003eGitHub Pages\u003c/a\u003e\u003c/li\u003e \u003cli\u003e\u003ca href=\"https://medium.com/\"\u003eMedium\u003c/a\u003e\u003c/li\u003e \u003cli\u003e\u003ca href=\"https://wordpress.com/\"\u003eWordpress\u003c/a\u003e\u003c/li\u003e \u003c/ul\u003e  \u003cp\u003eDifferent platforms have different pros and cons, so do a little research to decide what is best for you.\u003c/p\u003e  \u003ch2\u003eBlog Requirements\u003c/h2\u003e  \u003cp\u003eTo succeed in your career transition and graduate from Flatiron School, you must complete the following activities. These requirements are designed to give you the best opportunity to deepen your knowledge, practice communication skills, and showcase yourself to potential employers.\u003c/p\u003e  \u003cul\u003e \u003cli\u003e\u003cp\u003eSet up a publicly accessible blog \u003c/p\u003e\u003c/li\u003e \u003cli\u003e\u003cp\u003ePublish at least four blog posts on it, including \u003cstrong\u003eone per Phase for Phases 1-4\u003c/strong\u003e\u003c/p\u003e\u003c/li\u003e \u003cli\u003e\n\u003cp\u003eSubmit URLs to your posts \u003cstrong\u003eby the end of each Phase\u003c/strong\u003e in the Blog Post assignments\u003c/p\u003e  \u003cul\u003e \u003cli\u003eThese assignments are located in the Milestones topics of the Phase 1-4 Canvas courses\u003c/li\u003e \u003c/ul\u003e\n\u003c/li\u003e \u003cli\u003e\n\u003cp\u003eWrite blog posts that...\u003c/p\u003e  \u003cul\u003e \u003cli\u003eDiscuss data science topics\u003c/li\u003e \u003cli\u003eAre composed primarily of original material you wrote\u003c/li\u003e \u003cli\u003eInclude proper attribution\u003c/li\u003e \u003cli\u003eHave high-quality content and formatting\u003c/li\u003e \u003cli\u003eAre something you would proudly show to a potential employer\u003c/li\u003e \u003c/ul\u003e\n\u003c/li\u003e \u003c/ul\u003e  \u003cp\u003eAfter you submit your blog posts, your teacher will grade them as Complete or Incomplete. Your blogs must all be submitted on time and receive Complete grades in order to continue through your program.\u003c/p\u003e  \u003cp\u003eHave fun and happy blogging!\u003c/p\u003e","frontPage":false},{"exportId":"apache-spark-introduction","title":"Apache Spark - Introduction","type":"WikiPage","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-spark-introduction\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-spark-introduction/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eIn this section, you will be introduced to the idea of big data and the tools data scientists use to manage it.\u003c/p\u003e\n\n\u003ch2\u003eBig Data in PySpark\u003c/h2\u003e\n\n\u003cp\u003eBig data is undoubtedly one of the most hyped terms in data science these days. Big data analytics involves dealing with data that is large in volume and high in variety and velocity, making it challenging for data scientists to run their routine analysis activities. In this section, you'll learn the basics of dealing with big data through parallel and distributed computing. In particular, you will be introduced to Apache Spark, an open-source distributed cluster-computing framework. You'll learn how to use the popular Apache Spark Python API, \u003cstrong\u003ePySpark\u003c/strong\u003e.\u003c/p\u003e\n\n\u003ch3\u003eParallel and Distributed Computing with MapReduce\u003c/h3\u003e\n\n\u003cp\u003eBefore diving into PySpark, we start this section by providing more context on the ideas of parallel and distributed computing and MapReduce. When talking about distributed and parallel computing, we refer to the fact that complex (and big) data science tasks can be executed over a cluster of interconnected computers instead of on just one machine. You'll learn that MapReduce allows us to convert these big datasets into sets of tuples as key:value pairs, as we'll cover in more detail in this section.\u003c/p\u003e\n\n\u003ch3\u003eApache Spark\u003c/h3\u003e\n\n\u003cp\u003eAs mentioned before, Apache Spark makes it easier (and feasible) to use huge amounts of data! You'll read a scientific article on the advantages of Apache Spark to understand its use and benefits better.\u003c/p\u003e\n\n\u003ch3\u003eInstalling and Configuring PySpark with Docker\u003c/h3\u003e\n\n\u003cp\u003eA big part of PySpark is actually getting PySpark up and running on your machine. You'll get an overview of how to do this so you can get started exploring distributed computing!\u003c/p\u003e\n\n\u003ch3\u003ePySpark\u003c/h3\u003e\n\n\u003cp\u003eYou'll learn about distributed and parallel computing and the different PySpark modules needed to create this parallelization.\u003c/p\u003e\n\n\u003ch3\u003eRDDs (Resilient Distributed Datasets)\u003c/h3\u003e\n\n\u003cp\u003eResilient Distributed Datasets (RDDs) are the core concept in PySpark. RDDs are immutable distributed collections of data objects. Each dataset in RDD is divided into logical partitions, which may be computed on different computers (so-called \"nodes\") in the Spark cluster. In this section, you'll learn how RDDs in Spark work. Additionally, you'll learn that RDD operations can be split into actions and transformations. \u003c/p\u003e\n\n\u003ch3\u003eWord Count with MapReduce\u003c/h3\u003e\n\n\u003cp\u003eYou'll use MapReduce to solve a basic NLP task where you compare the attributes of different authors of various texts.\u003c/p\u003e\n\n\u003ch3\u003eMachine Learning with Spark\u003c/h3\u003e\n\n\u003cp\u003eAfter you've solved a basic MapReduce problem, you will learn about employing the machine learning modules of PySpark. You will perform both a regression and classification problem and get the chance to build a full parallelizable data science pipeline that can scale to work with big data. In this section, you'll also get a chance to work with PySpark DataFrames.\u003c/p\u003e\n\n\u003ch2\u003eSummary\u003c/h2\u003e\n\n\u003cp\u003eIn this section, you'll learn the foundations of Big Data and how to manage it with Apache Spark!\u003c/p\u003e\n\u003cfooter class=\"fis-footer\" style=\"visibility: hidden;\"\u003e\u003cdiv class=\"fis-feedback\"\u003e\n\u003ch5\u003eHow do you feel about this lesson?\u003c/h5\u003e\n\u003cimg id=\"thumbs-up\" data-repository=\"dsc-spark-introduction\" title=\"Thumbs up!\" alt=\"thumbs up\"\u003e\u003cimg id=\"thumbs-down\" data-repository=\"dsc-spark-introduction\" title=\"Thumbs down!\" alt=\"thumbs down\"\u003e\n\u003c/div\u003e\n\u003ch5\u003eHave specific feedback? \u003ca href=\"https://github.com/learn-co-curriculum/dsc-spark-introduction/issues/new\"\u003eTell us here!\u003c/a\u003e\n\u003c/h5\u003e\u003c/footer\u003e","frontPage":false},{"exportId":"understanding-recurrent-neural-networks","title":"Understanding Recurrent Neural Networks","type":"WikiPage","content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-understanding-recurrent-neural-networks\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-understanding-recurrent-neural-networks\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-understanding-recurrent-neural-networks/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eIn this lesson, you'll learn about a new type of model architecture you haven't seen yet  \u003cstrong\u003e\u003cem\u003eRecurrent Neural Networks\u003c/em\u003e\u003c/strong\u003e!\u003c/p\u003e\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\u003cp\u003eYou will be able to:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eExplain the role time steps play in RNN models\u003c/li\u003e\n\u003cli\u003eExplain back propagation through time\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003eData as Time Sequences\u003c/h2\u003e\n\u003cp\u003eThe hallmark of Recurrent Neural Networks is that they are used to evaluate \u003cstrong\u003e\u003cem\u003eSequences\u003c/em\u003e\u003c/strong\u003e of data, rather than just individual data points. So what is sequence data, and how do you distinguish it from other kinds of data, so that you know when to use an RNN?\u003c/p\u003e\n\u003cp\u003eTime series data is a classic example of sequence data. You care about the value over time, and any given point in time can really only be examined relative to the other points of time in that sequence. For instance, knowing the price of Google stock today doesn't provide enough information for us to classify it as a something you should or shouldn't buy  for that, you would need to examine today's price relative to the previous day(s) price to see if it's going up or down.\u003c/p\u003e\n\u003cp\u003eAnother great example of sequence data is text. All text data is sequence data by default  a letter only makes sense when it's words are in the proper order. You would lose all information if you made a \"Bag of Letters\". Words themselves are sequence data, and can be used for all kinds of novel sequence generation tasks. You've probably seen articles in popular culture about people using neural networks to generate novel band names, cookie names, Pokemon names, etc. These are always done with Recurrent Neural Networks, because they are a perfect fit for sequence data. For this reason, RNNs excel at NLP tasks, because they can take in text as full sequences of words, from a single sentence up to an entire document or book! Because of this, they do not suffer the same loss of information that comes from a traditional Bag-of-Words vectorization approach.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-understanding-recurrent-neural-networks/master/images/unrolled.gif\"\u003e\u003c/p\u003e\n\u003cp\u003eLet's take a look at the overall structure of an RNN to see how it interacts with this sequence data!\u003c/p\u003e\n\u003ch2\u003eBasic RNN Architecture\u003c/h2\u003e\n\u003cp\u003eA basic Recurrent Neural Network is just a neural network that passes it's output from a given example back into itself as input for the next example. Intuitively, this approach makes sense. If you want to predict what Google's stock price is going to be two days from now, the most important input you can give it is what you think the price will be one day from now!\u003c/p\u003e\n\u003cp\u003eWhen drawn as a diagram, RNNs are usually represented in an \u003cstrong\u003e\u003cem\u003eUnrolled\u003c/em\u003e\u003c/strong\u003e representation, which shows the components at each given timestep. The image on the left is a how an RNN is denoted in a diagram \"rolled up\", while the image on the right is \"unrolled\". The current timestep is denoted with the input node \u003cimg src=\"https://render.githubusercontent.com/render/math?math=X_t\"\u003e , which makes the previous timestep \u003cimg src=\"https://render.githubusercontent.com/render/math?math=X_%7Bt-1%7D\"\u003e and the next timestep \u003cimg src=\"https://render.githubusercontent.com/render/math?math=X_%7Bt%2b1%7D\"\u003e . \u003cimg src=\"https://render.githubusercontent.com/render/math?math=H_0\"\u003e represents the model's output for timestep 0, which will then be passed back into the model in \u003cimg src=\"https://render.githubusercontent.com/render/math?math=X_1\"\u003e .\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-understanding-recurrent-neural-networks/master/images/new-RNN-unrolled.png\"\u003e\u003c/p\u003e\n\u003ch2\u003eBackpropagation Through Time\u003c/h2\u003e\n\u003cp\u003eOne interesting aspect of working with RNNs is that they use a modified form of back propagation called \u003cstrong\u003e\u003cem\u003eBack Propagation Through Time (BPTT)\u003c/em\u003e\u003c/strong\u003e. Because the model is trained on sequence data, it has the potential to be right or wrong at every point in that sequence. This means that you need to adjust the model's weights at each time point to effectively learn from sequence data. Because the model starts at the most recent output, and then works backwards to calculate the loss and update the weights at each time step, the model is said to be going \"back in time\" to learn. Since you have to update every single weight at every single time step, that means that BPTT is much more computationally expensive than traditional back propagation. For instance, if a single data point is a sequence with 1000 time steps, then the model will perform a full round of back propagation for each of the 1000 points in that single sequence.\u003c/p\u003e\n\u003ch3\u003eTruncated Back Prop Through Time\u003c/h3\u003e\n\u003cp\u003eThis was a major hurdle for traditional RNN architectures, but a solution exists in the form of the \u003cstrong\u003e\u003cem\u003eTruncated Back Propagation Through Time (TBPTT)\u003c/em\u003e\u003c/strong\u003e algorithm! We won't go deep into the specifics, but essentially, this algorithm increases performance by breaking a big sequence of 1000 points into 50 sequences of 20. This significantly improves training time over regular BPTT, but is still significantly slower than vanilla back propagation.\u003c/p\u003e\n\u003cp\u003eFun Fact: Truncated Back Prop Through Time was invented in the dissertation of Ilya Sutskever, one of the lead researchers at Open AI!\u003c/p\u003e\n\u003ch2\u003eSummary\u003c/h2\u003e\n\u003cp\u003eIn this lesson, you learned about the sequence data. You also learned about the architecture of RNNs, and the modified back prop algorithm they use for training!\u003c/p\u003e","frontPage":false},{"exportId":"lstms-and-grus","title":"LSTMs and GRUs","type":"WikiPage","content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-lstms-and-grus\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-lstms-and-grus\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-lstms-and-grus/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eIn this lesson, you'll learn about two advanced types of neurons that typically outperform basic RNNs, \u003cstrong\u003e\u003cem\u003eLong Short Term Memory Cells\u003c/em\u003e\u003c/strong\u003e and \u003cstrong\u003e\u003cem\u003eGated Recurrent Units\u003c/em\u003e\u003c/strong\u003e! You'll explore the problems they solve that increase their effectiveness compared to traditional vanilla RNNs, and compare and contrast the two neurons types to get a feel for what exactly they do and how they do it!\u003c/p\u003e\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\u003cp\u003eYou will be able to:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eExplain why vanishing and exploding gradients exist when training RNNs\u003c/li\u003e\n\u003cli\u003eDescribe the basic architecture and function of a GRU\u003c/li\u003e\n\u003cli\u003eDescribe the architecture and function of an LSTM cell\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003eRNNs and Gradient Problems\u003c/h2\u003e\n\u003cp\u003eOne of the biggest problems with standard Recurrent Neural Networks is that they get \u003cstrong\u003e\u003cem\u003eSaturated\u003c/em\u003e\u003c/strong\u003e. The problem with this it that they use a sigmoid or tanh activation function, and there are large areas of each function where the derivative is very, very close to 0. When the derivatives are low, this means the weight updates are small, which means that the \"learning\" of the model slows to a crawl! This happens because after many, many weight updates, many weights will have been pushed into an extremely positive or extremely negative value. All you have to do is get past -5 or +5 to get to very small values.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-lstms-and-grus/master/images/new_vanishing_gradient.png\"\u003e\u003c/p\u003e\n\u003cp\u003eWhen gradients are close to 0 because the values are extremely low, this is called \u003cstrong\u003e\u003cem\u003eVanishing Gradient\u003c/em\u003e\u003c/strong\u003e. Similarly, networks can also get to the point where the gradients are much, much large, resulting in massive weight updates that cause the model to thrash between 1 extremely wrong answer and another. When this happens, it is called \u003cstrong\u003e\u003cem\u003eExploding Gradient\u003c/em\u003e\u003c/strong\u003e. In practice, you can easily solve exploding gradients by just \"clipping\" the weight updates by bounding them at a maximum value. However, there's no good solution for vanishing gradients!\u003c/p\u003e\n\u003cp\u003eAn intuitive way to think of this in terms of Information Theory -- the network is trying to encapsulate too much information from all of the time steps. Take a look at the following diagram, which you saw in the previous lesson. Pay attention to the colors that represent each word:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-lstms-and-grus/master/images/unrolled.gif\"\u003e\u003c/p\u003e\n\u003cp\u003eNotice how the further along the sequence goes, the less overall area the navy blue color (for the first word, \"What\") gets. As each new word in the sequence gets processed, the amount of \"room\" the RNN has to remember things gets saturated. It turns out, remembering too many things is a pretty surefire way to get your model to crash and burn. This makes it hard for dealing with long-term dependencies in the data. For instance, consider the following sentence:\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\"Marilyn studied in France during the summer and fall semesters of college in 2016. As a result, she speaks fluent {_}\"\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eIf you were to use a traditional RNN to predict the next word in this sentence, it would likely have trouble figuring out the answer because of the number of time steps between the word to predict and the word that contains the information necessary to make a prediction, \"France\".\u003c/p\u003e\n\u003ch2\u003eRemembering and Forgetting\u003c/h2\u003e\n\u003cp\u003eThis is where the modern versions of RNNs come in. In practice, when building models for sequence data, people rarely use traditional RNN architectures anymore. Instead they make use of \u003cstrong\u003e\u003cem\u003eLSTMs\u003c/em\u003e\u003c/strong\u003e and \u003cstrong\u003e\u003cem\u003eGRUs\u003c/em\u003e\u003c/strong\u003e. Both of these models can be thought of as special types of neurons that can be used in an RNN. Although they work a little differently, they have the same strength -- the ability to \u003cstrong\u003e\u003cem\u003eforget information\u003c/em\u003e\u003c/strong\u003e! By constantly updating their internal state, they can learn what is important to remember, and when it is okay to forget it.\u003c/p\u003e\n\u003cp\u003eConsider the word prediction example you just looked at. You clearly need to remember the word \"France\", but there are plenty of words in between France and the word you need to predict that aren't that important, and you can safely ignore, such as \"during the\", \"and\", \"of\", etc. Furthermore, let's assume that the model learns enough to answer this question, but the next thousand words in the sequence is about something completely different. Do you really still need to hold on to the information about where Marilyn studied? How can you tell when you need to remember something and when you need to forget something? This is where GRUs and LSTMs have different approaches. Let's take a quick look at how they both work.\u003c/p\u003e\n\u003ch2\u003eGated Recurrent Units (GRUs)\u003c/h2\u003e\n\u003cp\u003e\u003cstrong\u003e\u003cem\u003eGated Recurrent Units\u003c/em\u003e\u003c/strong\u003e, or \u003cstrong\u003e\u003cem\u003eGRUs\u003c/em\u003e\u003c/strong\u003e, are a special type of cell that passes along it's internal state at each time step. However, not every part of the internal state is passed along, but only the important stuff! GRUs make use of two \"gate\" functions: a \u003cstrong\u003e\u003cem\u003eReset Gate\u003c/em\u003e\u003c/strong\u003e, which determines what should be removed from the cell's internal state before passing itself along to the next time step, and an \u003cstrong\u003e\u003cem\u003eUpdate Gate\u003c/em\u003e\u003c/strong\u003e, which determines how much of the state from the previous time step should be used in the current time step.\u003c/p\u003e\n\u003cp\u003eThe following technical diagram shows the internal operations of how a GRU cell works. Don't worry about trying to understand what every part of this diagram means. Internally, its just some equations for the update and reset operations, coupled with matrix multiplication and sigmoid functions. Instead, focus on the the \u003cimg src=\"https://render.githubusercontent.com/render/math?math=S_t\"\u003e line, which moves from left to right and denotes the state being updated and passed onto the next layer.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-lstms-and-grus/master/images/new_gru.png\" width=\"400\"\u003e\u003c/p\u003e\n\u003ch2\u003eLong Short Term Memory Cells (LSTMs)\u003c/h2\u003e\n\u003cp\u003e\u003cstrong\u003e\u003cem\u003eLong Short Term Memory Cells\u003c/em\u003e\u003c/strong\u003e, or \u003cstrong\u003e\u003cem\u003eLSTMs\u003c/em\u003e\u003c/strong\u003e, are another sort of specialized neurons for use in RNNs that are able to effectively learn what to remember and what to forget in sequence models.\u003c/p\u003e\n\u003cp\u003eLSTMs are generally like GRUs, except that they use three gates instead of two. LSTMs have:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003ean \u003cstrong\u003e\u003cem\u003eInput Gate\u003c/em\u003e\u003c/strong\u003e, which determines how much of the cell state that was passed along should be kept\u003c/li\u003e\n\u003cli\u003ea \u003cstrong\u003e\u003cem\u003eForget Gate\u003c/em\u003e\u003c/strong\u003e, which determines how much of the current state should be forgotten\u003c/li\u003e\n\u003cli\u003ean \u003cstrong\u003e\u003cem\u003eOutput Gate\u003c/em\u003e\u003c/strong\u003e, which determines how much of the current state should be exposed to the next layers in the network\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eAs you can see, they essentially accomplish the same thing as GRUs, but they do it in a slightly different way. Both models do a great job learning patterns from sequences, even when they are long and extremely complex! You'll find a diagram of a LSTM cell below. Just like with GRUs, don't worry about what the symbols mean or the math behind it. You can always pick that up later if you're curious. Instead, try to focus on how the information flows through this diagram from left to right, and where the various gates are for each function performed!\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-lstms-and-grus/master/images/new_LSTM3_chain.png\" width=\"800\"\u003e\u003c/p\u003e\n\u003cp\u003eThere's no good answer yet as to whether GRUs or LSTMs are superior to one another. In practice, GRUs tend to have a slight advantage in many use cases, but this is far from guaranteed. The best thing to do is to build a model with each and see which one does better.\u003c/p\u003e\n\u003ch2\u003eSummary\u003c/h2\u003e\n\u003cp\u003eIn this lesson, you learned about how LSTMs and GRUs can help the models avoid problems such as vanishing and exploding gradients when working with large sequences of data. You also learned about the structure of LSTMs and GRUs, and how they are able to \"forget\" information!\u003c/p\u003e","frontPage":false},{"exportId":"deep-neural-networks-introduction","title":"Deep Neural Networks - Introduction","type":"WikiPage","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-deep-learning-section-intro\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-deep-learning-section-intro/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eThis lesson summarizes the topics we'll be covering in this section and why they'll be important to you as a data scientist.\u003c/p\u003e\n\n\u003ch2\u003eMulti-Layer Perceptrons\u003c/h2\u003e\n\n\u003cp\u003eIn the previous section you learned a lot about how neural networks work. In this section, you'll learn why deeper networks sometimes lead to better results, and we'll generalize what you have learned before to get your matrix dimensions right for deep networks. You'll build deeper neural networks from scratch, and also learn how to build these using Keras.\u003c/p\u003e\n\n\u003ch2\u003eDeep Networks\u003c/h2\u003e\n\n\u003cp\u003eYou'll learn that deep representations are really good at automating what used to be a tedious and time-consuming process of feature engineering. In this section, you'll see that you can actually build a smaller but deeper neural network with exponentially less hidden units which performs even better than a network with more hidden units. The reason for this is that learning happens in each layer, and adding more layers (even with fewer limits) can lead to very powerful predictions! You'll learn about matrix notation for these deep networks and how to build a network like that from scratch.\u003c/p\u003e\n\n\u003ch2\u003eSummary\u003c/h2\u003e\n\n\u003cp\u003eIn this section, you'll extend your deep learning knowledge by learning about deeper neural networks. You'll also learn how to use Keras to build deep learning models!\u003c/p\u003e\n\u003cfooter class=\"fis-footer\" style=\"visibility: hidden;\"\u003e\u003cdiv class=\"fis-feedback\"\u003e\n\u003ch5\u003eHow do you feel about this lesson?\u003c/h5\u003e\n\u003cimg id=\"thumbs-up\" data-repository=\"dsc-deep-learning-section-intro\" title=\"Thumbs up!\" alt=\"thumbs up\"\u003e\u003cimg id=\"thumbs-down\" data-repository=\"dsc-deep-learning-section-intro\" title=\"Thumbs down!\" alt=\"thumbs down\"\u003e\n\u003c/div\u003e\n\u003ch5\u003eHave specific feedback? \u003ca href=\"https://github.com/learn-co-curriculum/dsc-deep-learning-section-intro/issues/new\"\u003eTell us here!\u003c/a\u003e\n\u003c/h5\u003e\u003c/footer\u003e","frontPage":false},{"exportId":"common-problems-with-clustering-algorithms","title":"Common Problems with Clustering Algorithms","type":"WikiPage","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-common-problems-with-clustering\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-common-problems-with-clustering/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eIn this lesson, we'll discuss some of the common problems often seen when attempting clustering with k-means or hierarchical agglomerative clustering.\u003c/p\u003e\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\u003cp\u003eYou will be able to:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eIdentify the problems that can arise from bad centroid initializations in k-means and bad initial groups in HAC\u003c/li\u003e\n\u003cli\u003eCompare and contrast k-means and hierarchical agglomerative clustering methodologies\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003eCommon Problems with Clustering\u003c/h2\u003e\n\u003cp\u003eWhen working with clustering algorithms, there are certain problems that we should always be aware of in order to help us prevent situations where we unknowingly accept the results of a bad clustering. Understanding the potential problems that can arise with each clustering algorithm also tends to provide greater insight into how each algorithm works.\u003c/p\u003e\n\u003cp\u003eThe most common issue is one that is applicable to all forms of clustering -- we have no way of verifying if the results of the cluster analysis are correct or not! Always try to keep this in mind when working with clustering algorithms, and never make the mistake of treating the results of a cluster analysis as ground-truth.\u003c/p\u003e\n\u003cp\u003eTo further drive this point home, let's spend some time looking at common problems with the two kinds of clustering algorithms we've talked about so far so that we can gain insight into the situations where clustering algorithms fall short.\u003c/p\u003e\n\u003ch2\u003eAdvantages \u0026amp; Disadvantages of K-Means Clustering\u003c/h2\u003e\n\u003cp\u003eThe advantages of the k-means clustering approach are:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eVery easy to implement!\u003c/li\u003e\n\u003cli\u003eWith many features, k-means is usually faster than HAC (as long as \u003cimg src=\"https://render.githubusercontent.com/render/math?math=k\"\u003e is reasonably small)\u003c/li\u003e\n\u003cli\u003eObjects are locked into the cluster they are first assigned to and can change as the centroids move around\u003c/li\u003e\n\u003cli\u003eClusters are often tighter than those formed by HAC\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eHowever, this algorithm often comes with several disadvantages:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eQuality of results depends on picking the right value for \u003cimg src=\"https://render.githubusercontent.com/render/math?math=k\"\u003e . This can be a problem when we don't know how many clusters to expect in our dataset\u003c/li\u003e\n\u003cli\u003eScaling our dataset will completely change the results\u003c/li\u003e\n\u003cli\u003eInitial start points of each centroid have a very strong impact on our final results. A bad start point can cause sub-optimal clusters (see example below)\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-common-problems-with-clustering/master/images/bad-centroid-start.gif\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"http://shabal.in/visuals/kmeans/right.gif\"\u003egif courtesy of Andrey A. Shabalin\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eThe animation above shows what can happen when we get a bad centroid initialization. Because of the random points that the centroids were initialized at, this led to one centroid cluster containing no points, while another cluster centroid has combined two clusters by being located in between them! Even though we had the correct value for \u003cimg src=\"https://render.githubusercontent.com/render/math?math=k\"\u003e (since we have 4 centroids, and data clearly contains 4 clusters), we ended up with incorrect results.\u003c/p\u003e\n\u003cp\u003eSince every dataset is different, and centroids are generated randomly, there is no way to make sure that we have good centroid initialization every time. One way to deal with this is to run a clustering algorithm multiple times, and keep track of how many times the same results come up. The good news here is that bad centroid initializations are typically much less likely than good centroid initializations, so the chances of getting bad results due to poor centroid initialization multiple times in a row are somewhat unlikely.\u003c/p\u003e\n\u003cp\u003eNow, let's take a look at HAC.\u003c/p\u003e\n\u003ch2\u003eAdvantages \u0026amp; Disadvantages of HAC\u003c/h2\u003e\n\u003cp\u003eHAC is useful as a clustering algorithm because:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eIt produces an ordered relationship between clusters, which can be useful when visualized\u003c/li\u003e\n\u003cli\u003eSmaller clusters are created. This allows us to get a very granular understanding of our dataset, and zoom in at the level where the clusters make the most sense to us (note the coloration of the lines in the example dendrogram above)\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eHowever, this algorithm is also built on some assumptions which can be disadvantages:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eResults are usually dependent upon the distance metric used\u003c/li\u003e\n\u003cli\u003eObjects can be grouped 'incorrectly' early on, with no way to relocate them. For instance, consider two points that belong to separate clusters, but are both nearer to each other than the center of the cluster they actually belong to (both are near the \"boundary\" between their cluster and the opposing cluster). These will be incorrectly grouped as a cluster, which will throw off the clustering of the groups they actually belong to, as well\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eLet's look at an example. Consider the circled points in the following plot:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-common-problems-with-clustering/master/images/new_bad-hac.png\" width=\"600\"\u003e\u003c/p\u003e\n\u003cp\u003eThe two points circled are from different clusters. However, they are right on the boundary between the two clusters, which has significant overlap between them. Because of this, there is a good chance that the clusters will meet the linkage criteria, and the HAC algorithm will group them together. The centroid of this new (incorrect) cluster is also close to many points on the boundary, meaning that it is quite likely that those points will be merged and the incorrect cluster will grow bigger. Early mistakes with the HAC algorithm tend to act as a bit of a slippery slope, and since HAC doesn't constantly reassign points like k-means does, this means that things can go from bad to worse if mistakes are made early on.\u003c/p\u003e\n\u003ch2\u003eA Note on Visualization\u003c/h2\u003e\n\u003cp\u003eSo far, we've checked our work by looking at visualizations of the clusters and using our eyes and our judgment to check if we agree with the results of the algorithm. However, it's worth remembering that this is highly unlikely to be an option on real-world data since we can't visualize any data with more than 3 dimensions. Because of this, it's often much harder to tell when a clustering algorithm has made a mistake, because we aren't able to use our eyes to confirm or deny the results!\u003c/p\u003e\n\u003ch2\u003eSummary\u003c/h2\u003e\n\u003cp\u003eIn this lesson, we learned about some of the challenges that come with clustering, and the relative advantages and disadvantages of k-means and hierarchical agglomerative clustering.\u003c/p\u003e","frontPage":false},{"exportId":"unsupervised-learning","title":"Unsupervised Learning","type":"WikiPage","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-unsupervised-learning\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-unsupervised-learning/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eIn this lesson, you'll get a high-level overview of unsupervised learning, an entire class of algorithms in machine learning. To date, you've only seen examples of supervised learning tasks such as regression and classification. As the name implies, unsupervised learning is a bit different than these tasks. In supervised learning, you define an \u003ccode\u003eX\u003c/code\u003e and \u003ccode\u003ey\u003c/code\u003e, and the algorithm attempts to generalize this transformation in order to predict \u003ccode\u003ey\u003c/code\u003e given \u003ccode\u003eX\u003c/code\u003e. In unsupervised learning, you do not define an \u003ccode\u003eX\u003c/code\u003e or \u003ccode\u003ey\u003c/code\u003e. Instead, you feed in a given dataset and the unsupervised learning algorithm returns some new representation of the data based on the structure and patterns within the data itself.\u003c/p\u003e\n\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\n\u003cp\u003eYou will be able to: \u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eDefine unsupervised learning \u003c/li\u003e\n\u003cli\u003eCompare and contrast supervised and unsupervised learning \u003c/li\u003e\n\u003cli\u003eIdentify real-world scenarios in which you would use unsupervised learning \u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch2\u003eSupervised vs. Unsupervised Learning\u003c/h2\u003e\n\n\u003cp\u003eThe main difference between supervised and unsupervised learning are their goals. Supervised learning needs concrete, ground-truth labels to train models that answer very specific questions. Unsupervised learning differs in that the task it is trying to accomplish is much less well-defined - it can usually be summed up as \"are there any natural patterns in this data that are recognizable?\"  To illustrate this, assume that you have a basket of various different kinds of fruit. A supervised learning task would be building an apple classifier that tells us if a given fruit is or isn't an apple, based on the size, shape, color, texture, taste, and any other data that you've encoded for each piece of fruit. An unsupervised learning task on the same data would analyze only the features, and sort them into groups without being told what type of fruit each was. In general, supervised learning uses data to accomplish a clear task while unsupervised learning has no clear task, but is instead used to identify patterns.\u003c/p\u003e\n\n\u003ch2\u003eUnsupervised Learning Tasks\u003c/h2\u003e\n\n\u003cp\u003eThe two most common unsupervised learning tasks are clustering and dimensionality reduction. Clustering groups data into homogeneous groups, where members share common traits. Dimensionality reduction attempts to reduce the overall number of features of a dataset while preserving as much information as possible. With that, let's take a deeper look into some general notes on each.\u003c/p\u003e\n\n\u003ch3\u003eClustering\u003c/h3\u003e\n\n\u003cp\u003eThere are a few different kinds of clustering algorithms, but they all do the same thing - finding different ways to group a dataset based on patterns in the data.  One common use-case for clustering is market segmentation. In market segmentation, you would try to decompose an audience into subsets for more precise targeting for business purposes, such as advertising. Even though there's no way to verify that these groups are correct, in practice it usually does quite well, often providing useful subgroups which can then be individually examined.\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-unsupervised-learning/master/images/kmeans.gif\"\u003e\u003c/p\u003e\n\n\u003cblockquote\u003e\n\u003cp\u003eSource: \u003ca href=\"https://dashee87.github.io/data%20science/general/Clustering-with-Scikit-with-GIFs/\"\u003eGIF by David Sheehan\u003c/a\u003e\u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003ch3\u003eDimensionality Reduction\u003c/h3\u003e\n\n\u003cp\u003eThe most common dimensionality reduction algorithm is Principal Component Analysis (PCA). Dimensionality reduction algorithms work by projecting data from its current n-dimensional subspace into a smaller subspace, while losing as little information as possible in the process. Dimensionality reduction algorithms still lose \u003cem\u003esome\u003c/em\u003e information, but you can quantify this information loss to make an informed decision about the number of dimensions reduced versus the overall information lost. Dimensionality reduction algorithms are a must-have in any data scientist's toolbox, because they provide a way for us to deal with the \u003cstrong\u003eCurse of Dimensionality\u003c/strong\u003e. The curse of dimensionality is a key concept as datasets scale. In short, as the number of features in a dataset increases, the processing power and search space required to optimize a given machine learning algorithm explodes exponentially. Because this often creates intractable computational problems, dimensionality reduction techniques such as PCA can be an essential preprocessing technique.\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-unsupervised-learning/master/images/pca.gif\"\u003e\u003c/p\u003e\n\n\u003cblockquote\u003e\n\u003cp\u003eSource: \u003ca href=\"https://stats.stackexchange.com/questions/2691/making-sense-of-principal-component-analysis-eigenvectors-eigenvalues/140579#140579\"\u003eGIF by amoeba\u003c/a\u003e\u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003ch2\u003eSummary\u003c/h2\u003e\n\n\u003cp\u003eIn this lesson, you explored the differences between supervised and unsupervised learning. You also learned about the types of problems we can solve with unsupervised learning, including clustering and dimensionality reduction. \u003c/p\u003e\n\u003cfooter class=\"fis-footer\" style=\"visibility: hidden;\"\u003e\u003cdiv class=\"fis-feedback\"\u003e\n\u003ch5\u003eHow do you feel about this lesson?\u003c/h5\u003e\n\u003cimg id=\"thumbs-up\" data-repository=\"dsc-unsupervised-learning\" title=\"Thumbs up!\" alt=\"thumbs up\"\u003e\u003cimg id=\"thumbs-down\" data-repository=\"dsc-unsupervised-learning\" title=\"Thumbs down!\" alt=\"thumbs down\"\u003e\n\u003c/div\u003e\n\u003ch5\u003eHave specific feedback? \u003ca href=\"https://github.com/learn-co-curriculum/dsc-unsupervised-learning/issues/new\"\u003eTell us here!\u003c/a\u003e\n\u003c/h5\u003e\u003c/footer\u003e","frontPage":false},{"exportId":"introduction-to-graph-theory","title":"Introduction to Graph Theory","type":"WikiPage","content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-intro-graph-theory\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-intro-graph-theory\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-intro-graph-theory/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eIn this lesson, you'll get an introduction to some basic terminology regarding graphs and graph theory. To start, here's a graph!\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-intro-graph-theory/master/images/graph1.png\"\u003e\u003c/p\u003e\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\u003cp\u003eYou will be able to:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eExplain what nodes and edges are in graph theory\u003c/li\u003e\n\u003cli\u003eExplain the difference between directed and undirected graphs\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003eNodes and Edges: The Building Blocks of Graphs\u003c/h2\u003e\n\u003cp\u003eTo start, graphs are composed of two primary objects: \u003cstrong\u003enodes\u003c/strong\u003e and \u003cstrong\u003eedges\u003c/strong\u003e. In the picture above, the nodes are the circles, while the lines that connect them are edges. Typically, nodes represent some entity such as a person, businesses, places, or webpages. In turn, edges then represent the relationships between these entities. For example, you might have a graph of a social network in which each node represents a person, and each edge represents whether those two individuals are connected or friends within the network.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-intro-graph-theory/master/images/graph2.png\"\u003e\u003c/p\u003e\n\u003cp\u003eAs you can see, Jen is a well connected character in this scenario: she has a connecting edge with literally every other node in the graph! On the other hand, Jake is the least connected. He has no other connections other than Jen.\u003c/p\u003e\n\u003ch2\u003eDirected vs Undirected Graphs\u003c/h2\u003e\n\u003cp\u003eAnother important concept in graph theory is the difference between directed and undirected graphs. The previous two examples have demonstrated undirected graphs. As the name implies, the edges in an undirected graph represent a mutual connection between two nodes. For example, the previous undirected graph could represent a mutual relationship such as \"Friends\" on Facebook or \"Connections\" on LinkedIn. In contrast, a direct graph looks like this:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-intro-graph-theory/master/images/graph3.png\"\u003e\u003c/p\u003e\n\u003cp\u003eAs you can see, each of the edges now has an arrow indicating a direction. This scenario could represent an alternative type of social network such as Twitter in which individual's relationships are not necessarily mutual. Instead, Twitter users can follow other users to stay up to date with their activity. In the graph depicted above, Sally isn't following anyone. However, both Bob and Jen are following Sally. There is also one mutual relationship depicted: Jake is following Jen and she is also following him.\u003c/p\u003e\n\u003ch2\u003eConnectedness\u003c/h2\u003e\n\u003cp\u003eConnectedness aims to quantify the number of edges attached to a node. In the graphs above, Jen is undoubtedly the most connected of the individuals depicted. In the undirected graph, she was connected to everyone. Similarly, if your goal is to become an \u003cem\u003einfluencer\u003c/em\u003e, you're going to need to develop quite the following and become a very connected node. You'll explore more details in how connectedness is quantified in the upcoming lessons. For now, take some time to think about other implications of connectedness. For example, how might you be able to use connectedness to determine friend circles or cliques in social networks?\u003c/p\u003e\n\u003ch2\u003ePath Searching\u003c/h2\u003e\n\u003cp\u003ePath searching algorithms aim to find the shortest distance between any two nodes. This can then be used as a distance metric between nodes. Additionally, this can have interesting implications. For example, in a graph network of a website, a path searching algorithm might outline how many steps are required for a customer to move from the homepage, to browsing for an item, all the way through completing their purchase at checkout. You've actually already seen some basic examples of path searching algorithms in your work with traversing JSON files. There, you took a look at developing breadth-first versus depth-first recursive procedures to create an outline of the structure of an arbitrary JSON file.\u003c/p\u003e\n\u003ch2\u003eSummary\u003c/h2\u003e\n\u003cp\u003eIn this lesson, you got a brief introduction to graph theory, including some basic definitions and foundational concepts. Remember that graphs are composed of primary objects called nodes and the relationships between those objects, known as edges. Additionally, graphs can be directed or undirected depending on the nature of the edges and the relationships between nodes.\u003c/p\u003e","frontPage":false},{"exportId":"the-curse-of-dimensionality","title":"The Curse of Dimensionality","type":"WikiPage","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-curse-of-dimensionality\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-curse-of-dimensionality/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eThe curse of dimensionality is an interesting paradox for data scientists. On the one hand, one often hopes to garner more information to improve the accuracy of a machine learning algorithm. However, there are also some interesting phenomena that come along with larger datasets. In particular, the curse of dimensionality is based on the exploding volume of n-dimensional spaces as the number of dimensions, n, increases.\u003c/p\u003e\n\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\n\u003cp\u003eYou will be able to:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eExplain what is meant by the curse of dimensionality and its implications when training machine learning algorithms \u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch2\u003eSparseness in N-Dimensional Space\u003c/h2\u003e\n\n\u003cp\u003ePoints in n-dimensional space become increasingly sparse as the number of dimensions increases. That is, the distance between points will continue to grow as the number of dimensions grows. This can be problematic in a number of machine learning algorithms, in particular, when clustering points into groups. Due to the exploding nature of n-dimensional space, there is also an unwieldy number of possible combinations when searching for optimal parameters for a machine learning algorithm. \u003c/p\u003e\n\n\u003cp\u003eTo demonstrate this, you'll generate this graph in the upcoming lab:  \u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-curse-of-dimensionality/master/images/sparsity.png\"\u003e\u003c/p\u003e\n\n\u003cp\u003eThis image demonstrates how the average distance between points and the origin continues to grow as the number of dimensions increases, even though each dimension has a fixed range. Simply increasing the number of dimensions continues to make individual points more and more sparse.\u003c/p\u003e\n\n\u003ch2\u003eImplications\u003c/h2\u003e\n\n\u003cp\u003eThe main implication of the curse dimensionality is that optimization problems can become infeasible as the number of features increases. The practical limit will vary based on your particular computer and the time that you have to invest in a problem. As you'll see in the upcoming lab, this relationship is exponential. For machine learning algorithms that involve backpropagation, or iterative convergence, including Lasso and Ridge regression, this will drastically impact the size of feasible solvable problems.\u003c/p\u003e\n\n\u003cp\u003eThe sparsity of points also has additional consequences. Due to the sheer scale of potential points in an n-dimensional space, as n continues to grow, the probability of seeing a particular point (or even nearby point) continues to plummet. Therefore, it is likely that there are entire regions of an n-dimensional space that have yet to be explored. As such, if no such information from the training set is available regarding such cases, then making predictions regarding these cases will be guesswork. Put another way, with the increasing sparsity of points, you have an ever decreasing proportionate sample of the space. For example, a thousand observations in a 3-dimensional space might be quite powerful and provide sufficient information to determine a relevant classification or regression model. However, a thousand observations in a million-dimensional space is likely to be utterly useless in determining which features are most influential and to what degree. \u003c/p\u003e\n\n\u003ch2\u003eSummary\u003c/h2\u003e\n\n\u003cp\u003eThe curse of dimensionality presents an intriguing paradox. On the one hand, more features allow one to account for variance and nuances required to accurately model a given machine learning model. On the other hand, as the number of dimensions increases, the accompanying volume of the hyperspace explodes exponentially. As such, the potential amount of information required to accurately model such a space becomes increasingly complex. (This is not always the case; a simple line can still exist in a 10-dimensional space, but the problems one is likely to be tackling when employing 10 features are most likely more complex than a 2-dimensional model.) With this, more and more observations will be required to produce an adequate model.\u003c/p\u003e\n\u003cfooter class=\"fis-footer\" style=\"visibility: hidden;\"\u003e\u003cdiv class=\"fis-feedback\"\u003e\n\u003ch5\u003eHow do you feel about this lesson?\u003c/h5\u003e\n\u003cimg id=\"thumbs-up\" data-repository=\"dsc-curse-of-dimensionality\" title=\"Thumbs up!\" alt=\"thumbs up\"\u003e\u003cimg id=\"thumbs-down\" data-repository=\"dsc-curse-of-dimensionality\" title=\"Thumbs down!\" alt=\"thumbs down\"\u003e\n\u003c/div\u003e\n\u003ch5\u003eHave specific feedback? \u003ca href=\"https://github.com/learn-co-curriculum/dsc-curse-of-dimensionality/issues/new\"\u003eTell us here!\u003c/a\u003e\n\u003c/h5\u003e\u003c/footer\u003e","frontPage":false},{"exportId":"market-segmentation-with-clustering","title":"Market Segmentation with Clustering","type":"WikiPage","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-market-segmentation-clustering\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-market-segmentation-clustering/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eIn this lesson, we'll learn about one of the most popular use cases for clustering in the business world -- market segmentation!\u003c/p\u003e\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\u003cp\u003eYou will be able to:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eExplain market segmentation and how clustering can be used for it\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003eWhat is Market Segmentation?\u003c/h2\u003e\n\u003cp\u003ePerhaps the most common use case for clustering algorithms in the real world, \u003cstrong\u003e\u003cem\u003eMarket Segmentation\u003c/em\u003e\u003c/strong\u003e refers to using \u003cstrong\u003e\u003cem\u003eCluster Analysis\u003c/em\u003e\u003c/strong\u003e to segment a customer base into different \u003cem\u003emarket segments\u003c/em\u003e using the clustering techniques we've learned.\u003c/p\u003e\n\u003cp\u003eConsider the following scenario: You're a movie executive, and you have a new superhero film coming out. You need to decide how to best allocate your advertising budget in order to attract the most customers. This film is a sequel, so you have good demographic data on who went to see the last film. The advertising options available to you are TV, newspaper, radio, and internet. How do you best allocate your advertising budget to ensure that the movie does as well as possible?\u003c/p\u003e\n\u003cp\u003eThe answer depends on your data. A regression analysis on last year's data can give you a general idea of how much you can expect to make overall, assuming that there aren't major differences between last year and this year. However, regression just tells you what you can expect \u003cem\u003eoverall\u003c/em\u003e -- what if we're trying to optimize where we spend our money, rather than just predict what the returns will be, based on the overall amount of money we spent?\u003c/p\u003e\n\u003cp\u003eThe answer lies in knowing who your customer is. All forms of advertising are not consumed equally by every age group or demographic. By identifying \u003cstrong\u003e\u003cem\u003esegments\u003c/em\u003e\u003c/strong\u003e in our customer data, we can look for trends that identify one group or another, and create personalized regression models for each group.\u003c/p\u003e\n\u003cp\u003eIn order to understand this better, let's take a sample question that market segmentation can help us answer. For our TV advertising budget, we still have to decide what channel to run our commercials on. What effect will advertising on the Disney channel have on a person's likelihood of coming to see our superhero movie? If the person in question is 12 years old, then it's probably very likely that our commercial convinces this person to see our movie. But what about if they're 68 years old? In that case, advertising during a cartoon on the Disney channel might not be the most effective way to reach that person. If we're worried about reaching this customer, the first question we should ask is what kind of customer they are. In the case of a superhero movie, we can likely assume that all things equal, a 12-year-old child is more likely to be interested in seeing a superhero movie after seeing our commercial than a 68-year-old, so we should probably pay attention to what the data tells us about how 12-year-olds are affected by each type of media advertisement we can use!\u003c/p\u003e\n\u003cp\u003e\u0026nbsp;\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-market-segmentation-clustering/master/images/new_old-man-little-boy-talking.png\" width=\"600\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e\u003cem\u003eTwo potential customers deep in conversation about what movie to see\u003c/em\u003e\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eYou can bet that movie studio executives have complex, well-defined models to predict their Return on Investment (RoI) for things as granular as advertising on Disney, versus advertising on the History channel. This is because different market segments of customers behave differently, and market segmentation allows us to zoom in on those groups!\u003c/p\u003e\n\u003ch3\u003eIdentifying Market Segmentation\u003c/h3\u003e\n\u003cp\u003eAt the most basic level, market segmentation allows us to look at our data and identify which customers belong to which groups. Once we have this information, we can examine each individual segment and use it to answer important questions and build individual, targeted models for each segment.\u003c/p\u003e\n\u003cp\u003eOnce we understand our market segments, then we can begin making informed decisions that are specific to each segment. So how do we find these market segments?\u003c/p\u003e\n\u003cp\u003eWith clustering, of course! By definition, market segments are groups within our dataset with substantive differences between them. A segment only matters to us if it is different from other groups -- we don't really care about identifying the segment of children with red hair versus children with brown hair in our data if they both act the same way and have the same level of interest in our movie. Before data scientists became commonplace, this sort of segmentation was usually handled by marketers using their intuition about their customer base to create \u003cem\u003ecustomer personas\u003c/em\u003e, and then seek out data to back up their assumptions. As data scientists, we know that the best option is not to seek data to confirm our beliefs -- instead, it is to pull our beliefs from evidence in the data. Clustering provides a great way for us to allow the data to tell us what is and isn't significant -- lest we get caught up chasing down market segments that aren't actually all that different -- or worse, don't actually exist at all!\u003c/p\u003e\n\u003ch2\u003eSegmentation and Targeting\u003c/h2\u003e\n\u003cp\u003eIn modern business analytics, segmentation is only the first step.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-market-segmentation-clustering/master/images/new_marketing-strategy.png\" width=\"700\"\u003e\u003c/p\u003e\n\u003cp\u003eAfter we've identified the different market segments, the next step is to build individualized strategies to \u003cstrong\u003e\u003cem\u003eTarget\u003c/em\u003e\u003c/strong\u003e them! In the movie example we used above, we would first start by answering questions such as \"which market segment is most valuable to us?\" This can be answered through research or through analyzing our data, or a combination of both. Once we realized that the 12-to-18-year-old demographic is most valuable to us, we can then decide how to target them in the most effective way possible. This brings us back to our earlier question -- how do we allocate our advertising budget? If we've used regression to determine that we're most likely to get the return on investment for our advertising dollars with the 12-to-18-year-old age group, then our next step is to determine which ad channels are most effective to us. We'll likely find that TV advertisements and internet ads are very effective at reaching this particular market segment, but radio is less effective (since a solid portion of the target segment can't yet drive), and newspaper ads are unlikely to reach them at all (because when is the last time you saw a 12-year-old read a newspaper?).\u003c/p\u003e\n\u003cp\u003eThe third step in this process is a bit outside the scope of clustering. This is where the marketing team really shines -- figuring out how to position our product to make it both as desirable as possible to a given segment, while also making our product stand out from competitors.\u003c/p\u003e\n\u003cp\u003eLet's look at one more example to consider what this looks like: car advertisements!\u003c/p\u003e\n\u003cp\u003eScenario: You are the newest data scientist at Tesla Motors. Next year, you are introducing a new SUV in the $30-50k price range. The SUV is roomy, spacious, fast, and affordable, in addition to having a very high safety rating and a ton of technological bells and whistles. One day, Elon Musk asks you (presumably, on Twitter) who your valuable market segments are, and what parts of the car he should highlight in several upcoming interviews. How do you answer this question?\u003c/p\u003e\n\u003cp\u003ePresumably, the first thing you would do is to look at the results of your market segmentation and identify the most profitable market segments to target. Once you know who these segments are, you can target them with ads -- but this only brings us to the second step in our diagram above. The third step means personalizing these ads to have maximum impact on a given targeted segment. Is your target market middle-class families? Then maybe it makes sense to highlight the car's affordability, space, and safety rating. What about if your target is upper-middle-class customers between 30 and 40 years of age that enjoy luxury cars? In that case, you'd probably focus on the speed, luxury, and looks of the car, because they're more likely to care about these qualities than the others.\u003c/p\u003e\n\u003cp\u003e\u0026nbsp;\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-market-segmentation-clustering/master/images/new_market_seg.png\" width=\"70%\" height=\"70%\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e\u003cem\u003eWhen you know your market segment, you can market to them in the most effective way possible!\u003c/em\u003e\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eStep 3 in this process is usually done with the help of survey data, under the umbrella of \u003cem\u003eUser Research\u003c/em\u003e. This is not something that data scientists typically have to worry about too much, as it is a different domain of expertise. However, the first two stages are very much something that data scientists can expect to do multiple times in their career!\u003c/p\u003e\n\u003ch2\u003eSummary\u003c/h2\u003e\n\u003cp\u003eIn this lesson, we learned about how cluster analysis can be applied to determine market segmentation, and how these market segments are used in the real world to plan and execute effective business strategies!\u003c/p\u003e\n\u003cfooter class=\"fis-footer\" style=\"visibility: hidden;\"\u003e\n\u003cdiv class=\"fis-feedback\"\u003e\n\u003ch5\u003eHow do you feel about this lesson?\u003c/h5\u003e\n\u003cimg id=\"thumbs-up\" title=\"Thumbs up!\" alt=\"thumbs up\" data-repository=\"dsc-market-segmentation-clustering\"\u003e\u003cimg id=\"thumbs-down\" title=\"Thumbs down!\" alt=\"thumbs down\" data-repository=\"dsc-market-segmentation-clustering\"\u003e\n\u003c/div\u003e\n\u003ch5\u003eHave specific feedback? \u003ca href=\"https://github.com/learn-co-curriculum/dsc-market-segmentation-clustering/issues/new\"\u003eTell us here!\u003c/a\u003e\n\u003c/h5\u003e\n\u003c/footer\u003e","frontPage":false},{"exportId":"arma-models","title":"ARMA Models","type":"WikiPage","content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-arma-models\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-arma-models\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-arma-models/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eYou've seen two basic time series models now, the random walk and white noise models. In this lesson, you'll learn about two other very important time series models that are widely used to understand and predict future values in stochastic processes: the Autoregressive (AR) and Moving Average (MA) models.\u003c/p\u003e\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\u003cp\u003eYou will be able to:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eExplain what autoregressive means in an autoregressive model\u003c/li\u003e\n\u003cli\u003eExplain what a moving average model means\u003c/li\u003e\n\u003cli\u003eDescribe how AR and MA can be combined to form an ARMA model\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003eThe Autoregressive Model\u003c/h2\u003e\n\u003cp\u003eAn autoregressive (AR) model is when a value from a time series is regressed on previous values from the same time series.\u003c/p\u003e\n\u003cp\u003eIn words, the mathematical idea is the following:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Ctext%7BToday%20=%20constant%20%2b%20slope%7D%20%5Ctimes%20%5Ctext%7Byesterday%20%2b%20noise%7D\"\u003e\u003c/p\u003e\n\u003cp\u003eOr, mathematically:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Clarge%20Y_t%20=%20%5Cmu%20%2b%20%5Cphi%20*%20Y_%7Bt-1%7D%2b%5Cepsilon_t\"\u003e\u003c/p\u003e\n\u003cp\u003eSome notes based on this formula: - If the slope is 0, the time series is a white noise model with mean \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cmu\"\u003e - If the slope is not 0, the time series is autocorrelated - Bigger slope means bigger autocorrelation - When there is a negative slope, the time series follows an oscillatory process\u003c/p\u003e\n\u003cp\u003eWe simulated some time series below. Have a look at them, and make sure this follows your intuition looking at the formula.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-arma-models/master/images/AR_model.png\"\u003e\u003c/p\u003e\n\u003cp\u003eNote that simply having a value for \u003cem\u003ephi\u003c/em\u003e ( \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cphi\"\u003e ) slightly bigger than 1, the time series clearly goes in one direction. Note the scale of the y-axis, where the y-axis scale for all the other processes is between -10 and 10, the last time series goes down to values of -100.\u003c/p\u003e\n\u003cp\u003eLet's look at the autocorrelation plots as well.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-arma-models/master/images/AR_ACF.png\"\u003e\u003c/p\u003e\n\u003cp\u003eThe oscillatory process of the time series with \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cphi=0.9\"\u003e is reflected in the autocorrelation function, returning an oscillatory autocorrelation function as well. \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cphi=0.2\"\u003e leads to a very low, insignificant, autocorrelation. \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cphi=0.8\"\u003e leads to a strong autocorrelation for the first few lags and then incurs a steep decline. Having a \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cphi=1.02\"\u003e (just slightly bigger than 1) leads to strong and long-lasting autocorrelation.\u003c/p\u003e\n\u003cp\u003eNext, let's look at the partial autocorrelation plots.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-arma-models/master/images/AR_PACF.png\"\u003e\u003c/p\u003e\n\u003cp\u003eFor each of these PACFs, we notice a high value for 1 lag, then autocorrelations of 0, except for the second one. This is no big surprise, as the slope parameter is fairly small, so the relationship between a value and the next one is fairly limited.\u003c/p\u003e\n\u003ch2\u003eThe Moving Average Model\u003c/h2\u003e\n\u003cp\u003eThe Moving Average model can be described as the weighted sum of today's and yesterday's noise.\u003c/p\u003e\n\u003cp\u003eIn words, the mathematical idea is the following:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Ctext%7BToday%20=%20Mean%20%2b%20Noise%20%2b%20Slope%7D%20%5Ctimes%20%5Ctext%7Byesterday's%20noise%7D\"\u003e\u003c/p\u003e\n\u003cp\u003eOr, mathematically:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Clarge%20Y_t%20=%20%5Cmu%20%2b%5Cepsilon_t%20%2b%20%5Ctheta%20*%20%5Cepsilon_%7Bt-1%7D\"\u003e\u003c/p\u003e\n\u003cp\u003eSome notes based on this formula: - If the slope is 0, the time series is a white noise model with mean \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cmu\"\u003e - If the slope is not 0, the time series is autocorrelated and depends on the previous white noise process - Bigger slope means bigger autocorrelation - When there is a negative slope, the time series follow an oscillatory process\u003c/p\u003e\n\u003cp\u003eFor the Moving Average Model we also simulated some time series with varying parameters below.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-arma-models/master/images/MA_model.png\"\u003e\u003c/p\u003e\n\u003cp\u003eWhen there is a positive \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Ctheta\"\u003e there is a certain persistence in level, meaning that each observation is generally close to its neighbors. This is more pronounced for higher values of \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Ctheta\"\u003e . MA series with negative coefficients, however, show oscillatory patterns. Recall that when \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Ctheta=0\"\u003e , the process is a true white noise process!\u003c/p\u003e\n\u003cp\u003eLet's look at the ACF plots.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-arma-models/master/images/MA_ACF.png\"\u003e\u003c/p\u003e\n\u003cp\u003eRemember that MA processes have autocorrelations, but because of the structure of the MA formula (regressing it on the noise term of the previous observation) there is only a dependence for one period, and the autocorrelation is zero for lags 2 and higher.\u003c/p\u003e\n\u003cp\u003eIf \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Ctheta%20\u003e0%22\u003e%20the%20lag%20one%20autocorrelation%20is%20positive,%20if%20%20\u003cimg%20src=\"\u003e the lag one autocorrelation is negative.\u003c/p\u003e\n\u003cp\u003eNext, let's look at the partial autocorrelation plots.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-arma-models/master/images/MA_PACF.png\"\u003e\u003c/p\u003e\n\u003cp\u003eFor PACFs, a typical structure is that there is a strong correlation with the 1-period lag (strength depending on \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Ctheta\"\u003e ), and then the PACF gradually tails off. You can particularly observe this for \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Ctheta=0.9\"\u003e and \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Ctheta=-0.95\"\u003e .\u003c/p\u003e\n\u003ch2\u003eHigher-order AR and MA models\u003c/h2\u003e\n\u003cp\u003eLet's look at the formulas of AR and MA again:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eAR: \u003cimg src=\"https://render.githubusercontent.com/render/math?math=Y_t%20=%20%5Cmu%20%2b%20%5Cphi%20*%20Y_%7Bt-1%7D%2b%5Cepsilon_t\"\u003e\n\u003c/li\u003e\n\u003cli\u003eMA: \u003cimg src=\"https://render.githubusercontent.com/render/math?math=Y_t%20=%20%5Cmu%20%2b%5Cepsilon_t%20%2b%20%5Ctheta%20*%20%5Cepsilon_%7Bt-1%7D\"\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eNote that these models are constructed in a way that processes only depend directly on the previous observation in the process. These are known as \"1st order models\", and denoted by AR(1) and MA(1) processes respectively. Let's look at AR(2) and MA(2).\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eAR(2): \u003cimg src=\"https://render.githubusercontent.com/render/math?math=Y_t%20=%20%5Cmu%20%2b%20%5Cphi_1%20*%20Y_%7Bt-1%7D%2b%5Cphi_2%20*%20Y_%7Bt-2%7D%2b%5Cepsilon_t\"\u003e\n\u003c/li\u003e\n\u003cli\u003eMA(2): \u003cimg src=\"https://render.githubusercontent.com/render/math?math=Y_t%20=%20%5Cmu%20%2b%5Cepsilon_t%20%2b%20%5Ctheta_1%20*%20%5Cepsilon_%7Bt-1%7D%2b%20%5Ctheta_2%20*%20%5Cepsilon_%7Bt-2%7D\"\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eNeedless to say, this can be extended to higher-orders as well! Generally, the order of an AR model is denoted \u003cimg src=\"https://render.githubusercontent.com/render/math?math=p\"\u003e , and the order of an MA model is denoted \u003cimg src=\"https://render.githubusercontent.com/render/math?math=q\"\u003e .\u003c/p\u003e\n\u003ch2\u003eACF and PACF intuition for AR(p) and MA(q)\u003c/h2\u003e\n\u003cp\u003eA quick overview of how higher order models affect the ACF and PACF:\u003c/p\u003e\n\u003ch3\u003eAR(p)\u003c/h3\u003e\n\u003cp\u003eConsidering a time series that was generated by an autoregression (AR) process with an order of \u003cimg src=\"https://render.githubusercontent.com/render/math?math=p\"\u003e , we would expect the ACF plot for the AR(p) time series to be strong to a lag of \u003cimg src=\"https://render.githubusercontent.com/render/math?math=p\"\u003e and remain stagnant for subsequent lag values, trailing off at some point as the effect is weakened. The PACF, on the other hand, describes the direct relationship between an observation and its lag. This generally leads to no correlation for lag values beyond \u003cimg src=\"https://render.githubusercontent.com/render/math?math=p\"\u003e .\u003c/p\u003e\n\u003ch3\u003eMA(q)\u003c/h3\u003e\n\u003cp\u003eWith a time series generated by a moving average (MA) process with an order \u003cimg src=\"https://render.githubusercontent.com/render/math?math=q\"\u003e , we would expect the ACF for the MA(q) process to show a strong correlation with recent values up to the lag of \u003cimg src=\"https://render.githubusercontent.com/render/math?math=q\"\u003e , then an immediate decline to minimal or no correlation. For the PACF, we would expect the plot to show a strong relationship to the lag and then a tailing off to no correlation from the lag onwards.\u003c/p\u003e\n\u003ch2\u003eARMA models\u003c/h2\u003e\n\u003cp\u003eNow that we've seen AR and MA models, it is important to note that \u003cstrong\u003ethere is no reason why AR and MA models would not coexist\u003c/strong\u003e. That's where ARMA models come in, which basically means that in this model, a regression on past values takes place (AR part) and also that the error term is modeled as a linear combination of error terms of the recent past (MA part). Generally, one denotes ARMA as ARMA(p, q).\u003c/p\u003e\n\u003cp\u003eAn ARMA(2,1) model is given by:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=Y_t%20=%20%5Cmu%20%2b%20%5Cphi_1%20Y_%7Bt-1%7D%2b%5Cphi_2%20Y_%7Bt-2%7D%2b%20%5Ctheta%20%5Cepsilon_%7Bt-1%7D%2b%5Cepsilon_t\"\u003e\u003c/p\u003e\n\u003cp\u003eA short table to summarize ACF and PACF for AR(p), MA(q), and ARMA(p, q):\u003c/p\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003e\u003c/th\u003e\n\u003cth\u003eAR(p)\u003c/th\u003e\n\u003cth\u003eMA(q)\u003c/th\u003e\n\u003cth\u003eARMA(p, q)\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd\u003eACF\u003c/td\u003e\n\u003ctd\u003eTails off\u003c/td\u003e\n\u003ctd\u003eCuts off after lag q\u003c/td\u003e\n\u003ctd\u003eTails off\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003ePACF\u003c/td\u003e\n\u003ctd\u003eCuts off after lag p\u003c/td\u003e\n\u003ctd\u003eTails off\u003c/td\u003e\n\u003ctd\u003eTails off\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003ch2\u003eNote on modeling\u003c/h2\u003e\n\u003cp\u003eSeeing the table above, you might get an idea of why ACF and PACF are so useful when modeling! What you generally will try to do for any time series analysis is:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eDetrend your time series using differencing. ARMA models represent stationary processes, so we have to make sure there are no trends in our time series\u003c/li\u003e\n\u003cli\u003eLook at ACF and PACF of the time series\u003c/li\u003e\n\u003cli\u003eDecide on the AR, MA, and order of these models\u003c/li\u003e\n\u003cli\u003eFit the model to get the correct parameters and use for prediction\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003eAdditional resources\u003c/h2\u003e\n\u003cp\u003eTo learn more about AR, MA, and ARMA, have a look at lessons 1 and 2 \u003ca href=\"https://onlinecourses.science.psu.edu/stat510/node/41/\"\u003ehere\u003c/a\u003e.\u003c/p\u003e\n\u003ch2\u003eSummary\u003c/h2\u003e\n\u003cp\u003eGreat! Now that you have learned the basics of AR, MA, and ARMA models, let's look at some time series and how to model them in the next lesson!\u003c/p\u003e","frontPage":false},{"exportId":"introduction-to-recommendation-systems","title":"Introduction to Recommendation Systems","type":"WikiPage","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-recommendation-system-introduction\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-recommendation-system-introduction/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eThis lesson will give you a brief introduction to recommendation system modeling approaches. We will develop intuition into how these systems work and how collaborative filtering is used to make accurate recommendation systems that can harness the power of big data.\u003c/p\u003e\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\u003cp\u003eYou will be able to:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eDescribe the role and rationale for recommendation systems\u003c/li\u003e\n\u003cli\u003eDescribe collaborative filtering recommender systems and their benefits/limitations\u003c/li\u003e\n\u003cli\u003eDescribe content-based recommenders and their benefits/limitations\u003c/li\u003e\n\u003cli\u003eDefine implicit and explicit rating systems\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003eProblem Domain\u003c/h2\u003e\n\u003cp\u003eThe goal of a recommendation system is to expose people to items that they will like. In exact terms, a recommendation system predicts the future preference of a set of items for a user, and recommends the top items from this set. In today's world, due to the internet and its global reach, people have more options to choose from than ever before.\u003c/p\u003e\n\u003cp\u003eConsider buying an album from a traditional music store where the options are always limited and mainly depend upon size and type of the store. There is a physical limitation to how many songs, albums, and artists can be offered. An online product like Spotify, however has a much higher ceiling in terms of storage space. With this new method of selecting products, recommendation systems are a popular way for users to sort through millions of songs to find the ones that are customized exactly for them. Recommendation systems cast a direct impact on profitability and customer satisfaction for most businesses today. With the nearly limitless options consumers have for products online, they need some guidance!\u003c/p\u003e\n\u003cp\u003eThis idea can be represented by a concept called the \"Long Tail,\" which is a set of statistical distributions that have a very long \"tail\" of the distribution, representing many occurrences of low frequency things. In the context of consumer products, there are some products that everyone is going to buy: light bulbs, toilet paper, bread etc. There are also items that are far more obscure: specific toys, sports equipment, movies. Recommendation systems are made to help consumers tap into this long tail to assist them in picking from the endless number of options that are made available to them via the internet.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-recommendation-system-introduction/master/images/LongTailConcept.png\" width=\"500\"\u003e\u003c/p\u003e\n\u003cp\u003eHere's a formal definition of recommendation systems from authors \u003ca href=\"https://misq.org/e-commerce-product-recommendation-agents-use-characteristics-and-impact.html\"\u003eBo Xiao and Izak Benbasat, 2017\u003c/a\u003e :\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\"Recommendation Systems are software agents that elicit the interests and preferences of individual consumers [] and make recommendations accordingly. They have the potential to support and improve the quality of the decisions consumers make while searching for and selecting products online.\"\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch2\u003eApplications of Recommendation Systems\u003c/h2\u003e\n\u003cp\u003eLets understand what all recommendation systems can do for businesses:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eHelp in suggesting the merchants/items which a customer might be interested in after buying a product in a marketplace\u003c/li\u003e\n\u003cli\u003eEstimate profit \u0026amp; loss of many competing items and make recommendations to the customer (e.g. buying and selling stocks)\u003c/li\u003e\n\u003cli\u003eBased on the experience of the customer, recommend a customer centric or product centric offering\u003c/li\u003e\n\u003cli\u003eEnhance customer engagement by providing offers which can be highly appealing to the customer\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003eRecommendation Systems Approaches\u003c/h2\u003e\n\u003cp\u003eThere are two main types of recommendation systems: unpersonalized and personalized. In the majority of this section, we will focus on personalized recommendation systems because that's where data scientists can provide the most value to companies, but to start off, let's investigate some unpersonalized systems because they can be productive in their own right.\u003c/p\u003e\n\u003ch3\u003eUnpersonalized Recommendations\u003c/h3\u003e\n\u003cp\u003eUnpersonalized recommendation systems have been happening since way before machine learning was ever in the public knowledge base. An example of an unpersonalized recommendation would be on YouTube when it recommends the most viewed videos. These are videos that the most people have watched. For the most part, these recommendations aren't too bad. After all, there's a reason why things are popular. This approach, however, is not going to help more niche videos get exposure. It also won't be immensely beneficial to those who have very particular tastes. Of course, there are times when a simple approach like this might be best. An example of a simple popularity recommender working well is with the news. There's a high chance that everyone who visits a news website is going to want to see whatever is the most popular at that moment in time.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-recommendation-system-introduction/master/images/news.png\" width=\"900\"\u003e\u003c/p\u003e\n\u003cp\u003eBecause unpersonalized recommendations are based on the entire user pool, whatever item is the most popular at any given time would be recommended to you, even if it's something you are completely uninterested in. There are so many items that are far too obscure to be the \"most popular\" item that might make someone's day. To make more informed recommendations, personalized recommendation systems make use of big data to ensure that users are getting items tailored towards there personal interests, no matter how niche they are.\u003c/p\u003e\n\u003ch3\u003ePersonalized Recommendations\u003c/h3\u003e\n\u003cp\u003eThe general problem of personalized recommendation systems can be summarized as:\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eGiven\u003c/strong\u003e:\u003c/p\u003e\n\u003cp\u003eThe profile of the \"active\" user and possibly some situational context, i.e. user browsing a product or making a purchase etc.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eRequired\u003c/strong\u003e:\u003c/p\u003e\n\u003cp\u003eCreating a set of items, and a score for each recommendable item in that set\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eProfile\u003c/strong\u003e:\u003c/p\u003e\n\u003cp\u003eUser profile may contain past purchases, ratings in either implicit or explicit form, demographics and interest scores for item features\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eThere are two ways to gather such data. The first method is to ask for explicit ratings from a user, typically on a concrete rating scale (such as rating a movie from one to five stars). The second is to gather data implicitly as the user is in the domain of the system - that is, to log the actions of a user on the site.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003e\u003cstrong\u003eProblem\u003c/strong\u003e:\u003c/p\u003e\n\u003cp\u003eWe want to learn a function that predicts the relevance score for a given (typically unseen) item based on user user profile and context\u003c/p\u003e\n\u003cp\u003eWithin personalized recommendation systems there are many different possible algorithms. We're going to go over the important ones now.\u003c/p\u003e\n\u003cp\u003eEach of these techniques make use of different similarity metrics to determine how \"similar\" items are to one another. The most common similarity metrics are \u003ca href=\"https://en.wikipedia.org/wiki/Euclidean_distance\"\u003eEuclidean distance\u003c/a\u003e, \u003ca href=\"https://en.wikipedia.org/wiki/Cosine_similarity\"\u003ecosine similarity\u003c/a\u003e, \u003ca href=\"https://en.wikipedia.org/wiki/Pearson_correlation_coefficient\"\u003ePearson correlation\u003c/a\u003e and the \u003ca href=\"https://en.wikipedia.org/wiki/Jaccard_index\"\u003eJaccard index (useful with binary data)\u003c/a\u003e. Each one of these distance metrics has its advantages and disadvantages depending on the type of ratings you are using and the characteristics of your data.\u003c/p\u003e\n\u003ch3\u003eContent-Based Recommenders\u003c/h3\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003eMain Idea\u003c/strong\u003e: If you like an item, you will also like \"similar\" items.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-recommendation-system-introduction/master/images/content_based.png\" width=\"500\"\u003e\u003c/p\u003e\n\u003cp\u003eThese systems are based on the characteristics of the items themselves. If you ever see a banner ad saying \"try other items like this\", it is most likely a content-based recommender systems. The advantages of content-based recommender systems is that it is a recommender system that gives the user a bit more information as to why they are seeing these recommendations. If they are on a page of a book they very much like, they will be happy to see another book that is similar to it. If they are told that this book is similar to their favorite book, they're more than likely to get that book. A disadvantage of content-based recommender systems is that they often require manual or semi-manual tagging of each of products. More advanced versions of content-based recommender systems allow for the development of an average of all the items a user has liked. This allows for a more nuanced approach to incorporate more than one item when calculating which items are most similar.\u003c/p\u003e\n\u003ch3\u003eCollaborative Filtering Systems\u003c/h3\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003eMain Idea\u003c/strong\u003e: If user A likes items 5, 6, 7, and 8 and user B likes items 5, 6, and 7, then it is highly likely that user B will also like item 8.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-recommendation-system-introduction/master/images/collaborative_filtering.png\" width=\"450\"\u003e\u003c/p\u003e\n\u003cp\u003eCollaborative filtering systems use a collection of user rating of items to make recommendations. The issue with collaborative filtering is that you have what is called the \"cold start problem.\" The idea behind it is, how to recommend something based off of user activity if you do not have any user activity to begin with! This can be overcome through various techniques. The most important thing to realize is that there is no one best recommendation system technique. In the end, what matters most is what system actually gets people to get recommendations that they will act upon. It might be that on the aggregate, recommending the most popular items is the most cost effective way to introduce users to new products.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eThe key idea behind collaborative filtering is that similar users share similar interests and that users tend to like items that are similar to one another.\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eWhile this may not be completely true on every occasion, if we have a large enough dataset, if there are patterns present, they will start to emerge.\u003c/p\u003e\n\u003cp\u003eAssume there are some users who have bought certain items, we can use a matrix with size \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Ctext%7Bnum_users%7D%20*%20%5Ctext%7Bnum_items%7D\"\u003e to denote the past behavior of users. Each cell in the matrix represents the associated opinion that a user holds. Such matrix is called a \u003cstrong\u003eUtility Matrix\u003c/strong\u003e. For instance, \u003cimg src=\"https://render.githubusercontent.com/render/math?math=M_%7Bi,%20j%7D\"\u003e denotes how user \u003cimg src=\"https://render.githubusercontent.com/render/math?math=u\"\u003e likes item \u003cimg src=\"https://render.githubusercontent.com/render/math?math=i\"\u003e . Sometimes these individual ratings are written as \u003cimg src=\"https://render.githubusercontent.com/render/math?math=r_%7Bui%7D\"\u003e for a rating for a given user and a given item. Using the table below as a reference point, if we replaced the \u003cimg src=\"https://render.githubusercontent.com/render/math?math=u\"\u003e and \u003cimg src=\"https://render.githubusercontent.com/render/math?math=i\"\u003e variable subscripts with actual values it would look like \u003cimg src=\"https://render.githubusercontent.com/render/math?math=r_%7B%5Ctext%7BMike%7D,%5Ctext%7BLittle%20Mermaid%7D%7D%20=%203\"\u003e .\u003c/p\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003e\u003c/th\u003e\n\u003cth\u003eToy Story\u003c/th\u003e\n\u003cth\u003eCinderella\u003c/th\u003e\n\u003cth\u003eLittle Mermaid\u003c/th\u003e\n\u003cth\u003eLion King\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd\u003eMatt\u003c/td\u003e\n\u003ctd\u003e\u003c/td\u003e\n\u003ctd\u003e2\u003c/td\u003e\n\u003ctd\u003e\u003c/td\u003e\n\u003ctd\u003e5\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eLore\u003c/td\u003e\n\u003ctd\u003e2\u003c/td\u003e\n\u003ctd\u003e\u003c/td\u003e\n\u003ctd\u003e4\u003c/td\u003e\n\u003ctd\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eMike\u003c/td\u003e\n\u003ctd\u003e\u003c/td\u003e\n\u003ctd\u003e5\u003c/td\u003e\n\u003ctd\u003e3\u003c/td\u003e\n\u003ctd\u003e2\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eForest\u003c/td\u003e\n\u003ctd\u003e5\u003c/td\u003e\n\u003ctd\u003e\u003c/td\u003e\n\u003ctd\u003e1\u003c/td\u003e\n\u003ctd\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eTaylor\u003c/td\u003e\n\u003ctd\u003e1\u003c/td\u003e\n\u003ctd\u003e5\u003c/td\u003e\n\u003ctd\u003e\u003c/td\u003e\n\u003ctd\u003e2\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003cp\u003eThe task of a recommendation system would be to come up with ratings for users in the spots that are currently empty. As you can imagine, most of the time, these values will be largely empty. For user 1, our recommendation system would try and predict what user 1 would rate Toy Story and the Little Mermaid and then recommend whichever product our model predicts they would rate the highest. The utility matrix above is what's known as an explicit rating. Each person has rated the movies that they've seen. Frequently, we must infer some meaning from the data and use our own judgment to determine how to use it for a recommendation system. Assume that rather than ratings, we only knew whether or not users bought a movie from a streaming website. Let's take a look at what this table would look like:\u003c/p\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003e\u003c/th\u003e\n\u003cth\u003eToy Story\u003c/th\u003e\n\u003cth\u003eCinderella\u003c/th\u003e\n\u003cth\u003eLittle Mermaid\u003c/th\u003e\n\u003cth\u003eLion King\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd\u003eMatt\u003c/td\u003e\n\u003ctd\u003e\u003c/td\u003e\n\u003ctd\u003e1\u003c/td\u003e\n\u003ctd\u003e\u003c/td\u003e\n\u003ctd\u003e1\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eLore\u003c/td\u003e\n\u003ctd\u003e1\u003c/td\u003e\n\u003ctd\u003e\u003c/td\u003e\n\u003ctd\u003e1\u003c/td\u003e\n\u003ctd\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eMike\u003c/td\u003e\n\u003ctd\u003e\u003c/td\u003e\n\u003ctd\u003e1\u003c/td\u003e\n\u003ctd\u003e1\u003c/td\u003e\n\u003ctd\u003e1\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eForest\u003c/td\u003e\n\u003ctd\u003e1\u003c/td\u003e\n\u003ctd\u003e\u003c/td\u003e\n\u003ctd\u003e1\u003c/td\u003e\n\u003ctd\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eTaylor\u003c/td\u003e\n\u003ctd\u003e1\u003c/td\u003e\n\u003ctd\u003e1\u003c/td\u003e\n\u003ctd\u003e\u003c/td\u003e\n\u003ctd\u003e1\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003cp\u003eThese are \u003cstrong\u003eimplicit\u003c/strong\u003e ratings because we are assuming that because a person has bought something, they would like to buy other items like it. Of course, this is not necessarily true, but it's better than nothing!\u003c/p\u003e\n\u003cp\u003eWithin the domain of collaborative filtering, there are both memory-based approaches and model-based approaches that you will learn about in the upcoming lessons.\u003c/p\u003e\n\u003ch2\u003eFurther Reading\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"http://infolab.stanford.edu/%7Eullman/mmds/ch9.pdf\"\u003eChapter 9: Mining of Massive Datasets (MMDS)\u003c/a\u003e - A must read for in-depth knowledge about how recommendation systems work, their underlying algorithms and evaluation approaches. This covers most of the topic from this lesson and also the upcoming lessons in great detail.\u003c/p\u003e\n\u003ch2\u003eSummary\u003c/h2\u003e\n\u003cp\u003eIn this lesson, we looked at an overview of recommendation systems. Focusing on collaborative filtering systems, we will move on to developing user-based engines.\u003c/p\u003e","frontPage":false},{"exportId":"modeling-time-series-data-recap","title":"Modeling Time Series Data - Recap","type":"WikiPage","content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-time-series-models-section-recap\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-time-series-models-section-recap\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-time-series-models-section-recap/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\u003ch2\u003eKey Takeaways\u003c/h2\u003e\n\u003cp\u003eThe key takeaways from this section include:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eA white noise model has a fixed and constant mean and variance, and no correlation over time\u003c/li\u003e\n\u003cli\u003eA random walk model has no specified mean or variance, but has a strong dependence over time\u003c/li\u003e\n\u003cli\u003eThe Pandas \u003ccode\u003e.corr()\u003c/code\u003e method can be used to return the correlation between various time series in the DataFrame\u003c/li\u003e\n\u003cli\u003eAutocorrelation allows us to identify how strongly each time series observation is related to previous observations\u003c/li\u003e\n\u003cli\u003eThe Autocorrelation Function (ACF) is a function that represents autocorrelation of a time series as a function of the time lag\u003c/li\u003e\n\u003cli\u003eThe Partial Autocorrelation Function (or PACF) gives the partial correlation of a time series with its own lagged values, controlling for the values of the time series at all shorter lags\u003c/li\u003e\n\u003cli\u003eARMA (Autoregressive and Moving Average) modeling is a tool for forecasting time series values by regressing the variable on its own lagged (past) values\u003c/li\u003e\n\u003cli\u003eARMA models assume that you've already detrended your data and that there is no seasonality\u003c/li\u003e\n\u003c/ul\u003e","frontPage":false},{"exportId":"apache-spark-recap","title":"Apache Spark - Recap","type":"WikiPage","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-spark-section-recap\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-spark-section-recap/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\u003ch2\u003eKey Takeaways\u003c/h2\u003e\n\n\u003cp\u003eThe key takeaways from this section include:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eBig Data usually refers to datasets that grow so large that they become awkward to work with using traditional database management systems and analytical approaches\u003c/li\u003e\n\u003cli\u003eBig data refers to data that is terabytes (TB) to petabytes (PB) in size\u003c/li\u003e\n\u003cli\u003eMapReduce can be used to split big datasets up in smaller sets to be distributed over several machines to deal with Big Data Analytics \u003c/li\u003e\n\u003cli\u003eBefore starting to work, you need to install Docker and Kinematic on your environment\u003c/li\u003e\n\u003cli\u003eMake sure to test your installation so you're sure everything is working\u003c/li\u003e\n\u003cli\u003eWhen you start working with PySpark, you have to create a \u003ccode\u003eSparkContext()\u003c/code\u003e \u003c/li\u003e\n\u003cli\u003eThe creation or RDDs is essential when working with PySpark\u003c/li\u003e\n\u003cli\u003eExamples of actions and transformations include \u003ccode\u003ecollect()\u003c/code\u003e, \u003ccode\u003ecount()\u003c/code\u003e, \u003ccode\u003efilter()\u003c/code\u003e, \u003ccode\u003efirst()\u003c/code\u003e, \u003ccode\u003etake()\u003c/code\u003e, and \u003ccode\u003ereduce()\u003c/code\u003e \u003c/li\u003e\n\u003cli\u003eMachine Learning on the scale of big data can be done with Spark using the \u003ccode\u003eml\u003c/code\u003e library\u003c/li\u003e\n\u003c/ul\u003e\n\u003cfooter class=\"fis-footer\" style=\"visibility: hidden;\"\u003e\u003cdiv class=\"fis-feedback\"\u003e\n\u003ch5\u003eHow do you feel about this lesson?\u003c/h5\u003e\n\u003cimg id=\"thumbs-up\" data-repository=\"dsc-spark-section-recap\" title=\"Thumbs up!\" alt=\"thumbs up\"\u003e\u003cimg id=\"thumbs-down\" data-repository=\"dsc-spark-section-recap\" title=\"Thumbs down!\" alt=\"thumbs down\"\u003e\n\u003c/div\u003e\n\u003ch5\u003eHave specific feedback? \u003ca href=\"https://github.com/learn-co-curriculum/dsc-spark-section-recap/issues/new\"\u003eTell us here!\u003c/a\u003e\n\u003c/h5\u003e\u003c/footer\u003e","frontPage":false},{"exportId":"natural-language-processing-recap","title":"Natural Language Processing - Recap","type":"WikiPage","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-nlp-section-recap\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-nlp-section-recap/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\u003ch2\u003eKey Takeaways\u003c/h2\u003e\n\n\u003cp\u003eThe key takeaways from this section include:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eNLP has become increasingly popular over the past few years, and NLP researchers have achieved very insightful insights\u003cbr\u003e\n\u003c/li\u003e\n\u003cli\u003eThe Natural Language Tool Kit (NLTK) is one of the most popular Python libraries for NLP\u003c/li\u003e\n\u003cli\u003eRegular Expressions are an important part of NLP, which can be used for pattern matching and filtering\u003c/li\u003e\n\u003cli\u003eRegular Expressions can become confusing, so make sure to use our provided cheat sheet the first few times you work with regex\u003c/li\u003e\n\u003cli\u003eIt is strongly recommended you take some time to use regex tester websites to ensure you understand how changing your regex pattern affects your results when working towards a correct answer!\u003c/li\u003e\n\u003cli\u003eFeature Engineering is essential when working with text data, and to understand the dynamics of your text\u003c/li\u003e\n\u003cli\u003eCommon feature engineering techniques are removing stop words, stemming, lemmatization, and n-grams\u003c/li\u003e\n\u003cli\u003eWhen diving deeper into grammar and linguistics, context-free grammars and part-of-speech tagging is important\u003c/li\u003e\n\u003cli\u003eIn this context, parse trees can help computers when dealing with ambiguous words \u003c/li\u003e\n\u003cli\u003e\n\u003cem\u003eHow\u003c/em\u003e you clean and preprocess your data will have a major effect on the conclusions you'll be able to draw in your NLP classification problems \u003c/li\u003e\n\u003c/ul\u003e\n\u003cfooter class=\"fis-footer\" style=\"visibility: hidden;\"\u003e\u003cdiv class=\"fis-feedback\"\u003e\n\u003ch5\u003eHow do you feel about this lesson?\u003c/h5\u003e\n\u003cimg id=\"thumbs-up\" data-repository=\"dsc-nlp-section-recap\" title=\"Thumbs up!\" alt=\"thumbs up\"\u003e\u003cimg id=\"thumbs-down\" data-repository=\"dsc-nlp-section-recap\" title=\"Thumbs down!\" alt=\"thumbs down\"\u003e\n\u003c/div\u003e\n\u003ch5\u003eHave specific feedback? \u003ca href=\"https://github.com/learn-co-curriculum/dsc-nlp-section-recap/issues/new\"\u003eTell us here!\u003c/a\u003e\n\u003c/h5\u003e\u003c/footer\u003e","frontPage":false},{"exportId":"text-classification","title":"Text Classification","type":"WikiPage","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-text-classification\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-text-classification/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eIn this lesson, we'll discuss the general process for setting up text datasets for classification problems.\u003c/p\u003e\n\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\n\u003cp\u003eYou will be able to:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eList the steps for classifying text data \u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch2\u003eClassification for Text Data\u003c/h2\u003e\n\n\u003cp\u003eFor the final lab of this section, we'll use everything we've learned so far to build a classifier that works well with text data. As you've probably guessed, text data is significantly harder to work with than most traditional datasets, because of the sheer amount of preprocessing needed to get the data into a format acceptable to a machine learning algorithm. \u003c/p\u003e\n\n\u003cp\u003eThe main challenge in working with text data isn't just the preprocessing -- its the number of decisions you have to make about how you'll clean and structure the data. In a traditional dataset full of numerical and categorical features, the preprocessing steps are fairly straightforward. Generally, we normalize the numeric data, check for and deal with multicollinearity, convert categorical data to numerical format through one-hot encoding, and so forth. Although the steps themselves may not be easy, there's generally little ambiguity about \u003cem\u003ewhat needs to be done\u003c/em\u003e. Text data is a bit more ambiguous. Let's examine some of the decisions we generally need to make when working with text data.\u003c/p\u003e\n\n\u003ch2\u003eCleaning and Preprocessing Text Data\u003c/h2\u003e\n\n\u003cp\u003eOnce we have our data, the fun part begins. We'll need to begin by preprocessing and cleaning our text data. As you've seen throughout this section, preprocessing text data is a bit more challenging than working with more traditional data types because there's no clear-cut answer for exactly what sort of preprocessing and cleaning we need to do. When working with traditional datasets, our goals are generally pretty clear for this stage -- normalize and clean our numerical data, convert categorical data to a numeric format, check for and deal with multicollinearity, etc. The steps we take are largely dependent on what the data already looks like when we get a hold of it. Text data is different -- if we inspect a raw text dataset, we'll generally see that it only has one dimension -- the actual text, in the form of a string. This could be anything from a tweet to a full novel. This means that we need to make some decisions about how to preprocess our data. Before we can begin cleaning and preprocessing our text data, we need to make some decisions about things such as:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eDo we remove stop words or not?\u003cbr\u003e\n\u003c/li\u003e\n\u003cli\u003eDo we stem or lemmatize our text data, or leave the words as is?\u003cbr\u003e\n\u003c/li\u003e\n\u003cli\u003eIs basic tokenization enough, or do we need to support special edge cases through the use of regex?\u003cbr\u003e\n\u003c/li\u003e\n\u003cli\u003eDo we use the entire vocabulary, or just limit the model to a subset of the most frequently used words? If so, how many?\u003cbr\u003e\n\u003c/li\u003e\n\u003cli\u003eDo we engineer other features, such as bigrams, or POS tags, or Mutual Information Scores?\u003cbr\u003e\n\u003c/li\u003e\n\u003cli\u003eWhat sort of vectorization should we use in our model? Boolean Vectorization? Count Vectorization? TF-IDF? More advanced vectorization strategies such as Word2Vec?\u003cbr\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003eThese are all questions that we'll need to think about pretty much anytime we begin working with text data.\u003c/p\u003e\n\n\u003ch2\u003eFeature Engineering\u003c/h2\u003e\n\n\u003cp\u003eAnother common decision point when working with text data is exactly what features to include in the dataset. As we saw in a previous lab, NLTK makes it quite easy to do things like generate part-of-speech tags for words, or create word or character-level n-grams. In general, there's no great answer for exactly which features will improve the performance of your model, and which won't. This means that your best bet is to experiment, and treat the entire project as an iterative process! When working with text data, don't be afraid to try modeling on alternative forms of the text data, such as bigrams or n-grams. Similarly, we encourage you to explore how adding in additional features such as POS tags or mutual information scores affect the overall model performance. Sometimes, it has a great effect on performance. Other times, not much. Either way, you won't know until you try!\u003c/p\u003e\n\n\u003ch2\u003eSummary\u003c/h2\u003e\n\n\u003cp\u003eIn this lesson, we discussed the challenges that come with working with text data for classification, and the types of decisions we should be ready to make when cleaning and preprocessing a dataset!\u003c/p\u003e\n\u003cfooter class=\"fis-footer\" style=\"visibility: hidden;\"\u003e\u003cdiv class=\"fis-feedback\"\u003e\n\u003ch5\u003eHow do you feel about this lesson?\u003c/h5\u003e\n\u003cimg id=\"thumbs-up\" data-repository=\"dsc-text-classification\" title=\"Thumbs up!\" alt=\"thumbs up\"\u003e\u003cimg id=\"thumbs-down\" data-repository=\"dsc-text-classification\" title=\"Thumbs down!\" alt=\"thumbs down\"\u003e\n\u003c/div\u003e\n\u003ch5\u003eHave specific feedback? \u003ca href=\"https://github.com/learn-co-curriculum/dsc-text-classification/issues/new\"\u003eTell us here!\u003c/a\u003e\n\u003c/h5\u003e\u003c/footer\u003e","frontPage":false},{"exportId":"big-data-analytics-on-apache-spark","title":"Big Data Analytics on Apache Spark","type":"WikiPage","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-big-data-analytics-apache-spark\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-big-data-analytics-apache-spark/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eBig data analytics is an emerging area of interest both for business and academia. There are a lot of details around the characteristics of big data and how Apache Spark eases up the job of analyzing huge amounts of data using a simple programming paradigm. In this section, we will look at understanding and implementing a simple problem using MapReduce in PySpark. Real world problems, however, are much more complicated than this and you should be able to scale up the takeaways from the simple word count example we will complete to much bigger problems. This lesson aims to provide you with a wider understanding of MapReduce and big data computation in the Apache Spark environment.\u003c/p\u003e\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\u003cp\u003eYou will be able to:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eDescribe the role of Apache Spark in Big data analytics\u003c/li\u003e\n\u003cli\u003eList some of the Spark functionalities\u003c/li\u003e\n\u003cli\u003eDescribe the role of RDDs in spark\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eFor this lesson, you are required to read the following review article:\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://link.springer.com/article/10.1007/s41060-016-0027-9\"\u003ehttps://link.springer.com/article/10.1007/s41060-016-0027-9\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eInternational Journal of Data Science and Analytics, November 2016, Volume 1, Issue 34, pp 145164. Authors: Salman Salloum, Ruslan Dautov, Xiaojun Chen, Patrick Xiaogang Peng, Joshua Zhexue Huang\u003c/p\u003e\n\u003cp\u003e\u003cem\u003e\"In this paper, we present a technical review on big data analytics using Apache Spark. This review focuses on the key components, abstractions and features of Apache Spark. More specifically, it shows what Apache Spark has for designing and implementing big data algorithms and pipelines for machine learning, graph analysis and stream processing. In addition, we highlight some research and development directions on Apache Spark for big data analytics.\"\u003c/em\u003e - from the abstract. Here is an image from the paper giving a general overview of how the spark ecosystem functions:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-big-data-analytics-apache-spark/master/images/spark.gif\"\u003e\u003c/p\u003e\n\u003cp\u003eYou are expected to spend around 90 - 120 minutes reading this article. It is an excellent article and all the key aspects of spark computational environment are summarized and presented in an excellent manner.\u003c/p\u003e\n\u003ch2\u003eSummary\u003c/h2\u003e\n\u003cp\u003eIn this lesson, you read the scientific article \"Big Data Analytics on Apache Spark\", which covers the key aspects of Spark's computational environment. You'll now move on to working with Spark through Python.\u003c/p\u003e","frontPage":false},{"exportId":"feature-engineering-for-text-data","title":"Feature Engineering for Text Data","type":"WikiPage","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-feature-engineering-for-text-data\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-feature-engineering-for-text-data/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eIn this lesson, we'll examine some common approaches to feature engineering for text data. \u003c/p\u003e\n\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\n\u003cp\u003eYou will be able to:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eExplain what stop words are and why they are frequently removed \u003c/li\u003e\n\u003cli\u003eExplain stemming and lemmatization\u003cbr\u003e\n\u003c/li\u003e\n\u003cli\u003eDefine bigrams and n-grams \u003c/li\u003e\n\u003cli\u003eDefine mutual information in the context of NLP \u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch2\u003eCommon Approaches to NLP Feature Engineering\u003c/h2\u003e\n\n\u003cp\u003eAs you've likely noticed by now, working with text data comes with \u003cstrong\u003e\u003cem\u003ea lot\u003c/em\u003e\u003c/strong\u003e of ambiguity. When all we start with is an arbitrarily-sized string of words, there's no clear answer as to what sorts of features we should engineer, or even where we should start! The goal of this lesson is to provide a framework for working with text data, and help us figure out exactly what sorts of features we should create when working with text data. \u003c/p\u003e\n\n\u003cp\u003eIn this lesson, we'll focus on the following topics:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eStopword Removal\u003c/li\u003e\n\u003cli\u003eFrequency Distributions\u003c/li\u003e\n\u003cli\u003eStemming and Lemmatization\u003c/li\u003e\n\u003cli\u003eBigrams, N-grams, and Mutual Information Score\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch2\u003eRemoving Stop Words\u003c/h2\u003e\n\n\u003cp\u003eWhen working with text data, one of the first steps to try is to remove the \u003cstrong\u003e\u003cem\u003eStop Words\u003c/em\u003e\u003c/strong\u003e from the text. One common feature of text data (regardless of language!) is the inclusion of stop words for grammatical structure. Words such as \"a\", \"and\", \"but\", and \"or\" are examples of stop words. While a sentence would be both grammatically incorrect and hard to understand without them, from a modeling standpoint, stop words provide little to no actual value. If we create a \u003cstrong\u003e\u003cem\u003eFrequency Distribution\u003c/em\u003e\u003c/strong\u003e to see the number of times each word is used in a corpus, we'll almost always find that the top spots are dominated by stop words, which tell us nothing about the actual content of the corpus. Removing stop words allows us to reduce the overall dimensionality of our dataset (which is always a good thing), while also distilling the overall vocabulary of our bag-of-words down only to the words that really matter. \u003c/p\u003e\n\n\u003cp\u003e\u003cem\u003eNLTK\u003c/em\u003e makes it extremely easy to remove stopwords. The library includes a full corpus of all stopwords for all the languages NLTK supports. Since we usually only want the stopwords relevant to the language our text data is in, NLTK even makes it easy to filter out the unneeded stop words and grab only the ones that pertain to our problem. \u003c/p\u003e\n\n\u003cp\u003eThe following example shows how we can get all the stopwords for English from NLTK:\u003c/p\u003e\n\n\u003cpre\u003e\u003ccode class=\"python\"\u003efrom nltk.corpus import stopwords\nimport string\n\nstopwords_list = stopwords.words('english')\n\nstopwords_list += list(string.punctuation)\n\u003c/code\u003e\u003c/pre\u003e\n\n\u003cp\u003eOnce we have a list of stopwords, we can easily remove them from our text data after we've tokenized our data. Recall that we can easily tokenize text data using NLTK's \u003ccode\u003eword_tokenize()\u003c/code\u003e function. Once we have a list of word tokens, all we need to do is use a list comprehension, and omit any tokens that can be found in our stopwords list.  For example:\u003c/p\u003e\n\n\u003cpre\u003e\u003ccode class=\"python\"\u003efrom nltk import word_tokenize\n\ntokens = word_tokenize(some_text_data)\n\nstopped_tokens = [w.lower() for w in tokens if w not in stopwords_list]\n\u003c/code\u003e\u003c/pre\u003e\n\n\u003ch2\u003eFrequency Distributions\u003c/h2\u003e\n\n\u003cp\u003eOnce we have tokenized our data and removed all the stop words, the next step is usually to explore our text data through a \u003cstrong\u003e\u003cem\u003eFrequency Distribution\u003c/em\u003e\u003c/strong\u003e. This is just a fancy way of saying that we create a histogram that tells us the total number of times each word is used in a given corpus. \u003c/p\u003e\n\n\u003cp\u003eOnce we have tokenized our text data, we can use NLTK to easily create a frequency distribution using \u003ccode\u003enltk.FreqDist()\u003c/code\u003e. A frequency distribution is analogous to a Python dictionary, with a few more bells and whistles attached to make it easier to use for NLP tasks. Each key is a word token, and each value is the corresponding number of times that token appeared in the tokenized corpus given to the \u003ccode\u003eFreqDist\u003c/code\u003e object at instantiation. \u003c/p\u003e\n\n\u003cp\u003eWe can easily filter a \u003ccode\u003eFreqDist()\u003c/code\u003e object to see the most common words by using the \u003ccode\u003e.most_common()\u003c/code\u003e built-in method, as seen below:\u003c/p\u003e\n\n\u003cpre\u003e\u003ccode class=\"python\"\u003efrom  nltk import FreqDist\nfreqdist = FreqDist(tokens)\n\nmost_common = freqdist.most_common(200)\n\u003c/code\u003e\u003c/pre\u003e\n\n\u003cp\u003eOnce we have the most common words, we can easily use this to filter out the text and reduce the dimensionality of particularly large datasets, as needed. \u003c/p\u003e\n\n\u003ch2\u003eStemming and Lemmatization\u003c/h2\u003e\n\n\u003cp\u003eConsider the words 'run', 'running', 'ran', and 'runs'. If we create a basic frequency distribution, each of these words will be treated as a separate token. After all, they are different words. However, we know that they pretty much mean the same thing. Counting these words as individual separate tokens can sometimes hurt our model by needlessly increasing dimensionality, and hiding important information from our model. Although we instinctively know that those four words are all talking about the same action, our model will default to thinking that they are four completely different concepts. The way we deal with this is to remove suffixes through techniques such as \u003cstrong\u003e\u003cem\u003eStemming\u003c/em\u003e\u003c/strong\u003e or \u003cstrong\u003e\u003cem\u003eLemmatization\u003c/em\u003e\u003c/strong\u003e.\u003c/p\u003e\n\n\u003cp\u003ePeople often get stemming and lemmatization confused, because they are extremely similar. They generally accomplish the same task, but they use different means to do so. \u003c/p\u003e\n\n\u003cp\u003e\u003cstrong\u003e\u003cem\u003eStemming\u003c/em\u003e\u003c/strong\u003e follows a predetermined set of rules to reduce a word to its \u003cem\u003estem\u003c/em\u003e.  Words like 'running' and 'runs' will be reduced down to 'run', because the stemmer contains rules that understands how to deal with suffixes such as '-ing' and '-s'. The best stemmer currently available is the \u003cstrong\u003e\u003cem\u003ePorter Stemmer\u003c/em\u003e\u003c/strong\u003e. For code samples demonstrating how to use it, check out NLTK's documentation for the \u003ca href=\"http://www.nltk.org/howto/stem.html\"\u003ePorter Stemmer\u003c/a\u003e.\u003c/p\u003e\n\n\u003cp\u003e\u003cstrong\u003e\u003cem\u003eLemmatization\u003c/em\u003e\u003c/strong\u003e differs from stemming in that it reduces each word down to a linguistically valid \u003cstrong\u003e\u003cem\u003elemma\u003c/em\u003e\u003c/strong\u003e, or root word. It does this through stored linguistic mappings. Lemmatization is generally more complex, but also more accurate. This is because the rules that guide things like the Porter Stemmer are good, but far from perfect. For example, stemmers commonly deal with the suffix \u003ccode\u003e-ed\u003c/code\u003e by just  dropping it from the word. This usually works, until it runs into an edge case like the word 'agreed'. When stemmed, 'agreed' becomes 'agre'. Lemmatization does not make this mistake, because it contains a mapping for the word that tells it what 'agreed' should be reduced down to. Generally, most lemmatizers make use of the famous \u003cstrong\u003e\u003cem\u003eWordNet\u003c/em\u003e\u003c/strong\u003e lexical database. \u003c/p\u003e\n\n\u003cp\u003eNLTK makes it quite easy to make use of lemmatization, as demonstrated below:\u003c/p\u003e\n\n\u003cpre\u003e\u003ccode class=\"python\"\u003efrom nltk.stem.wordnet import WordNetLemmatizer\n\nlemmatizer = WordNetLemmatizer()\n\nlemmatizer.lemmatize('feet') # foot\nlemmatizer.lemmatize('running') # run\n\u003c/code\u003e\u003c/pre\u003e\n\n\u003ch2\u003eBigrams and Mutual Information Score\u003c/h2\u003e\n\n\u003cp\u003eAnother alternative to tokenization is to instead create \u003cstrong\u003e\u003cem\u003eBigrams\u003c/em\u003e\u003c/strong\u003e out of the text. A bigram is just a pair of adjacent words, treated as a single unit. \u003c/p\u003e\n\n\u003cp\u003eConsider the sentence \"the dog played outside\". If we created bigrams out of this sentence, we would get \u003ccode\u003e('the', 'dog'), ('dog', 'played'), ('played', 'outside')\u003c/code\u003e. From a modeling perspective, this can be quite useful, because sometimes pairs of words are greater than the sum of their parts. Note that bigrams are just a special case of \u003cstrong\u003e\u003cem\u003en-grams\u003c/em\u003e\u003c/strong\u003e -- we can choose any number of words for a sequence. Alternatively, it's quite common to create n-grams at the character level, rather than the word level. \u003c/p\u003e\n\n\u003cp\u003eOne handy feature of bigrams is that we can apply a frequency filter to only keep bigrams that show up more than a set number of times. In this way, we can get rid of all bigrams that only occur because of random chance, and keep the bigrams that must mean something, because they occur together multiple times. How strict your frequency filter should be depends on a number of factors, and generally, it's something you'll have to experiment with to get right. However, most experts tend to apply a minimum frequency filter of 5. \u003c/p\u003e\n\n\u003cp\u003eAnother way we can make use of bigrams is to calculate their \u003cstrong\u003e\u003cem\u003ePointwise Mutual Information Score\u003c/em\u003e\u003c/strong\u003e. This is a statistical measure from information theory that generally measures the mutual dependence between two words. In plain english, this measures how much information the bigram itself contains by computing the dependence between the two words in the bigram. For instance, the bigram \u003ccode\u003e('San', 'Francisco')\u003c/code\u003e would likely have a high mutual information score, because when these tokens appear in the text, it is highly likely that they appear together, and unlikely that they appear next other words. \u003c/p\u003e\n\n\u003cp\u003eIn practice, you don't need to worry too much about how to calculate mutual information, because NLTK provides an easy way to do this for us. We'll explore this in detail in the next lab. Instead, your main takeaway on this topic should be that mutual information scores are a type of feature that you can engineer for text data that may provide good information for you when it comes to exploring the text data or fitting a model to it. \u003c/p\u003e\n\n\u003ch2\u003eSummary\u003c/h2\u003e\n\n\u003cp\u003eIn this lesson, we learned about various types of feature engineering we can perform on text data, and what each one means!\u003c/p\u003e\n\u003cfooter class=\"fis-footer\" style=\"visibility: hidden;\"\u003e\u003cdiv class=\"fis-feedback\"\u003e\n\u003ch5\u003eHow do you feel about this lesson?\u003c/h5\u003e\n\u003cimg id=\"thumbs-up\" data-repository=\"dsc-feature-engineering-for-text-data\" title=\"Thumbs up!\" alt=\"thumbs up\"\u003e\u003cimg id=\"thumbs-down\" data-repository=\"dsc-feature-engineering-for-text-data\" title=\"Thumbs down!\" alt=\"thumbs down\"\u003e\n\u003c/div\u003e\n\u003ch5\u003eHave specific feedback? \u003ca href=\"https://github.com/learn-co-curriculum/dsc-feature-engineering-for-text-data/issues/new\"\u003eTell us here!\u003c/a\u003e\n\u003c/h5\u003e\u003c/footer\u003e","frontPage":false},{"exportId":"pca-recap","title":"PCA - Recap","type":"WikiPage","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-pca-summary\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-pca-summary/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\u003ch2\u003eKey Takeaways\u003c/h2\u003e\n\n\u003cp\u003eThe key takeaways from this section include: \u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003ePCA is an \u003cem\u003eunsupervised learning technique\u003c/em\u003e which does not require labeled data \u003c/li\u003e\n\u003cli\u003eIt is also a dimensionality reduction technique which can be used to compress data and experiment with its effects on machine learning algorithms as a preprocessing step \u003c/li\u003e\n\u003cli\u003eThere are four steps to conducting PCA:\n\n\u003cul\u003e\n\u003cli\u003eCenter each feature by subtracting the feature mean\u003c/li\u003e\n\u003cli\u003eCalculate the covariance matrix for your normalized dataset\u003c/li\u003e\n\u003cli\u003eCalculate the eigenvectors/eigenvalues for the covariance matrix\n\n\u003cul\u003e\n\u003cli\u003eReorder your eigenvectors based on their accompanying eigenvalues (in descending order of the eigenvalues)\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eTake the dot product of the transpose of the eigenvectors with the transpose of the normalized data\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eYou can also easily implement PCA using scikit-learn \u003c/li\u003e\n\u003c/ul\u003e\n\u003cfooter class=\"fis-footer\" style=\"visibility: hidden;\"\u003e\u003cdiv class=\"fis-feedback\"\u003e\n\u003ch5\u003eHow do you feel about this lesson?\u003c/h5\u003e\n\u003cimg id=\"thumbs-up\" data-repository=\"dsc-pca-summary\" title=\"Thumbs up!\" alt=\"thumbs up\"\u003e\u003cimg id=\"thumbs-down\" data-repository=\"dsc-pca-summary\" title=\"Thumbs down!\" alt=\"thumbs down\"\u003e\n\u003c/div\u003e\n\u003ch5\u003eHave specific feedback? \u003ca href=\"https://github.com/learn-co-curriculum/dsc-pca-summary/issues/new\"\u003eTell us here!\u003c/a\u003e\n\u003c/h5\u003e\u003c/footer\u003e","frontPage":false},{"exportId":"nlp-and-word-vectorization","title":"NLP and Word Vectorization","type":"WikiPage","content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-nlp-and-word-vectorization\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-nlp-and-word-vectorization\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-nlp-and-word-vectorization/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eIn this lesson, we'll learn about some foundational concepts in Natural Language Processing such as stemming and lemmatization, as well as various strategies for converting text data into word vectors!\u003c/p\u003e\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\u003cp\u003eYou will be able to:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eExplain stemming and lemmatization\u003c/li\u003e\n\u003cli\u003eExplain what stop words are and why they are frequently removed\u003c/li\u003e\n\u003cli\u003eDefine tokenization in the context of NLP\u003c/li\u003e\n\u003cli\u003eDefine TF-IDF vectorization and its components\u003c/li\u003e\n\u003cli\u003eDefine count vectorization and its relationship to bag of words\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003eWhat is Natural Language Processing?\u003c/h2\u003e\n\u003cp\u003e\u003cstrong\u003e\u003cem\u003eNatural Language Processing\u003c/em\u003e\u003c/strong\u003e, or \u003cstrong\u003e\u003cem\u003eNLP\u003c/em\u003e\u003c/strong\u003e, is the study of how computers can interact with humans through the use of human language. Although this is a field that is quite important to Data Scientists, it does not belong to Data Science alone. NLP has been around for quite a while, and sits at the intersection of \u003cem\u003eComputer Science\u003c/em\u003e, \u003cem\u003eArtificial Intelligence\u003c/em\u003e, \u003cem\u003eLinguistics\u003c/em\u003e, and \u003cem\u003eInformation Theory\u003c/em\u003e. In the early days of NLP, it mainly consisted of trying to program algorithms that contained many rules borrowed from the field of linguistics. However, in the 1980s, machine learning started to show great success with many NLP tasks, and many of these rule-based methods took a back seat to approaches involving machine learning and AI. Fast forward to now, and NLP has become an area of applied machine learning that Data Scientists all around the globe work in every day.\u003c/p\u003e\n\u003ch2\u003eNLP and Bayesian Statistics\u003c/h2\u003e\n\u003cp\u003eAs machine learning has come into its own, we've seen NLP products get better and better. For instance, in just a few decades, we've gone from rule-based chat bots with preprogrammed responses to things like Siri and \u003ca class=\"\" href=\"https://www.youtube.com/watch?v=D5VN56jQMWM\"\u003eGoogle Duplex\u003c/a\u003e (if you aren't familiar with Duplex, take a few minutes to follow that link and watch the demo on YouTube -- you won't be disappointed!). Much of the most exciting advancements currently happening in the field of NLP are due to Deep Learning. However, we can still do amazing things with machine learning and text data by making use of Bayesian methods. For instance, you may remember a time in the early 2000s when the problem of email spam was bad, and getting worse. This problem was eventually solved through the application of machine learning -- specifically, \u003cstrong\u003e\u003cem\u003eNaive Bayesian Classification\u003c/em\u003e\u003c/strong\u003e! For the remainder of this section, we'll focus on how we can apply our newfound knowledge of Bayesian methods to solve real-world NLP tasks such as \u003ca href=\"http://www.paulgraham.com/spam.html\"\u003espam filtering\u003c/a\u003e and text classification.\u003c/p\u003e\n\u003ch2\u003eWorking With Text Data\u003c/h2\u003e\n\u003cp\u003eWorking with text data comes with a unique set of problems and solutions that other types of datasets don't have. Often, text data requires more cleaning and preprocessing than normal data, in order to get it into a format where we can use statistical methods or machine learning to work with it. Let's explore some of the things we generally need to do to get text data into a form where we can work with it.\u003c/p\u003e\n\u003ch2\u003eCreating a Bag of Words\u003c/h2\u003e\n\u003cp\u003eThe most common approach to working with text is to vectorize it by creating a \u003cstrong\u003e\u003cem\u003eBag of Words\u003c/em\u003e\u003c/strong\u003e. In this case, the name \"Bag of Words\" is quite descriptive of the final product -- the bag contains information about all the important words in the text individually, but not in any particular order. It's as if we take every word in a \u003cstrong\u003e\u003cem\u003eCorpus\u003c/em\u003e\u003c/strong\u003e and throw them into a bag. With a large enough corpus, we'll often see certain patterns start to emerge -- for instance, a bag of words made out of Shakespeare's \u003cem\u003eHamlet\u003c/em\u003e is probably more similar to a bag of words made out of \u003cem\u003eMacbeth\u003c/em\u003e than it is to something like \u003cem\u003eThe Hunger Games\u003c/em\u003e. The simplest way to create a bag of words is to just count how many times each unique word is used in a given corpus. If we have a number for every word, then we have a way to treat each bag as a \u003cstrong\u003e\u003cem\u003evector\u003c/em\u003e\u003c/strong\u003e, which opens up all kinds of machine learning tools for use.\u003c/p\u003e\n\u003cp\u003eLet's explore some of the steps that must occur before we can fully vectorize a text and work with it.\u003c/p\u003e\n\u003ch3\u003eBasic Cleaning and Tokenization\u003c/h3\u003e\n\u003cp\u003eOne of the most basic problems seen when working with text data is things like punctuation and capitalization. Although counting how many times a word appears in a text sounds straightforward at first, it can actually be quite complicated at times, and will almost always require some decisions on our part. For instance, consider the following sentence:\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\"Apple shareholders have had a great year. Apple's stock price has gone steadily upwards -- Apple even broke a trillion-dollar valuation, continuing the dominance of this tech stock.\"\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eIf we were to count how many times each word appears in this sentence, we would likely say that \"Apple\" has a count of three. However, if we wrote a basic Python script to do this, our algorithm would tell us that the word \"Apple\" only appears twice! To a computer, \"Apple\" and \"Apple's\" are different words. Capitalization is also a problem -- \"apple\" would also be counted as a different word. Similarly, punctuation is also a problem. A basic counting algorithm would see \"stock\" and \"stock.\" as two completely different words.\u003c/p\u003e\n\u003cp\u003eFirst and foremost, cleaning a text dataset usually means removing punctuation, and lowercasing everything. However, this can be tricky, and require decisions on your part based on the text you're working with and your goals -- for instance, whether or not apostrophes should be removed.\u003c/p\u003e\n\u003cp\u003eThe goal of this step is to create word \u003cstrong\u003e\u003cem\u003etokens\u003c/em\u003e\u003c/strong\u003e. The sentence \"Where did you get those coconuts?\", when cleaned and tokenized, would probably look more like \u003ccode\u003e['where', 'did', 'you, 'get', 'those', 'coconuts']\u003c/code\u003e.\u003c/p\u003e\n\u003cp\u003eHowever, there are still other important decisions to make during the tokenization stage. For instance, should \"run\" and \"runs\" be counted as the same token, or as different tokens? How about \"ran\", or \"running\"?\u003c/p\u003e\n\u003ch3\u003eStemming, Lemmatization, and Stop Words\u003c/h3\u003e\n\u003cp\u003eSometimes, depending on the task, it may be best to leave \"run\" and \"runs\" as different tokens. However, this often is not the case -- especially with smaller datasets. NLP methods such as \u003cstrong\u003e\u003cem\u003eStemming\u003c/em\u003e\u003c/strong\u003e and \u003cstrong\u003e\u003cem\u003eLemmatization\u003c/em\u003e\u003c/strong\u003e help us deal with this problem, where we reduce each word token down to its root word. For cases such as \"run\", \"runs\", \"running\" and \"ran\", they are more similar than different -- we may want our algorithm to treat these as the same word, \"run\".\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e\u003cem\u003eStemming\u003c/em\u003e\u003c/strong\u003e accomplishes this by removing the ends of words where the end signals some sort of derivational change to the word. For instance, we know that adding an 's' to the end of a word makes it plural -- a stemming algorithm given the word \"cats\" would return \"cat\". Note that stems do not have to make sense as actual English words. For example, \"ponies\" would be reduced to \"poni\", not \"pony\". Stemming is a more crude, heuristic process that contains rule sets that tells the algorithm how to stem each word, and what it should be stemmed to. The process is more crude than lemmatization, but it's also easier to implement. For instance, take a look at this example subset of stemming rules from the \u003ca href=\"https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html\"\u003eStanford NLP Group\u003c/a\u003e:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-nlp-and-word-vectorization/master/images/new_stemming.png\" width=\"400\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e\u003cem\u003eLemmatization\u003c/em\u003e\u003c/strong\u003e accomplishes pretty much the same thing as stemming, but does it in a more complex way, by examining the \u003cstrong\u003e\u003cem\u003emorphology\u003c/em\u003e\u003c/strong\u003e of words and attempting to reduce each word to its most basic form, or \u003cstrong\u003e\u003cem\u003elemma\u003c/em\u003e\u003c/strong\u003e. Note that the results here often end up a bit different than stemming. See the following table for an example of the differences in results:\u003c/p\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth style=\"text-align: center;\"\u003eWord\u003c/th\u003e\n\u003cth style=\"text-align: center;\"\u003eStem\u003c/th\u003e\n\u003cth style=\"text-align: center;\"\u003eLemma\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd style=\"text-align: center;\"\u003eStudies\u003c/td\u003e\n\u003ctd style=\"text-align: center;\"\u003eStudi\u003c/td\u003e\n\u003ctd style=\"text-align: center;\"\u003eStudy\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd style=\"text-align: center;\"\u003eStudying\u003c/td\u003e\n\u003ctd style=\"text-align: center;\"\u003eStudy\u003c/td\u003e\n\u003ctd style=\"text-align: center;\"\u003eStudy\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003cp\u003eFinally, you may have intuited that many words in a text are pretty much useless and contain little to no actual information. For instance, words such as \"the\" and \"of\". These are called \u003cstrong\u003e\u003cem\u003eStop Words\u003c/em\u003e\u003c/strong\u003e, and are often removed after tokenization is complete in order to reduce the dimensionality of each corpus down to only the words that contain important information. Popular NLP frameworks and toolkits such as NLTK contain a list of stop words for most languages, which allow us to easily loop through our tokenized corpus and remove any stop words we find.\u003c/p\u003e\n\u003ch2\u003eVectorization Strategies\u003c/h2\u003e\n\u003cp\u003eOnce we cleaned and tokenized our text data, we can convert it to vectors. However, there are a few different ways we can do this. Depending on our goals and our dataset, some may be more useful than others.\u003c/p\u003e\n\u003ch3\u003eCount Vectorization\u003c/h3\u003e\n\u003cp\u003eOne of the most basic, but useful ways of vectorizing text data is to simply count the number of times each word appears in the corpus. If working with a single document, we just create a single vector, where each element in the vector corresponds to the count of a unique word in the document. If working with multiple documents, we would store everything in a DataFrame, with each column representing a unique word, while each row represents the count vector for a given document.\u003c/p\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth style=\"text-align: center;\"\u003eDocument\u003c/th\u003e\n\u003cth style=\"text-align: center;\"\u003eAardvark\u003c/th\u003e\n\u003cth style=\"text-align: center;\"\u003eApple\u003c/th\u003e\n\u003cth\u003e...\u003c/th\u003e\n\u003cth\u003eZebra\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd style=\"text-align: center;\"\u003e1\u003c/td\u003e\n\u003ctd style=\"text-align: center;\"\u003e0\u003c/td\u003e\n\u003ctd style=\"text-align: center;\"\u003e3\u003c/td\u003e\n\u003ctd\u003e...\u003c/td\u003e\n\u003ctd\u003e1\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd style=\"text-align: center;\"\u003e2\u003c/td\u003e\n\u003ctd style=\"text-align: center;\"\u003e1\u003c/td\u003e\n\u003ctd style=\"text-align: center;\"\u003e2\u003c/td\u003e\n\u003ctd\u003e...\u003c/td\u003e\n\u003ctd\u003e0\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003cp\u003eNote that we do not need to have a column for every word in the English language -- just a column for each word that shows up in the total vocabulary of our document or documents. If we have multiple documents, we just combine the unique words from each document to get the total dimensionality that allows us to represent each. If a word doesn't show up in a given document, that's fine -- that just means the count is 0 for that row and column.\u003c/p\u003e\n\u003ch3\u003eTF-IDF Vectorization\u003c/h3\u003e\n\u003cp\u003eTF-IDF stands for \u003cstrong\u003e\u003cem\u003eTerm Frequency-Inverse Document Frequency\u003c/em\u003e\u003c/strong\u003e. It is a combination of two individual metrics, which are the TF and IDF, respectively. TF-IDF is used when we have multiple documents. It is based on the idea that rare words contain more information about the content of a document than words that are used many times throughout all the documents. For instance, if we treated every article in a newspaper as a separate document, looking at the amount of times the word \"he\" or \"she\" is used probably doesn't tell us much about what that given article is about -- however, the amount of times \"touchdown\" is used can provide good signal that the article is probably about sports.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e\u003cem\u003eTerm Frequency\u003c/em\u003e\u003c/strong\u003e is calculated with the following formula:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Clarge%20%5Ctext%7BTerm%20Frequency%7D(t)%20=%20%5Cfrac%7B%5Ctext%7Bnumber%20of%20times%20t%20appears%20in%20a%20document%7D%7D%7B%5Ctext%7Btotal%20number%20of%20terms%20in%20the%20document%7D%7D\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e\u003cem\u003eInverse Document Frequency\u003c/em\u003e\u003c/strong\u003e is calculated with the following formula:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Clarge%20%5Ctext%7BIDF%7D(t)%20=%20log_e(%5Cfrac%7B%5Ctext%7BTotal%20Number%20of%20Documents%7D%7D%7B%5Ctext%7BNumber%20of%20Documents%20with%20t%20in%20it%7D%7D)\"\u003e\u003c/p\u003e\n\u003cp\u003eThe \u003cstrong\u003e\u003cem\u003eTF-IDF\u003c/em\u003e\u003c/strong\u003e value for a given word in a given document is just found by multiplying the two!\u003c/p\u003e\n\u003ch2\u003eSummary\u003c/h2\u003e\n\u003cp\u003eIn this lesson, you learned about some foundational concepts in Natural Language Processing such as stemming and lemmatization, as well as various strategies for converting text data into word vectors.\u003c/p\u003e","frontPage":false},{"exportId":"recommendation-systems-introduction","title":"Recommendation Systems - Introduction","type":"WikiPage","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-recommender-section-intro\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-recommender-section-intro/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eIn this section, you'll learn about machine learning algorithms that we encounter every day in our lives:  recommendation systems! \u003c/p\u003e\n\n\u003ch2\u003eDeveloping a Recommendation System in PySpark\u003c/h2\u003e\n\n\u003cp\u003eIn this section you'll learn about recommendation system modeling approaches and you'll even build your own recommendation system in PySpark! \u003c/p\u003e\n\n\u003ch3\u003eRecommendation Systems\u003c/h3\u003e\n\n\u003cp\u003eA recommendation system allows predicting the future preference list for a certain customer or user, and recommends the top preference for this user. Examples include: which books would a customer prefer to buy on Amazon, which Netflix movie or series would a user watch next, etc. You'll learn about several different types of recommendation system algorithms and how they work.\u003c/p\u003e\n\n\u003ch3\u003eCollaborative Filtering with Singular Value Decomposition\u003c/h3\u003e\n\n\u003cp\u003eCollaborative Filtering (CF) is currently the most widely used approach to build recommendation systems. It often uses matrix factorization under the hood, which you'll learn about in this section.\u003c/p\u003e\n\n\u003ch3\u003eImplementing Recommender Systems with \u003ccode\u003esurprise\u003c/code\u003e\n\u003c/h3\u003e\n\n\u003cp\u003e\u003ccode\u003esurprise\u003c/code\u003e is a library that is optimized to efficiently create recommendations. You'll get a chance to use this library to code up different implementations of collaborative filtering recommendation systems.\u003c/p\u003e\n\n\u003ch3\u003eMatrix Factorization with Alternating Least Squares\u003c/h3\u003e\n\n\u003cp\u003eWe'll also look at another matrix factorization technique called Alternating Least Squares (ALS). This method can prove to be much more effective and robust than SVD. You'll also learn how ALS is implemented in Spark's machine learning library \u003ccode\u003eml\u003c/code\u003e.\u003c/p\u003e\n\n\u003ch3\u003eBuilding a Recommendation System in PySpark\u003c/h3\u003e\n\n\u003cp\u003eYou'll end this section by building your own recommendation system in PySpark using the \u003ccode\u003eMovieLens\u003c/code\u003e rating dataset!\u003c/p\u003e\n\n\u003ch2\u003eSummary\u003c/h2\u003e\n\n\u003cp\u003eIn this section, you'll learn the basics of recommendation systems and how to implement them in \u003ccode\u003esurprise\u003c/code\u003e and Spark!\u003c/p\u003e\n\u003cfooter class=\"fis-footer\" style=\"visibility: hidden;\"\u003e\u003cdiv class=\"fis-feedback\"\u003e\n\u003ch5\u003eHow do you feel about this lesson?\u003c/h5\u003e\n\u003cimg id=\"thumbs-up\" data-repository=\"dsc-recommender-section-intro\" title=\"Thumbs up!\" alt=\"thumbs up\"\u003e\u003cimg id=\"thumbs-down\" data-repository=\"dsc-recommender-section-intro\" title=\"Thumbs down!\" alt=\"thumbs down\"\u003e\n\u003c/div\u003e\n\u003ch5\u003eHave specific feedback? \u003ca href=\"https://github.com/learn-co-curriculum/dsc-recommender-section-intro/issues/new\"\u003eTell us here!\u003c/a\u003e\n\u003c/h5\u003e\u003c/footer\u003e","frontPage":false},{"exportId":"natural-language-processing-introduction","title":"Natural Language Processing - Introduction","type":"WikiPage","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-nlp-section-intro\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-nlp-section-intro/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eThis lesson summarizes the topics we'll be covering in this section and why they'll be important to you as a data scientist.\u003c/p\u003e\n\n\u003ch2\u003eFoundations of Natural Language Processing (NLP)\u003c/h2\u003e\n\n\u003cp\u003eIn this section we will be covering Natural Language Processing (NLP), which refers to analytics tasks that deal with natural human language, in the form of text or speech.\u003c/p\u003e\n\n\u003ch3\u003eNatural Language Tool Kit (NLTK)\u003c/h3\u003e\n\n\u003cp\u003eWe'll start by providing more context on the Natural Language Tool Kit (NLTK), one of the most popular NLP libraries used in Python.  This library was developed by researchers at the University of Pennsylvania, and it has quickly become one of the most powerful and complete library of NLP tools available. \u003c/p\u003e\n\n\u003ch3\u003eRegular Expressions\u003c/h3\u003e\n\n\u003cp\u003eData preprocessing is an essential part of NLP, and that's why being very familiar with \u003cstrong\u003eregular expressions\u003c/strong\u003e is extremely important. Regular expressions, or \"Regex\" is extremely useful for NLP. We can use regex to quickly pattern match and filter through text documents. \u003c/p\u003e\n\n\u003ch3\u003eFeature Engineering for Text Data\u003c/h3\u003e\n\n\u003cp\u003eWorking with text data comes with a lot of ambiguity. Feature engineering for NLP is pretty specific, and in this section you'll learn some feature engineering techniques that are essential when working with text data. You'll learn how to remove stop words from your text, as well as how to create frequency distributions, representing histograms that give us an overview of the total number of times each word occurs in a given text corpus. \u003c/p\u003e\n\n\u003cp\u003eAdditionally, you'll learn about stemming and lemmatization, which is the technique of removing suffixes from our words (and can enhance our text insight by creating frequency histograms \u003cem\u003eafter\u003c/em\u003e having performed stemming or lemmatization!). You'll also learn how to create bigrams, which creates an insight on how often two words occur together!\u003c/p\u003e\n\n\u003ch3\u003eContext-Free Grammars and Part-of-Speech (POS) Tagging\u003c/h3\u003e\n\n\u003cp\u003eIn NLP, it is important to understand what context-free grammars and part-of-speech tagging are. Context-free grammars refer to bits of text that are grammatically correct, but feel like complete nonsense when considering the same bit of text on the semantic level. POS tagging refers to the act of helping a computer understand how to interpret a sentence. The context-free grammars (CFG) defines the rules of how sentences can exist. You'll see multiple examples on how to use both CFG and POS tagging, and why they are important!\u003c/p\u003e\n\n\u003ch3\u003eText Classification\u003c/h3\u003e\n\n\u003cp\u003eWe will finish off this section by explaining the general process to set text data up for classification problems.\u003c/p\u003e\n\n\u003ch2\u003eSummary\u003c/h2\u003e\n\n\u003cp\u003eIn this section, you'll learn the foundations of NLP and different techniques to make a computer understand text!\u003c/p\u003e\n\u003cfooter class=\"fis-footer\" style=\"visibility: hidden;\"\u003e\u003cdiv class=\"fis-feedback\"\u003e\n\u003ch5\u003eHow do you feel about this lesson?\u003c/h5\u003e\n\u003cimg id=\"thumbs-up\" data-repository=\"dsc-nlp-section-intro\" title=\"Thumbs up!\" alt=\"thumbs up\"\u003e\u003cimg id=\"thumbs-down\" data-repository=\"dsc-nlp-section-intro\" title=\"Thumbs down!\" alt=\"thumbs down\"\u003e\n\u003c/div\u003e\n\u003ch5\u003eHave specific feedback? \u003ca href=\"https://github.com/learn-co-curriculum/dsc-nlp-section-intro/issues/new\"\u003eTell us here!\u003c/a\u003e\n\u003c/h5\u003e\u003c/footer\u003e","frontPage":false},{"exportId":"deep-nlp-with-word-embeddings-introduction","title":"Deep NLP with Word Embeddings - Introduction","type":"WikiPage","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-deep-nlp-intro\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-deep-nlp-intro/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eIn this section you'll strengthen your deep learning and natural language processing skills by learning about word embeddings! Word embeddings are a unique coding schema for text corpora that preserve many underlying features, allowing for interesting geometric relations in this hyperspace. Specifically, you'll look at how similarity metrics can represent how two words relate to each other, and these transformations can be applied to multiple word pairs. For example, a similarity metric could encapsulate analogies like \"man is to woman as king is to ____\". \u003c/p\u003e\n\n\u003ch3\u003eWord Embeddings\u003c/h3\u003e\n\n\u003cp\u003eIn this section, you'll learn about the concept of word embeddings, and how you can use them to model the semantic meanings of words in a high-dimensional embedding space! Word embeddings use similarity metrics to represent how two words relate to each other. This way, we can understand the words in our corpus to a bigger extent. A typical example is the example of \"Man\" vs \"woman\" and \"king\" vs \"queen\": word embeddings can capture that the word \"man\" relates to the word \"woman\" the same way the word \"king\" relates to \"queen\"!\u003c/p\u003e\n\n\u003ch3\u003eUsing Word2Vec\u003c/h3\u003e\n\n\u003cp\u003eCreating word embeddings is not an easy task. Word embeddings can be created using so-called \"Word2Vec\" models that are  given enough training data. At its core, Word2Vec is just another deep neural network, that looks at sequences of words and words that are often used in similar contexts (or \u003cem\u003eclose\u003c/em\u003e to each other in sentences). In this section you'll learn how to train a Word2Vec model, and you'll explore the embedding space.\u003c/p\u003e\n\n\u003ch3\u003eClassification with Word Embeddings\u003c/h3\u003e\n\n\u003cp\u003eTo wrap up this section, we'll focus on the practical aspects of how Word2Vec and word embeddings can be used to improve our text classification models. We'll start by learning how transfer learning can be used by loading pre-trained word vectors into our Word2Vec model. Then, we'll learn about how we can get the word vectors we need and combine them into mean word vectors, and how we can streamline this process by writing our own vectorizer class that is compatible with scikit-learn pipelines. Next, we'll see how deep neural networks with their own embedding layers can be trained, and how Keras preprocesses the text data to make everything run smoothly!\u003c/p\u003e\n\n\u003ch2\u003eSummary\u003c/h2\u003e\n\n\u003cp\u003eIn this section, you'll dive deeper into NLP and get better classification results using word embeddings!\u003c/p\u003e\n\u003cfooter class=\"fis-footer\" style=\"visibility: hidden;\"\u003e\u003cdiv class=\"fis-feedback\"\u003e\n\u003ch5\u003eHow do you feel about this lesson?\u003c/h5\u003e\n\u003cimg id=\"thumbs-up\" data-repository=\"dsc-deep-nlp-intro\" title=\"Thumbs up!\" alt=\"thumbs up\"\u003e\u003cimg id=\"thumbs-down\" data-repository=\"dsc-deep-nlp-intro\" title=\"Thumbs down!\" alt=\"thumbs down\"\u003e\n\u003c/div\u003e\n\u003ch5\u003eHave specific feedback? \u003ca href=\"https://github.com/learn-co-curriculum/dsc-deep-nlp-intro/issues/new\"\u003eTell us here!\u003c/a\u003e\n\u003c/h5\u003e\u003c/footer\u003e","frontPage":false},{"exportId":"k-means-clustering","title":"K-means Clustering","type":"WikiPage","content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-k-means-clustering\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-k-means-clustering\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-k-means-clustering/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eIn this lesson, we'll learn about the most popular and widely-used clustering algorithm, K-means clustering. \u003c/p\u003e\n\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\n\u003cp\u003eYou will be able to:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eCompare the different approaches to clustering networks \u003c/li\u003e\n\u003cli\u003eExplain the steps behind the K-means clustering algorithm \u003c/li\u003e\n\u003cli\u003ePerform k-means clustering in scikit-learn \u003c/li\u003e\n\u003cli\u003eExplain how clusters are evaluated \u003c/li\u003e\n\u003cli\u003eDefine an \"elbow plot\" and how to interpret it \u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch2\u003eClustering\u003c/h2\u003e\n\n\u003cp\u003e\u003cstrong\u003e\u003cem\u003eClustering\u003c/em\u003e\u003c/strong\u003e techniques are among the most popular unsupervised machine learning algorithms. The main idea behind clustering is that you want to group objects into similar classes, in a way that:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eintra-class similarity is high (similarity amongst members of the same group is high)\u003c/li\u003e\n\u003cli\u003einter-class similarity is low (similarity of different groups is low)\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003eWhat does \u003cem\u003esimilarity\u003c/em\u003e mean? You should be thinking of it in terms of \u003cem\u003edistance\u003c/em\u003e, just like we did with the k-nearest-neighbors algorithm. The closer two points are, the more similar they are. It is useful to make a distinction between \u003cem\u003ehierarchical\u003c/em\u003e and \u003cem\u003enonhierarchical\u003c/em\u003e clustering algorithms:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003e\u003cp\u003eIn cluster analysis, an \u003cstrong\u003e\u003cem\u003eagglomerative hierarchical\u003c/em\u003e\u003c/strong\u003e algorithm starts with \u003cem\u003en\u003c/em\u003e clusters (where \u003cem\u003en\u003c/em\u003e is the number of observations, so each observation is a cluster), then combines the two most similar clusters, combines the next two most similar clusters, and so on. A \u003cstrong\u003e\u003cem\u003edivisive\u003c/em\u003e\u003c/strong\u003e hierarchical algorithm does the exact opposite, going from 1 to \u003cem\u003en\u003c/em\u003e clusters.\u003c/p\u003e\u003c/li\u003e\n\u003cli\u003e\u003cp\u003eA \u003cstrong\u003e\u003cem\u003enonhierarchical\u003c/em\u003e\u003c/strong\u003e algorithm chooses \u003cem\u003ek\u003c/em\u003e initial clusters and reassigns observations until no improvement can be obtained. How initial clusters and reassignments are done depends on the specific type of algorithm.\u003c/p\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003eAn essential understanding when using clustering methods is that you are basically trying to group data points together without knowing what the \u003cem\u003eactual\u003c/em\u003e cluster/classes are. This is also the main distinction between clustering and classification (which is a supervised learning method). This is why technically, you also don't know how many clusters you're looking for.\u003c/p\u003e\n\n\u003ch2\u003eNon-Hierarchical Clustering With K-Means Clustering\u003c/h2\u003e\n\n\u003cp\u003e\u003cstrong\u003e\u003cem\u003eK-means clustering\u003c/em\u003e\u003c/strong\u003e is the most well-known clustering technique, and it belongs to the class of non-hierarchical clustering methods. When performing k-means clustering, you're essentially trying to find  \u003cimg src=\"https://render.githubusercontent.com/render/math?math=k\"\u003e cluster centers as the mean of the data points that belong to these clusters. One challenging aspect of k-means is that the number \u003cem\u003ek\u003c/em\u003e needs to be decided upon before you start running the algorithm.\u003c/p\u003e\n\n\u003cp\u003eThe k-means clustering algorithm is an iterative algorithm that reaches for a pre-determined number of clusters within an unlabeled dataset, and basically works as follows:\u003c/p\u003e\n\n\u003col\u003e\n\u003cli\u003eSelect  \u003cimg src=\"https://render.githubusercontent.com/render/math?math=k\"\u003e initial seeds \u003c/li\u003e\n\u003cli\u003eAssign each observation to the cluster to which it is \"closest\"\u003c/li\u003e\n\u003cli\u003eRecompute the cluster centroids\u003c/li\u003e\n\u003cli\u003eReassign the observations to one of the clusters according to some rule\u003c/li\u003e\n\u003cli\u003eStop if there is no reallocation \u003c/li\u003e\n\u003c/ol\u003e\n\n\u003cp\u003eTwo assumptions are of main importance for the k-means clustering algorithm:\u003c/p\u003e\n\n\u003col\u003e\n\u003cli\u003eTo compute the \"cluster center\", you calculate the (arithmetic) mean of all the points belonging to the cluster.  Each cluster center is recalculated in the beginning of each new iteration\u003cbr\u003e\n\u003c/li\u003e\n\u003cli\u003eAfter the cluster center has been recalculated, if a given point is now closer to a different cluster center than the center of its current cluster, then that point is reassigned to the clostest center \u003c/li\u003e\n\u003c/ol\u003e\n\n\u003ch2\u003eVisualization of K-means Clustering Algorithm\u003c/h2\u003e\n\n\u003cp\u003eIn the animation below, the green dots are the centroids. Notice how they are randomly assigned at the beginning, and shift with each iteration as they are recalculated to match the center of the points assigned to their cluster. The clustering ends when the centroids find a position in which points are no longer reassigned, meaning that the centroids no longer need to move. \u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-k-means-clustering/master/images/good-centroid-start.gif\"\u003e\u003c/p\u003e\n\n\u003ch2\u003eImplementing K-means Clustering in scikit-learn\u003c/h2\u003e\n\n\u003cp\u003eImplementing k-means clustering with scikit-learn is quite simple because the API mirrors the same functionality that we've seen before. The same preprocessing steps used for supervised learning methods are required -- missing values must be dealt with and all data must be in numerical format (meaning that non-numerical columns must be dropped or one-hot encoded). \u003c/p\u003e\n\n\u003cpre\u003e\u003ccode class=\"python\"\u003efrom sklearn.cluster import KMeans\n\nk_means = KMeans(n_clusters=3) \n\nk_means.fit(some_df) \n\ncluster_assignments = k_means.predict(some_df) \n\n\u003c/code\u003e\u003c/pre\u003e\n\n\u003ch2\u003eEvaluating Cluster Fitness\u003c/h2\u003e\n\n\u003cp\u003eRunning K-means on a dataset is easy enough, but how do we know if we have the best value for  \u003cimg src=\"https://render.githubusercontent.com/render/math?math=k\"\u003e ?  The best bet is to use an accepted metric for evaluating cluster fitness such as \u003ca href=\"https://scikit-learn.org/stable/modules/generated/sklearn.metrics.calinski_harabasz_score.html\"\u003e\u003cstrong\u003e\u003cem\u003eCalinski Harabasz Score\u003c/em\u003e\u003c/strong\u003e\u003c/a\u003e, which is more often referred to by a simpler, \u003cstrong\u003e\u003cem\u003eVariance Ratio\u003c/em\u003e\u003c/strong\u003e.\u003c/p\u003e\n\n\u003ch3\u003eComputing Variance Ratios\u003c/h3\u003e\n\n\u003cp\u003eThe \u003cem\u003evariance ratio\u003c/em\u003e is a ratio of the variance of the points within a cluster, to the variance of a point to points in other clusters. Intuitively, we can understand that we want intra-cluster variance to be low (suggesting that the clusters are tightly knit), and inter-cluster variance to be high (suggesting that there is little to no ambiguity about which cluster the points belong to). \u003c/p\u003e\n\n\u003cp\u003eWe can easily calculate the variance ratio by importing a function from scikit-learn to calculate it for us, as shown below. To use this metric, we just need to pass in the points themselves, and the predicted labels given to each point by the clustering algorithm. The higher the score, the better the fit.\u003c/p\u003e\n\n\u003cpre\u003e\u003ccode class=\"python\"\u003efrom sklearn.metrics import calinski_harabasz_score\n\nprint(calinski_harabasz_score(some_df, cluster_assignments))\n\u003c/code\u003e\u003c/pre\u003e\n\n\u003cp\u003eThere are other metrics that can also be used to evaluate the fitness, such as \u003ca href=\"https://scikit-learn.org/stable/modules/generated/sklearn.metrics.silhouette_score.html#sklearn.metrics.silhouette_score\"\u003eSilhouette Score\u003c/a\u003e. No one metric is best -- they all have slightly different strengths and weaknesses depending on the given dataset and goals. Because of this, it's generally accepted that it's best to pick one metric and stick to it. \u003c/p\u003e\n\n\u003ch3\u003eFinding the Optimal Value of K\u003c/h3\u003e\n\n\u003cp\u003eNow that we have a way to evaluate how well our clusters fit the dataset, we can use this to find the optimal value for  \u003cimg src=\"https://render.githubusercontent.com/render/math?math=k\"\u003e . The best way to do this is to create and fit different k-means clustering objects for every value of  \u003cimg src=\"https://render.githubusercontent.com/render/math?math=k\"\u003e that we want to try, and then compare the variance ratio scores for each. \u003c/p\u003e\n\n\u003cp\u003eWe can then visualize the scores using an \u003cstrong\u003e\u003cem\u003eElbow Plot\u003c/em\u003e\u003c/strong\u003e:\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-k-means-clustering/master/images/new_elbow-method.png\" width=\"500\"\u003e\u003c/p\u003e\n\n\u003cp\u003eAn \u003cem\u003eelbow plot\u003c/em\u003e is a general term for plots like this where we can easily see where we hit a point of diminishing returns. In the plot above, we can see that performance peaks at \u003cem\u003ek=6\u003c/em\u003e, and then begins to drop off. That tells us that our data most likely has 6 naturally occurring clusters in our data. \u003c/p\u003e\n\n\u003cp\u003eElbow plots aren't exclusively used with variance ratios -- it's also quite common to calculate something like distortion (another clustering metric), which will result in a graph with a negative as opposed to a positive slope. \u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-k-means-clustering/master/images/new_elbow_2.png\" width=\"500\"\u003e\u003c/p\u003e\n\n\u003ch4\u003eUnderstanding the Elbow\u003c/h4\u003e\n\n\u003cp\u003eA note on elbow plots: higher scores aren't always better. Higher values of  \u003cimg src=\"https://render.githubusercontent.com/render/math?math=k\"\u003e mean introducing more overall complexity -- we will sometimes see elbow plots that look like this:\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-k-means-clustering/master/images/new_dim_returns.png\" width=\"500\"\u003e\u003c/p\u003e\n\n\u003cp\u003eIn the example above, although k=20 technically scores better than k=4, we choose k=4 because it is the \u003cstrong\u003e\u003cem\u003eElbow\u003c/em\u003e\u003c/strong\u003e on the graph. After the elbow, the metric we're trying to optimize for gets better at a much slower rate. Dealing with 20 clusters, when the fit is only slightly better, isn't worth it -- it's better to treat our data as having only 4 clusters, because that is the simplest overall model that provides the most value with the least complexity!\u003c/p\u003e\n\n\u003ch2\u003eSummary\u003c/h2\u003e\n\n\u003cp\u003eIn this lesson, we learned about different kinds of clustering and explored how the k-means clustering algorithm works. We also learned about how we can quantify the performance of a clustering algorithm using metrics such as variance ratios, and how we can use these metrics to find the optimal value for  \u003cimg src=\"https://render.githubusercontent.com/render/math?math=k\"\u003e by creating elbow plots!\u003c/p\u003e","frontPage":false},{"exportId":"amazon-web-services-introduction","title":"Amazon Web Services - Introduction","type":"WikiPage","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-productionizing-machine-learning-models-section-intro\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-productionizing-machine-learning-models-section-intro/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eIn this section, you'll be introduced to Amazon Web Services (AWS) - the most popular cloud service. \u003c/p\u003e\n\n\u003ch2\u003eMachine Learning and the Cloud\u003c/h2\u003e\n\n\u003cp\u003eWe'll begin this section by learning about all the ways that cloud computing services such as \u003cstrong\u003e\u003cem\u003eAmazon Web Services (AWS)\u003c/em\u003e\u003c/strong\u003e have made things better and easier for data scientists. We'll also explore why being able to productionize the machine learning models you create so that other people can use them is one of the most valuable skills you can have as a data scientist. \u003c/p\u003e\n\n\u003ch2\u003eAmazon Web Services (AWS)\u003c/h2\u003e\n\n\u003cp\u003eOne we understand the importance of cloud services and how they fit into the picture for data scientists, we'll jump right in to the most popular cloud service, AWS. We'll learn about what AWS ecosystem contains and how we can use it. We'll also create an account and learn our way around the AWS dashboard. \u003c/p\u003e\n\n\u003ch2\u003eAWS SageMaker\u003c/h2\u003e\n\n\u003cp\u003eOnce we know the basics of AWS, we'll learn how we can make use of the most important tool for Data Scientists, \u003cstrong\u003e\u003cem\u003eAWS SageMaker\u003c/em\u003e\u003c/strong\u003e! We'll see how we can incorporate AWS SageMaker into our workflow to simplify things like distributed training or model productionization! \u003c/p\u003e\n\n\u003ch2\u003eHands-On Practice Shipping Models\u003c/h2\u003e\n\n\u003cp\u003eFinally, we will train and ship real-world models using AWS SageMaker. We'll start by training and productionizing some classical machine learning models with scikit-learn and then set up endpoints with AWS SageMaker so that we can make them available for inference. Then, we'll move onto training a Deep Learning model with SageMaker, so that we can make use of distributed training to speed things up, and then ship the model to production. Finally, we'll use SageMaker to train and productionize a more advanced Convolutional Neural Network for image classification. \u003c/p\u003e\n\n\u003ch2\u003eSummary\u003c/h2\u003e\n\n\u003cp\u003eBy the end of this section, you'll know the basics of how to use AWS for Data Science projects, and you'll have hands-on experience training and productionizing three different machine learning models. This will set you up for success with your capstone project!\u003c/p\u003e\n\u003cfooter class=\"fis-footer\" style=\"visibility: hidden;\"\u003e\u003cdiv class=\"fis-feedback\"\u003e\n\u003ch5\u003eHow do you feel about this lesson?\u003c/h5\u003e\n\u003cimg id=\"thumbs-up\" data-repository=\"dsc-productionizing-machine-learning-models-section-intro\" title=\"Thumbs up!\" alt=\"thumbs up\"\u003e\u003cimg id=\"thumbs-down\" data-repository=\"dsc-productionizing-machine-learning-models-section-intro\" title=\"Thumbs down!\" alt=\"thumbs down\"\u003e\n\u003c/div\u003e\n\u003ch5\u003eHave specific feedback? \u003ca href=\"https://github.com/learn-co-curriculum/dsc-productionizing-machine-learning-models-section-intro/issues/new\"\u003eTell us here!\u003c/a\u003e\n\u003c/h5\u003e\u003c/footer\u003e","frontPage":false},{"exportId":"exploring-time-series-data-recap","title":"Exploring Time Series Data - Recap","type":"WikiPage","content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-time-series-section-recap\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-time-series-section-recap\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-time-series-section-recap/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\u003ch2\u003eKey Takeaways\u003c/h2\u003e\n\u003cp\u003eThe key takeaways from this section include:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eWhen you import time series data into Pandas, make sure to use the time/date information as index values using either a Pandas \u003ccode\u003etimestamp\u003c/code\u003e or Python \u003ccode\u003edatetime\u003c/code\u003e data type\u003c/li\u003e\n\u003cli\u003eThere are a range of built-in methods in Pandas for easily downsampling or upsampling time series data\u003c/li\u003e\n\u003cli\u003eLine plots and dot plots can be useful for getting a sense of how a time series dataset changes over time\u003c/li\u003e\n\u003cli\u003eHistograms and density plots can be useful for getting a sense of the time-independent distribution of a time series\u003c/li\u003e\n\u003cli\u003eBox and whisker plots per year (or other seasonality period - day, week, month, etc) can be a great way to easily see trends in the distribution of time series data over time\u003c/li\u003e\n\u003cli\u003eHeat maps can also be useful for comparing changes of time series data across a couple of dimensions. For example, with months on one axis and years on another, they can be a great way to see both seasonality and year on year trends\u003c/li\u003e\n\u003cli\u003eA time series is said to be stationary if its statistical properties such as mean and variance remain constant over time\u003c/li\u003e\n\u003cli\u003eMost time series models work on the assumption that the time series are stationary (assumption of homoscedasticity)\u003c/li\u003e\n\u003cli\u003eMany time series datasets \u003cem\u003edo\u003c/em\u003e have trends, violating the assumption of homoscedasticity\u003c/li\u003e\n\u003cli\u003eCommon examples are trends that include linear (straight line over time), exponential, and periodic. Some datasets also have increasing (or decreasing) variance over time\u003c/li\u003e\n\u003cli\u003eAny given dataset may exhibit multiple trends (e.g. linear, periodic, and reduction of variance)\u003c/li\u003e\n\u003cli\u003eRolling statistics can be used to test for trends to see whether the centrality and/or dispersion of time series changes over time\u003c/li\u003e\n\u003cli\u003eThe Dickey-Fuller test is a common test for determining whether a time series contains trends\u003c/li\u003e\n\u003cli\u003eCommon approaches for removing trends and seasonality include taking a log-transform, subtracting the rolling mean, and differencing\u003c/li\u003e\n\u003cli\u003eDecomposing allows you to separately view \u003cem\u003eseasonality\u003c/em\u003e (which could be daily, weekly, annual, etc), \u003cem\u003etrend\u003c/em\u003e, and \u003cem\u003erandom\u003c/em\u003e, which is the variability in time series after removing the effects of the seasonality and trend\u003c/li\u003e\n\u003c/ul\u003e","frontPage":false},{"exportId":"clustering-introduction","title":"Clustering - Introduction","type":"WikiPage","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-clustering-intro\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-clustering-intro/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eIn this section, you'll learn about a useful unsupervised learning technique: clustering. This lesson summarizes the topics you'll be covering in this section.\u003c/p\u003e\n\u003ch2\u003eClustering\u003c/h2\u003e\n\u003cp\u003eClustering techniques are very powerful when you want to group data with similar characteristics together, but have no pre-specified labels. The main goal of clustering is to create clusters that have a high similarity between the data belonging to one cluster while aiming for minimal similarity between clusters.\u003c/p\u003e\n\u003ch3\u003eK-Means Clustering\u003c/h3\u003e\n\u003cp\u003eWe start by providing a basic intuition of the K-means clustering algorithm. When using the K-means clustering algorithm, the number of clusters that you want to obtain is specified upfront and the algorithm aims at the most \"optimal\" cluster centers, given that there are \u003cimg src=\"https://render.githubusercontent.com/render/math?math=K\"\u003e clusters.\u003c/p\u003e\n\u003ch3\u003eHierarchical Agglomerative Clustering\u003c/h3\u003e\n\u003cp\u003eA second branch of clustering algorithms is hierarchical agglomerative clustering. Using hierarchical clustering, unlike K-means clustering, you don't decide on the number of clusters beforehand. Instead, you start with \u003cimg src=\"https://render.githubusercontent.com/render/math?math=n\"\u003e clusters, where \u003cimg src=\"https://render.githubusercontent.com/render/math?math=n\"\u003e is the number of data points, and at each step you join two clusters. You stop joining clusters when a certain criterion is reached.\u003c/p\u003e\n\u003ch3\u003eMarket Segmentation with Clustering\u003c/h3\u003e\n\u003cp\u003eA very common and useful application of clustering is market segmentation. You'll practice your clustering skills on a market segmentation dataset!\u003c/p\u003e\n\u003ch3\u003eSemi-Supervised Learning\u003c/h3\u003e\n\u003cp\u003eAt the end of this section, you'll learn how semi-supervised learning techniques, which are increasingly popular in machine learning, combines both concepts of supervised and unsupervised learning.\u003c/p\u003e\n\u003ch2\u003eSummary\u003c/h2\u003e\n\u003cp\u003eIn this section, you'll learn how to use clustering techniques which are very useful for finding patterns and grouping unlabeled data together.\u003c/p\u003e","frontPage":false},{"exportId":"semi-supervised-learning-and-look-alike-models","title":"Semi-Supervised Learning and Look-Alike Models","type":"WikiPage","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-semi-supervised-learning-and-look-alike-models\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-semi-supervised-learning-and-look-alike-models/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eIn this lesson, we'll learn about some unsupervised learning techniques we can use to supplement our supervised learning techniques.\u003c/p\u003e\n\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\n\u003cp\u003eYou will be able to:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eIdentify appropriate use cases for semi-supervised learning \u003c/li\u003e\n\u003cli\u003eIdentify appropriate use cases for look-alike models \u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch2\u003eCombining Supervised and Unsupervised Learning\u003c/h2\u003e\n\n\u003cp\u003eFor the majority of this section, we've focused exclusively on popular unsupervised learning techniques and their most common use cases. However, in the real world, there are also plenty of examples where it works to our advantage to bring supervised and unsupervised learning algorithms together to supplement each other. In this lesson, we'll look at two common areas combining supervised and unsupervised learning algorithms that allow us to be more effective than just using them on their own. \u003c/p\u003e\n\n\u003ch2\u003eUse Case 1: Look-Alike Models\u003c/h2\u003e\n\n\u003cp\u003eAs we've learned when working with clustering algorithms, one of their most common use cases is for market segmentation. A more advanced, but similar use case is to then use these market segments to create \u003cstrong\u003e\u003cem\u003elook-alike models\u003c/em\u003e\u003c/strong\u003e to help us identify more customers or market segments that we can plausibly assume are equally valuable, due to their similarity with valuable customers or market segments we've already identified. \u003c/p\u003e\n\n\u003cp\u003eTake a look at the following infographic that provides a visual representation of look-alike modeling:\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-semi-supervised-learning-and-look-alike-models/master/images/new_look-alike-model.png\"\u003e\u003c/p\u003e\n\n\u003cp\u003eIn the example above, the dark blue smiley faces represent customer segments that we already know are valuable. These are customers that we have identified in our data, and know for a fact have been good for us. Under normal circumstances, this would mean that we can divide our customers (or, more often, potential customers) into two groups: the group we know is valuable, and everyone else, who are all unknown to us. \u003c/p\u003e\n\n\u003cp\u003eThis is where \u003cem\u003elook-alike modeling\u003c/em\u003e comes in. A look-alike model uses a distance metric of our choice to rate the similarity of each customer in our group of unknowns to customers in our known, valuable group. For customers that look extremely similar to customers in own known valuable group, we can assume with a very high likelihood that these customers will also be valuable, and should direct resources at capturing them! We'll likely also see customers that are only somewhat similar to our valuable group, which tells us that they \u003cem\u003ecould possibly be valuable\u003c/em\u003e, but we aren't sure. And finally, customers that look nothing like our known valuable customers segment, should probably be left alone.  \u003c/p\u003e\n\n\u003cp\u003eIf this sounds suspiciously like clustering to you, you are absolutely correct! Although this could also be framed as a classification or regression problem, it's quite common to see clustering used to help determine similarity. After all, if we want to build a supervised learning model to predict if an unknown customer looks like our known valuable customers, then we need plenty of labeled examples, and we don't always have that luxury! \u003c/p\u003e\n\n\u003cp\u003eIn the real-world, using look-alike models to find other customers that could potentially be valuable to us is often referred to as \u003cstrong\u003e\u003cem\u003eprospecting\u003c/em\u003e\u003c/strong\u003e. Viewed in terms of the infographic above, we would choose direct resources to market to the customers that look like our valuable customers to increase our \u003cstrong\u003e\u003cem\u003etop-of-funnel\u003c/em\u003e\u003c/strong\u003e, meaning that we are trying to increase the number of potential customers that haven't shown interest in our product or company yet but are likely to, due to their similarity to customers that already have. \u003c/p\u003e\n\n\u003ch2\u003eUse Case 2: Semi-Supervised Learning\u003c/h2\u003e\n\n\u003cp\u003eThe second use case we'll talk about combines supervised and unsupervised learning to allow us access to more (pseudo) labeled data so that we can better train our supervised learning models. This technique is called \u003cstrong\u003e\u003cem\u003esemi-supervised learning\u003c/em\u003e\u003c/strong\u003e.  You may also hear it commonly referred to as \u003cstrong\u003e\u003cem\u003eweakly supervised learning\u003c/em\u003e\u003c/strong\u003e, but it means the same thing. \u003c/p\u003e\n\n\u003cp\u003ePicture the following scenario: \u003c/p\u003e\n\n\u003cp\u003eWe are trying to build a supervised learning model, and we have 100,000 observations in our dataset. However, labels are exceedingly expensive, so only 5,000 of these 100,000 observations are labeled. In traditional supervised learning, this means that in a practical sense, we really only have a dataset of 5,000 observations, because we can't do anything with the 95,000 unlabeled examples -- or can we?\u003c/p\u003e\n\n\u003cp\u003eThe main idea behind \u003cem\u003esemi-supervised learning\u003c/em\u003e is to generate \u003cstrong\u003e\u003cem\u003epseudo-labels\u003c/em\u003e\u003c/strong\u003e that are possibly correct (at least better than random chance). To do this, we don't usually use clustering algorithms -- instead, we use our supervised learning algorithms in an unsupervised way. \u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-semi-supervised-learning-and-look-alike-models/master/images/new_semi-supervised.png\"\u003e\u003c/p\u003e\n\n\u003cp\u003eSupervised learning typically follows a set pattern:\u003c/p\u003e\n\n\u003col\u003e\n\u003cli\u003e\u003cp\u003e\u003cstrong\u003e\u003cem\u003eTrain your model on your labeled training data\u003c/em\u003e\u003c/strong\u003e. In the case of our example above, we would build the best model possible with our tiny dataset of 5,000 labeled examples. \u003c/p\u003e\u003c/li\u003e\n\u003cli\u003e\u003cp\u003e\u003cstrong\u003e\u003cem\u003eUse your trained model to generate pseudo-labels for your unlabeled data\u003c/em\u003e\u003c/strong\u003e. This means having our trained model make predictions on our 95,000 unlabeled examples. Since our trained model does better than random chance, this means that our generated pseudo-labels will be at least somewhat more correct than random chance. We can even put a number to this, by looking at the performance our trained model had on the test set. For example, if our trained model had an accuracy of ~70%, then we can assume that ~70% of the pseudo-labels will be correct, ~30% will be incorrect. \u003c/p\u003e\u003c/li\u003e\n\u003cli\u003e\u003cp\u003e\u003cstrong\u003e\u003cem\u003eCombine your labeled data and your pseudo-labeled data into a single, new dataset.\u003c/em\u003e\u003c/strong\u003e. This means that we concatenate all our labeled data of 5,000 examples with the 95,000 pseudo-labeled examples. \u003c/p\u003e\u003c/li\u003e\n\u003cli\u003e\u003cp\u003e\u003cstrong\u003e\u003cem\u003eRetrain your model on the new dataset\u003c/em\u003e\u003c/strong\u003e. Although some of the pseudo-labeled data will certainly be wrong, it's likely that the amount that is correct will be more useful, and the signal that these correctly pseudo-labeled examples provide will outweigh the incorrectly labeled ones, thereby resulting in better overall model performance. \u003c/p\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\n\u003ch3\u003eBenefits and Drawbacks of Semi-Supervised Learning\u003c/h3\u003e\n\n\u003cp\u003eIf semi-supervised learning sounds a bit risky to you, you're not wrong. When done correctly, semi-supervised learning can increase overall model performance by opening up access to much more data than we would have access to, and more data almost always results in better performance, but without the exorbitant costs of paying to have humans generate labels for the data needed. \u003c/p\u003e\n\n\u003cp\u003eHowever, there are definitely some problems that can arise from using a semi-supervised learning approach, if we're not careful and thoughtful throughout.\u003c/p\u003e\n\n\u003ch4\u003eFeedback Loops and Self-Fulfilling Prophecies\u003c/h4\u003e\n\n\u003cp\u003eSemi-supervised learning tends to work fairly well in many use cases and has become quite a popular technique in the field of Deep Learning, which requires massive amounts of labeled data that is often very expensive to obtain. But what happens when our dataset is extremely noisy to begin with? In that case, our incorrect pseudo-labels may skew the model by introducing more \"noise\" than \"signal\". This is partially because we can end up in a feedback loop of sorts. Think about an example where the model has generated an incorrect pseudo-label. If a model trained only on the real data with no pseudo-labels got this example wrong, then what happens when you train the model on the same example, but this time provide a pseudo-label that \"confirms\" this incorrect belief? When done correctly, we can hope that the signal provided by all the correctly pseudo-labeled examples will generalize to help the model correct its mistakes on the ones it got wrong. However, if the dataset is noisy, or the original model wasn't that good to begin with (or both), then it can be quite likely that we are introducing even more incorrect information than correct information, moving the model in the wrong direction.\u003c/p\u003e\n\n\u003cp\u003eSo how do we make sure that we're not making these mistakes when using a semi-supervised approach? \u003cstrong\u003e\u003cem\u003eUse a holdout set!\u003c/em\u003e\u003c/strong\u003e You should definitely have a test set that the model has never seen before to check the performance of your semi-supervised model. Obviously, make sure that your test set only contains actual, ground-truth labeled examples, no pseudo-labels allowed! Also, the noisier your dataset or more complicated your problem, the more likely you are to run into trouble with semi-supervised learning. When possible, try to structure your tasks as binary classification tasks, rather than multi-categorical, and make sure that your dataset is as clean as possible before attempting semi-supervised learning. Although it seems risky, there's a reason companies that are heavy into deep learning and AI research such as Google, Microsoft, and Facebook make heavy use of semi-supervised learning -- when done correctly, it works wonders, without costing an arm and a leg to pay for labeling!\u003c/p\u003e\n\n\u003ch2\u003eSummary\u003c/h2\u003e\n\n\u003cp\u003eIn this lesson, we learned about two popular methodologies for using unsupervised learning in applied, focused ways to help companies generate more revenue, get more customers, or increase model performance without paying for more labeled training data!\u003c/p\u003e\n\u003cfooter class=\"fis-footer\" style=\"visibility: hidden;\"\u003e\u003cdiv class=\"fis-feedback\"\u003e\n\u003ch5\u003eHow do you feel about this lesson?\u003c/h5\u003e\n\u003cimg id=\"thumbs-up\" data-repository=\"dsc-semi-supervised-learning-and-look-alike-models\" title=\"Thumbs up!\" alt=\"thumbs up\"\u003e\u003cimg id=\"thumbs-down\" data-repository=\"dsc-semi-supervised-learning-and-look-alike-models\" title=\"Thumbs down!\" alt=\"thumbs down\"\u003e\n\u003c/div\u003e\n\u003ch5\u003eHave specific feedback? \u003ca href=\"https://github.com/learn-co-curriculum/dsc-semi-supervised-learning-and-look-alike-models/issues/new\"\u003eTell us here!\u003c/a\u003e\n\u003c/h5\u003e\u003c/footer\u003e","frontPage":false},{"exportId":"introduction-to-amazon-sagemaker","title":"Introduction to Amazon SageMaker","type":"WikiPage","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-introduction-to-aws-sagemaker\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-introduction-to-aws-sagemaker/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eIn this lesson, we'll learn about \u003cstrong\u003e\u003cem\u003eAmazon SageMaker\u003c/em\u003e\u003c/strong\u003e, and explore some of the common use cases it covers for data scientists. \u003c/p\u003e\n\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\n\u003cul\u003e\n\u003cli\u003eList the use cases of Amazon SageMaker \u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch2\u003eWhat is SageMaker?\u003c/h2\u003e\n\n\u003cp\u003eSageMaker is a platform created by Amazon to centralize all the various services related to Data Science and Machine Learning. If you're a data scientist working on AWS, chances are that you'll be spending most (if not all) of your time in SageMaker getting things done. You can get to SageMaker by just searching for \"SageMaker\" inside the spotlight search bar in the AWS Console. \u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-introduction-to-aws-sagemaker/master/images/sagemaker.png\"\u003e\u003c/p\u003e\n\n\u003ch2\u003eSageMaker Use Cases\u003c/h2\u003e\n\n\u003cp\u003eWhen you visit the page for SageMaker, you'll notice that the following graphic highlighting the various use cases SageMaker can help with:\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-introduction-to-aws-sagemaker/master/images/use_cases.png\"\u003e\u003c/p\u003e\n\n\u003cp\u003eYou'll also notice these same categories on the sidebar on the left side of the screen, with more detailed links to services that fall under each category:\n\u003cbr\u003e\n\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-introduction-to-aws-sagemaker/master/images/sidebar.png\"\u003e\u003c/p\u003e\n\n\u003cp\u003eHere's a brief explanation of what each of these service areas are used for in a professional data science setting.\u003c/p\u003e\n\n\u003ch3\u003eGround Truth\u003c/h3\u003e\n\n\u003cp\u003eOne of the hardest, most expensive, and most tedious parts of data science is getting the labels needed for supervised learning projects. For projects inside companies, it's quite common to start by gathering the proprietary data needed in order to train a model that can answer the business question and/or provide the service your company needs. One of the major use cases SageMaker provides is a well-structured way to manage data labeling projects. \u003cstrong\u003e\u003cem\u003eSageMaker GroundTruth\u003c/em\u003e\u003c/strong\u003e allows you to manage private teams, in case your information is sensitive, or to manage public teams by leveraging \u003cstrong\u003e\u003cem\u003eAWS Mechanical Turk\u003c/em\u003e\u003c/strong\u003e, which crowdsources labels from an army of public contractors that have signed up and are paid by the label. \u003c/p\u003e\n\n\u003cp\u003eRecently, Amazon launched an automated labeling service that makes use of machine learning models to generate labels in a human-in-the-loop format, where only labels that are above a particular confidence threshold (which you set yourself) are auto-generated by the model. This allows your contractors to focus only on the tough examples, and saves you from having to pay as much for labels for the easy examples which a model can handle. \u003c/p\u003e\n\n\u003ch3\u003eNotebooks\u003c/h3\u003e\n\n\u003cp\u003eThese are exactly what they sound like -- cloud-based jupyter notebooks, a data scientist's 'bread and butter'!  SageMaker notebooks are just like regular jupyter notebooks, with a bit more added functionality. For instance, it's quite easy to choose from a bunch of pre-configured kernels to select which version of Python/TensorFlow/etc. you want to use. You can start a notebook from scratch inside SageMaker and do all of your work in the cloud, or you can upload preexisting notebooks into SageMaker, allowing you to do you work on a local machine and move it over to the cloud when you're ready for training!\u003c/p\u003e\n\n\u003cp\u003eWe strongly recommend you take a minute to poke around inside a SageMaker notebook to get a feel for what it looks like and what it can do. They're pretty amazing!\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-introduction-to-aws-sagemaker/master/images/notebook.png\"\u003e\u003c/p\u003e\n\n\u003ch3\u003eTraining\u003c/h3\u003e\n\n\u003cp\u003eSageMaker's training services allow you to easily leverage cloud computing with AWS's specialized GPU and TPU servers, allowing you to train massive models that simply wouldn't be possible on a local machine. There are a ton of configuration options, and you can easily set budgets, limits, training times, and even auto-tune your hyperparameters! Although this is outside the scope of our lessons on AWS, Amazon provides some pretty amazing (and fast!) tutorials about how to use more specific services like cloud training or \u003ca href=\"https://aws.amazon.com/blogs/aws/sagemaker-automatic-model-tuning/\"\u003emodel tuning\u003c/a\u003e once you've completed this section! \u003c/p\u003e\n\n\u003ch3\u003eInference\u003c/h3\u003e\n\n\u003cp\u003eArguably the most important part of the data science pipeline, \u003cstrong\u003e\u003cem\u003eInference\u003c/em\u003e\u003c/strong\u003e services focus on allowing you to create endpoints so that people can consume your models over the internet! One of the most handy parts of SageMaker's approach to inference is the fact that you can productionize your own model, or just use one of theirs! While there are certainly times where you'll need to create, train, and host your own model, AWS has made things simple by allowing you to use their own models and charging you on a per-use basis. For instance, let's say that you needed to make some time series forecasts. While you could go down the very complicated route of training your own model, you could also just make use of AWS SageMaker's \u003cem\u003eDeepAR\u003c/em\u003e model, which uses the most cutting-edge time series model available to make forecasts on your data. \u003c/p\u003e\n\n\u003ch2\u003eSummary\u003c/h2\u003e\n\n\u003cp\u003eIn this lesson, we learned about Amazon SageMaker, and explored some of the common use cases it covers for data scientists!\u003c/p\u003e\n\u003cfooter class=\"fis-footer\" style=\"visibility: hidden;\"\u003e\u003cdiv class=\"fis-feedback\"\u003e\n\u003ch5\u003eHow do you feel about this lesson?\u003c/h5\u003e\n\u003cimg id=\"thumbs-up\" data-repository=\"dsc-introduction-to-aws-sagemaker\" title=\"Thumbs up!\" alt=\"thumbs up\"\u003e\u003cimg id=\"thumbs-down\" data-repository=\"dsc-introduction-to-aws-sagemaker\" title=\"Thumbs down!\" alt=\"thumbs down\"\u003e\n\u003c/div\u003e\n\u003ch5\u003eHave specific feedback? \u003ca href=\"https://github.com/learn-co-curriculum/dsc-introduction-to-aws-sagemaker/issues/new\"\u003eTell us here!\u003c/a\u003e\n\u003c/h5\u003e\u003c/footer\u003e","frontPage":false},{"exportId":"deep-nlp-with-word-embeddings-recap","title":"Deep NLP with Word Embeddings - Recap","type":"WikiPage","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-deep-nlp-recap\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-deep-nlp-recap/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eCongratulations! You know have a myriad of powerful NLP tools that you can begin to tap into and further explore. With that, take a minute to review some of the key concepts you were exposed to in this section. \u003c/p\u003e\n\n\u003ch2\u003eRNNs and Word Embeddings\u003c/h2\u003e\n\n\u003cp\u003eRemember that word embeddings are a type of vectorization strategy that computes word vectors from a text corpus. They use similarity metrics, which can reveal how certain words relate to each other, or \"semantic relationships\".\u003c/p\u003e\n\n\u003cp\u003eUnlike TF-IDF vectorization, the size of word embeddings is a tunable parameter, which can help overcoming the curse of dimensionality. Word embeddings can be created using Word2Vec models -- given enough training data. \u003c/p\u003e\n\n\u003cp\u003eSince deep learning is used to create Word2Vec models, training word embeddings can be really time consuming, and when building a predictive model you'd want to avoid spending a lot of time here. Pretrained word vectors are very useful here, and GLoVe is the most commonly used model. When using GLoVe, and moving towards a vector representation for any arbitrarily-sized block of text, mean word embeddings can be used.\u003c/p\u003e\n\n\u003ch2\u003eGRUs and LSTMs\u003c/h2\u003e\n\n\u003cp\u003eBuilding on this, you then took a look at some new architectures for neural nets. Aside from having a temporal aspect as with RNNs, GRUs (Gated Recurrent Units) and LSTMs (Long Short Term Memory Cells) have capabilities for both summarizing important information seen prior and forgetting needless details to free up memory. This acts as an analogy to human memory and allows for improved performance in many tasks. \u003c/p\u003e\n\n\u003ch2\u003eSummary\u003c/h2\u003e\n\n\u003cp\u003eIn this section you learned about advanced deep network architectures including RNNs, GRUs, and LSTMs. You also saw how to create word embeddings, an alternative methodology for encoding textual data into numerical spaces. With that, you also saw how to use transfer learning to apply Word2Vec models and improve NLP classification algorithms.\u003c/p\u003e\n\u003cfooter class=\"fis-footer\" style=\"visibility: hidden;\"\u003e\u003cdiv class=\"fis-feedback\"\u003e\n\u003ch5\u003eHow do you feel about this lesson?\u003c/h5\u003e\n\u003cimg id=\"thumbs-up\" data-repository=\"dsc-deep-nlp-recap\" title=\"Thumbs up!\" alt=\"thumbs up\"\u003e\u003cimg id=\"thumbs-down\" data-repository=\"dsc-deep-nlp-recap\" title=\"Thumbs down!\" alt=\"thumbs down\"\u003e\n\u003c/div\u003e\n\u003ch5\u003eHave specific feedback? \u003ca href=\"https://github.com/learn-co-curriculum/dsc-deep-nlp-recap/issues/new\"\u003eTell us here!\u003c/a\u003e\n\u003c/h5\u003e\u003c/footer\u003e","frontPage":false},{"exportId":"data-science-and-machine-learning-engineering","title":"Data Science and Machine Learning Engineering","type":"WikiPage","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-data-science-and-machine-learning-engineering\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-data-science-and-machine-learning-engineering/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eIn this lesson, we'll learn about the importance of being able to write production-quality code to make the models you've trained usable, and the leading cloud environments that make this possible!\u003c/p\u003e\n\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\n\u003cul\u003e\n\u003cli\u003eExplain why cloud computing and putting models into production is important to data scientists \u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch2\u003eData Science Skills and the Job Market\u003c/h2\u003e\n\n\u003cp\u003eYou're almost done with your studies, and will soon be ready to begin the job hunt. Since this is one of the most important topics a Data Scientist can know, we've elected to leave it until the very end, so that it'll be fresh in your mind going into your capstone project and the start of your career -- \u003cstrong\u003e\u003cem\u003eputting models into production\u003c/em\u003e\u003c/strong\u003e. \u003c/p\u003e\n\n\u003cp\u003eAs you start to look at job postings, you probably notice that a \"Data Scientist\" job can mean many, many different things. In some companies, it means a data analyst or a DBA focused on databases or data pipelines. In others, it means someone with a scientific mindset skilled with running A/B tests. Yet others may be highly specialized machine learning roles in areas like NLP, Computer Vision, or Deep Learning -- and these roles may break down further into specializations focused on either research or implementation. As a Junior Data Scientist entering the workforce, it's most likely that you'll land in a generalist role, spending the first few years of your career working on various tasks that focus on all of these areas at least a little bit. Specialization happens later in your career. Out of the gate, the best thing that you can be is a strong generalist, with the demonstrated ability to contribute to many different sorts of projects that might be expected of a Data Science team. \u003c/p\u003e\n\n\u003cp\u003eOver the course of your studies, you've picked up many different skills in many different domain areas that will allow you to contribute to data science projects in a professional environment. However, when it comes to the job market, not all Data Science skills are created equal. While different data scientists or recruiters may rank these skills differently based on their own experiences or needs, one thing most agree on is that the ability to \u003cstrong\u003e\u003cem\u003eproductionize a model\u003c/em\u003e\u003c/strong\u003e is both very valuable and very rare when it comes to Junior Data Scientists. This presents a massive opportunity for you -- if you can become proficient in productionizing the models you've created (and demonstrate this proficiency in your portfolio of projects!), you become a much more interesting candidate for any role.  \u003c/p\u003e\n\n\u003ch2\u003eProductionizing Models as a Career Skill\u003c/h2\u003e\n\n\u003cp\u003eAt large companies such as Google and Facebook, Data Scientists typically run experiments and train models until they have found a solution that works. Once they have trained a validated the model, they typically then hand off productionization of the model to \u003cstrong\u003e\u003cem\u003eMachine Learning Engineers\u003c/em\u003e\u003c/strong\u003e. Whereas the Data Scientist creates the basic prototype, the Machine Learning Engineer's job is to put that model into a production system in a performant and maintainable manner. Whereas Data Scientists at large companies focus on the \"big picture\" by finding solutions to business problems, Machine Learning Engineers focus on the details, implementing the solutions created by the data scientists in the best way possible. Data Scientists focus more on analytics and statistics, whereas Machine Learning Engineers will have a stronger command of backend engineering, data structures and algorithms, and software engineering overall. The following diagram lays out the relationship between different technical roles well:\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-data-science-and-machine-learning-engineering/master/images/new-venn-diagram.png\" height=\"80%\" width=\"80%\"\u003e\u003c/p\u003e\n\n\u003cp\u003eAs you can see from the overlap between \u003cem\u003eData Scientist\u003c/em\u003e and \u003cem\u003eMachine Learning Engineer\u003c/em\u003e , there is still significant overlap between the two -- a Data Scientist should be able to productionize a basic machine learning model, just as a Machine Learning Engineer should able to train a model, validate results, and deal with overfitting. \u003c/p\u003e\n\n\u003ch3\u003eBeing a 'Scrappy' Data Scientist\u003c/h3\u003e\n\n\u003cp\u003eMany junior data scientists have at least one or two areas where they have significant holes in their knowledge. There are many paths into data science, and many junior data scientists are from backgrounds that have overlap with certain parts of data science. For instance, it's not uncommon for bioinformatics professionals or statisticians to rebrand themselves as a data scientist to take advantage of the higher salary in this field. While their backgrounds may give them a very strong understanding of analytics, scientific experimentation, or understanding how machine learning models work, these professionals often have little exposure to engineering, and thus can create and train models, but aren't able to put them into production so that the company can actually use them. Similarly, many junior data scientists on the job market have nothing more than a bachelor's degree in computer science and a passing understanding of machine learning -- in this case, productionizing a model is easy, but they may lack depth of understanding when it comes to the model itself. This isn't an insurmountable problem -- large companies almost always have some role where a candidate's skills can be useful, and they can invest in training the employee and skilling them up in areas where they're a bit weak. \u003c/p\u003e\n\n\u003cp\u003eWith small and medium-sized companies, this is a much bigger problem. Data Scientists in smaller organizations are expected to be a bit more independent, and will likely have to \"wear more hats\". For a data science role at a startup, it's a common expectation for their data scientists to handle every part of a data science project. This means starting by interviewing key stakeholders and identifying the problem to be solved, followed by rapidly prototyping a solution until you've trained/tuned/validated a model that meets your standards, followed by actually putting that model in production!  This means that it's very important to be 'scrappy', and be able to handle anything that's thrown at you as a data scientist. Smaller companies often don't have the funds or the infrastructure for a separate Machine Learning Engineering team to handle the details of implementation. In this respect, being able to productionize a machine learning model is the most practical, useful skill you can have in your Data Science toolbox. For all but the largest companies, it doesn't matter how great you are at training models -- until you put that model into production so that the rest of the organization can actually \u003cem\u003euse\u003c/em\u003e it, it might as well not exist! \u003c/p\u003e\n\n\u003cp\u003eThe TL;DR here is quite simple:\u003c/p\u003e\n\n\u003col\u003e\n\u003cli\u003eMany data scientists don't know how to put machine learning models into production.\u003cbr\u003e\n\u003c/li\u003e\n\u003cli\u003ePutting a model into production is a mandatory skill for data scientists at most small to medium-sized companies.\u003c/li\u003e\n\u003cli\u003eBeing able to productionize models will make you a much more attractive candidate to employers, and give you a competitive advantage!\u003c/li\u003e\n\u003c/ol\u003e\n\n\u003ch2\u003eData Science and Cloud Computing\u003c/h2\u003e\n\n\u003cp\u003eWe've established that being able to productionize machine learning models is a valuable skill -- so how do we do it? This answer has changed in recent years thanks to cloud computing platforms such as \u003cstrong\u003e\u003cem\u003eAmazon Web Services\u003c/em\u003e\u003c/strong\u003e. A decade ago, productionizing a machine learning model would have meant building your own web server with something like \u003ca href=\"http://flask.pocoo.org/\"\u003eFlask\u003c/a\u003e or \u003ca href=\"https://www.djangoproject.com/\"\u003eDjango\u003c/a\u003e and hosting somewhere, just like you would with any web app. However, the creation of cloud computing platforms changed things in a big way, and data science is no exception. Now, we don't even need to worry about things like server code -- instead, we can use preexisting services from AWS that were created specifically to simplify the process of productionizing machine learning solutions!\u003c/p\u003e\n\n\u003cp\u003eFor the remainder of this section, we're going to dig deep into all the amazing tools AWS provides, and learn how we can use them to make you more effective data scientists! \u003c/p\u003e\n\n\u003ch2\u003eSummary\u003c/h2\u003e\n\n\u003cp\u003eIn this lesson, we learned about the similarities and differences between Data Scientists and Machine Learning Engineers. We also learned why the ability to productionize a machine learning model is a crucial skill for data scientists, as well as how this skill can provide a great competitive advantage when applying for jobs!\u003c/p\u003e\n\u003cfooter class=\"fis-footer\" style=\"visibility: hidden;\"\u003e\u003cdiv class=\"fis-feedback\"\u003e\n\u003ch5\u003eHow do you feel about this lesson?\u003c/h5\u003e\n\u003cimg id=\"thumbs-up\" data-repository=\"dsc-data-science-and-machine-learning-engineering\" title=\"Thumbs up!\" alt=\"thumbs up\"\u003e\u003cimg id=\"thumbs-down\" data-repository=\"dsc-data-science-and-machine-learning-engineering\" title=\"Thumbs down!\" alt=\"thumbs down\"\u003e\n\u003c/div\u003e\n\u003ch5\u003eHave specific feedback? \u003ca href=\"https://github.com/learn-co-curriculum/dsc-data-science-and-machine-learning-engineering/issues/new\"\u003eTell us here!\u003c/a\u003e\n\u003c/h5\u003e\u003c/footer\u003e","frontPage":false},{"exportId":"tuning-neural-networks-introduction","title":"Tuning Neural Networks - Introduction","type":"WikiPage","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-tuning-neural-networks-intro\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-tuning-neural-networks-intro/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eNow that you have a general sense of the architecture of neural networks and some of their underlying concepts, its time to further investigate how to properly tune a model for optimal performance. Specifically, you'll take a look at two main techniques: regularization and normalization. \u003c/p\u003e\n\n\u003ch2\u003eRegularization\u003c/h2\u003e\n\n\u003cp\u003eYou've seen regularization before in many other models including linear regression. For example, recall the L1 and L2 penalties which modify ordinary linear regression. These updated loss functions can help tune models so they do not overfit to the training data. For neural networks, you'll use a surprisingly similar process in order to achieve well trained models that are neither overfit nor underfit.\u003c/p\u003e\n\n\u003ch2\u003eNormalization and Tuning Neural Networks\u003c/h2\u003e\n\n\u003cp\u003eAnother modeling problem occurs when one gets trapped into a local minimum when searching for an optimal solution using an iterative approach such as gradient descent. One technique for counteracting this scenario is normalizing features. Normalization in deep learning models can drastically decrease computation time, mitigate common issues such as vanishing or exploding gradients, and increase model performance.\u003c/p\u003e\n\n\u003ch3\u003eOptimization\u003c/h3\u003e\n\n\u003cp\u003eFinally, you'll look at alternative optimization algorithms. These are of primary interest when one encounters local minimum. Knowing when one has hit such a pitfall can be challenging and typically requires experimenting with different optimization approaches and learning rates.\u003c/p\u003e\n\n\u003ch2\u003eSummary\u003c/h2\u003e\n\n\u003cp\u003eIn this section, you'll extend your deep learning knowledge by learning about regularization and optimizing your neural network models. \u003c/p\u003e\n\u003cfooter class=\"fis-footer\" style=\"visibility: hidden;\"\u003e\u003cdiv class=\"fis-feedback\"\u003e\n\u003ch5\u003eHow do you feel about this lesson?\u003c/h5\u003e\n\u003cimg id=\"thumbs-up\" data-repository=\"dsc-tuning-neural-networks-intro\" title=\"Thumbs up!\" alt=\"thumbs up\"\u003e\u003cimg id=\"thumbs-down\" data-repository=\"dsc-tuning-neural-networks-intro\" title=\"Thumbs down!\" alt=\"thumbs down\"\u003e\n\u003c/div\u003e\n\u003ch5\u003eHave specific feedback? \u003ca href=\"https://github.com/learn-co-curriculum/dsc-tuning-neural-networks-intro/issues/new\"\u003eTell us here!\u003c/a\u003e\n\u003c/h5\u003e\u003c/footer\u003e","frontPage":false},{"exportId":"pca-introduction","title":"PCA - Introduction","type":"WikiPage","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-pca-introduction\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-pca-introduction/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eIn this section, you'll learn about principal component analysis, or PCA, one of the most famous \u003cem\u003eUnsupervised Learning Techniques\u003c/em\u003e. PCA is a dimensionality reduction technique. It allows you to compress a dataset into a lower dimensional space with fewer features while maintaining as much of the original information as possible.\u003c/p\u003e\n\n\u003ch2\u003eThe Curse of Dimensionality\u003c/h2\u003e\n\n\u003cp\u003eThe curse of dimensionality is a general mathematical problem relating to the exploding size of space as you continue to add additional dimensions. This can be particularly problematic when dealing with large datasets. The more features you have, the more data you have about the scenario, but the more difficult it might be to exhaustively explore combinations of these features.\u003c/p\u003e\n\n\u003ch2\u003ePCA Use Cases\u003c/h2\u003e\n\n\u003cp\u003eThe curse of dimensionality is certainly one motivating factor for PCA. If you can't process all of the information at your disposal, then an alternative path around is necessary. Dimensionality reduction techniques such as PCA can be essential in such situations. PCA can also help improve regression and classification algorithms in many cases. In particular, algorithms are less prone to overfitting when the underlying data itself has first been compressed, reducing noise or other anomalies. Finally, PCA can also be helpful for visualizing the structure of large datasets. After all, you are limited to 2 or 3 dimensions when visualizing data. As such, reducing a dataset to 2 or 3 primary features is monumental in creating a visualization.\u003c/p\u003e\n\n\u003ch2\u003eSummary\u003c/h2\u003e\n\n\u003cp\u003eIn this section, you'll explore PCA in depth using scikit-learn, and coding your own version from scratch using NumPy. Throughout this section, keep in mind use cases for PCA such as the curse of dimensionality.\u003c/p\u003e\n\u003cfooter class=\"fis-footer\" style=\"visibility: hidden;\"\u003e\u003cdiv class=\"fis-feedback\"\u003e\n\u003ch5\u003eHow do you feel about this lesson?\u003c/h5\u003e\n\u003cimg id=\"thumbs-up\" data-repository=\"dsc-pca-introduction\" title=\"Thumbs up!\" alt=\"thumbs up\"\u003e\u003cimg id=\"thumbs-down\" data-repository=\"dsc-pca-introduction\" title=\"Thumbs down!\" alt=\"thumbs down\"\u003e\n\u003c/div\u003e\n\u003ch5\u003eHave specific feedback? \u003ca href=\"https://github.com/learn-co-curriculum/dsc-pca-introduction/issues/new\"\u003eTell us here!\u003c/a\u003e\n\u003c/h5\u003e\u003c/footer\u003e","frontPage":false},{"exportId":"introduction-to-regular-expressions","title":"Introduction to Regular Expressions","type":"WikiPage","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-introduction-to-regular-expressions\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-introduction-to-regular-expressions/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eIn this lesson, we'll learn about how we can use \u003cstrong\u003e\u003cem\u003eRegular Expressions\u003c/em\u003e\u003c/strong\u003e for pattern matching and filtering when working with text data. \u003c/p\u003e\n\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\n\u003cp\u003eYou will be able to:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eIdentify common use cases where regular expressions are useful \u003c/li\u003e\n\u003cli\u003eCreate regex code to capture meaningful patterns found in text \u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch2\u003eWhat Are Regular Expressions?\u003c/h2\u003e\n\n\u003cp\u003e\u003cstrong\u003e\u003cem\u003eRegular Expressions\u003c/em\u003e\u003c/strong\u003e are a type of pattern that describe some text. We can use these regular expressions to quickly match patterns and filter through text documents. Regular Expressions (or regex, for short) are an important tool anytime we need to pull information from a larger text document without manually reading the entire thing. For data scientists, regex is extremely useful for data gathering. With regex, we can quickly scrape webpages by using regex to search through the html and find the info needed. \u003c/p\u003e\n\n\u003ch3\u003eUse Cases for NLP\u003c/h3\u003e\n\n\u003cp\u003eRegex is especially useful for Natural Language Processing. By definition, just about any text document you work with on an NLP task is going to be one that contains a large amount of text. One of the more common NLP-specific use cases for regex is to use regex during the tokenization stage to define the rules for where we should split strings into separate tokens. As an example, NLTK's basic \u003ccode\u003eword_tokenize()\u003c/code\u003e function would split a word that contains an apostrophe into 3 separate tokens -- \u003ccode\u003e'they're'\u003c/code\u003e gets broken into \u003ccode\u003e[\"they\", \"'\", \"re\"]\u003c/code\u003e. This is because the word tokenizer has instructions to just grab sequences of letters as the basic tokens, and an apostrophe isn't a letter. When preprocessing text data, it's quite common to use some small regex patterns to create a more intelligent tokenization scheme to avoid problems like this, so that our tokenizer treats words like \u003ccode\u003e'they're'\u003c/code\u003e as a single token. \u003c/p\u003e\n\n\u003ch2\u003eCreating Basic Patterns\u003c/h2\u003e\n\n\u003cp\u003eRegex is only as good as the \u003cstrong\u003e\u003cem\u003ePatterns\u003c/em\u003e\u003c/strong\u003e we create. We can use these patterns to find, or to replace text. There are many, many things we can do with regex, and covering them all is outside the scope of this lesson. Instead, we'll just focus on some of the more useful, basic patterns that allow us to begin using regex to work with text data. \u003c/p\u003e\n\n\u003cp\u003eLet's take a look at a basic regex pattern, to get a feel for what they look like. \u003c/p\u003e\n\n\u003cpre\u003e\u003ccode class=\"python\"\u003eimport re\nsentence = 'he said that she said \"hello\".'\npattern = 'he'\np = re.compile(sentence)\np.findall() # Output will be ['he', 'he, 'he']\n\u003c/code\u003e\u003c/pre\u003e\n\n\u003cp\u003eWe define a pattern by a Python string. We can then use the regular expressions library, \u003ccode\u003ere\u003c/code\u003e, to compile this pattern. Once we have a compiled pattern, we just need to pass in a string and the pattern will find every instance of that pattern in the string. \u003c/p\u003e\n\n\u003cp\u003eFor people new to regex, the results from the pattern above might be surprising at first. The pattern successfully matches the word 'he', but it also matches the letters 'he' that are found inside of the words 'she' and 'hello'.  Subsequences inside of larger sequences are fair game to regex. If we just wanted to match the word 'he', we would need to specify that the pattern needs to start and end with a space, or use of \u003cstrong\u003e\u003cem\u003eanchors\u003c/em\u003e\u003c/strong\u003e for things like word boundaries. \u003c/p\u003e\n\n\u003ch2\u003eRanges, Groups, and Quantifiers\u003c/h2\u003e\n\n\u003cp\u003eObviously, we don't want to have to explicitly type every valid match for any search into our pattern. That would defeat the purpose. Luckily, we don't have to type every possible uppercase letter to match on uppercase letters. Instead, we can use a \u003cstrong\u003e\u003cem\u003eRange\u003c/em\u003e\u003c/strong\u003e such as \u003ccode\u003e[A-Z]\u003c/code\u003e. This will match any uppercase letter. Ranges are always inside of square brackets. We can put many things inside of ranges at the same time, and regex will match on any of them. For instance, if we wanted to find any uppercase letter, lowercase letter, or digit, we could use \u003ccode\u003e[A-Za-z0-9]\u003c/code\u003e. \u003c/p\u003e\n\n\u003ch3\u003eCharacter Classes\u003c/h3\u003e\n\n\u003cp\u003eCharacter classes are a special case of ranges. Since it's quite a common task to use ranges to do things like match on words or numbers, regex actually includes character classes as a shortcut. For instance, we could use \u003ccode\u003e\\d\u003c/code\u003e to match any digit -- this is equivalent to using \u003ccode\u003e[0-9]\u003c/code\u003e. We could also use \u003ccode\u003e\\w\u003c/code\u003e to match on any word. In the same vein, we can use \u003ccode\u003e\\D\u003c/code\u003e to get anything that \u003cem\u003eisn't\u003c/em\u003e a digit, or \u003ccode\u003e\\W\u003c/code\u003e to match on everything that isn't a word. There are a few other types of character classes as well. For a full list, check out the cheat sheet below!\u003c/p\u003e\n\n\u003ch3\u003eGroups and Quantifiers\u003c/h3\u003e\n\n\u003cp\u003eGroups are kind of like ranges, but they specify an exact pattern to match on. Groups are denoted by parentheses. Whereas \u003ccode\u003e[A-Z0-9]\u003c/code\u003e matches on any uppercase letter or any digit, \u003ccode\u003e(A-Z0-9)\u003c/code\u003e will only match on the sequence \u003ccode\u003e'A-Z0-9'\u003c/code\u003e exactly. This becomes much more useful when paired with \u003cstrong\u003e\u003cem\u003eQuantifiers\u003c/em\u003e\u003c/strong\u003e, which allows us to specify how many times a group should happen in a row. If we want to specify an exact number of times, we can use curly braces. For instance, a group followed by \u003ccode\u003e{3}\u003c/code\u003e will only match on patterns that have that group repeated exactly 3 times. The most common quantifiers are usually:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003e\n\u003ccode\u003e*\u003c/code\u003e (0 or more times)\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003e+\u003c/code\u003e (1 or more times)\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003e?\u003c/code\u003e (0 or 1 times)\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003eIn this way, we can fill a grouping with any pattern, tell and specify the number of times we can expect to see that pattern. When we include things like ranges, groupings, and quantifiers together, it becomes easy to write a pattern that can match complex things, like email addresses -- take a look at the example provided below, and see if you can figure out how it works!\u003c/p\u003e\n\n\u003cp\u003e\u003ccode\u003e'([A-Za-z]+)@([A-Za-z]+)\\.com'\u003c/code\u003e \u003c/p\u003e\n\n\u003cp\u003eThis pattern matches basic email addresses like '\u003ca href=\"mailto:joe@gmail.com\"\u003ejoe@gmail.com\u003c/a\u003e', but not '\u003ca href=\"mailto:john.doe@gmail.com\"\u003ejohn.doe@gmail.com\u003c/a\u003e', or '\u003ca href=\"mailto:joe@stanford.edu\"\u003ejoe@stanford.edu\u003c/a\u003e'. Take a look at the pattern again -- how would you need to modify the pattern in order for it to match either of those, as well?\u003c/p\u003e\n\n\u003ch2\u003eAlways Keep A Cheat Sheet Handy\u003c/h2\u003e\n\n\u003cp\u003eRegex is confusing, but it gets easier. With that being said, don't worry about trying to memorize all of the different symbols and metacharacters. Instead, focus on how patterns work, and just look up the symbols when you need them. The internet is filled with great regex cheatsheets. Here's an easy one to keep on hand for future reference:\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-introduction-to-regular-expressions/master/images/regex_cheat_sheet.png\"\u003e\u003c/p\u003e\n\n\u003ch2\u003eSummary\u003c/h2\u003e\n\n\u003cp\u003eIn this lesson, we learned about what regular expressions are, how they are used in NLP for specific tasks, and some common patterns and tools in regex. \u003c/p\u003e\n\u003cfooter class=\"fis-footer\" style=\"visibility: hidden;\"\u003e\u003cdiv class=\"fis-feedback\"\u003e\n\u003ch5\u003eHow do you feel about this lesson?\u003c/h5\u003e\n\u003cimg id=\"thumbs-up\" data-repository=\"dsc-introduction-to-regular-expressions\" title=\"Thumbs up!\" alt=\"thumbs up\"\u003e\u003cimg id=\"thumbs-down\" data-repository=\"dsc-introduction-to-regular-expressions\" title=\"Thumbs down!\" alt=\"thumbs down\"\u003e\n\u003c/div\u003e\n\u003ch5\u003eHave specific feedback? \u003ca href=\"https://github.com/learn-co-curriculum/dsc-introduction-to-regular-expressions/issues/new\"\u003eTell us here!\u003c/a\u003e\n\u003c/h5\u003e\u003c/footer\u003e","frontPage":false},{"exportId":"project-submission-and-review-online","title":"Project Submission \u0026 Review (Online)","type":"WikiPage","content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-project-submissions-online\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-project-submissions-online\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-project-submissions-online/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e  \u003ch2\u003eIntroduction\u003c/h2\u003e  \u003cp\u003eIn this lesson, we review the requirements, submission, and review process for the Phase Projects.\u003c/p\u003e  \u003ch2\u003eObjectives\u003c/h2\u003e  \u003cp\u003eYou will be able to:\u003c/p\u003e  \u003cul\u003e \u003cli\u003eCreate project deliverables that meet Flatiron School requirements\u003c/li\u003e \u003cli\u003eSubmit your project deliverables in Canvas\u003c/li\u003e \u003cli\u003ePrepare for your project review\u003c/li\u003e \u003c/ul\u003e  \u003ch2\u003eSchedule Your Review ASAP\u003c/h2\u003e  \u003cp\u003e\u003cstrong\u003eReach out to an instructor immediately via Slack to let them know you've started your project and schedule your project review.\u003c/strong\u003e If you're not sure who to schedule with, please ask in your cohort channel in Slack.\u003c/p\u003e  \u003ch2\u003eCreate Your Project Deliverables\u003c/h2\u003e  \u003cp\u003eComplete the deliverables for your project, guided by the rubric at the bottom of the main project assignment. Keep in mind that the audience for these deliverables is not only your teacher, but also potential employers. Employers will look at your project deliverables to evaluate multiple skills, including coding, modeling, communication, and domain knowledge. You will want to polish these as much as you can, both during the course and afterwards.\u003c/p\u003e  \u003ch3\u003eGitHub Repository\u003c/h3\u003e  \u003cp\u003eYour GitHub repository is the public-facing version of your project that your instructors and potential employers will see - make it as accessible as you can. At a minimum, it should contain all your project files and a README.md file that summarizes your project and helps visitors navigate the repository.\u003c/p\u003e  \u003ch3\u003eJupyter Notebook\u003c/h3\u003e  \u003cp\u003eYour Jupyter Notebook is the primary source of information about your analysis. At a minimum, it should contain or import all of the code used in your project and walk the reader through your project from start to finish. You may choose to use multiple Jupyter Notebooks in your project, but you should have one that provides a full project overview as a point of entry for visitors.\u003c/p\u003e  \u003ch3\u003eNon-Technical Presentation\u003c/h3\u003e  \u003cp\u003eYour non-technical presentation is your opportunity to communicate clearly and concisely about your project and it's real-world relevance. The target audience should be people with limited technical knowledge who may be interested in leveraging your project. We recommend using Google Slides, PowerPoint or Keynote to create your presentation slides. You will then record yourself delivering the presentation.\u003c/p\u003e  \u003ch2\u003eSubmit Your Project\u003c/h2\u003e  \u003cp\u003eTo submit your project in Canvas, you will create and upload PDF versions of three project deliverables, then upload a recording of your video presentation. You will also submit the URL to your GitHub repository in a separate assignment.\u003c/p\u003e  \u003ch3\u003ePresentation Slides PDF Creation\u003c/h3\u003e  \u003col\u003e \u003cli\u003eExport your presentation as a PDF from the program in which you created it.\u003c/li\u003e \u003cli\u003eGive it a short descriptive file name (e.g. \u003ccode\u003epresentation.pdf\u003c/code\u003e).\u003c/li\u003e \u003cli\u003ePlace a copy of the PDF in your GitHub repository.\u003c/li\u003e \u003c/ol\u003e  \u003ch3\u003eGitHub Repository PDF Creation\u003c/h3\u003e  \u003col\u003e \u003cli\u003eNavigate to the root directory of your project repository on GitHub, using your browser (we recommend Google Chrome).\u003c/li\u003e \u003cli\u003eSave the webpage as a PDF using the browser's Print functionality (\u003ca href=\"https://www.wikihow.com/Save-a-Web-Page-as-a-PDF-in-Google-Chrome\"\u003eGoogle Chrome Save to PDF instructions\u003c/a\u003e)\u003c/li\u003e \u003cli\u003eGive it a short descriptive file name (e.g. \u003ccode\u003egithub.pdf\u003c/code\u003e).\u003c/li\u003e \u003c/ol\u003e  \u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-project-submissions-online/master/repo_pdf.gif\" alt=\"Repository PDF Creation\"\u003e\u003c/p\u003e  \u003ch3\u003eJupyter Notebook PDF Creation\u003c/h3\u003e  \u003col\u003e \u003cli\u003eOpen your Notebook in your browser (we recommend Google Chrome).\u003c/li\u003e \u003cli\u003e\n\u003cstrong\u003eRun the Notebook from start to finish\u003c/strong\u003e so that your output is visible.\u003c/li\u003e \u003cli\u003eSave the page as a PDF using the browser's Print functionality (\u003ca href=\"https://www.wikihow.com/Save-a-Web-Page-as-a-PDF-in-Google-Chrome\"\u003eGoogle Chrome Save to PDF instructions\u003c/a\u003e)\u003c/li\u003e \u003cli\u003eGive it a short descriptive file name (e.g. \u003ccode\u003enotebook.pdf\u003c/code\u003e).\u003c/li\u003e \u003c/ol\u003e  \u003cp\u003eIf you have difficulty creating a PDF version of your notebook, you can use \u003ca href=\"https://htmtopdf.herokuapp.com/ipynbviewer/\"\u003ethis tool\u003c/a\u003e instead. Set the Results Format to HTML + PDF. Then click View and Convert. Once its done, you should see links to .html and .pdf versions above the View and Convert button.\u003c/p\u003e  \u003ch3\u003ePDF Submission in Canvas\u003c/h3\u003e  \u003cp\u003eYou will need to submit all three PDF files as a single submission:\u003c/p\u003e  \u003col\u003e \u003cli\u003eClick \"Submit Assignment\" at the top of the \"Phase X Project\" assignment in the \"Milestones\" topic.\u003c/li\u003e \u003cli\u003eIn the \"File Upload\" box, click \"Choose File\" button to upload a single file.\u003c/li\u003e \u003cli\u003eClick the \"Add Another File\" link to upload an additional file.\u003c/li\u003e \u003cli\u003eRepeat Step 3 to upload one more file. After this is done, all three files should be uploaded.\u003c/li\u003e \u003cli\u003eHit the blue \"Submit Assignment\" button.\u003c/li\u003e \u003c/ol\u003e  \u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-project-submissions-online/master/project_3pdf_submission.gif\" alt=\"Project PDF Submission\"\u003e\u003c/p\u003e  \u003ch3\u003ePresentation Recording and Submission\u003c/h3\u003e  \u003cp\u003eAfter you've submitted the PDF files for the project assignment, you will upload a recording of your presentation as a media comment on your submission:\u003c/p\u003e  \u003col\u003e \u003cli\u003eRecord your live presentation to a video file on your computer. We recommend using Zoom to record your live presentation to a local video file (\u003ca href=\"https://support.zoom.us/hc/en-us/articles/201362473-Local-recording\"\u003einstructions here\u003c/a\u003e). Video files must be under 500 MB and formatted as 3GP, ASF, AVI, FLV, M4V, MOV, MP4, MPEG, QT, or WMV.\u003c/li\u003e \u003cli\u003eClick \"Submission Details\" on the top right of the \"Phase X Project\" assignment in the \"Milestones\" topic.\u003c/li\u003e \u003cli\u003eClick \"Media Comment\" beneath the \"Add a Comment\" box on the right of the page.\u003c/li\u003e \u003cli\u003eClick \"Upload Media\" and \"Select Video File\" to upload your file.\u003c/li\u003e \u003cli\u003eThe thumbnail for your video will appear as a blue rectangle while Zoom processes your file - return to this page later to confirm that your recording uploaded successfully.\u003c/li\u003e \u003c/ol\u003e  \u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-project-submissions-online/master/video_recording_submission.gif\" alt=\"Video Recording Submission\"\u003e\u003c/p\u003e  \u003ch3\u003eURL Submission in Canvas\u003c/h3\u003e  \u003cp\u003eThere is an additional Canvas assignment where you will just enter the URL for your project's GitHub repository. This is located in the \"Milestones\" topic in this course as \"Phase X Project - GitHub Repository URL.\"\u003c/p\u003e  \u003ch2\u003ePrepare For Project Review\u003c/h2\u003e  \u003cp\u003eProject reviews are focused on preparing you for technical interviews. Treat project reviews as if they were technical interviews, in both attitude and technical presentation \u003cem\u003e(sometimes technical interviews will feel arbitrary or unfair - if you want to get the job, commenting on that is seldom a good choice)\u003c/em\u003e.\u003c/p\u003e  \u003cp\u003eThe project review is comprised of a 45 minute 1:1 session with one of the instructors. During your project review, be prepared to:\u003c/p\u003e  \u003ch3\u003e1. Deliver your PDF presentation to a non-technical stakeholder.\u003c/h3\u003e  \u003cp\u003eIn this phase of the review (~10 mins) your instructor will play the part of a non-technical stakeholder that you are presenting your findings to. The presentation  should not exceed 5 minutes, giving the \"stakeholder\" 5 minutes to ask questions.\u003c/p\u003e  \u003cp\u003eIn the first half of the presentation (2-3 mins), you should summarize your methodology in a way that will be comprehensible to someone with no background in data science and that will increase their confidence in you and your findings. In the second half (the remaining 2-3 mins) you should summarize your findings and be ready to answer a couple of non-technical questions from the audience. The questions might relate to technical topics (sampling bias, confidence, etc) but will be asked in a non-technical way and need to be answered in a way that does not assume a background in statistics or machine learning. You can assume a smart, business stakeholder, with a non-quantitative college degree.\u003c/p\u003e  \u003ch3\u003e2. Go through the Jupyter Notebook, answering questions about how you made certain decisions. Be ready to explain things like:\u003c/h3\u003e \u003cpre\u003e\u003ccode\u003e* \"How did you pick the question(s) that you did?\"\u003cbr\u003e* \"Why are these questions important from a business perspective?\"\u003cbr\u003e* \"How did you decide on the data cleaning options you performed?\"\u003cbr\u003e* \"Why did you choose a given method or library?\"\u003cbr\u003e* \"Why did you select those visualizations and what did you learn from each of them?\"\u003cbr\u003e* \"Why did you pick those features as predictors?\"\u003cbr\u003e* \"How would you interpret the results?\"\u003cbr\u003e* \"How confident are you in the predictive quality of the results?\"\u003cbr\u003e* \"What are some of the things that could cause the results to be wrong?\" \u003c/code\u003e\u003c/pre\u003e \u003cp\u003eThink of the first phase of the review (~30 mins) as a technical boss reviewing your work and asking questions about it before green-lighting you to present to the business team. You should practice using the appropriate technical vocabulary to explain yourself. Don't be surprised if the instructor jumps around or sometimes cuts you off - there is a lot of ground to cover, so that may happen.\u003c/p\u003e  \u003cp\u003eIf any requirements are missing or if significant gaps in understanding are uncovered, be prepared to do one or all of the following: * Perform additional data cleanup, visualization, feature selection, modeling and/or model validation * Submit an improved version * Meet again for another Project Review\u003c/p\u003e  \u003cp\u003eWhat won't happen: * You won't be yelled at, belittled, or scolded * You won't be put on the spot without support * There's nothing you can do to instantly fail or blow it\u003c/p\u003e  \u003ch2\u003eGrading\u003c/h2\u003e  \u003cp\u003eYour teacher will use the rubric at the bottom of the main project assignment to grade your project. In order to pass, you must properly submit your project and score \"Accomplished\" or \"Exemplary\" on nearly all rubric elements. You will receive a score of P (Pass) or NP (No Pass) - you must pass in order to move to the next phase with your cohort. Your teacher will grade your submission sometime after your review.\u003c/p\u003e  \u003ch2\u003eConclusion\u003c/h2\u003e  \u003cp\u003eThank you for your hard work on this project - you're going to do great! Remember that future employers will also look at your projects when deciding whether to hire you, so having complete, polished projects will help you tremendously not only to pass this assignment, but also to get the job you want after you graduate.\u003c/p\u003e  \u003cp\u003eIf you have any questions about the project submission or review process, don't hesitate to ask your teacher.\u003c/p\u003e","frontPage":false},{"exportId":"convolutional-neural-networks-recap","title":"Convolutional Neural Networks - Recap","type":"WikiPage","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-cnn-recap\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-cnn-recap/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eWell done! In this section you learned all about convolutional neural networks! You should now have enough of an introductory primer to be able to do some image recognition tasks on your own! \u003c/p\u003e\n\n\u003ch2\u003eKey Takeaways\u003c/h2\u003e\n\n\u003cp\u003eRemember that the essence of a CNN is the convolutional operation. A window is slided across the image based on a stride size. Padding can be used to prevent shrinkage and to make sure pixels at the edge of an image deserve the necessary attention. Each convolution then works to adjust the weights of the kernel through backpropagation  during training. Going back to the general architecture, max pooling is typically used between convolutional layers to reduce the dimensionality. \u003c/p\u003e\n\n\u003cp\u003eOverall, CNNs are a useful model for image recognition due to their ability to recognize visual patterns at varying scales. After developing the convolutional and pooling layers to form a base, the end of the network architecture still connects back to a densely connected network to perform classification.\u003c/p\u003e\n\n\u003ch2\u003eSummary\u003c/h2\u003e\n\n\u003cp\u003eIn this section you learned all about convolutional neural networks! From here, you'll learn more about tuning neural networks and other neural network architectures!\u003c/p\u003e\n\u003cfooter class=\"fis-footer\" style=\"visibility: hidden;\"\u003e\u003cdiv class=\"fis-feedback\"\u003e\n\u003ch5\u003eHow do you feel about this lesson?\u003c/h5\u003e\n\u003cimg id=\"thumbs-up\" data-repository=\"dsc-cnn-recap\" title=\"Thumbs up!\" alt=\"thumbs up\"\u003e\u003cimg id=\"thumbs-down\" data-repository=\"dsc-cnn-recap\" title=\"Thumbs down!\" alt=\"thumbs down\"\u003e\n\u003c/div\u003e\n\u003ch5\u003eHave specific feedback? \u003ca href=\"https://github.com/learn-co-curriculum/dsc-cnn-recap/issues/new\"\u003eTell us here!\u003c/a\u003e\n\u003c/h5\u003e\u003c/footer\u003e","frontPage":false},{"exportId":"exploring-time-series-data-introduction","title":"Exploring Time Series Data - Introduction","type":"WikiPage","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-time-series-section-intro\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-time-series-section-intro/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eIn this section, you will learn about working with an important and ever-present type of data: time series! Stock market prices, weather, and economic indicators like GDP are a few examples of time series data.\u003c/p\u003e\n\n\u003ch2\u003eTime Series Data\u003c/h2\u003e\n\n\u003cp\u003e\"Time series\" data refers to datasets where the progress of time is an important dimension in the dataset. For example, working with the changes in stock prices, oil flow through a pipeline or even climate data over time requires an understanding of how to work with time series data. We introduce the concept of time series data, look at how to manage and visualize time series data, introduce the types of trends and the idea of \"time series decomposition\". In the next section, we'll introduce techniques for modeling time series data.\u003c/p\u003e\n\n\u003ch3\u003eIntroduction to Time Series\u003c/h3\u003e\n\n\u003cp\u003eWe start by importing daily minimum temperatures for Melbourne, Australia and introduce the importance of using dates as index values when importing time series data into Pandas. We then go through how to downsample and upsample a dataset and show some of the built-in methods for easily selecting and slicing time series data. We also provide an introduction to some of the most common plots for time series such as a line plot and a dot plot, and approaches to grouping and visualizing time series data.\u003c/p\u003e\n\n\u003cp\u003eWe also introduce the use of time series histograms and density plots for visualizing the distribution of the values without considering the times at which the values were measured and suggest time series box and whisker plots on a per-year basis to get a sense of trends over time. Finally, we introduce time series heat maps which can be a great way of getting a sense of how time series data changes across a couple of dimensions (e.g. month to month and year to year).\u003c/p\u003e\n\n\u003ch3\u003eTypes of Trends\u003c/h3\u003e\n\n\u003cp\u003eBasic regression tests are often not capable of capturing and predicting time-dependent patterns, so we introduce the concept of trends and stationarity, and explain the Dickey-Fuller test for performing statistical testing for time series stationarity.\u003c/p\u003e\n\n\u003ch3\u003eRemoving Trends\u003c/h3\u003e\n\n\u003cp\u003eMost time series modeling techniques assume stationarity, so we look at some of the techniques available for removing (or reducing) trends and/or seasonality using techniques such as a log transformation, rolling means, and differencing.\u003c/p\u003e\n\n\u003ch3\u003eTime Series Decomposition\u003c/h3\u003e\n\n\u003cp\u003eFinally, we end the section by introducing the concept of decomposition - another approach to removing trends and seasonality from a time series dataset.\u003c/p\u003e\n\n\u003ch2\u003eSummary\u003c/h2\u003e\n\n\u003cp\u003eThis section will provide you with the foundational knowledge for loading and working with time series data, so you'll have the skills required to start to perform time series modeling!\u003c/p\u003e\n\u003cfooter class=\"fis-footer\" style=\"visibility: hidden;\"\u003e\u003cdiv class=\"fis-feedback\"\u003e\n\u003ch5\u003eHow do you feel about this lesson?\u003c/h5\u003e\n\u003cimg id=\"thumbs-up\" data-repository=\"dsc-time-series-section-intro\" title=\"Thumbs up!\" alt=\"thumbs up\"\u003e\u003cimg id=\"thumbs-down\" data-repository=\"dsc-time-series-section-intro\" title=\"Thumbs down!\" alt=\"thumbs down\"\u003e\n\u003c/div\u003e\n\u003ch5\u003eHave specific feedback? \u003ca href=\"https://github.com/learn-co-curriculum/dsc-time-series-section-intro/issues/new\"\u003eTell us here!\u003c/a\u003e\n\u003c/h5\u003e\u003c/footer\u003e","frontPage":false},{"exportId":"using-word2vec","title":"Using Word2Vec","type":"WikiPage","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-using-word2vec\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-using-word2vec/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eIn this lesson, you'll take a look at how the \u003cstrong\u003e\u003cem\u003eWord2Vec\u003c/em\u003e\u003c/strong\u003e model actually works, and then learn how you can make use of Word2Vec using the open-source \u003ccode\u003egensim\u003c/code\u003e library!\u003c/p\u003e\n\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\n\u003cp\u003eYou will be able to:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eDescribe the tunable parameters of a Word2Vec model \u003c/li\u003e\n\u003cli\u003eDescribe the architecture of the Word2Vec model \u003c/li\u003e\n\u003cli\u003eTrain a Word2Vec model and transform words into vectors \u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch2\u003eHow Word2Vec Works\u003c/h2\u003e\n\n\u003cp\u003eBy now, you've gained an understanding of what a word embedding space is, and you've learned a little bit about how the words are represented as Dense vectors. However, we haven't touched on how the model actually learns the correct values for all the word vectors in the embedding space. To put it another way, how does the Word2Vec model learn exactly \u003cem\u003ewhere\u003c/em\u003e to embed each word vector inside the high dimensional embedding space?\u003c/p\u003e\n\n\u003cp\u003eNote that this explanation will stay fairly high-level, since you don't actually need to understand every part of how the Word2Vec model works in order to use it effectively for Data Science tasks. If you'd like to dig deeper in to how the model actually works, we recommend you start by reading this tutorial series from Chris McCormick (\u003ca href=\"http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/\"\u003epart 1\u003c/a\u003e and \u003ca href=\"http://mccormickml.com/2017/01/11/word2vec-tutorial-part-2-negative-sampling/\"\u003epart 2\u003c/a\u003e), and then moving onto the actual \u003ca href=\"https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf\"\u003eWord2Vec White Paper by Mikolov et al\u003c/a\u003e. The graphics used in this lesson are actually from Chris McCormick's excellent blog posts explaining how Word2Vec actually works.\u003c/p\u003e\n\n\u003ch3\u003eWindow Size and Training Data\u003c/h3\u003e\n\n\u003cp\u003eAt its core, Word2Vec is just another deep neural network. It's not even a particularly complex neural network -- the model contains an input layer, a single hidden layer, and and an output layer that uses the softmax activation function, meaning that the model is meant for multiclass classification. The model examines a \u003cstrong\u003e\u003cem\u003ewindow\u003c/em\u003e\u003c/strong\u003e of words, which is a tunable parameter that you can set when working with the model. Let's take a look at a graphic that explains how this all actually looks on a real example of data:\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-using-word2vec/master/images/training_data.png\"\u003e\u003c/p\u003e\n\n\u003cp\u003eIn the example above, the model has a window size of 5, meaning that the model considers a word, and the two words to the left and right of this word.  \u003c/p\u003e\n\n\u003ch3\u003eThe Skip-Gram Architecture\u003c/h3\u003e\n\n\u003cp\u003eSo what exactly is this deep neural network predicting?\u003c/p\u003e\n\n\u003cp\u003eThe most clever thing about the Word2Vec model is the type of problem it trains the network to solve, which creates the dense vectors for every word as a side effect! A typical task for a neural network is sentence completion. A trained model should be able to take in a sentence like \"the cat sat on the\" and output the most likely next word in the sentence, which should be something like \"mat\", or \"floor\". This is a form of \u003cstrong\u003e\u003cem\u003eSequence Generation\u003c/em\u003e\u003c/strong\u003e. Given a certain context (the words that came before), the model should be able to generate the next most plausible word (or words) in the sequence. \u003c/p\u003e\n\n\u003cp\u003eWord2Vec takes this idea, and flips it on its head. Instead of predicting the next word given a context, the model trains to predict the context surrounding a given word! This means that given the example word \"fox\" from above, the model should learn to predict the words \"quick\", \"brown\", \"jumps\", and \"over\", although crucially, not in any particular order. You're likely asking yourself why a model like this would be useful -- there are a massive amount of correct contexts that can surround a given word, which means that the output trained model itself likely isn't very useful to us. This intuition is correct -- the \u003cem\u003eoutput\u003c/em\u003e of the model is pretty useless to us.  However, in the case of Word2Vec, it's not the model that we're interested in. It turns out that by training to predict the context window for a given word, the neurons in the hidden layer end up learning the embedding space!  This is the reason why the size of the word vectors output by a Word2Vec model are a parameter that you can set ourselves. If you want word vectors of size 300, then you just include 300 neurons in our hidden layer. If you want vectors of size 100, then you include 100 neurons, and so on. Take a look at the following diagram of the Word2Vec model's architecture:\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-using-word2vec/master/images/new_skip_gram_net_arch.png\" width=\"800\"\u003e\u003c/p\u003e\n\n\u003ch3\u003eHidden Layers as a \"Lookup Table\"\u003c/h3\u003e\n\n\u003cp\u003eTo recap, the Word2Vec model learns to solve a \"fake\" problem, which you don't actually care about. The input layer of the network contains one neuron for every word in the vocabulary. If there are 10,000 words, then there are 10,000 input neurons, with each one corresponding to a unique word in the vocabulary. Since these input neurons feed into a dense hidden layer, this means that each neuron will have a unique weight for each of the 10,000 words in the vocabulary. If there are 10,000 words and you want vectors of size 300, then this means the hidden layer will be of shape \u003ccode\u003e[10000, 300]\u003c/code\u003e. To put it another way -- each of the 10,000 words will have it's own unique vector of weights, which will be of size 300, since there are 300 neurons.  \u003c/p\u003e\n\n\u003cp\u003eOnce you've trained the model, you don't actually need the output layer anymore -- all that matters is the hidden layer, which will now act as a \"Lookup Table\" that allows us to quickly get the vector for any given word in the vocabulary. \u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-using-word2vec/master/images/new_word2vec_weight_matrix_lookup_table.png\" width=\"600\"\u003e\u003c/p\u003e\n\n\u003cp\u003eHere's the beautiful thing about this lookup table -- when you input a given word, it is passed into the model in a one-hot encoded format. This means that in a vocabulary of 10,000 words, you'll have a \u003ccode\u003e1\u003c/code\u003e at the element that corresponds to the word that we're looking up the word vector for, and \u003ccode\u003e0\u003c/code\u003e for every other element in the vector. If you multiply this one-hot encoded vector by the weight matrix that is our hidden layer, then the vector for every word will be zeroed out, except for the vector that corresponds to the word that you are most interested in!\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-using-word2vec/master/images/matrix_mult_w_one_hot.png\"\u003e\u003c/p\u003e\n\n\u003ch3\u003eUnderstanding the Intuition Behind Word2Vec\u003c/h3\u003e\n\n\u003cp\u003eSo how does the model actually learn the correct weights for each word in a way that captures their semantic context and meaning? The intuition behind Word2Vec is actually quite simple, when you think about the idea of the context window that it's learning to predict. Recall the following quote, which you've seen before:\u003c/p\u003e\n\n\u003cblockquote\u003e\n\u003cp\u003e\"You shall know a word by the company it keeps.\"  -- J.R. Firth, Linguist\u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003cp\u003eIn the case of the Word2Vec model, the \"company\" a word keeps are the words surrounding it, and the model is learning to predict these companions! By exploring many different contexts, the model attempts to decipher which words are appropriate in which contexts. For example, consider the sentence \"we have two cats as pets\". You could easily substitute the word \"cats\" for \"dogs\" and the entire sentence would still make perfect sense. While the meaning of the sentence is undoubtedly changed, there is also a lesson regarding the fact that both are nouns and pets. Without even worrying about the embedding space, you can easily understand that words that have similar meanings will likely also be used in many of the same kinds of sentences. The more similar words are, the more sentences in which they are likely to share context windows! This is exactly what the model is learning, and this is why words that are similar end up near each other inside the embedding space. The ways that they are \u003cem\u003enot\u003c/em\u003e similar also helps the model learn to differentiate between them, since there will be patterns here as well. For instance, consider \"ran\" and \"run\", and \"walk\" and \"walked\". They differ only in tense. From the perspective of the sentences present in a large text corpus (models are commonly trained on all of Wikipedia, to give you an idea of the sheer size and scale of most datasets), the model will see numerous examples of how \"ran\" is similar to \"walked\", as well as examples of how the context windows for \"ran\" are different from \"run\" in the same ways that the context windows for \"walked\" are different from \"walk\"! \u003c/p\u003e\n\n\u003ch2\u003eTraining A Word2Vec Model with \u003ccode\u003egensim\u003c/code\u003e\n\u003c/h2\u003e\n\n\u003cp\u003eNow, take look at how you can apply the Word2Vec model using the \u003ccode\u003egensim\u003c/code\u003e library!\u003c/p\u003e\n\n\u003cp\u003eTo train a Word2Vec model, you first need to import the model from the \u003ccode\u003egensim\u003c/code\u003e library and instantiate it. Upon instantiation, you'll need to provide the model with certain parameters including:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003ethe dataset you'll be training on\u003c/li\u003e\n\u003cli\u003ethe \u003ccode\u003esize\u003c/code\u003e of the word vectors you want to learn \u003c/li\u003e\n\u003cli\u003ethe \u003ccode\u003ewindow\u003c/code\u003e size to use when training the model\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003emin_count\u003c/code\u003e, which corresponds to the minimum number of times a word must be used in the corpus in order to be included in the training (for instance, \u003ccode\u003emin_count=5\u003c/code\u003e would only learn word embeddings for words that appear 5 or more times throughout the entire training set)\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003eworkers\u003c/code\u003e, the number of threads to use for training, which can speed up processing (\u003ccode\u003e4\u003c/code\u003e is typically used, since most processors nowadays have at least 4 cores). \u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003eOnce you've instantiated the model, you'll still need to call the model's \u003ccode\u003e.train()\u003c/code\u003e method, and pass in the following parameters:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eThe same dataset that you passed in at instantiation\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003etotal_examples\u003c/code\u003e, which is the number of words in the model. You don't need to calculate this manually -- instead, you can just pass in the instantiated model's \u003ccode\u003e.corpus_count\u003c/code\u003e attribute for this parameter.\u003c/li\u003e\n\u003cli\u003eThe number of \u003ccode\u003eepochs\u003c/code\u003e to train the model for. \u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003eThe following example demonstrates how to instantiate and train a Word2Vec model:\u003c/p\u003e\n\n\u003cpre\u003e\u003ccode class=\"python\"\u003efrom gensim.models import Word2Vec\n\nmodel = Word2Vec(data, size=100, window=5, min_count=1, workers=4)\n\nmodel.train(data, total_examples=model.corpus_count)\n\u003c/code\u003e\u003c/pre\u003e\n\n\u003ch3\u003eExploring the Embedding Space\u003c/h3\u003e\n\n\u003cp\u003eOnce you have trained the model, you can easily explore the embedding space using the built-in methods and functionality provided by gensim's \u003ccode\u003eWord2Vec\u003c/code\u003e class. \u003c/p\u003e\n\n\u003cp\u003eThe actual Word2Vec model itself is quite large. Normally, you only need the actual vectors and the words that correspond to them, which are stored inside of \u003ccode\u003emodel.wv\u003c/code\u003e as a \u003ccode\u003eWord2VecKeyedVectors\u003c/code\u003e object. To save time and space, it's usually easiest to just store the \u003ccode\u003emodel.wv\u003c/code\u003e inside it's own variable, and then work directly with that. You can then use this model for various sorts of functionality, which you'll demonstrate below!\u003c/p\u003e\n\n\u003cpre\u003e\u003ccode class=\"python\"\u003ewv = model.wv\n\nwv.most_similar('Cat')\n\nwv.most_similar(negative='Cat')\n\nwv['Cat']\n\nwv.vectors\n\nwv.most_similar(positive=['king', 'woman'], negative=['man'])\n\u003c/code\u003e\u003c/pre\u003e\n\n\u003cp\u003eIn the next lab, you'll train a Word2Vec model, and then explore the embedding space it has learned. \u003c/p\u003e\n\n\u003ch2\u003eSummary\u003c/h2\u003e\n\n\u003cp\u003eIn this lesson, you learned about how the Word2Vec model actually works, and how you can train and use a Word2Vec model using the \u003ccode\u003egensim\u003c/code\u003e library!\u003c/p\u003e\n\u003cfooter class=\"fis-footer\" style=\"visibility: hidden;\"\u003e\u003cdiv class=\"fis-feedback\"\u003e\n\u003ch5\u003eHow do you feel about this lesson?\u003c/h5\u003e\n\u003cimg id=\"thumbs-up\" data-repository=\"dsc-using-word2vec\" title=\"Thumbs up!\" alt=\"thumbs up\"\u003e\u003cimg id=\"thumbs-down\" data-repository=\"dsc-using-word2vec\" title=\"Thumbs down!\" alt=\"thumbs down\"\u003e\n\u003c/div\u003e\n\u003ch5\u003eHave specific feedback? \u003ca href=\"https://github.com/learn-co-curriculum/dsc-using-word2vec/issues/new\"\u003eTell us here!\u003c/a\u003e\n\u003c/h5\u003e\u003c/footer\u003e","frontPage":false},{"exportId":"image-classification-with-multi-layer-perceptrons","title":"Image Classification with Multi-Layer Perceptrons","type":"WikiPage","content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-image-classification-with-mlps\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-image-classification-with-mlps\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-image-classification-with-mlps/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eIn this lesson, you'll learn why deeper networks sometimes lead to better results, and we'll generalize what you have learned before to get your matrix dimensions right in deep networks.\u003c/p\u003e\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\u003cp\u003eYou will be able to:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eExplain what is meant by \"deep representations\" of images\u003c/li\u003e\n\u003cli\u003eMathematically represent forward and back propagation in a deep neural network\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003eWhy deep representations?\u003c/h2\u003e\n\u003cp\u003eDeep representations are really good at automating what used to be a tedious process of feature engineering. Not only would modelers need to have complex programming and analytical skills, they would also often require domain knowledge in order to manually build features that would then be passed on to a regression or classification algorithm. With deep representations, this time consuming process is often severely diminished.\u003c/p\u003e\n\u003cp\u003eFor example, the deep layers of a neural network for computer might look like this:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003efirst layer detects edges in pictures\u003c/li\u003e\n\u003cli\u003esecond layer groups edges together and starts to detect different parts\u003c/li\u003e\n\u003cli\u003emore layers: group even bigger parts together, etc.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eor in the case of audio:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003efirst layer: low lever wave features\u003c/li\u003e\n\u003cli\u003esecond layer: basic units of sounds, \"phonemes\"\u003c/li\u003e\n\u003cli\u003ethird: word recognition\u003c/li\u003e\n\u003cli\u003efourth: sentence recognition\u003c/li\u003e\n\u003cli\u003e...\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThe general idea is shallow networks detect \"simple\" things, and the deeper you go, the more complex things can be detected.\u003c/p\u003e\n\u003cp\u003eYou can build a smaller but deeper neural network that needs exponentially less hidden units but performs better, because learning happens in each layer!\u003c/p\u003e\n\u003ch2\u003eDeep Network Architecture and Notation\u003c/h2\u003e\n\u003cp\u003eLet's try to generalize all the notation to get things straight and know the dimensions of all matrices we'll be working with. Let's have a look at this 3-layer network:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-image-classification-with-mlps/master/images/new_classwmips.png\" width=\"800\"\u003e\u003c/p\u003e\n\u003cp\u003eImagine that there are 300 cases, or observations (m = 300). What do our matrices look like?\u003c/p\u003e\n\u003cp\u003eLet's start with \u003cimg src=\"https://render.githubusercontent.com/render/math?math=Z%5E%7B%5B1%5D%7D%20=%20W%5E%7B%5B1%5D%7D%20X%20%2bb%5E%7B%5B1%5D%7D\"\u003e .\u003c/p\u003e\n\u003cp\u003eWhile not shown above in the diagram, Z is the output of the linear part of one of our hidden layers.\u003c/p\u003e\n\u003cp\u003eBreaking this down, we have:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cimg src=\"https://render.githubusercontent.com/render/math?math=W%5E%7B%5B1%5D%7D\"\u003e is the weights matrix with dimensions (4 x 2)\u003c/li\u003e\n\u003cli\u003eIf we look at all our samples, \u003cimg src=\"https://render.githubusercontent.com/render/math?math=x\"\u003e is a (2 x 300)-matrix\u003c/li\u003e\n\u003cli\u003e\n\u003cimg src=\"https://render.githubusercontent.com/render/math?math=Z%5E%7B%5B1%5D%7D\"\u003e is a (4 x 300)-matrix\u003c/li\u003e\n\u003cli\u003e\n\u003cimg src=\"https://render.githubusercontent.com/render/math?math=b%5E%7B%5B1%5D%7D\"\u003e is a (4 x 1)-matrix. Due to broadcasting in Python, this matrix will be duplicated into a (4 x 300)-matrix\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eSimilarly, the second hidden layer also has a linear function attached.\u003c/p\u003e\n\u003cp\u003eIn \u003cimg src=\"https://render.githubusercontent.com/render/math?math=Z%5E%7B%5B2%5D%7D%20=%20W%5E%7B%5B2%5D%7D%20A%5E%7B%5B1%5D%7D%20%2bb%5E%7B%5B2%5D%7D\"\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eThe dimension of \u003cimg src=\"https://render.githubusercontent.com/render/math?math=A%5E%7B%5B1%5D%7D\"\u003e is the same as the dimension of \u003cimg src=\"https://render.githubusercontent.com/render/math?math=Z%5E%7B%5B1%5D%7D\"\u003e : (4 x 300)\u003c/li\u003e\n\u003cli\u003e\n\u003cimg src=\"https://render.githubusercontent.com/render/math?math=W%5E%7B%5B2%5D%7D\"\u003e is the weights matrix with dimensions (3 x 4)\u003c/li\u003e\n\u003cli\u003e\n\u003cimg src=\"https://render.githubusercontent.com/render/math?math=Z%5E%7B%5B2%5D%7D\"\u003e is a (3 x 300)-matrices\u003c/li\u003e\n\u003cli\u003e\n\u003cimg src=\"https://render.githubusercontent.com/render/math?math=b%5E%7B%5B2%5D%7D\"\u003e is a (3 x 1)-matrix. Due to broadcasting in Python, this matrix will be duplicated into a (3 x 300)-matrix\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003eGeneralizing Notation\u003c/h2\u003e\n\u003cp\u003eFrom here, we wish to generalize our notation to a deep network with \u003cimg src=\"https://render.githubusercontent.com/render/math?math=L\"\u003e layers as opposed to 2. For each of these layers, we have parameters associated with the linear transformation of the layer, and parameters associated with the activation function applied to the output of this linear transformation.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eParameters for the linear transformation:\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=W%5E%7B%5Bl%5D%7D:%20(n%5E%7B%5Bl%5D%7D,%20n%5E%7B%5Bl-1%5D%7D)\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=b%5E%7B%5Bl%5D%7D:%20(n%5E%7B%5Bl%5D%7D,%201)\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=dW%5E%7B%5Bl%5D%7D:%20(n%5E%7B%5Bl%5D%7D,%20n%5E%7B%5Bl-1%5D%7D)\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=db%5E%7B%5Bl%5D%7D:%20(n%5E%7B%5Bl%5D%7D,%201)\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eParameters for the activation function:\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=a%5E%7B%5Bl%5D%7D,%20z%5E%7B%5Bl%5D%7D:%20(n%5E%7B%5Bl%5D%7D,%201)\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=Z%5E%7B%5Bl%5D%7D,%20A%5E%7B%5Bl%5D%7D:%20(n%5E%7B%5Bl%5D%7D,%20m)\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=dZ%5E%7B%5Bl%5D%7D,%20dA%5E%7B%5Bl%5D%7D:%20(n%5E%7B%5Bl%5D%7D,%20m)\"\u003e\u003c/p\u003e\n\u003ch2\u003eForward Propagation\u003c/h2\u003e\n\u003cp\u003eRecall that deep networks work by performing forward propagation; evaluating a cost function associated with the output of the neural network by successively calculating the output of each layer given initial parameter values, and passing this output on to the next layer until a finalized output has been calculated and the cost function can then be evaluated.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eInput is \u003cimg src=\"https://render.githubusercontent.com/render/math?math=a%5E%7B%5Bl-1%5D%7D\"\u003e\n\u003c/li\u003e\n\u003cli\u003eOutput \u003cimg src=\"https://render.githubusercontent.com/render/math?math=a%5E%7B%5Bl%5D%7D\"\u003e , save \u003cimg src=\"https://render.githubusercontent.com/render/math?math=z%5E%7B%5Bl%5D%7D,%20w%5E%7B%5Bl%5D%7D,%20b%5E%7B%5Bl%5D%7D,%20a%5E%7B%5Bl-1%5D%7D\"\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eHere's some more details about how the forward propagation calculation is performed:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=Z%5E1\"\u003e is the output of the linear transformation of the initial input \u003cimg src=\"https://render.githubusercontent.com/render/math?math=A%5E1\"\u003e (the observations). In successive layers, \u003cimg src=\"https://render.githubusercontent.com/render/math?math=A%5El\"\u003e is the output from the previous hidden layer. In all of these cases, \u003cimg src=\"https://render.githubusercontent.com/render/math?math=W%5El\"\u003e is a matrix of weights to be optimized to minimize the cost function. \u003cimg src=\"https://render.githubusercontent.com/render/math?math=b%5El\"\u003e is also optimized but is a vector as opposed to a matrix.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=g%5El\"\u003e is the activation function which takes the output of this linear transformation and yields the input to the next hidden layer.\u003c/p\u003e\n\u003cp\u003eMathematically we have:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=Z%5E%7B%5Bl%5D%7D=%20W%5E%7B%5Bl%5D%7D%20A%5E%7B%5Bl-1%5D%7D%20%2b%20b%5E%7B%5Bl%5D%7D\"\u003e \u003cbr\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=A%5E%7B%5Bl%5D%7D=%20g%5E%7B%5Bl%5D%7D%20(%20Z%5E%7B%5Bl%5D%7D)\"\u003e\u003c/p\u003e\n\u003cp\u003eHere, \u003cimg src=\"https://render.githubusercontent.com/render/math?math=Z%5E%7B%5Bl%5D%7D,%20A%5E%7B%5Bl%5D%7D\"\u003e both have a shape of \u003cimg src=\"https://render.githubusercontent.com/render/math?math=(n%5E%7B%5Bl%5D%7D,%20m)\"\u003e\u003c/p\u003e\n\u003ch2\u003eBackward Propagation\u003c/h2\u003e\n\u003cp\u003eOnce an output for the neural network given the current parameter weights has been calculated, we must back propagate to calculate the gradients of layer parameters with respect to the cost function. This will allow us to apply an optimization algorithm such as gradient descent in order to make small adjustments to the parameters in order to minimize our cost (and improve our predictions).\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eInput: \u003cimg src=\"https://render.githubusercontent.com/render/math?math=da%20%5E%7B%5Bl%5D%7D\"\u003e\n\u003c/li\u003e\n\u003cli\u003eOutput: \u003cimg src=\"https://render.githubusercontent.com/render/math?math=da%5E%7B%5Bl-1%5D%7D\"\u003e , \u003cimg src=\"https://render.githubusercontent.com/render/math?math=dW%5E%7B%5Bl%5D%7D,%20db%5E%7B%5Bl%5D%7D\"\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eIn terms of formulas, the gradients for our respective parameters in each activation layer are given by:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=dZ%5E%7B%5Bl%5D%7D=%20dA%20%5E%7B%5Bl%5D%7D%20*%20g%5E%7B%5Bl%5D'%7D%20(Z%5E%7B%5Bl%5D%7D)\"\u003e \u003cbr\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=dW%5E%7B%5Bl%5D%7D%20=%20%5Cdfrac%7B1%7D%7Bm%7D%20dZ%5E%7B%5Bl%5D%7D*%20A%5E%7B%5Bl-1%5DT%7D\"\u003e \u003cbr\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=db%5E%7B%5Bl%5D%7D%20=%20%5Cdfrac%7B1%7D%7Bm%7D%20np.sum(dZ%5E%7B%5Bl%5D%7D,%20axis=1,%20keepdims=True)\"\u003e \u003cbr\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=dA%5E%7B%5Bl-1%5D%7D%20=%20W%5E%7B%5Bl%5DT%7D*dZ%5E%7B%5Bl%5D%7D\"\u003e\u003c/p\u003e\n\u003ch2\u003eProcess Overview\u003c/h2\u003e\n\u003cp\u003eTo summarize the process once more, we begin by defining a model architecture which includes the number of hidden layers, activation functions, and the number of units in each of these.\u003c/p\u003e\n\u003cp\u003eWe then initialize parameters for each of these layers (typically randomly). After the initial parameters are set, forward propagation evaluates the model giving a prediction, which is then used to evaluate a cost function. Forward propagation involves evaluating each layer and then piping this output into the next layer.\u003c/p\u003e\n\u003cp\u003eEach layer consists of a linear transformation and an activation function. The parameters for the linear transformation in \u003cstrong\u003eeach\u003c/strong\u003e layer include \u003cimg src=\"https://render.githubusercontent.com/render/math?math=W%5El\"\u003e and \u003cimg src=\"https://render.githubusercontent.com/render/math?math=b%5El\"\u003e . The output of this linear transformation is represented by \u003cimg src=\"https://render.githubusercontent.com/render/math?math=Z%5El\"\u003e . This is then fed through the activation function (again, for each layer) giving us an output \u003cimg src=\"https://render.githubusercontent.com/render/math?math=A%5El\"\u003e which is the input for the next layer of the model.\u003c/p\u003e\n\u003cp\u003eAfter forward propagation is completed and the cost function is evaluated, back propogation is used to calculate gradients of the initial parameters with respect to this cost function. Finally, these gradients are then used in an optimization algorithm, such as gradient descent, to make small adjustments to the parameters and the entire process of forward propagation, back propagation, and parameter adjustments is repeated until the modeller is satisfied with the results.\u003c/p\u003e\n\u003ch2\u003eAdditional Resources\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"https://www.coursera.org/learn/neural-networks-deep-learning/lecture/rz9xJ/why-deep-representations\"\u003ehttps://www.coursera.org/learn/neural-networks-deep-learning/lecture/rz9xJ/why-deep-representations\u003c/a\u003e\u003c/p\u003e\n\u003ch2\u003eSummary\u003c/h2\u003e\n\u003cp\u003eIn this brief lesson, we gave an intuitive justification behind using deep network structures and reviewed the architecture for neural nets in general. In upcoming lessons, we will begin to extend our previous work in creating a single layer neural network in order to build a deeper more powerful model.\u003c/p\u003e","frontPage":false},{"exportId":"sequence-model-use-cases","title":"Sequence Model Use Cases","type":"WikiPage","content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-sequence-model-use-cases\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-sequence-model-use-cases\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-sequence-model-use-cases/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eIn this lesson, you'll learn about \u003cstrong\u003e\u003cem\u003eSequence Models\u003c/em\u003e\u003c/strong\u003e, and what makes them different from traditional multi-layer perceptrons. You'll also examine some of the common things sequence models can be used for!\u003c/p\u003e\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\u003cp\u003eYou will be able to:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eDefine a Sequence Model\u003c/li\u003e\n\u003cli\u003eList some of the use cases for Sequence Models\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003eWhat is a Sequence Model?\u003c/h2\u003e\n\u003cp\u003eA \u003cstrong\u003e\u003cem\u003eSequence Model\u003c/em\u003e\u003c/strong\u003e is a general term for a special class of deep neural networks that work with time series of data as input (or any data where you want the model to consider the data one point at a time, in order). This means that they are great for problems where the order of the data matters - for instance, stock price data or text. In both cases, the data only makes sense in order. For instance, scrambling the words in a sentence destroys the meaning of the sentence, and it's impossible to predict if a stock price is going to go up or down if you don't see the prices in sequential order. In both cases, the sequence of the data matters.\u003c/p\u003e\n\u003cp\u003eConsider the following problem: you are given the sentence \"you are going to\" and asked to complete the sentence by generating at least 5 more words. The second word that you choose will depend heavily on the first word that you choose. The third word that you choose will depend heavily on the first and second words that you choose, and so on. Because of this, it is crucial that the models \u003cem\u003eremember\u003c/em\u003e the previous words that they generated. In computer science, you call this being \u003cstrong\u003e\u003cem\u003estateful\u003c/em\u003e\u003c/strong\u003e. This means that when the model is generating the second word, it needs to know what it generated as the first word! To do this, \u003cstrong\u003e\u003cem\u003eRecurrent Neural Networks\u003c/em\u003e\u003c/strong\u003e feed their output for timestep \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Clarge%20x_t\"\u003e back into the model as input for timestep \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Clarge%20x_%7Bt%20%2b%201%7D\"\u003e !\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-sequence-model-use-cases/master/images/rnn.gif\"\u003e\u003c/p\u003e\n\u003cp\u003eThere are many different kinds of sequence models, and they are most generally referred to as \u003cstrong\u003e\u003cem\u003eRecurrent Neural Networks\u003c/em\u003e\u003c/strong\u003e, or \u003cstrong\u003e\u003cem\u003eRNNs\u003c/em\u003e\u003c/strong\u003e. In the next lesson, you'll dig into how they work. Let's examine some of the things that RNNs can do!\u003c/p\u003e\n\u003ch2\u003eSequence Model Use Cases\u003c/h2\u003e\n\u003ch3\u003eText Classification\u003c/h3\u003e\n\u003cp\u003eOne of the most common applications of RNNs is for plain old text classification. Recall that all the models that you've used so far for text generation have been incapable of focusing on the order of the words, which means that they're likely to miss out on more advanced pieces of information such as connotation, context, sarcasm, etc. However, since RNNs examine the words one at a time and remember what they've seen at each time step, they're able to capture this information quite effectively in most cases! As the final part of this section, we'll actually build one of these models which will be able to detect toxic comments from real-world Wikipedia comments!\u003c/p\u003e\n\u003ch3\u003eSequence Generation\u003c/h3\u003e\n\u003cp\u003eSequence generation is probably some of the most incredible things you can do with neural networks, because they excel at coming up with wacky, almost-human sounding names for things when fed the right data. For instance, all of the following cookie names were generated by feeding a dataset of actual cookie names from recipes. The model was built to generate it's own cookie names letter by letter, based on what it saw in the recipe names. Since the model is responsible for generating its own output letter by letter, one at a time, this makes it a prime example of \u003cstrong\u003e\u003cem\u003eSequence Generation\u003c/em\u003e\u003c/strong\u003e!\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-sequence-model-use-cases/master/images/rnn_cookie_names.png\"\u003e\u003c/p\u003e\n\u003ch3\u003eSequence-to-Sequence Models\u003c/h3\u003e\n\u003cp\u003eIf you've ever used Google Translate before, then you've already interacted with a \u003cstrong\u003e\u003cem\u003eSequence to Sequence Model\u003c/em\u003e\u003c/strong\u003e. These models learn to map an input sequence to an output sequence, usually through an \u003cstrong\u003e\u003cem\u003eEncoder-Decoder\u003c/em\u003e\u003c/strong\u003e architecture. Note that although going from a sequence of English words to the corresponding sequence of French words is probably the basic example of Sequence to Sequence models, there are many other kinds of problems that are Sequence to Sequence that aren't immediately obvious. For instance, check out this example of a neural network that completes drawings of a mosquito based on how you start drawing the bug!\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-sequence-model-use-cases/master/images/multi_sketch_mosquito.gif\"\u003e\u003c/p\u003e\n\u003cp\u003eHere's another example from \u003ca href=\"https://phillipi.github.io/pix2pix/\"\u003epix2pix\u003c/a\u003e. Now, stop what you're doing, follow that link, and take a few minutes to play around with pix2pix -- watching it generate photos from your own drawings is really cool!\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-sequence-model-use-cases/master/images/pix2pix.gif\"\u003e\u003c/p\u003e\n\u003ch2\u003eSummary\u003c/h2\u003e\n\u003cp\u003eIn this lesson, you learned about sequence models, and some of their more common use cases.\u003c/p\u003e","frontPage":false},{"exportId":"graph-theory-recap","title":"Graph Theory - Recap","type":"WikiPage","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-networks-recap\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-networks-recap/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eIn this section you explored a new data structure: networks! While network analysis is a deep topic with many additional topics to explore, you should have a good initial introduction and enough to conduct some preliminary analyses for social networks and building recommendation systems.\u003c/p\u003e\n\n\u003ch2\u003eNetworks\u003c/h2\u003e\n\n\u003cp\u003eYou've seen that networks can represent a range of different underlying data. From directions, social networks, and customer databases, networks are a wonderful way to represent the relationships between individuals. They also make for some snazzy visuals!\u003c/p\u003e\n\n\u003ch2\u003ePaths\u003c/h2\u003e\n\n\u003cp\u003eThe first stop along your journey was paths! Here, you investigated Dijkstra's algorithm to find the shortest path between nodes. This harked back to some of your experience scraping the web when you used recursive functions to perform breadth and depth based search techniques to transverse a json file. While you didn't directly explore this application, networks are also a natural representation for exploring internet traffic and web page structures.\u003c/p\u003e\n\n\u003ch2\u003eCentrality\u003c/h2\u003e\n\n\u003cp\u003eOnce you had a metric to calculate the distance between nodes, you then started to investigate other important concepts of networks such as which nodes were most influential or connected within a graph. You saw how alternative metrics can provide different insights on node structure. As a quick recap:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003e\n\u003cstrong\u003eDegree-centrality\u003c/strong\u003e: The number of edges attached to a node\u003c/li\u003e\n\u003cli\u003e\n\u003cstrong\u003eCloseness-centrality\u003c/strong\u003e: The reciprocal of the sum of the distances to all other nodes in the network \u003c/li\u003e\n\u003cli\u003e\n\u003cstrong\u003eBetweeness-centrality\u003c/strong\u003e: The number of shortest paths between all node pairs the node lies on divided by the maximum number of shortests-paths any one node in the network lies on \u003c/li\u003e\n\u003cli\u003e\n\u003cstrong\u003eEigenvalue-centrality\u003c/strong\u003e: An iterative algorithm which assigns relative influence to a node based on the number and importance of connected nodes. Can be very computationally expensive to compute for large networks. Google's PageRank algorithm is a variation of eigenvalue-centrality \u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch2\u003eClustering\u003c/h2\u003e\n\n\u003cp\u003eAfter discussing centrality, you then focused on larger structures within a network, breaking apart nodes into clusters to examine subgroups. While this is a common and useful application, it is an ill-defined problem mathematically, often making it difficult to definitively determine an optimal clustering schema. \u003c/p\u003e\n\n\u003ch2\u003eRecommendations\u003c/h2\u003e\n\n\u003cp\u003eFinally, you rounded out the section by investigating how networks can be used to provide recommendations to users. To do this, you investigated a preliminary approach known as collaborative filtering, specifically exploring user-based collaborative filtering in which similar users are identified and their preferences are used to generate recommendations to the user in question. There are many alternative approaches to recommendations systems such as using Singular Value Decomposition. \u003c/p\u003e\n\n\u003ch2\u003eSummary\u003c/h2\u003e\n\n\u003cp\u003eA lot was covered in this section! From this, you should have a solid introduction to networks, and some of their applications. Going forward, continue to explore ongoing developments in clustering social networks, and generating recommendations from these fascinating data structures.\u003c/p\u003e\n\u003cfooter class=\"fis-footer\" style=\"visibility: hidden;\"\u003e\u003cdiv class=\"fis-feedback\"\u003e\n\u003ch5\u003eHow do you feel about this lesson?\u003c/h5\u003e\n\u003cimg id=\"thumbs-up\" data-repository=\"dsc-networks-recap\" title=\"Thumbs up!\" alt=\"thumbs up\"\u003e\u003cimg id=\"thumbs-down\" data-repository=\"dsc-networks-recap\" title=\"Thumbs down!\" alt=\"thumbs down\"\u003e\n\u003c/div\u003e\n\u003ch5\u003eHave specific feedback? \u003ca href=\"https://github.com/learn-co-curriculum/dsc-networks-recap/issues/new\"\u003eTell us here!\u003c/a\u003e\n\u003c/h5\u003e\u003c/footer\u003e","frontPage":false},{"exportId":"word-embeddings","title":"Word Embeddings","type":"WikiPage","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-word-embeddings\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-word-embeddings/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eIn this lesson, you'll learn about the concept of \u003cstrong\u003e\u003cem\u003eWord Embeddings\u003c/em\u003e\u003c/strong\u003e, and how you can use them to model the semantic meanings of words in a high-dimensional embedding space!\u003c/p\u003e\n\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\n\u003cp\u003eYou will be able to:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eDemonstrate how word vectors are structured \u003c/li\u003e\n\u003cli\u003eCompare and contrast word vector embeddings with other text vectorization strategies \u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch2\u003eWhat Are Word Embeddings?\u003c/h2\u003e\n\n\u003cp\u003e\u003cstrong\u003e\u003cem\u003eWord Embeddings\u003c/em\u003e\u003c/strong\u003e are a type of vectorization strategy that computes word vectors from a text corpus by training a neural network, which results in a high-dimensional embedding space, where each word in the corpus is a unique vector in that space. In this embedding space, the position of the vector relative to the other vectors captures semantic meaning. This method of creating distributed representations of words in a high-dimensional embedding space was first introduced in a landmark paper from members of the Google Brain team in 2013 at the Neural Information Processing Systems (NeurIPS, for short). You can read the full paper from Mikolov et al by following \u003ca href=\"https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf\"\u003ethis link\u003c/a\u003e.\u003c/p\u003e\n\n\u003ch3\u003eCapturing Semantic Relationships\u003c/h3\u003e\n\n\u003cp\u003eSo far, the vectorization strategies you've learned have focused only on how often a word appears in a given text, but they don't focus at all on capturing the semantic meaning. This is one area where using the Word2Vec model to create \u003cstrong\u003e\u003cem\u003eWord Vector Embeddings\u003c/em\u003e\u003c/strong\u003e really shines, because it will capture those semantic relationships between words, for instance, a Word2Vec model that is given enough data and training will learn that there is a semantic relationship between the word 'person' and 'people'. Furthermore, vector one would need to travel to get from the singular 'person' to the plural 'people' will be the same vector that will get you from the singular version of a word to it's plural - meaning that our model will 'learn' how to model the relationship between singular and plural versions of the same word. Take a look at the examples below:\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-word-embeddings/master/images/embeddings.png\"\u003e\u003c/p\u003e\n\n\u003cp\u003eAs you can see in the diagram above, the embedding space shows that the model has positioned the words 'king' and 'queen' in the same relationship that the vector 'man' has to 'woman'. The vector that gets you from 'king' to 'queen' or from 'man' to 'woman' is the vector for gender! You can see other examples show that the model also learns representations for verb tense, or even for countries and their capitals. This is more impressive when you realize that the model learns these relationships from reading a large enough corpus of text, without being given an explicit direction or instruction - that is, the researchers did not expressly feed the model sentences like \"Madrid is the capital of Spain\".  \u003c/p\u003e\n\n\u003cp\u003eSince the words are all embedded in the same high-dimensional space, you can use the same similarity metrics you've used before, such as things like \u003cem\u003eCosine Similarity\u003c/em\u003e or even \u003cem\u003eEuclidean Distance\u003c/em\u003e. In a future lab, you'll experiment with using a trained Word2Vec model for tasks like finding the most similar word(s) to a given word. Trained Word2Vec models also excel at things like the analogies questions that were made famous by the SAT test.\u003c/p\u003e\n\n\u003cp\u003eLet's end this lesson by taking a look at how the word vectors are actually structured. \u003c/p\u003e\n\n\u003ch2\u003eA Small Example\u003c/h2\u003e\n\n\u003cp\u003eSo far, you've learned vectorization strategies such as \u003cem\u003eCount Vectorization\u003c/em\u003e and \u003cem\u003eTF-IDF Vectorization\u003c/em\u003e. Recall that the vectors created by these algorithms are \u003cstrong\u003e\u003cem\u003eSparse Vectors\u003c/em\u003e\u003c/strong\u003e. The length of a vector created by TF-IDF or Count Vectorization is the length of the total vocabulary of the text corpus. In these vectors, the vast majority of elements in the vector are 0, which is a massive waste of space, and a ton of extra dimensionality that can hurt our model's performance (recall the \u003cstrong\u003e\u003cem\u003eCurse of Dimensionality\u003c/em\u003e\u003c/strong\u003e)! If you were to use TF-IDF vectorization to turn the word 'apple' into a vector representation with a text corpus containing 100,000 words, then our word vector would contain a value at the element that corresponds to the word 'apple', and then 99,999 \u003cem\u003e0\u003c/em\u003es!\u003c/p\u003e\n\n\u003cp\u003eVectors created through word embeddings are different - the size of the vector is a tunable parameter you can set. \u003c/p\u003e\n\n\u003cp\u003eLet's look at a toy example. Consider the diagram below. First, pay attention to what each of the columns mean. Let's assume that you built a model to 'rate' each of the animals across each of these four categories, relative to one another.  \u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-word-embeddings/master/images/vectors.png\" width=\"800\"\u003e\u003c/p\u003e\n\n\u003cp\u003eIn this embedding space, the vectorized representation of the word 'dog' would be \u003ccode\u003e[-0.4, 0.37, 0.02, -0.34]\u003c/code\u003e. As you'll see when you study the actual Word2Vec model, you can use some nifty tricks to train a neural network to act as a sort of 'lookup table', where you can get the vector out for any given word. In the next lesson, you'll spend a bit more time understanding exactly how the model learns the correct values for each word. \u003c/p\u003e\n\n\u003ch2\u003eSummary\u003c/h2\u003e\n\n\u003cp\u003eIn this lesson, you learned about the concept of \u003cstrong\u003e\u003cem\u003eWord Embeddings\u003c/em\u003e\u003c/strong\u003e, and explored how they work. \u003c/p\u003e\n\u003cfooter class=\"fis-footer\" style=\"visibility: hidden;\"\u003e\u003cdiv class=\"fis-feedback\"\u003e\n\u003ch5\u003eHow do you feel about this lesson?\u003c/h5\u003e\n\u003cimg id=\"thumbs-up\" data-repository=\"dsc-word-embeddings\" title=\"Thumbs up!\" alt=\"thumbs up\"\u003e\u003cimg id=\"thumbs-down\" data-repository=\"dsc-word-embeddings\" title=\"Thumbs down!\" alt=\"thumbs down\"\u003e\n\u003c/div\u003e\n\u003ch5\u003eHave specific feedback? \u003ca href=\"https://github.com/learn-co-curriculum/dsc-word-embeddings/issues/new\"\u003eTell us here!\u003c/a\u003e\n\u003c/h5\u003e\u003c/footer\u003e","frontPage":false},{"exportId":"introduction-to-nlp-with-nltk","title":"Introduction to NLP with NLTK","type":"WikiPage","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-introduction-to-nltk\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-introduction-to-nltk/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eIn this lesson, we'll discuss a general overview of Natural Language Processing, and the popular Python library for NLP, \u003cstrong\u003e\u003cem\u003eNatural Language Tool Kit\u003c/em\u003e\u003c/strong\u003e (NLTK).\u003c/p\u003e\n\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\n\u003cp\u003eYou will be able to:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eIdentify ways we can use NLTK to simplify and accelerate common preprocessing tasks for text data\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch2\u003eWhat is Natural Language Processing?\u003c/h2\u003e\n\n\u003cp\u003e\u003cstrong\u003e\u003cem\u003eNatural Language Processing\u003c/em\u003e\u003c/strong\u003e, or \u003cstrong\u003e\u003cem\u003eNLP\u003c/em\u003e\u003c/strong\u003e, refers to analytics tasks that deal with natural human language, in the form of text or speech. These tasks usually involve some sort of machine learning, whether for text classification or for feature generation, but NLP isn't just machine learning. Tasks such as text preprocessing and cleaning also fall under the NLP umbrella. \u003c/p\u003e\n\n\u003cp\u003eThe most common Python library used for NLP tasks is \u003cstrong\u003e\u003cem\u003eNatural Language Tool Kit\u003c/em\u003e\u003c/strong\u003e, or NLTK for short. This library was developed by researchers at the University of Pennsylvania, and quickly became the most powerful and complete library of NLP tools available. \u003c/p\u003e\n\n\u003ch2\u003eUsing NLTK\u003c/h2\u003e\n\n\u003cp\u003eNLTK is a sort of \"one-stop shop\" for all things NLP. It contains many sample corpora, with everything from full texts from Project Gutenberg to transcripts of State of the Union speeches from US Presidents. This library contains functions and tools for everything from data cleaning and preprocessing, to linguistic analysis, to feature generation and extraction. NLTK even contains its own Bayesian classifiers for quick testing (although realistically, you'll likely want to continue using scikit-learn for these sorts of tasks). \u003c/p\u003e\n\n\u003cp\u003eNLP is unique in that in addition to statistics and math, it also relies heavily on the field of \u003cstrong\u003e\u003cem\u003eLingustics\u003c/em\u003e\u003c/strong\u003e. Many of the concepts you'll run into will be grounded in linguistics. Some of them will seem a bit foreign to you if you haven't studied languages or grammar yet, but don't worry! The reality of it all is that you don't need deep expertise in linguistics to work with text data, because NLTK was built by professionals to make it easier for everyone to access the linguistic tools and methods needed for working with text data. Although a linguist knows how to manually generate something like a \u003cstrong\u003e\u003cem\u003eParse Tree\u003c/em\u003e\u003c/strong\u003e for a sentence, NLTK provides this functionality for you in just a few lines of code. \u003c/p\u003e\n\n\u003cp\u003e\u003cstrong\u003eA sample Parse Tree created with NLTK\u003c/strong\u003e\u003c/p\u003e\n\n\u003cp\u003e\u003c/p\u003e \u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-introduction-to-nltk/master/images/new_parse_tree.png\" width=\"750\"\u003e \n\n\u003ch2\u003eWorking With Text, Simplified\u003c/h2\u003e\n\n\u003cp\u003eGenerally, projects that work with text data follow the same overall pattern as any other projects. The main difference is that text projects usually require a bit more cleaning and preprocessing than regular data, in order to get the text into a format that's usable for modeling. \u003c/p\u003e\n\n\u003cp\u003eHere are some of the ways that NLTK can make our lives easier when working with text data:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003e\u003cp\u003e\u003cstrong\u003e\u003cem\u003eStop Word Removal\u003c/em\u003e\u003c/strong\u003e: NLTK contains a full library of stop words, making it easy to remove the words that don't matter from our data.    \u003c/p\u003e\u003c/li\u003e\n\u003cli\u003e\u003cp\u003e\u003cstrong\u003e\u003cem\u003eFiltering and Cleaning\u003c/em\u003e\u003c/strong\u003e: NLTK provides simple, easy ways to create and filter frequency distributions, as well providing multiple ways to clean, stem, lemmatize, or tokenize datasets.   \u003c/p\u003e\u003c/li\u003e\n\u003cli\u003e\u003cp\u003e\u003cstrong\u003e\u003cem\u003eFeature Selection and Feature Engineering\u003c/em\u003e\u003c/strong\u003e: NLTK contains tools to quickly generate features such as bigrams and n-grams. It also contains major libraries such as the \u003cstrong\u003e\u003cem\u003ePenn Tree Bank\u003c/em\u003e\u003c/strong\u003e to allow quick feature engineering, such as generating part-of-speech tags, or sentence polarity. \u003c/p\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003eAgain, don't worry if you're not sure what things like 'lemmatize' mean yet -- we'll cover all of that soon! With effective use of NLTK, we can quickly process and work with text data, allowing us to quickly get our data into the shape needed for tasks we're familiar with, such as classification!\u003c/p\u003e\n\n\u003cp\u003eFor the remainder of this section, we're going to spend some time getting comfortable with NLTK, while also learning about foundational concepts of linguistics that underpin many of the tasks in NLP. We'll learn to effectively use NLTK to clean and preprocess data in a variety of ways. We'll gain some practice filtering data with regular expressions, generate text statistics to compare text documents, and quickly engineer features to help us better train classifiers for text classification!\u003c/p\u003e\n\n\u003ch2\u003eSummary\u003c/h2\u003e\n\n\u003cp\u003eIn this lesson, we learned about what NLP is, and how the NLTK package can save us time and make us more effective when working with text data. \u003c/p\u003e\n\u003cfooter class=\"fis-footer\" style=\"visibility: hidden;\"\u003e\u003cdiv class=\"fis-feedback\"\u003e\n\u003ch5\u003eHow do you feel about this lesson?\u003c/h5\u003e\n\u003cimg id=\"thumbs-up\" data-repository=\"dsc-introduction-to-nltk\" title=\"Thumbs up!\" alt=\"thumbs up\"\u003e\u003cimg id=\"thumbs-down\" data-repository=\"dsc-introduction-to-nltk\" title=\"Thumbs down!\" alt=\"thumbs down\"\u003e\n\u003c/div\u003e\n\u003ch5\u003eHave specific feedback? \u003ca href=\"https://github.com/learn-co-curriculum/dsc-introduction-to-nltk/issues/new\"\u003eTell us here!\u003c/a\u003e\n\u003c/h5\u003e\u003c/footer\u003e","frontPage":false},{"exportId":"classification-with-word-embeddings","title":"Classification with Word Embeddings","type":"WikiPage","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-classification-with-word-embeddings\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-classification-with-word-embeddings/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eIn this lesson, you'll look at the practical aspects of how you can use word embeddings and Word2Vec models for text classification!\u003c/p\u003e\n\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\n\u003cp\u003eYou will be able to:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eDescribe what an embedding layer is in a neural network \u003c/li\u003e\n\u003cli\u003eUse pretrained word embeddings from popular pretrained models such as GloVe \u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch2\u003eGetting Started\u003c/h2\u003e\n\n\u003cp\u003eIn this lesson, you'll start by reviewing \u003cstrong\u003e\u003cem\u003eTransfer Learning\u003c/em\u003e\u003c/strong\u003e and loading pre-trained word vectors. Then, you'll learn about how to get important word vectors, combine them into \u003cstrong\u003e\u003cem\u003eMean Word Vectors\u003c/em\u003e\u003c/strong\u003e, and streamline this process by writing a custom vectorizer class compatible with scikit-learn pipelines. Finally, you'll end the lesson by examining how to train deep neural networks that include their own word embedding layers, and how you can use Keras to preprocess text data conveniently!\u003c/p\u003e\n\n\u003ch2\u003eUsing Pretrained Word Vectors With GloVe\u003c/h2\u003e\n\n\u003cp\u003ePerhaps the single best way to improve performance for text classification is to make use of weights from a Word2Vec model that has been trained for a very long time on a massive amount of text data. With deep learning, more data is almost always the single best thing that can improve model performance, and the embedded word vectors created by a Word2Vec model are no exception. For this reason, it's almost always a good idea to load one of the top-tier, industry-standard models that been open sourced for this exact purpose. The most common model to use for this is the \u003cstrong\u003e\u003cem\u003eGloVe\u003c/em\u003e\u003c/strong\u003e (short for \u003cstrong\u003e\u003cem\u003eGlobal Vectors for Word Representation\u003c/em\u003e\u003c/strong\u003e) model by the Stanford NLP Group. This model is trained on massive datasets, such as the entirety of Wikipedia, for a very long time on server clusters with multiple GPUs. It would be absolutely impossible for us to train a model of similar quality on our own machines. However, because the model weights are open-source, you don't need to! Instead, you'll simply download the weights and go from there. \u003c/p\u003e\n\n\u003cp\u003eFor text classification purposes, loading the weights precludes the need for us to instantiate or train a Word2Vec model entirely -- instead, you just:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eGet the total vocabulary in our dataset\u003c/li\u003e\n\u003cli\u003eDownload and unzip the GloVe file needed from the Stanford NLP Group's website\u003c/li\u003e\n\u003cli\u003eRead the GloVe file, and save only the vectors that correspond to the words that appear in the vocabulary of our dataset \u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003eThis can be a fairly involved process, so the code for this is provided for you in the next lab. That said, it's important to take some time and examine this code until you have at least general idea of what's going on!\u003c/p\u003e\n\n\u003ch2\u003eMean Word Embeddings\u003c/h2\u003e\n\n\u003cp\u003eLoading a pretrained model like GloVe may provide you with the most accurate word vectors we could possibly hope, but each vector is still just a single word. This isn't very conducive to classification as is at this stage, because it's highly likely that any text classification will be focused on arbitrarily-sized blobs of text, such as sentences or paragraphs. With that, the question is how to get these sentences and paragraphs into a format that can be used for classification, while making use of the word vectors from GloVe?\u003c/p\u003e\n\n\u003cp\u003eThe answer is to compute a \u003cstrong\u003e\u003cem\u003eMean Word Embedding\u003c/em\u003e\u003c/strong\u003e. The idea behind this is simple. To get the vector representation for any arbitrarily-sized block of text, all you need to do is get the vector for every individual word that appears in that block of text, and average them together! The benefit of this is that no matter how big or small that block of text is, the mean word embedding of that sentence will be the same size as all of the others, because the vectors you're averaging together all have the exact same dimensionality! This makes it a simple matter to get a block of text into a format that we can use with traditional supervised learning models such as Support Vector Machines or Gradient Boosted Trees. \u003c/p\u003e\n\n\u003ch3\u003eWorking With scikit-learn pipelines\u003c/h3\u003e\n\n\u003cp\u003eAs you'll see in the next lab, it's worth the extra bit of work to build a class that works with the requirements of a scikit-learn \u003ccode\u003ePipeline()\u003c/code\u003e class, so that you can pass the data straight in and generate the mean word embeddings on the fly. This way, you don't need to write the same set of code twice to generate mean word embeddings for both the training and test set. This is also important if the dataset is too large to fit into your computer's memory, as it will allow you to partially train models and load in different chunks of the dataset. By building a vectorizer class that handles creating the mean word embeddings rather than just writing the code procedurally, you'll save yourself a lot of work in the long run!\u003c/p\u003e\n\n\u003cp\u003eThe code for the mean embedding vectorizer class is also provided for you in the next lab. As you'll see, the class requires both \u003ccode\u003e.fit()\u003c/code\u003e and \u003ccode\u003e.transform()\u003c/code\u003e methods to be compliant with scikit-learn's \u003ccode\u003ePipeline()\u003c/code\u003e class. Take some time to study this code until you understand what it's doing -- it isn't complex, and understanding how to do this yourself will pay dividends in the long run. After all, writing clean, reusable code always does!\u003c/p\u003e\n\n\u003ch2\u003eDeep Learning \u0026amp; Embedding Layers\u003c/h2\u003e\n\n\u003cp\u003eOne problem you may have noticed with the mean word embedding strategy is that by combining all the words, you lose some information that is contained in the sequence of the words. In natural language, the position and phrasing of words in a sentence can often contain information that we pick up on. This is a downside to this approach, and one of the reasons why \u003cstrong\u003e\u003cem\u003eSequence Models\u003c/em\u003e\u003c/strong\u003e tend to outperform all of the 'shallow' algorithms (note: this term just refers to any machine learning algorithms that do not fall under the umbrella of deep learning -- it doesn't make any judgments about whether they are better or worse, as that is almost always dependent on the situation!). In the next lesson, you'll learn about sequence models including \u003cstrong\u003e\u003cem\u003eRecurrent Neural Networks\u003c/em\u003e\u003c/strong\u003e and \u003cstrong\u003e\u003cem\u003eLong Short Term Memory Cells\u003c/em\u003e\u003c/strong\u003e. Moreover, in the next lab, you'll also see a preview example of these, so that you can see how to use \u003cstrong\u003e\u003cem\u003eEmbedding Layers\u003c/em\u003e\u003c/strong\u003e directly within neural networks!\u003c/p\u003e\n\n\u003cp\u003eAn \u003cstrong\u003e\u003cem\u003eEmbedding Layer\u003c/em\u003e\u003c/strong\u003e is just a layer that learns the word embeddings for our dataset on the fly, right there inside the neural network. Essentially, its a way to make use of all the benefits of Word2Vec, without worrying about finding a way to include a separately trained Word2Vec model's output into our neural networks (which are probably already complicated enough!). You'll see an example of an \u003cstrong\u003e\u003cem\u003eEmbedding Layer\u003c/em\u003e\u003c/strong\u003e in the next lab. You should make note of a couple caveats that come with using embedding layers in your neural network -- namely:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eThe embedding layer must always be the first layer of the network, meaning that it should immediately follow the \u003ccode\u003eInput()\u003c/code\u003e layer \u003c/li\u003e\n\u003cli\u003eAll words in the text should be integer-encoded, with each unique word encoded as it's own unique integer\u003cbr\u003e\n\u003c/li\u003e\n\u003cli\u003eThe size of the embedding layer must always be greater than the total vocabulary size of the dataset! The first parameter denotes the vocabulary size, while the second denotes the size of the actual word vectors\u003c/li\u003e\n\u003cli\u003eThe size of the sequences passed in as data must be set when creating the layer (all data will be converted to padded sequences of the same size during the preprocessing step)\u003cbr\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003eIn the next lab, you'll make use of Keras' text preprocessing tools to convert the data from text to a tokenized format. Then, you'll convert the tokenized sentences to sequences. Finally, you'll pad the sequences, so that they're all the same length. During this step, you'll exclusively make use of the preprocessing tools provided by Keras. Don't worry if this all seems a bit complex right now, as you'll soon see, this is actually the most straightforward part of the next lab!\u003c/p\u003e\n\n\u003cp\u003eFor a full rundown of how to use embedding layers in Keras, see the \u003ca href=\"https://keras.io/layers/embeddings/\"\u003eKeras Documentation for Embedding Layers\u003c/a\u003e.\u003c/p\u003e\n\n\u003ch2\u003eSummary\u003c/h2\u003e\n\n\u003cp\u003eIn this lesson, you focused on the practical and pragmatic elements of using Word2Vec and word embeddings for text classification. You learned about how to load professional-quality pretrained word vectors with the Stanford NLP Group's open source GloVe data, as well as how to generate mean word embeddings that work with scikit-learn pipelines, and how to add embedding layers into neural networks with Keras!\u003c/p\u003e\n\u003cfooter class=\"fis-footer\" style=\"visibility: hidden;\"\u003e\u003cdiv class=\"fis-feedback\"\u003e\n\u003ch5\u003eHow do you feel about this lesson?\u003c/h5\u003e\n\u003cimg id=\"thumbs-up\" data-repository=\"dsc-classification-with-word-embeddings\" title=\"Thumbs up!\" alt=\"thumbs up\"\u003e\u003cimg id=\"thumbs-down\" data-repository=\"dsc-classification-with-word-embeddings\" title=\"Thumbs down!\" alt=\"thumbs down\"\u003e\n\u003c/div\u003e\n\u003ch5\u003eHave specific feedback? \u003ca href=\"https://github.com/learn-co-curriculum/dsc-classification-with-word-embeddings/issues/new\"\u003eTell us here!\u003c/a\u003e\n\u003c/h5\u003e\u003c/footer\u003e","frontPage":false},{"exportId":"context-free-grammars-and-pos-tagging","title":"Context-Free Grammars and POS Tagging","type":"WikiPage","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-context-free-grammars-and-POS-tagging\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-context-free-grammars-and-POS-tagging/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eIn this lesson, we'll explore the concept of context-free grammars, and the role they play in linguistics and NLP, particularly in relation to part-of-speech tagging.\u003c/p\u003e\n\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\n\u003cp\u003eYou will be able to:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eDescribe how you might manually create rules related to context-free grammar \u003c/li\u003e\n\u003cli\u003eDefine context-free grammars \u003c/li\u003e\n\u003cli\u003eExplain parts of speech (POS) tagging, and why it is important in NLP \u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch2\u003eWhat is a Context-Free Grammar?\u003c/h2\u003e\n\n\u003cp\u003eConsider the following sentence: \u003c/p\u003e\n\n\u003cblockquote\u003e\n\u003cp\u003e\"Colorless green ideas sleep furiously.\"\u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003cp\u003eThis is a sentence dreamed up by the famous linguist \u003ca href=\"https://en.wikipedia.org/wiki/Noam_Chomsky\"\u003eNoam Chomsky\u003c/a\u003e. This sentence, while correct at the \u003cstrong\u003e\u003cem\u003egrammatical\u003c/em\u003e\u003c/strong\u003e or \u003cstrong\u003e\u003cem\u003esyntactic\u003c/em\u003e\u003c/strong\u003e level, is just a bunch of nonsense when we consider it at the \u003cstrong\u003e\u003cem\u003esemantic\u003c/em\u003e\u003c/strong\u003e level. The sentence follows all the proper rules for a sentence in English, although in reality, it's complete nonsense. This was one of Chomsky's big ideas -- that speech contains an underlying \"deep structure\" that we recognize, regardless of the actual content of the sentence. We don't need any context about what the sentence is actually about to determine if the grammar is correct -- hence the name, \u003cstrong\u003e\u003cem\u003eContext-Free Grammar\u003c/em\u003e\u003c/strong\u003e, which we'll refer to as 'CFG' for short, for the remainder of this lesson. \u003c/p\u003e\n\n\u003cp\u003eIn order to understand CFGs, we first need to back up and gain a little background knowledge about linguistics. According to linguistics, there are five different levels of language:\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-context-free-grammars-and-POS-tagging/master/images/new_LevelsOfLanguage-Graph.png\"\u003e\u003c/p\u003e\n\n\u003cp\u003eWhen talking about CFGs, we're focusing on the \u003cstrong\u003e\u003cem\u003esyntactic level\u003c/em\u003e\u003c/strong\u003e. This level worries only about the structure of the sentence, not the informational content. \u003c/p\u003e\n\n\u003cp\u003eSo why do CFGs matter to us? For starters, they are an important part of computer science as a whole, as any code we write gets fed through a parser to determine what we want the computer to actually do. For NLP specifically, they are important because they describe a way that we can write a grammar to interpret sentences at the syntactic level. This is an approach that can be used when we want to generate \u003cstrong\u003e\u003cem\u003ePart-Of-Speech (POS) Tags\u003c/em\u003e\u003c/strong\u003e. Consider the word \"run\". This word can be interpreted as either a noun or a verb. As a noun, we may be talking about the concept of going for a jog, or a run scored in a baseball game. As a verb, we may be talking about the action of running. On its own, we don't know this. Part of the way we know which meaning to interpret for the word is our understanding of where the word fits into the sentence, and the part of speech it occupies in that sentence -- we implicitly recognize that the sentence \"I run in the mornings\" uses run as a verb, while the sentence \"The Yankees scored a run\" uses it as a noun, all based on it's placement in the sentence. \u003c/p\u003e\n\n\u003cp\u003eThis brings us to the concept of \u003cstrong\u003e\u003cem\u003eParse Trees\u003c/em\u003e\u003c/strong\u003e. \u003c/p\u003e\n\n\u003ch2\u003eParse Trees and Sentence Structure\u003c/h2\u003e\n\n\u003cp\u003eIn English, sentences consist of a \u003cstrong\u003e\u003cem\u003eNoun Phrase\u003c/em\u003e\u003c/strong\u003e followed by a \u003cstrong\u003e\u003cem\u003eVerb Phrase\u003c/em\u003e\u003c/strong\u003e, which may optionally be followed by a \u003cstrong\u003e\u003cem\u003ePrepositional Phrase\u003c/em\u003e\u003c/strong\u003e. This seems simple, but it gets more tricky when we realize that there is a recursive structure to these phrases. A noun phrase may consist of multiple smaller noun phrases, and in some cases, even a verb phrase. Similarly, a verb phrase can consist of multiple smaller verb phrases and noun phrases, which can themselves be made up of smaller noun phrases and verb phrases. \u003c/p\u003e\n\n\u003cp\u003eThis leads levels of \u003cstrong\u003e\u003cem\u003eambiguity\u003c/em\u003e\u003c/strong\u003e that can be troublesome for computers. NLTK's documentation explains this by examining the classic Groucho Marx joke:\u003c/p\u003e\n\n\u003cblockquote\u003e\n\u003cp\u003e\"While hunting in Africa, I shot an elephant in my pajamas. How he got into my pajamas, I don't know.\"\u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003cp\u003eThere are two different ways that we can interpret the first sentence. The common way that we interpret it is that a person shot an elephant while wearing pajamas. However, the alternative interpretation that is still correct (and the source of Marx's timeless punchline) is that Marx shot an elephant that was actually \u003cem\u003ein\u003c/em\u003e his pajamas. While we humans immediately understand the correct interpretation of the sentence (and hopefully get the joke), a computer has no way of knowing which of the two is the correct interpretation. \u003c/p\u003e\n\n\u003cp\u003eThe difference between the two interpretations can be most easily understood by comparing the \u003cstrong\u003e\u003cem\u003eParse Tree\u003c/em\u003e\u003c/strong\u003e for each. Take a look at this diagram from the \u003ca href=\"https://www.nltk.org/book/ch08.html\"\u003eNLTK Book's chapter on analyzing sentence structure\u003c/a\u003e:\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-context-free-grammars-and-POS-tagging/master/images/parse_tree.png\"\u003e\u003c/p\u003e\n\n\u003cp\u003eLet's break these diagrams down piece by piece. The first, most natural interpretation of the phrase \"I shot an elephant in my pajamas\" breaks down the sentence as such:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eNoun phrase: \u003ccode\u003e['I']\u003c/code\u003e\n\u003c/li\u003e\n\u003cli\u003eVerb phrase: \u003ccode\u003e['shot', 'an', 'elephant']\u003c/code\u003e\n\u003c/li\u003e\n\u003cli\u003ePrepositional phrase: \u003ccode\u003e['in', 'my', 'pajamas']\u003c/code\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003eThis is the basic sentence structure that we are used to hearing. The noun phrase tells us the subject of the sentence, the verb phrase tells us what the subject did, and the prepositional phrase offers more information about the circumstances of the action, e.g. where, when, how, etc. Note that the verb phrase here is made up of a verb ('shot'), followed by a noun phrase ('an elephant'), much in the same way that the prepositional phrase consists of a preposition ('in'), followed by a noun phrase ('my pajamas'). This nested structure is \u003cstrong\u003e\u003cem\u003erecursive\u003c/em\u003e\u003c/strong\u003e.\u003c/p\u003e\n\n\u003cp\u003eHowever, the ambiguity that Marx plays off of uses the second parse tree's structure:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eNoun phrase: \u003ccode\u003e['I']\u003c/code\u003e\n\u003c/li\u003e\n\u003cli\u003eVerb phrase: \u003ccode\u003e['shot', 'an', 'elephant', 'in', 'my', 'pajamas']\u003c/code\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003eIf we compare the two parse trees visually, the difference becomes clear. Whereas in the first interpretation, the verb phrase consists of a verb phrase and a prepositional phrase, the second interpretation is different, treating the prepositional phrase as part of a noun phrase, which is, in turn, part of the noun phrase contained within that verb phrase. If all the grammar terms are making your head spin a little, don't worry, that's normal! The simple explanation here is that the first interpretation treats 'elephant' and 'in my pajamas' as belonging to different things, while the second treats 'elephant in my pajamas' as a single phrase. \u003c/p\u003e\n\n\u003ch2\u003eWhy Does This Matter?\u003c/h2\u003e\n\n\u003cp\u003eYou may be wondering why any of this actually matters to a Data Scientist. At a glance, it mostly just seems like a rehashing of a bunch of grade-school grammar rules. The answer is that using parse trees to understand sentence structure can help us determine meaning when working with human speech. It also helps highlight why this is such a complicated task -- computers do not have the ability to judge the meaning of a sentence based on things like semantic context like we do. Put simply, we know what an elephant is, what pajamas are, and understand that it's highly unlikely that an elephant could fit in pajamas. This helps us determine how we understand that sentence on the fly -- computers don't have this luxury, so they don't know which to choose!\u003c/p\u003e\n\n\u003ch2\u003ePOS Tagging and CFGs\u003c/h2\u003e\n\n\u003cp\u003eThis brings us to part of speech tagging. One way that we can help a computer understand how to interpret a sentence is to create a CFG for it to use when parsing. The CFG defines the rules of how sentences can exist. We do this by labeling different word tokens as their grammatical types, and then defining which combinations of grammatical types are valid examples of verb phrases, noun phrases, etc. \u003c/p\u003e\n\n\u003cp\u003eLet's take a look at the example CFG from the NLTK link provided above:\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-context-free-grammars-and-POS-tagging/master/images/cfg.png\"\u003e\u003c/p\u003e\n\n\u003cp\u003eLet's break down this CFG, and see if we can understand it a bit better. \u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003e\n\u003ccode\u003eS -\u0026gt; NP VP\u003c/code\u003e A sentence (S) consists of a Noun Phrase (NP) followed by a Verb Phrase (VP).\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003ePP -\u0026gt; P NP\u003c/code\u003e A Prepositional Phrase (PP) consists of a Preposition (P) followed by a Noun Phrase (NP)\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003eNP -\u0026gt; Det N | Det N PP | 'I'\u003c/code\u003e A Noun Phrase (NP) can consist of:\n\n\u003cul\u003e\n\u003cli\u003ea Determiner (Det) followed by a Noun (N), or (as denoted by \u003ccode\u003e|\u003c/code\u003e) \u003c/li\u003e\n\u003cli\u003ea Determiner (Det) followed by a Noun (N), followed by a Prepositional Phrase (PP), or\u003c/li\u003e\n\u003cli\u003eThe token \u003ccode\u003e'I'\u003c/code\u003e.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003eVP -\u0026gt; V NP | VP PP\u003c/code\u003e A Verb Phrase can consist of:\n\n\u003cul\u003e\n\u003cli\u003ea Verb (V) followed by a Noun Phrase (NP) or\u003c/li\u003e\n\u003cli\u003ea Verb Phrase (VP) followed by a Prepositional Phrase (PP)\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003eDet -\u0026gt; 'an' | 'my'\u003c/code\u003e Determiners are the tokens 'an' or 'my'\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003eN -\u0026gt; 'elephant' | 'pajamas'\u003c/code\u003e Nouns are the tokens 'elephant' or 'pajamas'\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003eV -\u0026gt; 'shot'\u003c/code\u003e Verbs are the token 'shot'\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003eP -\u0026gt; 'in'\u003c/code\u003e Prepositions are the token 'in'\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003eAs we can see, the CFG provides explicit rules as to both:\n1. How sentences, noun phrases, verb phrases, and prepositional phrases may be structured\n2. What parts of speech each token belongs to \u003c/p\u003e\n\n\u003cp\u003eThis defines a very small CFG that allows the parser to successfully generate parse trees for the Groucho Marx's sentence. Note that both the parse trees seen above are valid, according to the rules defined in this grammar. Even though this grammar is quite explicit, both of them work. \u003c/p\u003e\n\n\u003cp\u003eSo what happens if this CFG runs across a sentence structure it doesn't understand, or a token that it doesn't have a POS label for? It fails! True CFGs are quite complex. This was a toy example. \u003c/p\u003e\n\n\u003cp\u003eIn the next lab, we'll gain some practice writing some toy CFGs for a few target sentences. We'll also learn how we can skip all this fun stuff and get existing POS tags for our tokens straight from NLTK whenever we need them, thanks to databases such as the \u003cstrong\u003e\u003cem\u003ePenn Tree Bank\u003c/em\u003e\u003c/strong\u003e!\u003c/p\u003e\n\n\u003ch2\u003eSummary\u003c/h2\u003e\n\n\u003cp\u003eIn this lesson, we dove into linguistics to understand the concept of a \u003cstrong\u003e\u003cem\u003eContext-Free Grammar\u003c/em\u003e\u003c/strong\u003e, and explored how they can be used to create parse trees for sentences.\u003c/p\u003e\n\u003cfooter class=\"fis-footer\" style=\"visibility: hidden;\"\u003e\u003cdiv class=\"fis-feedback\"\u003e\n\u003ch5\u003eHow do you feel about this lesson?\u003c/h5\u003e\n\u003cimg id=\"thumbs-up\" data-repository=\"dsc-context-free-grammars-and-POS-tagging\" title=\"Thumbs up!\" alt=\"thumbs up\"\u003e\u003cimg id=\"thumbs-down\" data-repository=\"dsc-context-free-grammars-and-POS-tagging\" title=\"Thumbs down!\" alt=\"thumbs down\"\u003e\n\u003c/div\u003e\n\u003ch5\u003eHave specific feedback? \u003ca href=\"https://github.com/learn-co-curriculum/dsc-context-free-grammars-and-POS-tagging/issues/new\"\u003eTell us here!\u003c/a\u003e\n\u003c/h5\u003e\u003c/footer\u003e","frontPage":false},{"exportId":"tuning-neural-networks-with-regularization","title":"Tuning Neural Networks with Regularization","type":"WikiPage","content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-tuning-neural-networks-with-regularization\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-tuning-neural-networks-with-regularization\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-tuning-neural-networks-with-regularization/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eNow that you've learned about neural networks and some streamlined methods for building such models, it's time to further explore how to tune and optimize the performance of these networks. One important aspect is reducing the time and resources needed to train these models. In previous lessons, when importing the Santa images, you immediately reduced each image to an extremely pixelated 64x64 representation. On top of that, you further down-sampled the dataset to reduce the number of observations. This was because training neural networks is resource intensive and is often a time consuming process as a result. Typically you also want to improve the accuracy and performance of these models. In this lesson, you will begin to examine various techniques related to these goals, beginning with the discussion of validation sets.\u003c/p\u003e\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\u003cp\u003eYou will be able to:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eExplain the relationship between bias and variance in neural networks\u003c/li\u003e\n\u003cli\u003eExplain how regularization affects the nodes of a neural network\u003c/li\u003e\n\u003cli\u003eExplain L1, L2, and dropout regularization in a neural network\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003eHyperparameters and iterative deep learning\u003c/h2\u003e\n\u003cp\u003eFirst, there are many hyperparameters you can tune. These include:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003enumber of hidden units\u003c/li\u003e\n\u003cli\u003enumber of layers\u003c/li\u003e\n\u003cli\u003elearning rate ( \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Calpha\"\u003e )\u003c/li\u003e\n\u003cli\u003eactivation function\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThe question then becomes, how do you choose these parameters? One primary method is to develop validation sets to strike a balance between specificity and generalization.\u003c/p\u003e\n\u003ch2\u003eTraining, Validation, and Test Sets\u003c/h2\u003e\n\u003cp\u003eWhen tuning neural networks it typically helps to split the data into three distinct partitions as follows: - You train algorithms on the training set - You'll use a validation set to decide which one will be your final model after parameter tuning - After having chosen the final model (and having evaluated long enough), you'll use the test set to get an unbiased estimate of the classification performance (or whatever your evaluation metric will be)\u003c/p\u003e\n\u003cp\u003eRemeber that it is \u003cstrong\u003eVERY IMPORTANT\u003c/strong\u003e to make sure that the holdout (validation) and test samples come from the same distribution: eg. same resolution of Santa pictures.\u003c/p\u003e\n\u003ch2\u003eBias and Variance in Deep Learning\u003c/h2\u003e\n\u003cp\u003eFinding a balance between generalization and specificity is at the heart of the bias-variance trade off. To further examine this process for tuning neural networks, let's return to a simple example you've seen before.\u003c/p\u003e\n\u003ch3\u003eThe Circles Example\u003c/h3\u003e\n\u003cp\u003eIn classical machine learning, you often need to consider \"bias-variance trade-off\". You'll investigate these concepts here, and see how deep learning is slightly different and a trade-off isn't always present!\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eBias = underfitting\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eHigh variance = overfitting\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eGood fit --\u0026gt; somewhere in between\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eTo start, take another look at the two circles data, the data looked like this:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-tuning-neural-networks-with-regularization/canvas/images/example.png\" alt=\"title\"\u003e\u003c/p\u003e\n\u003cp\u003eRecall that you fit a logistic regression model to the data here. You got something that looked like the picture below. The model didn't do a particularly good job at discriminating between the yellow and purple dots. You could say this is a model with a \u003cstrong\u003ehigh bias\u003c/strong\u003e, the model is \u003cstrong\u003eunderfitting\u003c/strong\u003e.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-tuning-neural-networks-with-regularization/canvas/images/underfitting.png\" alt=\"title\"\u003e\u003c/p\u003e\n\u003cp\u003eWhen using a neural network, what you reached in the end was a pretty good decision boundary, a circle discriminating between the yellow and purple dots:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-tuning-neural-networks-with-regularization/canvas/images/good.png\" alt=\"title\"\u003e\u003c/p\u003e\n\u003cp\u003eAt the other end of the spectrum, you might experience \u003cstrong\u003eoverfitting\u003c/strong\u003e, where you create a circle which is super sensitive to small deviations of the colored dots, like the example below. You can also call this a model with \u003cstrong\u003ehigh variance\u003c/strong\u003e.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-tuning-neural-networks-with-regularization/canvas/images/overfitting.png\" alt=\"title\"\u003e\u003c/p\u003e\n\u003ch2\u003eThe Santa Example\u003c/h2\u003e\n\u003cp\u003e\u0026nbsp;\u003c/p\u003e\n\u003cp\u003e\u003cimg style=\"height: 220px;\" src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-tuning-neural-networks-with-regularization/master/images/S_4.jpg\" alt=\"Santa\"\u003e \u003cimg style=\"height: 220px;\" src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-tuning-neural-networks-with-regularization/master/images/NS_1.jpg\" alt=\"CD\"\u003e\u003c/p\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003e\u003c/th\u003e\n\u003cth\u003eHigh variance\u003c/th\u003e\n\u003cth\u003eHigh bias\u003c/th\u003e\n\u003cth\u003eHigh variance \u0026amp; bias\u003c/th\u003e\n\u003cth\u003eLow variance and bias\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd\u003etrain set error\u003c/td\u003e\n\u003ctd\u003e12%\u003c/td\u003e\n\u003ctd\u003e26%\u003c/td\u003e\n\u003ctd\u003e26%\u003c/td\u003e\n\u003ctd\u003e12%\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003evalidation set error\u003c/td\u003e\n\u003ctd\u003e25%\u003c/td\u003e\n\u003ctd\u003e28%\u003c/td\u003e\n\u003ctd\u003e40%\u003c/td\u003e\n\u003ctd\u003e13%\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003cp\u003eAssume that our best model can get to a validation set accuracy of 87%. Note that \"high\" and \"low\" are relative! Also, in deep learning there is less of a bias variance trade-off!\u003c/p\u003e\n\u003ch2\u003eRules of Thumb Regarding Bias / Variance\u003c/h2\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003eHigh Bias? (training performance)\u003c/th\u003e\n\u003cth\u003eHigh variance? (validation performance)\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd\u003eUse a bigger network\u003c/td\u003e\n\u003ctd\u003eMore data\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eTrain longer\u003c/td\u003e\n\u003ctd\u003eRegularization\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eLook for other existing NN architextures\u003c/td\u003e\n\u003ctd\u003eLook for other existing NN architextures\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003ch2\u003eRegularization\u003c/h2\u003e\n\u003cp\u003eUse regularization when the model overfits to the data.\u003c/p\u003e\n\u003ch3\u003eL1 and L2 regularization\u003c/h3\u003e\n\u003ch4\u003e\u003cstrong\u003eIn Logistic Regression\u003c/strong\u003e\u003c/h4\u003e\n\u003cp\u003eLet's look back at the logistic regression example with lambda, a regularization parameter (another hyperparameter you have to tune).\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=J%20(w,b)%20=%20%5Cdfrac%7B1%7D%7Bm%7D%20%5Cdisplaystyle%5Csum%5Em_%7Bi=1%7D%5Cmathcal%7BL%7D(%5Chat%20y%5E%7B(i)%7D,%20y%5E%7B(i)%7D)%2b%20%5Cdfrac%7B%5Clambda%7D%7B2m%7D%7C%7Cw%7C%7C_2%5E2\"\u003e \u003cbr\u003e\u003cbr\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%7C%7Cw%7C%7C_2%5E2%20=%20%5Cdisplaystyle%5Csum%5E%7Bn_x%7D_%7Bj=1%7Dw_j%5E2=%20w%5ETw\"\u003e\u003c/p\u003e\n\u003cp\u003eThis is called L2-regularization. You can also add a regularization term for \u003cimg src=\"https://render.githubusercontent.com/render/math?math=b\"\u003e , but \u003cimg src=\"https://render.githubusercontent.com/render/math?math=b\"\u003e is just one parameter. L2-regularization is the most common type of regularization.\u003c/p\u003e\n\u003cp\u003eL1-regularization is where you just add a term:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cdfrac%7B%5Clambda%7D%7Bm%7D%7C%7Cw%7C%7C_1\"\u003e (could also be 2 in the denominator)\u003c/p\u003e\n\u003ch4\u003e\u003cstrong\u003eIn A Neural Network\u003c/strong\u003e\u003c/h4\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=J%20(w%5E%7B%5B1%5D%7D,b%5E%7B%5B1%5D%7D,...,w%5E%7B%5BL%5D%7D,b%5E%7B%5BL%5D%7D)%20=%20%5Cdfrac%7B1%7D%7Bm%7D%20%20%5Cdisplaystyle%5Csum%5Em_%7Bi=1%7D%5Cmathcal%7BL%7D(%5Chat%20y%5E%7B(i)%7D,%20y%5E%7B(i)%7D)%2b%20%5Cdfrac%7B%5Clambda%7D%7B2m%7D%20%5Cdisplaystyle%5Csum%5EL_%7Bl=1%7D%7C%7Cw%5E%7B%5Bl%5D%7D%7C%7C%5E2\"\u003e \u003cbr\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%7C%7Cw%5E%7B%5Bl%5D%7D%7C%7C%5E2%20=%20%20%5Cdisplaystyle%5Csum%5E%7Bn%5E%7B%5Bl-1%5D%7D%7D_%7Bi=1%7D%20%20%5Cdisplaystyle%5Csum%5E%7Bn%5E%7B%5Bl%5D%7D%7D_%7Bj=1%7D%20(w_%7Bij%7D%5E%7B%5Bl%5D%7D)%5E2\"\u003e\u003c/p\u003e\n\u003cp\u003eThis matrix norm is called the \"Frobenius norm\", also referred to as \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%7C%7Cw%5E%7B%5Bl%5D%7D%7C%7C%5E2%20_F\"\u003e\u003c/p\u003e\n\u003cp\u003eHow does backpropagation change now?\u003c/p\u003e\n\u003cp\u003eWhichever expression you have from the backpropagation, and add \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cdfrac%7B%5Clambda%7D%7Bm%7D%20w%5E%7B%5Bl%5D%7D\"\u003e .\u003c/p\u003e\n\u003cp\u003eSo,\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=dw%5E%7B%5Bl%5D%7D%20=%20%5Ctext%7B%5Bbackpropagation%20derivatives%5D%20%7D%2b%20%5Cdfrac%7B%5Clambda%7D%7Bm%7D%20w%5E%7B%5Bl%5D%7D\"\u003e\u003c/p\u003e\n\u003cp\u003eAfterwards, \u003cimg src=\"https://render.githubusercontent.com/render/math?math=w%5E%7B%5Bl%5D%7D\"\u003e is updated again as \u003cimg src=\"https://render.githubusercontent.com/render/math?math=w%5E%7B%5Bl%5D%7D%20:=%20w%5E%7B%5Bl%5D%7D%20-%20%5Calpha%20dw%5E%7B%5Bl%5D%7D\"\u003e\u003c/p\u003e\n\u003cp\u003eL2-regularization is called weight decay, because regularization will make your load smaller:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=w%5E%7B%5Bl%5D%7D:=%20w%5E%7B%5Bl%5D%7D%20-%20%5Calpha%20%5Cbigr(%20%5Ctext%7B%5Bbackpropagation%20derivatives%5D%20%7D%2b%20%5Cdfrac%7B%5Clambda%7D%7Bm%7D%20w%5E%7B%5Bl%5D%7D%5Cbigr)\"\u003e \u003cbr\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=w%5E%7B%5Bl%5D%7D:=%20w%5E%7B%5Bl%5D%7D%20-%20%5Cdfrac%7B%5Calpha%5Clambda%7D%7Bm%7Dw%5E%7B%5Bl%5D%7D%20-%20%5Calpha%20%5Ctext%7B%5Bbackpropagation%20derivatives%5D%7D\"\u003e\u003c/p\u003e\n\u003cp\u003ehence your weights will become smaller by a factor \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cbigr(1-%20%5Cdfrac%7B%5Calpha%5Clambda%7D%7Bm%7D%5Cbigr)\"\u003e.\u003c/p\u003e\n\u003cp\u003eIntuition for regularization: the weight matrices will be penalized from being too large. Actually, the network will be forced to almost be simplified.\u003c/p\u003e\n\u003cp\u003eAlso: e.g., \u003cem\u003etanh\u003c/em\u003e function, if \u003cimg src=\"https://render.githubusercontent.com/render/math?math=w\"\u003e is small, the activation function will be mostly operating in the linear region and not \"explode\" as easily.\u003c/p\u003e\n\u003ch2\u003eDropout Regularization\u003c/h2\u003e\n\u003cp\u003eWhen you apply the Dropout technique, a random subset of nodes (also called the units) in a layer are ignored (their weights set to zero) during each phase of training. Below is an image from the \u003ca href=\"http://www.jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf\"\u003eoriginal paper\u003c/a\u003e that introduced this technique.\u003c/p\u003e\n\u003cp\u003eOn the left you can see a standard neural network with four layers (one input layer, two hidden layers, and an output layer). On the right, you can see the network after Dropout is applied during one step of training. This technique is very effective because it allows us to train neural networks on different parts of the data, thus ensuring that our model is not overly sensitive noise in the data.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-tuning-neural-networks-with-regularization/canvas/images/dropout.png\" alt=\"title\"\u003e\u003c/p\u003e\n\u003cp\u003eIn Keras, you specify \u003cem\u003eDropout\u003c/em\u003e using the \u003ccode\u003eDropout\u003c/code\u003e layer, which is applied to input and hidden layers. The \u003ccode\u003eDropout\u003c/code\u003e layers requires one argument, \u003ccode\u003erate\u003c/code\u003e, which specifies the fraction of units to drop, usually between 0.2 and 0.5.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"python\"\u003emodel = models.Sequential()\nmodel.add(layers.Dense(5, activation='relu', input_shape=(500,)))\nmodel.add(layers.Dropout(0.3))\nmodel.add(layers.Dense(5, activation='relu'))\nmodel.add(layers.Dropout(0.3))\nmodel.add(layers.Dense(1, activation='sigmoid'))\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eIn different iterations through the training set, different nodes will be zeroed out!\u003c/p\u003e\n\u003ch2\u003eSummary\u003c/h2\u003e\n\u003cp\u003eIn this lesson you began to explore how to further tune and optimize out of the box neural networks built with Keras. This included regularization analogous to previous machine learning work you've seen, as well dropout regularization, which can be used to further prune your networks. In the upcoming lab you'll get a chance to experiment with these concepts in practice and observe their effect on your models outputs.\u003c/p\u003e","frontPage":false},{"exportId":"convolutional-neural-networks-introduction","title":"Convolutional Neural Networks - Introduction","type":"WikiPage","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-cnn-intro\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-cnn-intro/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eUntil now, you've learned about densely connected networks and how they can be super powerful for classification problems when you have unstructured data such as text or images. In one of the previous labs, you analyzed images and used densely connected neural networks to classify images according to whether they contained Santa or not. In this section you'll learn about another type of neural networks that work particularly well on image data: Convolutional Neural Networks.\u003c/p\u003e\n\n\u003ch3\u003eConvolutional Neural Networks\u003c/h3\u003e\n\n\u003cp\u003eThere are several issues when using densely connected neural networks on image data. Firstly, dense layers learn global patterns rather than local patterns, and densely connected networks can really grow very big if we have high resolution images. In this section, you'll see why Convolutional Neural Networks are often preferred over densely connected networks for image processing. Additionally, you'll learn what a convolution operation is, the different building blocks of convolutional neural networks (including filters, padding schemes, strided convolutions, etc.), and the types of network layers that are part of your convolutional neural networks.\u003c/p\u003e\n\n\u003ch3\u003eBuilding a CNN from Scratch\u003c/h3\u003e\n\n\u003cp\u003eOnce you understand how CNNs work, you'll practice building one from scratch. You'll learn how to preprocess your image data so your model can be trained using Keras. Just like with densely connected networks, Keras provides an extremely user-friendly tool to build CNNs.\u003c/p\u003e\n\n\u003ch3\u003eVisualizing Intermediate Activations\u003c/h3\u003e\n\n\u003cp\u003eAs with densely connected networks, CNNs are complicated networks that are considered a \"black box\" tool with little insight in what's happening in the network layers. However, when using CNNs, you're essentially changing your image through filters in every layer. You'll learn to get some insight in your black box models by visualizing the intermediate layers in your CNNs!\u003c/p\u003e\n\n\u003ch2\u003eSummary\u003c/h2\u003e\n\n\u003cp\u003eIn this section, you'll extend your deep learning knowledge by learning about convolutional neural networks. \u003c/p\u003e\n\u003cfooter class=\"fis-footer\" style=\"visibility: hidden;\"\u003e\u003cdiv class=\"fis-feedback\"\u003e\n\u003ch5\u003eHow do you feel about this lesson?\u003c/h5\u003e\n\u003cimg id=\"thumbs-up\" data-repository=\"dsc-cnn-intro\" title=\"Thumbs up!\" alt=\"thumbs up\"\u003e\u003cimg id=\"thumbs-down\" data-repository=\"dsc-cnn-intro\" title=\"Thumbs down!\" alt=\"thumbs down\"\u003e\n\u003c/div\u003e\n\u003ch5\u003eHave specific feedback? \u003ca href=\"https://github.com/learn-co-curriculum/dsc-cnn-intro/issues/new\"\u003eTell us here!\u003c/a\u003e\n\u003c/h5\u003e\u003c/footer\u003e","frontPage":false},{"exportId":"tuning-neural-networks-with-normalization","title":"Tuning Neural Networks with Normalization","type":"WikiPage","content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-tuning-neural-networks-with-normalization\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-tuning-neural-networks-with-normalization\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-tuning-neural-networks-with-normalization/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eNow that we've investigated some methods for tuning our networks, we will investigate some further methods and concepts regarding reducing training time. These concepts will begin to form a more cohesive framework for choices along the modelling process.\u003c/p\u003e\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\u003cp\u003eYou will be able to:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eExplain what normalization does to training time with neural networks and why\u003c/li\u003e\n\u003cli\u003eExplain what a vanishing or exploding gradient is, and how it is related to model convergence\u003c/li\u003e\n\u003cli\u003eCompare the different optimizer strategies for neural networks\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003eNormalized Inputs: Speed up Training\u003c/h2\u003e\n\u003cp\u003eOne way to speed up training of your neural networks is to normalize the input. In fact, even if training time were not a concern, normalization to a consistent scale (typically 0 to 1) across features should be used to ensure that the process converges to a stable solution. Similar to some of our previous work in training models, one general process for standardizing our data is subtracting the mean and dividing by the standard deviation.\u003c/p\u003e\n\u003ch2\u003eVanishing or Exploding Gradients\u003c/h2\u003e\n\u003cp\u003eNot only will normalizing your inputs speed up training, it can also mitigate other risks inherent in training neural networks. For example, in a neural network, having input of various ranges can lead to difficult numerical problems when the algorithm goes to compute gradients during forward and back propogation. This can lead to untenable solutions and will prevent the algorithm from converging to a solution. In short, make sure you normalize your data! Here's a little more mathematical background:\u003c/p\u003e\n\u003cp\u003eTo demonstrate, imagine a very deep neural network. Assume \u003cimg src=\"https://render.githubusercontent.com/render/math?math=g(z)=z\"\u003e (so no transformation, just a linear activation function), and biases equal to 0.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Chat%20y%20=%20w%5E%7B%5BL%5D%7Dw%5E%7B%5BL-1%5D%7Dw%5E%7B%5BL-2%5D%7D...%20w%5E%7B%5B3%5D%7Dw%5E%7B%5B2%5D%7Dw%5E%7B%5B1%5D%7Dx\"\u003e\u003c/p\u003e\n\u003cp\u003eRecall that \u003cimg src=\"https://render.githubusercontent.com/render/math?math=z%5E%7B%5B1%5D%7D%20=w%5E%7B%5B1%5D%7Dx\"\u003e , and that \u003cimg src=\"https://render.githubusercontent.com/render/math?math=a%5E%7B%5B1%5D%7D=g(z%5E%7B%5B1%5D%7D)=z%5E%7B%5B1%5D%7D\"\u003e\u003c/p\u003e\n\u003cp\u003eSimilarly, \u003cimg src=\"https://render.githubusercontent.com/render/math?math=a%5E%7B%5B2%5D%7D=g(z%5E%7B%5B2%5D%7D)=g(w%5E%7B%5B2%5D%7Da%5E%7B%5B1%5D%7D)\"\u003e\u003c/p\u003e\n\u003cp\u003eImagine two nodes in each layer, and w = \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cbig%5B%20%7B1.3%20%5Cquad%200%7D%5Catop%7B0%20%5Cquad%201.3%7D%20%5Cbig%5D\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Chat%20y%20=%20w%5E%7B%5BL%5D%7D%20%5Cbig%5B%7B1.3%20%5Cquad%200%7D%5Catop%7B0%20%5Cquad%201.3%7D%20%5Cbig%5D%20%5E%7BL-1%7D%20%20%20x\"\u003e\u003c/p\u003e\n\u003cp\u003eEven if the \u003cimg src=\"https://render.githubusercontent.com/render/math?math=w\"\u003e 's are slightly smaller than 1 or slightly larger, the activations will explode when there are many layers in the network!\u003c/p\u003e\n\u003ch2\u003eOther Solutions to Vanishing and Exploding Gradients\u003c/h2\u003e\n\u003cp\u003eAside from normalizing the data, you can also investigate the impact of changing the initialization parameters when you first launch the gradient descent algorithm.\u003c/p\u003e\n\u003cp\u003eFor initialization, the more input features feeding into layer l, the smaller you want each \u003cimg src=\"https://render.githubusercontent.com/render/math?math=w_i\"\u003e to be.\u003c/p\u003e\n\u003cp\u003eA common rule of thumb is:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=Var(w_i)%20=%201/n\"\u003e\u003c/p\u003e\n\u003cp\u003eor\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=Var(w_i)%20=%202/n\"\u003e\u003c/p\u003e\n\u003cp\u003eOne common initialization strategy for the relu activation function is:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ew^{[l]} = np.random.randn(shape)*np.sqrt(2/n_(l-1)) \n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eLater, we'll discuss other initialization strategies pertinent to other activation fuctions.\u003c/p\u003e\n\u003ch2\u003eOptimization\u003c/h2\u003e\n\u003cp\u003eIn addition, you could even use an alternative convergence algorithm instead of gradient descent. One issue with gradient descent is that it oscillates to a fairly big extent, because the derivative is bigger in the vertical direction.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-tuning-neural-networks-with-normalization/master/images/new_optimizer.png\" width=\"600\"\u003e\u003c/p\u003e\n\u003cp\u003eWith that, here are some optimization algorithms that work faster than gradient descent:\u003c/p\u003e\n\u003ch3\u003eGradient Descent with Momentum\u003c/h3\u003e\n\u003cp\u003eCompute an exponentially weighthed average of the gradients and use that gradient instead. The intuitive interpretation is that this will successively dampen oscillations, improving convergence.\u003c/p\u003e\n\u003cp\u003eMomentum:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003ecompute \u003cimg src=\"https://render.githubusercontent.com/render/math?math=dW\"\u003e and \u003cimg src=\"https://render.githubusercontent.com/render/math?math=db\"\u003e on the current minibatch\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003ecompute \u003cimg src=\"https://render.githubusercontent.com/render/math?math=V_%7Bdw%7D%20=%20%5Cbeta%20V_%7Bdw%7D%20%2b%20(1-%5Cbeta)dW\"\u003e and\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003ecompute \u003cimg src=\"https://render.githubusercontent.com/render/math?math=V_%7Bdb%7D%20=%20%5Cbeta%20V_%7Bdb%7D%20%2b%20(1-%5Cbeta)db\"\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cblockquote\u003e\n\u003cp\u003eThese are the moving averages for the derivatives of \u003cimg src=\"https://render.githubusercontent.com/render/math?math=W\"\u003e and \u003cimg src=\"https://render.githubusercontent.com/render/math?math=b\"\u003e\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=W:=%20W-%20%5Calpha%20Vdw\"\u003e \u003cbr\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=b:=%20b-%20%5Calpha%20Vdb\"\u003e\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eThis averages out gradient descent, and will \"dampen\" oscillations. Generally, \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cbeta=0.9\"\u003e is a good hyperparameter value.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch3\u003eRMSprop\u003c/h3\u003e\n\u003cp\u003eRMSprop stands for \"root mean square\" prop. It slows down learning in one direction and speed up in another one. On each iteration, it uses exponentially weighted average of the squares of the derivatives.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=S_%7Bdw%7D%20=%20%5Cbeta%20S_%7Bdw%7D%20%2b%20(1-%5Cbeta)dW%5E2\"\u003e \u003cbr\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=S_%7Bdb%7D%20=%20%5Cbeta%20S_%7Bdw%7D%20%2b%20(1-%5Cbeta)db%5E2\"\u003e \u003cbr\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=W:=%20W-%20%5Calpha%20%5Cdfrac%7Bdw%7D%7B%5Csqrt%7BS_%7Bdw%7D%7D%7D\"\u003e and \u003cbr\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=b:=%20b-%20%5Calpha%20%5Cdfrac%7Bdb%7D%7B%5Csqrt%7BS_%7Bdb%7D%7D%7D\"\u003e\u003c/p\u003e\n\u003cp\u003eIn the direction where we want to learn fast, the corresponding \u003cimg src=\"https://render.githubusercontent.com/render/math?math=S\"\u003e will be small, so dividing by a small number. On the other hand, in the direction where we will want to learn slow, the corresponding \u003cimg src=\"https://render.githubusercontent.com/render/math?math=S\"\u003e will be relatively large, and updates will be smaller.\u003c/p\u003e\n\u003cp\u003eOften, add small \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cepsilon\"\u003e in the denominator to make sure that you don't end up dividing by 0.\u003c/p\u003e\n\u003ch3\u003eAdam Optimization Algorithm\u003c/h3\u003e\n\u003cp\u003e\"Adaptive Moment Estimation\", basically using the first and second moment estimations. Works very well in many situations! It takes momentum and RMSprop to put it together!\u003c/p\u003e\n\u003cp\u003eInitialize:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=V_%7Bdw%7D=0,%20S_%7Bdw%7D=0,%20V_%7Bdb%7D=0,%20S_%7Bdb%7D=0\"\u003e\u003c/p\u003e\n\u003cp\u003eFor each iteration:\u003c/p\u003e\n\u003cp\u003eCompute \u003cimg src=\"https://render.githubusercontent.com/render/math?math=dW,%20db\"\u003e using the current mini-batch:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=V_%7Bdw%7D%20=%20%5Cbeta_1%20V_%7Bdw%7D%20%2b%20(1-%5Cbeta_1)dW\"\u003e , \u003cimg src=\"https://render.githubusercontent.com/render/math?math=V_%7Bdb%7D%20=%20%5Cbeta_1%20V_%7Bdb%7D%20%2b%20(1-%5Cbeta_1)db\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=S_%7Bdw%7D%20=%20%5Cbeta_2%20S_%7Bdw%7D%20%2b%20(1-%5Cbeta_2)dW%5E2\"\u003e , \u003cimg src=\"https://render.githubusercontent.com/render/math?math=S_%7Bdb%7D%20=%20%5Cbeta_2%20S_%7Bdb%7D%20%2b%20(1-%5Cbeta_2)db%5E2\"\u003e\u003c/p\u003e\n\u003cp\u003eIs like momentum and then RMSprop. We need to perform a correction! This is sometimes also done in RSMprop, but definitely here too.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=V%5E%7Bcorr%7D_%7Bdw%7D=%20%5Cdfrac%7BV_%7Bdw%7D%7D%7B1-%5Cbeta_1%5Et%7D\"\u003e , \u003cimg src=\"https://render.githubusercontent.com/render/math?math=V%5E%7Bcorr%7D_%7Bdb%7D=%20%5Cdfrac%7BV_%7Bdb%7D%7D%7B1-%5Cbeta_1%5Et%7D\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=S%5E%7Bcorr%7D_%7Bdw%7D=%20%5Cdfrac%7BS_%7Bdw%7D%7D%7B1-%5Cbeta_2%5Et%7D\"\u003e , \u003cimg src=\"https://render.githubusercontent.com/render/math?math=S%5E%7Bcorr%7D_%7Bdb%7D=%20%5Cdfrac%7BS_%7Bdb%7D%7D%7B1-%5Cbeta_2%5Et%7D\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=W:=%20W-%20%5Calpha%20%5Cdfrac%7BV%5E%7Bcorr%7D_%7Bdw%7D%7D%7B%5Csqrt%7BS%5E%7Bcorr%7D_%7Bdw%7D%2b%5Cepsilon%7D%7D\"\u003e and\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=b:=%20b-%20%5Calpha%20%5Cdfrac%7BV%5E%7Bcorr%7D_%7Bdb%7D%7D%7B%5Csqrt%7BS%5E%7Bcorr%7D_%7Bdb%7D%2b%5Cepsilon%7D%7D\"\u003e\u003c/p\u003e\n\u003cp\u003eHyperparameters:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Calpha\"\u003e\u003c/li\u003e\n\u003cli\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cbeta_1%20=%200.9\"\u003e\u003c/li\u003e\n\u003cli\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cbeta_2%20=%200.999\"\u003e\u003c/li\u003e\n\u003cli\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cepsilon%20=%2010%5E%7B-8%7D\"\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eGenerally, only \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Calpha\"\u003e gets tuned.\u003c/p\u003e\n\u003ch3\u003eLearning Rate Decay\u003c/h3\u003e\n\u003cp\u003eLearning rate decreases across epochs.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Calpha%20=%20%5Cdfrac%7B1%7D%7B1%2b%5Ctext%7Bdecay_rate%20*%20epoch_nb%7D%7D*%20%5Calpha_0\"\u003e\u003c/p\u003e\n\u003cp\u003eother methods:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Calpha%20=%200.97%20%5E%7B%5Ctext%7Bepoch_nb%7D%7D*%20%5Calpha_0\"\u003e (or exponential decay)\u003c/p\u003e\n\u003cp\u003eor\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Calpha%20=%20%5Cdfrac%7Bk%7D%7B%5Csqrt%7B%5Ctext%7Bepoch_nb%7D%7D%7D*%20%5Calpha_0\"\u003e\u003c/p\u003e\n\u003cp\u003eor\u003c/p\u003e\n\u003cp\u003eManual decay!\u003c/p\u003e\n\u003ch2\u003eHyperparameter Tuning\u003c/h2\u003e\n\u003cp\u003eNow that you've seen some optimization algorithms, take another look at all the hyperparameters that need tuning:\u003c/p\u003e\n\u003cp\u003eMost important: - \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Calpha\"\u003e\u003c/p\u003e\n\u003cp\u003eNext: - \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cbeta\"\u003e (momentum) - Number of hidden units - mini-batch-size\u003c/p\u003e\n\u003cp\u003eFinally: - Number of layers - Learning rate decay\u003c/p\u003e\n\u003cp\u003eAlmost never tuned: - \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cbeta_1\"\u003e , \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cbeta_2\"\u003e , \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cepsilon\"\u003e (Adam)\u003c/p\u003e\n\u003cp\u003eThings to do:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eDon't use a grid, because hard to say in advance which hyperparameters will be important\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003eAdditional Resources\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://www.coursera.org/learn/deep-neural-network/lecture/lXv6U/normalizing-inputs\"\u003ehttps://www.coursera.org/learn/deep-neural-network/lecture/lXv6U/normalizing-inputs\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://www.coursera.org/learn/deep-neural-network/lecture/y0m1f/gradient-descent-with-momentum\"\u003ehttps://www.coursera.org/learn/deep-neural-network/lecture/y0m1f/gradient-descent-with-momentum\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003eSummary\u003c/h2\u003e\n\u003cp\u003eIn this lesson you began learning about issues regarding the convergence of neural networks training. This included the need for normalization as well as initialization parameters and some optimization algorithms. In the upcoming lab, you'll further investigate these ideas in practice and observe their impacts from various perspectives.\u003c/p\u003e","frontPage":false},{"exportId":"recommendation-systems-recap","title":"Recommendation Systems - Recap","type":"WikiPage","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-recommendation-section-recap\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-recommendation-section-recap/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\u003ch2\u003eKey Takeaways\u003c/h2\u003e\n\n\u003cp\u003eThe key takeaways from this section include:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eRecommendation approaches can consist of simply recommending popular items (without personalization), or using algorithms which takes into account past customer behavior\u003c/li\u003e\n\u003cli\u003eWhen using algorithms, the two main types are content-based algorithms (recommending new content based on similar \u003cem\u003econtent\u003c/em\u003e), or collaborative filtering based (recommending new content based on similar types of \u003cem\u003eusers\u003c/em\u003e)\u003c/li\u003e\n\u003cli\u003eCollaborative Filtering (CF) is currently the most widely used approach to build recommendation systems\u003c/li\u003e\n\u003cli\u003eThe key idea behind CF is that similar users have similar interests and that a user generally likes items that are similar to other items they like\u003c/li\u003e\n\u003cli\u003eCF is filling an \"empty cell\" in the utility matrix based on the similarity between users or item. Matrix factorization or decomposition can help us solve this problem by determining what the overall \"topics\" are when a matrix is factored\u003c/li\u003e\n\u003cli\u003eMatrix decomposition can be reformulated as an optimization problem with loss functions and constraints\u003c/li\u003e\n\u003cli\u003eMatrix decomposition can be done using either Singular Value Decomposition (SVD) or Alternating Least Squares (ALS)\u003c/li\u003e\n\u003cli\u003eSpark's ALS implementation can be used to build a scalable and efficient recommendation system \u003c/li\u003e\n\u003c/ul\u003e\n\u003cfooter class=\"fis-footer\" style=\"visibility: hidden;\"\u003e\u003cdiv class=\"fis-feedback\"\u003e\n\u003ch5\u003eHow do you feel about this lesson?\u003c/h5\u003e\n\u003cimg id=\"thumbs-up\" data-repository=\"dsc-recommendation-section-recap\" title=\"Thumbs up!\" alt=\"thumbs up\"\u003e\u003cimg id=\"thumbs-down\" data-repository=\"dsc-recommendation-section-recap\" title=\"Thumbs down!\" alt=\"thumbs down\"\u003e\n\u003c/div\u003e\n\u003ch5\u003eHave specific feedback? \u003ca href=\"https://github.com/learn-co-curriculum/dsc-recommendation-section-recap/issues/new\"\u003eTell us here!\u003c/a\u003e\n\u003c/h5\u003e\u003c/footer\u003e","frontPage":false},{"exportId":"neural-networks-recap","title":"Neural Networks - Recap","type":"WikiPage","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-neural-networks-section-recap\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-neural-networks-section-recap/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\u003ch2\u003eKey Takeaways\u003c/h2\u003e\n\n\u003cp\u003eThe key takeaways from this section include:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eNeural networks are powerful models that can be customized and tweaked using various amounts of nodes, layers, ...\u003c/li\u003e\n\u003cli\u003eThe most basic neural networks are single-layer densely connected neural networks, which have very similar properties as logistic regression models\u003c/li\u003e\n\u003cli\u003eCompared to more traditional statistics and ML techniques, neural networks perform particularly well when using unstructured data\u003c/li\u003e\n\u003cli\u003eApart from densely connected networks, other types of neural networks include convolutional neural networks, recurrent neural networks, and generative adversarial neural networks \u003c/li\u003e\n\u003cli\u003eWhen working with image data, it's important to understand how image data is stored when working with them in Python\u003c/li\u003e\n\u003cli\u003eLogistic regression can be seen as a single-layer neural network with a sigmoid activation function\u003c/li\u003e\n\u003cli\u003eNeural networks use loss and cost functions to minimize the \"loss\", which is a function that summarizes the difference between the actual outcome (eg. pictures contain santa or not) and the model prediction (whether the model correctly identifies pictures with santas)\u003c/li\u003e\n\u003cli\u003eBackward and forward propagation are used to estimate the so-called \"model weights\"\u003c/li\u003e\n\u003cli\u003eAdding more layers to neural networks can substantially increase model performance\u003c/li\u003e\n\u003cli\u003eSeveral activations can be used in model nodes, you can explore with different types and evaluate how it affects performance\u003c/li\u003e\n\u003c/ul\u003e\n\u003cfooter class=\"fis-footer\" style=\"visibility: hidden;\"\u003e\u003cdiv class=\"fis-feedback\"\u003e\n\u003ch5\u003eHow do you feel about this lesson?\u003c/h5\u003e\n\u003cimg id=\"thumbs-up\" data-repository=\"dsc-neural-networks-section-recap\" title=\"Thumbs up!\" alt=\"thumbs up\"\u003e\u003cimg id=\"thumbs-down\" data-repository=\"dsc-neural-networks-section-recap\" title=\"Thumbs down!\" alt=\"thumbs down\"\u003e\n\u003c/div\u003e\n\u003ch5\u003eHave specific feedback? \u003ca href=\"https://github.com/learn-co-curriculum/dsc-neural-networks-section-recap/issues/new\"\u003eTell us here!\u003c/a\u003e\n\u003c/h5\u003e\u003c/footer\u003e","frontPage":false},{"exportId":"convolutional-neural-networks","title":"Convolutional Neural Networks","type":"WikiPage","content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-convolutional-neural-networks\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-convolutional-neural-networks\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-convolutional-neural-networks/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eConvolutional Neural Networks (CNNs), build upon the fully connected neural networks you've seen to date. Since detailed images can have incredibly high dimensions based on the number of pixels, CNNs provide an alternative formulation for analyzing groups of pixels. Without the convolutional operation, fitting neural networks to medium to large images would be infeasible for all but the most powerful computers. For example, given a color image with 500 x 500 pixels, you would have 500 x 500 x 3 = 750,000 input features, \u003cimg src=\"https://render.githubusercontent.com/render/math?math=(x_1,...,x_%7B750,000%7D)\"\u003e . From there, even having 2000 hidden units (3% of the input), in the first hidden layer, would result in roughly 1.5 billion parameters!\u003c/p\u003e\n\u003cp\u003eCNNs have certain features that identify patterns in images because of \"convolution operation\" including:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eDense layers learn global patterns in their input feature space\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eConvolution layers learn local patterns, and this leads to the following interesting features:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eUnlike with densely connected networks, when a convolutional neural network recognizes a pattern in one region, these insights can be shared and applied to other regions.\u003c/li\u003e\n\u003cli\u003eDeeper convolutional neural networks can learn spatial hierarchies. A first layer will learn small local patterns, a second layer will learn larger patterns using features of the first layer patterns, etc.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eBecause of these properties, CNNs are great for tasks like:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eImage classification\u003c/li\u003e\n\u003cli\u003eObject detection in images\u003c/li\u003e\n\u003cli\u003ePicture neural style transfer\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\u003cp\u003eYou will be able to:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eDefine what a convolution is, as it relates to CNNs\u003c/li\u003e\n\u003cli\u003eExplain how convolutions work using RGB images\u003c/li\u003e\n\u003cli\u003eDescribe what a pooling layer is in a neural network\u003c/li\u003e\n\u003cli\u003eExplain how padding works with convolution layers of a neural network\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003eBuilding CNNs in Keras\u003c/h2\u003e\n\u003cp\u003eBuilding a CNN in Keras is very similar to the previous neural networks that you've built to date. To start, you will initialize a sequential model as before and go on adding layers. However, rather then simply adding additional dense layers or dropouts between them, we will now start to investigate other potential layer architectures including convolutional layers.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-convolutional-neural-networks/master/images/Image_158CNN.png\"\u003e\u003c/p\u003e\n\u003ch2\u003eThe Convolution Operation\u003c/h2\u003e\n\u003cp\u003eThe idea behind the convolutional operation is to detect complex building blocks, or features, that can aid in the larger task such as image recognition. For example, we'll detect vertical or horizontal edges present in the image. Let's look at what horizontal edge detection would look like:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-convolutional-neural-networks/master/images/conv.png\"\u003e\u003c/p\u003e\n\u003cp\u003eThis is a simplified 5 x 5 pixel image (greyscale!). You use a so-called \"filter\" (denoted on the right) to perform a convolution operation. This particular filter operation will detect horizontal edges. The matrix in the left should have number in it (from 1-255, or let's assume we rescaled it to number 1-10). The output is a 3 x 3 matrix. (\u003cem\u003eThis example is for computational clarity, no clear edges\u003c/em\u003e)\u003c/p\u003e\n\u003cp\u003eIn Keras, function for the convolution step is \u003ccode\u003eConv2D\u003c/code\u003e.\u003c/p\u003e\n\u003cp\u003eThe convolutional operation applies this filter (typically 3x3 or 5x5) to each possible 3x3 or 5x5 region of the original image. The graphic below demonstrates this process.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-convolutional-neural-networks/master/images/convolution-layer-a.png\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://stanford.edu/%7Eshervine/teaching/cs-230/cheatsheet-convolutional-neural-networks\"\u003egif courtesy of Stanford University\u003c/a\u003e\u003c/p\u003e\n\u003ch2\u003ePadding\u003c/h2\u003e\n\u003cp\u003eThere are some issues with using filters on images including:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eThe image shrinks with each convolution layer: you're throwing away information in each layer! For example:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eStarting from a 5 x 5 matrix, and using a 3 x 3 matrix, you end up with a 3 x 3 image\u003c/li\u003e\n\u003cli\u003eStarting from a 10 x 10 matrix, and using a 3 x 3 matrix, you end up with a 8 x 8 image, etc.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eThe pixels around the edges are used much less in the outputs due to the filter\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eFor example, if you apply 3x3 filters to a 5x5 image, the original 5x5 image contains 25 pixels, but tiling the 3x3 filter only has 9 possible locations. Here's the 4 of the 9 possible locations for the 3x3 filter on a 5x5 image:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-convolutional-neural-networks/master/images/5by5_3by3_1.jpeg\" width=\"200\"\u003e \u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-convolutional-neural-networks/master/images/5by5_3by3_2.jpeg\" width=\"200\"\u003e \u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-convolutional-neural-networks/master/images/5by5_3by3_3.jpeg\" width=\"200\"\u003e \u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-convolutional-neural-networks/master/images/5by5_3by3_4.jpeg\" width=\"200\"\u003e\u003c/p\u003e\n\u003cp\u003eFortunately, padding solves both of these problems! Just one layer of pixels around the edges preserves the image size when having a 3 x 3 filter. We can also use bigger filters, but generally the dimensions are odd!\u003c/p\u003e\n\u003cp\u003eSome further terminology regarding padding that you should be aware of includes:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\"Valid\" - no padding\u003c/li\u003e\n\u003cli\u003e\"Same\" - padding such that output is same as the input size\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eBy adding padding to our 5x5 image, (now a 6x6 image by adding a border of pixels) we can add padding so that each pixel of our original 5x5 image can be the center of a 3x3 convolution window filter.\u003c/p\u003e\n\u003ch2\u003eStrided convolutions\u003c/h2\u003e\n\u003cp\u003eAnother method to change the output of your convolutions is to change the stride. The stride is how the convolution filter is moved over the original image. In our above example, we moved the filter one pixel to the right starting from the upper left hand corner, and then began to do this again after moving the filter one pixel down. Alternatively, by changing the stride, we could move our filter by 2 pixels each time, resulting in a smaller number of possible locations for the filter.\u003c/p\u003e\n\u003cp\u003eStrided convolutions are rarely used in practice but a good feature to be aware of for some models.\u003c/p\u003e\n\u003ch2\u003eConvolutions on RGB images\u003c/h2\u003e\n\u003cp\u003eInstead of 5 x 5 grayscale, imagine a 7 x 7 RGB image, which boils down to having a 7 x 7 x 3 tensor. (The image itself is compromised by a 7 by 7 matrix of pixels, each with 3 numerical values for the RGB values.) From there, you will need to use a filter that has the third dimension equal to 3 as well, let's say, 3 x 3 x 3 (a 3D \"cube\").\u003c/p\u003e\n\u003cp\u003eThis allows you to detect horizontal edges in the blue channel.\u003c/p\u003e\n\u003cp\u003eThen, in each layer, you can convolve with several 3D filters. Afterwards, you stack every output of the result together, giving you a matrix of shape 5 x 5 x \u003ccode\u003enumber_of_filters\u003c/code\u003e.\u003c/p\u003e\n\u003cp\u003eIf you think of it, the filter plays the same role as the w^{[1]} in our densely connected networks.\u003c/p\u003e\n\u003cp\u003eThe advantage is, while your image may be huge, the amount of parameters you have still only depends on how many filters you're using!\u003c/p\u003e\n\u003cp\u003eImagine 20 (3 x 3 x 3) --\u0026gt; 20 * 27 + a bias for each filter (1* 20) = 560 parameters.\u003c/p\u003e\n\u003cp\u003eNotation:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cimg src=\"https://render.githubusercontent.com/render/math?math=f%5E%7B%5Bl%5D%7D\"\u003e = size of the filter\u003c/li\u003e\n\u003cli\u003e\n\u003cimg src=\"https://render.githubusercontent.com/render/math?math=p%5E%7B%5Bl%5D%7D\"\u003e = padding\u003c/li\u003e\n\u003cli\u003e\n\u003cimg src=\"https://render.githubusercontent.com/render/math?math=s%5E%7B%5Bl%5D%7D\"\u003e = amount of stride\u003c/li\u003e\n\u003cli\u003e\n\u003cimg src=\"https://render.githubusercontent.com/render/math?math=n_c%5E%7B%5Bl%5D%7D\"\u003e = number of filters\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003efilter: \u003cimg src=\"https://render.githubusercontent.com/render/math?math=f%5E%7B%5Bl%5D%7D\"\u003e x \u003cimg src=\"https://render.githubusercontent.com/render/math?math=f%5E%7B%5Bl%5D%7D\"\u003e x \u003cimg src=\"https://render.githubusercontent.com/render/math?math=n_c%5E%7B%5Bl-1%5D%7D\"\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eInput = \u003cimg src=\"https://render.githubusercontent.com/render/math?math=n_h%5E%7B%5Bl-1%5D%7D%20*%20n_w%5E%7B%5Bl-1%5D%7D%20*%20n_c%5E%7B%5Bl-1%5D%7D\"\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eOutput = \u003cimg src=\"https://render.githubusercontent.com/render/math?math=n_h%5E%7B%5Bl%5D%7D%20*%20n_w%5E%7B%5Bl%5D%7D%20*%20n_c%5E%7B%5Bl%5D%7D\"\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eHeight and width are given by:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=n_h%5E%7B%5Bl%5D%7D=%20%5CBigr%5Clfloor%5Cdfrac%7Bn_h%5E%7B%5Bl-1%5D%7D%2b2p%5E%7B%5Bl%5D%7D-f%5E%7B%5Bl%5D%7D%7D%7Bs%5E%7B%5Bl%5D%7D%7D%2b1%5CBigr%5Crfloor\"\u003e \u003cbr\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=n_w%5E%7B%5Bl%5D%7D=%20%5CBigr%5Clfloor%5Cdfrac%7Bn_w%5E%7B%5Bl-1%5D%7D%2b2p%5E%7B%5Bl%5D%7D-f%5E%7B%5Bl%5D%7D%7D%7Bs%5E%7B%5Bl%5D%7D%7D%2b1%5CBigr%5Crfloor\"\u003e\u003c/p\u003e\n\u003cp\u003eActivations: \u003cimg src=\"https://render.githubusercontent.com/render/math?math=a%5E%7B%5Bl%5D%7D\"\u003e is of dimension \u003cimg src=\"https://render.githubusercontent.com/render/math?math=n_h%5E%7B%5Bl%5D%7D%20*%20n_w%5E%7B%5Bl%5D%7D%20*%20n_c%5E%7B%5Bl%5D%7D\"\u003e\u003c/p\u003e\n\u003ch2\u003ePooling layer\u003c/h2\u003e\n\u003cp\u003eThe last element in a CNN architecture (before fully connected layers as we have previously discussed in other neural networks) is the pooling layer. This layer is meant to substantially downsample the previous convolutional layers. The idea behind this is that the previous convolutional layers will find patterns such as edges or other basic shapes present in the pictures. From there, pooling layers such as Max pooling (the most common) will take a summary of the convolutions from a larger section. In practice, Max pooling (taking the max of all convolutions from a larger area of the original image) works better than average pooling as we are typically looking to detect whether a feature is present in that region. Downsampling is essential in order to produce viable execution times in the model training.\u003c/p\u003e\n\u003cp\u003eMax pooling has some important hyperparameters: - \u003cimg src=\"https://render.githubusercontent.com/render/math?math=f\"\u003e (filter size) - \u003cimg src=\"https://render.githubusercontent.com/render/math?math=S\"\u003e (stride)\u003c/p\u003e\n\u003cp\u003eCommon hyperparameters include: \u003ccode\u003ef=2\u003c/code\u003e, \u003ccode\u003es=2\u003c/code\u003e and \u003ccode\u003ef=3\u003c/code\u003e, \u003ccode\u003es=2\u003c/code\u003e, this shrinks the size of the representations. If a feature is detected anywhere in the quadrants, a high number will appear, so max pooling preserves this feature.\u003c/p\u003e\n\u003ch2\u003eFully Connected Layers in CNN\u003c/h2\u003e\n\u003cp\u003eOnce you have added a number of convolutional layers and pooling layers, you then will add fully connected (dense) layers as we did before in previous neural network models. This now allows the network to learn a final decision function based on these transformed informative inputs generating from the convolutional and pooling layers.\u003c/p\u003e\n\u003ch2\u003eSummary\u003c/h2\u003e\n\u003cp\u003eIn this lesson, you learned about the basic concepts behind CNNs including their use cases and general architecture. In the upcoming lab, you'll begin to look at how you can build these models in Python using Keras.\u003c/p\u003e","frontPage":false},{"exportId":"transfer-learning-introduction","title":"Transfer Learning - Introduction","type":"WikiPage","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-transfer-learning-intro\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-transfer-learning-intro/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eIn this section you'll learn all about transfer learning and how it could be specifically applied to convolutional neural networks. There are also other applications of transfer learning like NLP.\u003c/p\u003e\n\n\u003ch2\u003eConvolutional Neural Networks (Continued)\u003c/h2\u003e\n\n\u003cp\u003eIn an earlier section, you learned about the fundamentals of convolutional neural networks and how to use them. In this section, you'll deepen your CNN knowledge and learn about concepts that will allow you to reuse pretrained models from other image recognition tasks. This will help you solve problems where only limited data is available.\u003c/p\u003e\n\n\u003ch3\u003eUsing Pretrained Networks\u003c/h3\u003e\n\n\u003cp\u003eYou will learn about the concept of \"convolutional bases\" and why they are useful. The use of a convolutional base, or a \"pretrained network\" has the advantage that hierarchical features that already have been \"pre-learned\" by this network can act as a generic model. Because of that reason, these networks can be used for a wide variety of computer vision tasks, even if your new problem involves completely different classes of images. You'll learn about the pretrained networks that are available in Keras, the use of pretrained networks through feature extraction (meaning that you run your new data through the pretrained network and training a new classifier on top of the pretrained network), and the use of pretrained networks through finetuning.\u003c/p\u003e\n\n\u003ch3\u003eImage Classification\u003c/h3\u003e\n\n\u003cp\u003eAt the end of this section, you'll work through a lab that combines the knowledge you gained in this section and the previous one. You'll work on a dog breed classification problem, a dataset used in a Kaggle competition, and build both a convolutional neural network from scratch, and a CNN using a pretrained network.\u003c/p\u003e\n\n\u003ch2\u003eSummary\u003c/h2\u003e\n\n\u003cp\u003eIn this section, you'll extend your deep learning knowledge by learning about transfer learning. \u003c/p\u003e\n\u003cfooter class=\"fis-footer\" style=\"visibility: hidden;\"\u003e\u003cdiv class=\"fis-feedback\"\u003e\n\u003ch5\u003eHow do you feel about this lesson?\u003c/h5\u003e\n\u003cimg id=\"thumbs-up\" data-repository=\"dsc-transfer-learning-intro\" title=\"Thumbs up!\" alt=\"thumbs up\"\u003e\u003cimg id=\"thumbs-down\" data-repository=\"dsc-transfer-learning-intro\" title=\"Thumbs down!\" alt=\"thumbs down\"\u003e\n\u003c/div\u003e\n\u003ch5\u003eHave specific feedback? \u003ca href=\"https://github.com/learn-co-curriculum/dsc-transfer-learning-intro/issues/new\"\u003eTell us here!\u003c/a\u003e\n\u003c/h5\u003e\u003c/footer\u003e","frontPage":false},{"exportId":"the-aws-ecosystem","title":"The AWS Ecosystem","type":"WikiPage","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-the-aws-ecosystem\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-the-aws-ecosystem/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eIn this lesson, we'll get set up to use \u003cstrong\u003e\u003cem\u003eAmazon Web Services\u003c/em\u003e\u003c/strong\u003e, and then get to know our way around the platform before digging into AWS SageMaker in the next lesson. \u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-the-aws-ecosystem/master/images/awscloud.svg\"\u003e\u003c/p\u003e\n\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\n\u003cul\u003e\n\u003cli\u003eSet up an AWS account and explore the Amazon Resource Center \u003c/li\u003e\n\u003cli\u003eExplain what the \"regions\" are in AWS and why it is important to choose the right one \u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch2\u003eGetting Started\u003c/h2\u003e\n\n\u003cp\u003eBefore we can begin exploring everything AWS has to offer, we'll need to create an account on the platform. To do this, start by following this link to \u003ca href=\"https://aws.amazon.com/\"\u003eAmazon Web Services\u003c/a\u003e. While you're there, you may want to take the time to bookmark it -- chances are this is a website you'll use frequently in your career as a Data Scientist!\u003c/p\u003e\n\n\u003ch3\u003eWill This Cost Money?\u003c/h3\u003e\n\n\u003cp\u003eAlthough you will need a credit card to register for AWS, working through this section will not cost any money. AWS provides a free tier for learning and prototyping on the platform -- this is the tier we'll use for everything going forward. As long as you correctly register for the free tier, this will not cost you any money. \u003c/p\u003e\n\n\u003ch3\u003eRegister Your Email\u003c/h3\u003e\n\n\u003cp\u003eBegin by clicking the \"Sign Up\" button in the top right-hand corner of the page. \u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-the-aws-ecosystem/master/images/aws-1.png\"\u003e\u003c/p\u003e\n\n\u003cp\u003eNext, create an account by adding your email and password. You'll also need to set an \u003cstrong\u003e\u003cem\u003eAWS Account Name\u003c/em\u003e\u003c/strong\u003e. \u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-the-aws-ecosystem/master/images/aws-2.png\"\u003e\u003c/p\u003e\n\n\u003cp\u003eOn the next screen, enter your contact information. \u003cstrong\u003e\u003cem\u003eMake sure you set your account type to 'Personal'!\u003c/em\u003e\u003c/strong\u003e \u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-the-aws-ecosystem/master/images/aws-3.png\"\u003e\u003c/p\u003e\n\n\u003cp\u003eThis next page is especially important -- be sure to select the \u003cstrong\u003e\u003cem\u003eBasic Plan\u003c/em\u003e\u003c/strong\u003e! As a reminder, you will be asked to enter a credit card number during the next few steps. Although we will only be making use of the free tier of services for AWS, be aware that you will still need to enter a credit card number in order to complete the registration process. \u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-the-aws-ecosystem/master/images/aws-4.png\"\u003e\u003c/p\u003e\n\n\u003cp\u003eNow that you're all signed up, click the \"Sign in to the Console\" button to actually enter the AWS Console. \u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-the-aws-ecosystem/master/images/aws-5.png\"\u003e\u003c/p\u003e\n\n\u003cp\u003eAlright, you've now created an AWS Account! Let's take a look around. \u003c/p\u003e\n\n\u003ch2\u003eThe AWS Console\u003c/h2\u003e\n\n\u003cp\u003eNow that you're signed in, you'll see the \u003cstrong\u003e\u003cem\u003eAWS Console\u003c/em\u003e\u003c/strong\u003e. This is your \"home screen\" for AWS -- it allows you to quickly navigate through the thousands of services offered on AWS to find what you need. The easiest way to find what you need is the \"Find Services\" search bar at the top of the body of the page. \u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-the-aws-ecosystem/master/images/aws-6.png\"\u003e\u003c/p\u003e\n\n\u003cp\u003eYou can also click the \"See All Services\" dropdown to see a full list of services you can use in AWS. There are \u003cstrong\u003ea ton\u003c/strong\u003e of services, but don't let yourself get overwhelmed -- you'll probably never end up using the vast majority of these, as only a few apply to the work of a data scientist. \u003c/p\u003e\n\n\u003ch2\u003eUse Cases for Data Scientists\u003c/h2\u003e\n\n\u003cp\u003eWe've now created an account for AWS, so that we can take advantage of the Cloud. As data scientists, we'll find that a cloud computing service like AWS is very helpful in a number of ways. Aside from productionizing the model as a whole, the most important thing the cloud enables data scientists to do is to train much, much larger models by distributing training across entire clusters of servers. Without cloud computing, it would be impossible to train some of the larger deep learning models that exist today. The ability to distribute training of a neural network across a GPU allowed AI researchers to create massive models in a reasonable amount of time by creating a server cluster full of hundreds of GPUs. While this works, building a server like this is cost prohibitive to all but major companies and universities. Thankfully, services like AWS allow us to rent time on these servers per minute, making distributed training available for anybody at extremely cheap prices, paying only for what we use. AWS provides other great uses for data scientists beyond speedy training times -- it also plays a major part with databases. In your job as a data scientist, the databases you connect to in order to get your data will almost certainly be stored on AWS, or a competitor cloud platform. AWS servers also allow for companies to make use of big data frameworks such as Hadoop or Spark across a cluster of servers. \u003c/p\u003e\n\n\u003ch2\u003eUsing the Amazon Resource Center\u003c/h2\u003e\n\n\u003cp\u003eAs platforms go, you won't find many with more options than AWS. It has an amazing amount of offerings, with more getting added all the time. While AWS is great for basic use cases like hosting a server or a website, it also has all kinds of different offerings in areas such as Databases, Machine Learning, Data Analytics and other areas useful to Data Scientists. It's not possible for us to cover how to use every service in AWS in this section -- but luckily, we don't need to, because Amazon already has! The \u003ca href=\"https://aws.amazon.com/getting-started/\"\u003eGetting Started Resource Center\u003c/a\u003e contains a ton of awesome tutorials, demonstrations, and sample projects for just about everything you would ever want to know about any service on AWS. We \u003cstrong\u003e\u003cem\u003estrongly recommend\u003c/em\u003e\u003c/strong\u003e bookmarking this page, as the tutorials they offer are very high quality, and free!\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-the-aws-ecosystem/master/images/aws-7.png\"\u003e\u003c/p\u003e\n\n\u003ch2\u003eA Note On Regions\u003c/h2\u003e\n\n\u003cp\u003eBefore we move onto digging into \u003cstrong\u003e\u003cem\u003eAWS SageMaker\u003c/em\u003e\u003c/strong\u003e in the next lesson, it's worth taking a moment to explain \"Regions\" and what they have to do with AWS. AWS has data centers all over the world, and they are \u003cstrong\u003enot\u003c/strong\u003e interchangeable when it comes to your projects. Click on the \"Region\" tab in the top right corner of the navigation bar, and you should see a dropdown of all the different data centers you can choose from. It is \u003cstrong\u003e\u003cem\u003every important\u003c/em\u003e\u003c/strong\u003e that you always choose the same region to connect to with your projects. Each region is its own unique data center, and anything you do on in that region is only in that region. One of the most common mistakes newcomers to AWS make is thinking they've lost their project because they are connected to a different data center and don't realize it. We'll remind you of this again later, but it can't hurt to say it twice: always make sure you're connected to the correct data center! This goes doubly for when you're creating a new project. \u003c/p\u003e\n\n\u003ch2\u003eSummary\u003c/h2\u003e\n\n\u003cp\u003eIn this lesson, we signed up for Amazon Web Services and explored some of the different options on the platform. We also learned about where AWS fits in the data science process. \u003c/p\u003e\n\u003cfooter class=\"fis-footer\" style=\"visibility: hidden;\"\u003e\u003cdiv class=\"fis-feedback\"\u003e\n\u003ch5\u003eHow do you feel about this lesson?\u003c/h5\u003e\n\u003cimg id=\"thumbs-up\" data-repository=\"dsc-the-aws-ecosystem\" title=\"Thumbs up!\" alt=\"thumbs up\"\u003e\u003cimg id=\"thumbs-down\" data-repository=\"dsc-the-aws-ecosystem\" title=\"Thumbs down!\" alt=\"thumbs down\"\u003e\n\u003c/div\u003e\n\u003ch5\u003eHave specific feedback? \u003ca href=\"https://github.com/learn-co-curriculum/dsc-the-aws-ecosystem/issues/new\"\u003eTell us here!\u003c/a\u003e\n\u003c/h5\u003e\u003c/footer\u003e","frontPage":false},{"exportId":"modeling-time-series-data-introduction","title":"Modeling Time Series Data - Introduction","type":"WikiPage","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-time-series-models-introduction\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-time-series-models-introduction/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eIn this section, you'll learn about modeling for time series data. \u003c/p\u003e\n\n\u003ch2\u003eTime Series Modeling\u003c/h2\u003e\n\n\u003cp\u003eIn the previous section, we introduced the idea of time series data and provided some best practices for importing, managing, and visualizing time series data along with a number of techniques for removing trends and/or seasonality from a time series dataset. In this section, we're going to look at various types of models for time series data.\u003c/p\u003e\n\n\u003ch3\u003eBasic Time Series Models\u003c/h3\u003e\n\n\u003cp\u003eWe start off by introducing two basic time series models -- the white noise and random walk models.\u003c/p\u003e\n\n\u003ch3\u003eCorrelation, Autocorrelation, and Partial Autocorrelation\u003c/h3\u003e\n\n\u003cp\u003eWe will then move on to the concept of correlation as it relates to time series datasets, and plot the Autocorrelation Function (ACF) and Partial Autocorrelation Function (PACF) for a time series. \u003c/p\u003e\n\n\u003ch3\u003eARMA Models\u003c/h3\u003e\n\n\u003cp\u003eWe then move on to introduce two other key time series models that are widely used for predicting future values for time series data - the auto regressive (AR) and moving average (MA) models.\u003c/p\u003e\n\n\u003ch2\u003eSummary\u003c/h2\u003e\n\n\u003cp\u003eLet's get started! This section wraps up our introduction to time series analysis, giving you the modeling tools required to effectively forecast time series data.\u003c/p\u003e\n\u003cfooter class=\"fis-footer\" style=\"visibility: hidden;\"\u003e\u003cdiv class=\"fis-feedback\"\u003e\n\u003ch5\u003eHow do you feel about this lesson?\u003c/h5\u003e\n\u003cimg id=\"thumbs-up\" data-repository=\"dsc-time-series-models-introduction\" title=\"Thumbs up!\" alt=\"thumbs up\"\u003e\u003cimg id=\"thumbs-down\" data-repository=\"dsc-time-series-models-introduction\" title=\"Thumbs down!\" alt=\"thumbs down\"\u003e\n\u003c/div\u003e\n\u003ch5\u003eHave specific feedback? \u003ca href=\"https://github.com/learn-co-curriculum/dsc-time-series-models-introduction/issues/new\"\u003eTell us here!\u003c/a\u003e\n\u003c/h5\u003e\u003c/footer\u003e","frontPage":false},{"exportId":"amazon-web-services-recap","title":"Amazon Web Services - Recap","type":"WikiPage","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-productionizing-machine-learning-models-section-recap\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-productionizing-machine-learning-models-section-recap/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\u003ch2\u003eKey Takeaways\u003c/h2\u003e\n\n\u003cp\u003eThe key takeaways from this section include:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eAWS is a \u003cstrong\u003e\u003cem\u003eCloud-Computing Platform\u003c/em\u003e\u003c/strong\u003e which we can use for a variety of use cases in data science.\u003c/li\u003e\n\u003cli\u003eIn this section, we learned about how to sign up for AWS, and how to make sure that we have the right region selected when working in AWS.\u003c/li\u003e\n\u003cli\u003eAmazon has centralized all of the major data science services inside \u003cstrong\u003e\u003cem\u003eAmazon SageMaker\u003c/em\u003e\u003c/strong\u003e. SageMaker provides numerous services for things such as:\n\n\u003cul\u003e\n\u003cli\u003eData Labeling\u003c/li\u003e\n\u003cli\u003eCloud-based Notebooks\u003c/li\u003e\n\u003cli\u003eTraining and Model Tuning\u003c/li\u003e\n\u003cli\u003eInference\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eWe can set up our own models, or use the preexisting models provided by AWS. Similarly, we can set up our own inference endpoints, or make use of preexisting endpoints created by AWS. \u003c/li\u003e\n\u003cli\u003eCreating our own endpoint requires us to use a Docker instance, as we saw in the previous codealong. Much of the work required to create an endpoint for our own model is boilerplate, and we can use it again and again across multiple projects. \u003c/li\u003e\n\u003c/ul\u003e\n\u003cfooter class=\"fis-footer\" style=\"visibility: hidden;\"\u003e\u003cdiv class=\"fis-feedback\"\u003e\n\u003ch5\u003eHow do you feel about this lesson?\u003c/h5\u003e\n\u003cimg id=\"thumbs-up\" data-repository=\"dsc-productionizing-machine-learning-models-section-recap\" title=\"Thumbs up!\" alt=\"thumbs up\"\u003e\u003cimg id=\"thumbs-down\" data-repository=\"dsc-productionizing-machine-learning-models-section-recap\" title=\"Thumbs down!\" alt=\"thumbs down\"\u003e\n\u003c/div\u003e\n\u003ch5\u003eHave specific feedback? \u003ca href=\"https://github.com/learn-co-curriculum/dsc-productionizing-machine-learning-models-section-recap/issues/new\"\u003eTell us here!\u003c/a\u003e\n\u003c/h5\u003e\u003c/footer\u003e","frontPage":false},{"exportId":"hierarchical-agglomerative-clustering","title":"Hierarchical Agglomerative Clustering","type":"WikiPage","content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-hierarchical-agglomerative-clustering\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-hierarchical-agglomerative-clustering\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-hierarchical-agglomerative-clustering/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eIn this lesson, we'll learn about another popular class of clustering algorithms -- hierarchical agglomerative clustering!\u003c/p\u003e\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\u003cp\u003eYou will be able to:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eExplain the process behind hierarchical agglomerative clustering\u003c/li\u003e\n\u003cli\u003eDescribe the three different linkage criteria for hierarchical agglomerative clustering\u003c/li\u003e\n\u003cli\u003eDefine the purpose of a dendrogram\u003c/li\u003e\n\u003cli\u003eCompare and contrast k-means and hierarchical agglomerative clustering methodologies\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003eUnderstanding Hierarchical Clustering\u003c/h2\u003e\n\u003cp\u003eSo far, we've worked with a non-hierarchical clustering algorithm, k-means clustering. K-means works by taking a set parameter that tells it how many clusters we think exist in the data, and then uses the Expectation-Maximization (EM) algorithm to iteratively shift each cluster centroid to the best possible position by constantly calculating and recalculating the centroid's position by assigning each point to the cluster centroid they are closest to with each new step, and then moving the centroid to the center of all the points currently assigned to that centroid. With non-hierarchical algorithms, there can be no subgroups -- that is, no clusters within clusters.\u003c/p\u003e\n\u003cp\u003eThis is where agglomerative clustering algorithms come in. In agglomerative clustering, the algorithm starts with \u003cimg src=\"https://render.githubusercontent.com/render/math?math=n\"\u003e clusters (where \u003cimg src=\"https://render.githubusercontent.com/render/math?math=n\"\u003e is the number of data points) and proceeds by merging the most similar clusters, until some stopping criterion. in \u003ccode\u003escikit-learn\u003c/code\u003e, the stopping criterion that is implemented is \"number of clusters\". If left alone, the algorithm will work until it has merged every cluster into one giant cluster. We can also set the limit, if we want, to stop when there are only [x] clusters remaining.\u003c/p\u003e\n\u003ch3\u003eLinking Similar Clusters Together\u003c/h3\u003e\n\u003cp\u003eSeveral linkage criteria that have different definitions for \"most similar clusters\" can be used. The measure is always defined between two existing clusters up until that point, so the later in the algorithms, the bigger the clusters get.\u003c/p\u003e\n\u003cp\u003eScikit-learn provides three linkage criteria:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eward\u003c/strong\u003e (default): picks the two clusters to merge in a way that the variance within all clusters increases the least. Generally, this leads to clusters that are fairly equally sized.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eaverage\u003c/strong\u003e: merges the two clusters that have the smallest \u003cstrong\u003e\u003cem\u003eaverage\u003c/em\u003e\u003c/strong\u003e distance between all the points.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003ecomplete\u003c/strong\u003e (or maximum linkage): merges the two clusters that have the smallest \u003cstrong\u003e\u003cem\u003emaximum\u003c/em\u003e\u003c/strong\u003e distance between their points.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eAs we'll see in the next lab, these linkage criteria can definitely have an effect on how the clustering algorithm performs. As always seems to be the case, no one of these is \"best\" -- which one you should use often depends on the structure of your data, and/or your own goals.\u003c/p\u003e\n\u003ch3\u003eA Visual Example\u003c/h3\u003e\n\u003cp\u003eIt's often easier to understand what the HAC algorithm is doing when we look at the decisions it makes at each given step. The following diagram demonstrates the clusters created at each step for a dataset of 16 points. Take a look at the diagram and see if you can figure out what the algorithm is doing at each step as it merges clusters together:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-hierarchical-agglomerative-clustering/master/images/new_hac_iterative.png\"\u003e\u003c/p\u003e\n\u003cp\u003eAs we can see from the diagram above, in each step, the algorithm takes the two clusters that are closest together (and remember, we define \"closest together\" according to whichever linkage criteria we choose to use), and then \u003cstrong\u003e\u003cem\u003emerge\u003c/em\u003e\u003c/strong\u003e those two clusters together into a single cluster. We don't move the data points or anything like that -- we just consider them as a single unit, as opposed to two separate ones. This works at every stage because in the beginning, we treat each data point as a unique cluster.\u003c/p\u003e\n\u003cp\u003eThis becomes very intuitive when we look at the following gif -- pay attention to the image on the left:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-hierarchical-agglomerative-clustering/master/images/dendrogram_gif.gif\"\u003e\u003c/p\u003e\n\u003cp\u003eAs the dots disappear, the visualization is replacing them with the newly calculated center of that cluster, which will be used for linkage purposes. Now, let's end this lesson by talking about visualizations we can use to interpret results!\u003c/p\u003e\n\u003ch3\u003eDendrograms and Clustergrams\u003c/h3\u003e\n\u003cp\u003eOne advantage of HAC is that we can easily visualize the results \u003cstrong\u003e\u003cem\u003eat any given step\u003c/em\u003e\u003c/strong\u003e using visualizations such as \u003cstrong\u003e\u003cem\u003eDendrograms\u003c/em\u003e\u003c/strong\u003e and \u003cstrong\u003e\u003cem\u003eClustergrams\u003c/em\u003e\u003c/strong\u003e. Take another look at the gif above, but this time, pay attention to the image on the right. This is a \u003cem\u003edendrogram,\u003c/em\u003e which is used to visualize the hierarchical relationship between the various clusters that are computed throughout each step. Dendrograms are very useful to decide how clusters change depending on the euclidian distance. If you decide that your intra-cluster euclidian distance should be smaller than 3, you can draw a horizontal line at euclidian distance 3, and define which points belong to which cluster by looking at the dendrogram. For the gif above, this means that there are three clusters: cluster one contains \u003cimg src=\"https://render.githubusercontent.com/render/math?math=p_0\"\u003e , \u003cimg src=\"https://render.githubusercontent.com/render/math?math=p_1\"\u003e and \u003cimg src=\"https://render.githubusercontent.com/render/math?math=p_2\"\u003e , cluster two contains \u003cimg src=\"https://render.githubusercontent.com/render/math?math=p_3\"\u003e , cluster three contains \u003cimg src=\"https://render.githubusercontent.com/render/math?math=p_4\"\u003e , \u003cimg src=\"https://render.githubusercontent.com/render/math?math=p_5\"\u003e and \u003cimg src=\"https://render.githubusercontent.com/render/math?math=p_6\"\u003e .\u003c/p\u003e\n\u003cp\u003eWe can also visualize the same information by drawing lines representing each cluster at each step to create a \u003cem\u003eclustergram\u003c/em\u003e. Take a look at the following diagram below, which shows both a dendrogram and clustergram of the same HAC results:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-hierarchical-agglomerative-clustering/master/images/new_clustergram.png\" width=\"600\"\u003e\u003c/p\u003e\n\u003ch3\u003eHow is HAC used?\u003c/h3\u003e\n\u003cp\u003eHAC algorithms are used in generally the same way that K-means and other clustering algorithms are used: for tasks such as market segmentation, or for gaining a deeper understanding of a dataset through cluster analysis. However, there are special cases of things that fit quite well in a hierarchical agglomerative structure -- one of the most common use cases you'll see for HAC is the way that smartphones naturally sort photos inside their photos app! Take a look at your photos app on your phone, and the albums that it creates for you -- you'll likely see that the albums are sorted in a \u003cstrong\u003e\u003cem\u003ehierarchical\u003c/em\u003e\u003c/strong\u003e fashion! Perhaps the phone chooses to group photos by date first, and then by location, or even content! In this way, these can be viewed as natural clusters within clusters, in a way that makes intuitive sense to users. When we browse, we likely want to see photos that were taken around the same time, and then at the same place, and then narrow it down to photos about the same things, to quickly browse and find what we're looking for. This is a great example of HAC being used in the wild!\u003c/p\u003e\n\u003ch2\u003eSummary\u003c/h2\u003e\n\u003cp\u003eIn this lesson, we learned about how the HAC algorithm derives its clusters, including different linkage criteria that can be used to determine which clusters should be merged at any given point. We also examined some visualizations of HAC algorithms, in the forms of dendrograms and clustergrams!\u003c/p\u003e","frontPage":false},{"exportId":"deep-neural-networks-recap","title":"Deep Neural Networks - Recap","type":"WikiPage","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-deep-learning-section-recap\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-deep-learning-section-recap/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\u003ch2\u003eKey Takeaways\u003c/h2\u003e\n\n\u003cp\u003eThe key takeaways from this section include:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eDeep neural network representations can lighten the burden and automate certain tasks of heavy data preprocessing\u003c/li\u003e\n\u003cli\u003eDeep representations need exponentially fewer hidden units than shallow networks, to obtain the same performance\u003c/li\u003e\n\u003cli\u003eParameter initialization, forward propagation, cost function evaluation, and backward propagation are again the cornerstones of deep networks\u003c/li\u003e\n\u003cli\u003eTensors are the building blocks of neural networks and a good understanding of them and how to use them in Python is crucial\u003c/li\u003e\n\u003cli\u003eScalars can be seen as 0-D tensors. Vectors can be seen as 1-D tensors, and matrices as 2-D tensors\u003c/li\u003e\n\u003cli\u003eThe usage of tensors reaches beyond matrices: tensors can have N dimensions\u003c/li\u003e\n\u003cli\u003eTensors can be created and manipulated using Numpy\u003c/li\u003e\n\u003cli\u003eKeras makes building neural networks in Python easy, and you learned how to do that in this section\u003c/li\u003e\n\u003cli\u003eYou can use Keras to do some NLP as well, e.g. for tokenization \u003c/li\u003e\n\u003c/ul\u003e\n\u003cfooter class=\"fis-footer\" style=\"visibility: hidden;\"\u003e\u003cdiv class=\"fis-feedback\"\u003e\n\u003ch5\u003eHow do you feel about this lesson?\u003c/h5\u003e\n\u003cimg id=\"thumbs-up\" data-repository=\"dsc-deep-learning-section-recap\" title=\"Thumbs up!\" alt=\"thumbs up\"\u003e\u003cimg id=\"thumbs-down\" data-repository=\"dsc-deep-learning-section-recap\" title=\"Thumbs down!\" alt=\"thumbs down\"\u003e\n\u003c/div\u003e\n\u003ch5\u003eHave specific feedback? \u003ca href=\"https://github.com/learn-co-curriculum/dsc-deep-learning-section-recap/issues/new\"\u003eTell us here!\u003c/a\u003e\n\u003c/h5\u003e\u003c/footer\u003e","frontPage":false},{"exportId":"introduction-to-big-data","title":"Introduction to Big Data","type":"WikiPage","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-big-data-introduction\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-big-data-introduction/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eIn the information age, data in huge quantities has become available to analysts and decision-makers. Due to a vast increase in the amount of such data in recent times, a number of specialized platforms and development paradigms have been developed that can handle big data. Using such specialist approaches allows data scientists to gain valuable insights from complex data, ranging from daily transactions to customer interactions and social network data.\u003c/p\u003e\n\n\u003cp\u003eThis section aims to focus on some of the different analytical approaches and tools data scientists apply to big data in order to gain valuable insights that aid business decision making. \u003c/p\u003e\n\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\n\u003cp\u003eYou will be able to:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eList the domain areas where big data is particularly useful \u003c/li\u003e\n\u003cli\u003eList the technologies associated with big data \u003c/li\u003e\n\u003cli\u003eDescribe the 3 V's of big data and how they differentiate big data from routine data \u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch2\u003eWhat is Big Data\u003c/h2\u003e\n\n\u003cp\u003eThe topic of \"big data\" has received a lot of hype lately, accompanied by a huge amount of interest from big businesses as it can potentially provide them data-driven decision-making abilities. Big data is one of the most discussed topics in business today across industry sectors, although it was barely known a few years ago. This lesson will focus on what big data is, why it is important, and the benefits it brings. \u003c/p\u003e\n\n\u003cp\u003eBig data is no different than normal data that we have seen so far; it's only \"bigger.\" This changes the analytical landscape that must be used as the huge size increase of the data requires specialist tools, techniques, and platforms. It helps us solve new problems and find improved ways to find answers to old problems. \u003c/p\u003e\n\n\u003ch3\u003eDefining Big Data\u003c/h3\u003e\n\n\u003cp\u003eDespite all the hype around this topic, there is no clear consensus on how to define  \u003cstrong\u003ebig data\u003c/strong\u003e. The term often gets related to business analytics and data mining for identifying relationships and associations present in huge amounts of transaction data.  \u003c/p\u003e\n\n\u003cp\u003eIn the data science domain, big data usually refers to datasets that grow so large that they become awkward to work with using traditional database management systems and analytical approaches. They are datasets whose size is beyond the ability of commonly used software tools and storage systems to capture, store, manage, as well as process the data within a tolerable elapsed time.\u003c/p\u003e\n\n\u003ch4\u003eHow Big is \"Big\" Data?\u003c/h4\u003e\n\n\u003cp\u003eBig data sizes are constantly increasing, currently ranging from a few terabytes (TB) to many petabytes (PB) of data in a single dataset. Consequently, some of the difficulties related to big data include capturing, storing, searching, sharing, analyzing, and visualizing. Today, enterprises are exploring large volumes of highly detailed data to discover trends and pieces of information considered incapable of being captured before. \u003c/p\u003e\n\n\u003cp\u003eHere are some of the examples of big data:\n- Web traffic data: Data points such as number of page views, previous web page, user information, advertisement click-through rate, pages per visit, average visit duration\n- Text data: Emails, tweets, news reports, voice recordings, and text gathered from crawling the web can make massive datasets that are valuable to data scientists\n- Location and time data: GPS data helps Google determine which roads have higher traffic and which businesses will be busier at certain hours\n- Social network data: Using the information of relationships between users on Facebook, LinkedIn, Twitter, Reddit, and countless other websites and apps\n- Smart grid and sensor data: With the advent of the Internet of Things (IoT), more and more devices are able to record data at all times, making it possible to gather lots of data instantaneously\u003c/p\u003e\n\n\u003ch2\u003e3 V's of Big Data\u003c/h2\u003e\n\n\u003cp\u003eDoug Laney published a \u003ca href=\"https://blogs.gartner.com/doug-laney/files/2012/01/ad949-3D-Data-Management-Controlling-Data-Volume-Velocity-and-Variety.pdf\"\u003epaper\u003c/a\u003e on three defining characteristics of big data. Three main features characterize big data: volume, variety, and velocity, or the three Vs. The volume of the data is its size, and how enormous it is. Velocity refers to the rate with which data is changing, or how often it is created. Finally, variety includes the different formats and types of data, as well as the different kinds of uses and ways of analyzing the data:\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-big-data-introduction/master/./images/3_components.png\" width=\"500\"\u003e\u003c/p\u003e\n\n\u003cp\u003eLet's look a bit deeper into what these 3 V's refer to:\u003c/p\u003e\n\n\u003ch3\u003eVOLUME\u003c/h3\u003e\n\n\u003cp\u003eVolume refers to the \u003cstrong\u003eamount of data\u003c/strong\u003e generated through websites, portals, and online applications in a data-driven business. Especially for online retailers, volume encompasses the available data that are out there and need to be assessed for relevance. \u003c/p\u003e\n\n\u003cp\u003eConsider the following:\u003c/p\u003e\n\n\u003cp\u003eAs of 2019, Facebook has 2.32 billion users, Youtube: 1.9 billion users, WhatsApp: 1.6 billion users and Instagram: 1 billion users. Every day, these users contribute to billions of images, posts, videos, tweets, etc. You can now imagine the insanely large amount (or \u003cstrong\u003ev\u003c/strong\u003eolume) of data that is generated every minute around the world.\nData volume is the primary attribute of big data. Big data can be quantified by size in Terabytes (TBs) or Petabytes (PBs), as well as even the number of records, transactions, tables, or files. Additionally, one of the things that makes big data really big is that its coming from a greater variety of sources than ever before, including logs, clickstreams, and social media as we will see below. \u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-big-data-introduction/master/./images/rank_users.png\" width=\"650\"\u003e\u003c/p\u003e\n\n\u003ch3\u003eVELOCITY\u003c/h3\u003e\n\n\u003cp\u003eVelocity refers to the speed with which data is generated, and as internet speeds have increased and the number of users has increased, the velocity has also increased substantially.\u003c/p\u003e\n\n\u003cp\u003eThe following image created by \u003ca href=\"https://www.allaccess.com/merge/archive/29580/2019-this-is-what-happens-in-an-internet-minute\"\u003eLori Lewis and Chadd Callahan\u003c/a\u003e shows what happens on major social media platforms in one minute. \u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-big-data-introduction/master/./images/internet_minute.jpg\" width=\"500\"\u003e\u003c/p\u003e\n\n\u003cp\u003eVelocity is basically the frequency of data generation or the frequency of data delivery. The leading edge of\nbig data is streaming data, which is collected in real-time from the websites. \u003c/p\u003e\n\n\u003cp\u003eTools within the big data stack help companies hold this explosion in velocity, accept the incoming flow of data, and at the same time process it quickly enough so that it does not create bottlenecks.\u003c/p\u003e\n\n\u003ch3\u003eVARIETY\u003c/h3\u003e\n\n\u003cp\u003eVariety in big data refers to all the structured and unstructured data that has the possibility of getting generated either by humans or by machines. Structured data is whatever data you could store in a spreadsheet. It can easily be cataloged and summary statistics can be calculated for it. Unstructured data are raw things like texts, tweets, pictures, videos, emails, voice mails, hand-written text, ECG readings, and audio recordings. Humans can only make sense of data that is structured, and it is usually up to data scientists to create some organization and structure to unstructured data.\u003c/p\u003e\n\n\u003cp\u003eVariety is all about the ability to classify the incoming data into various categories and turn unstructured data into something with more structure.\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-big-data-introduction/master/./images/unstructured_data.png\" width=\"500\"\u003e\u003c/p\u003e\n\n\u003cp\u003eThis leads us to the most widely used definition in the industry by Gartner: \u003c/p\u003e\n\n\u003ch3\u003e\n\u003cem\u003eBig data is high-volume, high-velocity and/or high-variety information assets that demand cost-effective, innovative forms of information processing that enable enhanced insight, decision making, and process automation\u003c/em\u003e.\u003c/h3\u003e\n\n\u003cp\u003eAny data sources that fall under those 3 Vs are sources of big data, no matter how you define it.\u003c/p\u003e\n\n\u003cp\u003e\u003cstrong\u003eNOTE\u003c/strong\u003e: \u003cem\u003eSome researchers have discussed the addition of a fourth V, or \u003cstrong\u003eVeracity\u003c/strong\u003e. Veracity focuses on the quality of the data. This characterizes big data quality as good, bad, or undefined due to data inconsistency, incompleteness, ambiguity, latency, deception, and approximations\u003c/em\u003e\u003c/p\u003e\n\n\u003cp\u003eThe important thing to remember from this three-pronged definition of Big Data is that there is not a single component that makes data \"big\" or not. It is also a futile effort to try and make a definition of what the threshold is to make something \"big data\" rather than normal data. As technologies evolve and new distributed algorithms are created, what was once big data will no longer be big, and we will raise the bar.\u003c/p\u003e\n\n\u003ch2\u003eBig Data Analytics\u003c/h2\u003e\n\n\u003cp\u003eWith the evolution of technology and the increased amounts of data, as discussed above, the need for faster and more efficient ways of analyzing such data has also grown exponentially. Having big data \u003cstrong\u003ealone\u003c/strong\u003e is no longer enough to make efficient decisions at the right time. As we mentioned above, Big Data cannot be easily analyzed with traditional data management and analysis techniques and infrastructures. Therefore, there arises a need for new\ntools and methods specialized for big data analytics, as well as the required architectures for storing and managing such data. Accordingly, the emergence of big data has an effect on everything from the data itself to its collection, analysis, and visualization, as well as the final extracted decisions.\u003c/p\u003e\n\n\u003cp\u003eThe image below shows the technology stack, or the key tools and platforms heavily being employed in big data analytics today. \u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-big-data-introduction/master/./images/tech_stack.png\" width=\"800\"\u003e\u003c/p\u003e\n\n\u003cp\u003eExplaining each one of these tools/platforms etc. is outside the scope of this lesson. You are, however, encouraged to look up these technologies and see their role in big data analytics. Such a stack maps the different big data storage, management, analytics tools/methods, visualization, and evaluation tools to the different phases of the decision-making process. \u003c/p\u003e\n\n\u003cp\u003eThe key activities associated with big data analytics are reflected in four main areas: \u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eBig data warehousing and distribution\u003c/li\u003e\n\u003cli\u003eBig data storage\u003c/li\u003e\n\u003cli\u003eBig data computational platforms\u003c/li\u003e\n\u003cli\u003eBig data analyses, visualization, and evaluation\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003eSuch a framework can be applied for knowledge discovery and informed decision-making in big data-driven organizations.\u003c/p\u003e\n\n\u003ch3\u003eExample Business Applications of Big Data Analytics\u003c/h3\u003e\n\n\u003cp\u003eAlong with some of the most common advanced data analytics methods such as regression analysis, association rules, clustering, and classification, some additional analyses have become common with big data.\u003c/p\u003e\n\n\u003cp\u003eFor example, social media has recently become important for social networking and content sharing. Yet, the content that is generated from social media websites is enormous and remains largely unexploited. However, social media analytics can be used to analyze such data and extract useful information and predictions.\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-big-data-introduction/master/./images/social_media.png\" width=\"600\"\u003e\u003c/p\u003e\n\n\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003eSocial media analytics\u003c/strong\u003e is based on developing and evaluating informatics frameworks and tools in order to collect, monitor, summarize, analyze, as well as visualize social media data. \u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003cp\u003eSocial media analytics facilitates understanding the reactions and conversations between people in online communities, as well as extracting useful patterns and intelligence from their interactions and what they share on social media websites.\u003c/p\u003e\n\n\u003cp\u003eOn the other hand, \u003cstrong\u003etext mining\u003c/strong\u003e and \u003cstrong\u003eNLP\u003c/strong\u003e techniques are used to analyze a document or set of documents in order to understand the content within and the meaning of the information contained. Text mining has become very important nowadays since much of the information stored consists of text - in the form of emails, SMS texts, social media feeds, blogs, etc. While data mining deals with structured data, text presents special characteristics which basically follow a non-relational form and require wisely thought-out schemas to grant it more structure.\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-big-data-introduction/master/./images/nlp.png\" width=\"200\"\u003e\u003c/p\u003e\n\n\u003cp\u003e\u003cstrong\u003eSentiment analysis/opinion mining\u003c/strong\u003e is also becoming more and more important as online opinion data, such as blogs, product reviews, forums, and social data from social media sites, like Twitter and Facebook, grow tremendously. \u003c/p\u003e\n\n\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003eSentiment Analysis\u003c/strong\u003e focuses on analyzing and understanding emotions from subjective text patterns and is enabled through text mining. It identifies the opinions and attitudes of individuals towards certain topics, and it is useful in classifying viewpoints as positive or negative. \u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-big-data-introduction/master/./images/sentiment_2.png\" width=\"500\"\u003e\u003c/p\u003e\n\n\u003cp\u003eSentiment analysis uses NLP and text analytics in order to identify and extract information by finding words that are indicative of certain sentiments, as well as relationships between words so that sentiments can be accurately identified.\u003c/p\u003e\n\n\u003cp\u003eAnd finally, one of the leading applications in big data analytics is \u003cstrong\u003erecommendation systems\u003c/strong\u003e. Powerful recommendation engines can be built for anything from movies and videos to music, books, and products as offered by Netflix, Pandora, or Amazon. As customers of an online retailer browse through products, the Recommendation system offers recommendations of products they might be interested in. In our daily online browsing and shopping routine, most of us often come across messages like the one shown below. This is a recommendation system doing its job. \u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-big-data-introduction/master/./images/rec.png\" width=\"800\"\u003e\u003c/p\u003e\n\n\u003cp\u003eRecommendation systems have been immensely beneficial for both businesses and consumers. Big data is the driving force behind recommendation systems. A typical recommendation system cannot do its job without sufficient data and big data supplies plenty of user data such as past purchases, browsing history, and feedback for the recommendation systems to provide relevant and effective recommendations. In a nutshell, even the most advanced recommendations cannot be effective without big data.\u003c/p\u003e\n\n\u003ch3\u003eSo what's next?\u003c/h3\u003e\n\n\u003cp\u003eAfter this quick introduction, we will look at Map-Reduce, a distributed computation platform designed to incorporate big data analytics and how it is used by Hadoop/Apache Spark development environments to analyze big data. \u003c/p\u003e\n\n\u003ch2\u003eAdditional Reading\u003c/h2\u003e\n\n\u003cp\u003eBig data is a huge subject and incorporates a lot of underlying technologies and principles. You are advised to visit the following resources and read up on big data to develop a sound and holistic understanding of the domain. \u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003e\u003cp\u003e\u003ca href=\"https://www.youtube.com/watch?v=0cizsKDn3TI\"\u003eYoutube: Big Data Trap\u003c/a\u003e - Highly recommended, an excellent lecture on the social dimension of big data.\u003c/p\u003e\u003c/li\u003e\n\u003cli\u003e\u003cp\u003e\u003ca href=\"https://www.educba.com/big-data-vs-data-science/\"\u003eBig Data vs Data Science\u003c/a\u003e - How to relate big data analytics to routine analytics that we have so far!\u003c/p\u003e\u003c/li\u003e\n\u003cli\u003e\u003cp\u003e\u003ca href=\"https://pdfs.semanticscholar.org/d392/0f02dbb15da19b04d782fc0546ef113e0bf7.pdf\"\u003eBig Data Analytics\u003c/a\u003e - A great paper summarizing big-data-related terms, ideas, etc. \u003c/p\u003e\u003c/li\u003e\n\u003cli\u003e\u003cp\u003e\u003ca href=\"https://www.ntnu.no/iie/fag/big/lessons/lesson2.pdf\"\u003eIntroduction to Big Data\u003c/a\u003e - A paper discussing the basics of big data\u003c/p\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch2\u003eSummary\u003c/h2\u003e\n\n\u003cp\u003eIn this introductory lesson on big data, we looked at what data qualifies at \"big data\". We looked at how it is hard to come up with a standard definition of big data due to the variety of its applications and use cases. Up next, we will get into how we actually make parallelizable applications that are efficient with big data. \u003c/p\u003e\n\u003cfooter class=\"fis-footer\" style=\"visibility: hidden;\"\u003e\u003cdiv class=\"fis-feedback\"\u003e\n\u003ch5\u003eHow do you feel about this lesson?\u003c/h5\u003e\n\u003cimg id=\"thumbs-up\" data-repository=\"dsc-big-data-introduction\" title=\"Thumbs up!\" alt=\"thumbs up\"\u003e\u003cimg id=\"thumbs-down\" data-repository=\"dsc-big-data-introduction\" title=\"Thumbs down!\" alt=\"thumbs down\"\u003e\n\u003c/div\u003e\n\u003ch5\u003eHave specific feedback? \u003ca href=\"https://github.com/learn-co-curriculum/dsc-big-data-introduction/issues/new\"\u003eTell us here!\u003c/a\u003e\n\u003c/h5\u003e\u003c/footer\u003e","frontPage":false},{"exportId":"tuning-neural-networks-recap","title":"Tuning Neural Networks - Recap","type":"WikiPage","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-tuning-neural-networks-recap\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-tuning-neural-networks-recap/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\u003ch2\u003eKey Takeaways\u003c/h2\u003e\n\n\u003cp\u003eThe key takeaways from this section include: \u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eValidation and test sets are used when iteratively building deep neural networks\u003c/li\u003e\n\u003cli\u003eLike traditional machine learning models, we need to watch out for the bias variance trade-off when building deep learning models\u003c/li\u003e\n\u003cli\u003eSeveral regularization techniques can help us limit overfitting: L1 Regularization, L2 Regularization, Dropout Regularization, etc ...\u003c/li\u003e\n\u003cli\u003eTraining of deep neural networks can be sped up by using normalized inputs\u003c/li\u003e\n\u003cli\u003eNormalized inputs can also help mitigate a common issue of vanishing or exploding gradients \u003c/li\u003e\n\u003cli\u003eExamples of alternatives for gradient descent are: RMSprop, Adam, Gradient Descent with Momentum, etc. \u003c/li\u003e\n\u003cli\u003eHyperparameter tuning is of crucial importance when working with deep learning models, as setting the parameters right can lead to great improvements in model performance \u003c/li\u003e\n\u003c/ul\u003e\n\u003cfooter class=\"fis-footer\" style=\"visibility: hidden;\"\u003e\u003cdiv class=\"fis-feedback\"\u003e\n\u003ch5\u003eHow do you feel about this lesson?\u003c/h5\u003e\n\u003cimg id=\"thumbs-up\" data-repository=\"dsc-tuning-neural-networks-recap\" title=\"Thumbs up!\" alt=\"thumbs up\"\u003e\u003cimg id=\"thumbs-down\" data-repository=\"dsc-tuning-neural-networks-recap\" title=\"Thumbs down!\" alt=\"thumbs down\"\u003e\n\u003c/div\u003e\n\u003ch5\u003eHave specific feedback? \u003ca href=\"https://github.com/learn-co-curriculum/dsc-tuning-neural-networks-recap/issues/new\"\u003eTell us here!\u003c/a\u003e\n\u003c/h5\u003e\u003c/footer\u003e","frontPage":false},{"exportId":"transfer-learning-recap","title":"Transfer Learning - Recap","type":"WikiPage","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-transfer-learning-recap\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-transfer-learning-recap/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eIn this section, you learned how you can adapt pretrained models to improve the performance of neural networks when limited training data is available. While you specifically investigated CNNs and the VGG-19 model, these concepts are also applicable to other domains as well. For example, GloVe (Global Vectors for Word Representation) is a pretrained model that can be useful in a variety of natural language processing tasks.\u003c/p\u003e\n\n\u003cp\u003eRemember that the general process for transfer learning begins by taking a pretrained model like VGG-19 and freezing the weights so that they remain constant. From there, you can then append a standard densely connected classifier to perform the task at hand. In essence, the pretrained model acts as a form of feature engineering applied to the underlying dataset. \u003c/p\u003e\n\n\u003cp\u003eAfter the classifier is trained with the frozen pretrained model, a few of the top layers from the pretrained model can be unfrozen for fine tuning. Remember that you should only do this after training the classifier on top of the fully frozen model. Unfreezing parts of the pretrained model earlier is prone to overwriting any useful feature weights encoded in the pretrained model as there will be large gradients in forward and backward propagation passes until the densely connected layers converge to a stable solution. Also, remember that little is to be gained by unfreezing more than a few of the top layers from a pretrained model. Base layers of models such as VGG-19 will pick up very granular features such as colors or edges in image recognition. As such, these base layers are typically well formulated features across many domains. Unfreezing top layers has far more impact on tuning as these final layers often pick up domain specific features, so when adopting a model to a new problem domain such as predicting flower species instead of predicting animal kingdoms, these top layers can often be more impactful if retrained to the specific application.\u003c/p\u003e\n\n\u003ch2\u003eSummary\u003c/h2\u003e\n\n\u003cp\u003eIn this section, you got an overview of transfer learning and how to adapt pretrained models. From here, you'll continue to learn about other neural network architectures and build upon your growing deep learning knowledge.\u003c/p\u003e\n\u003cfooter class=\"fis-footer\" style=\"visibility: hidden;\"\u003e\u003cdiv class=\"fis-feedback\"\u003e\n\u003ch5\u003eHow do you feel about this lesson?\u003c/h5\u003e\n\u003cimg id=\"thumbs-up\" data-repository=\"dsc-transfer-learning-recap\" title=\"Thumbs up!\" alt=\"thumbs up\"\u003e\u003cimg id=\"thumbs-down\" data-repository=\"dsc-transfer-learning-recap\" title=\"Thumbs down!\" alt=\"thumbs down\"\u003e\n\u003c/div\u003e\n\u003ch5\u003eHave specific feedback? \u003ca href=\"https://github.com/learn-co-curriculum/dsc-transfer-learning-recap/issues/new\"\u003eTell us here!\u003c/a\u003e\n\u003c/h5\u003e\u003c/footer\u003e","frontPage":false},{"exportId":"graph-theory-introduction","title":"Graph Theory - Introduction","type":"WikiPage","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-network-introduction\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-network-introduction/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eIn this section, you'll investigate a new data structure: networks! Networks are a useful data structure to map a range of applications from driving directions to social networks.\u003c/p\u003e\n\n\u003ch2\u003eNetwork Graphs\u003c/h2\u003e\n\n\u003cp\u003eNetworks are another way of representing data that you have yet to fully investigate. In their most simple case, a network contains \u003cstrong\u003enodes\u003c/strong\u003e connected by \u003cstrong\u003eedges\u003c/strong\u003e like this:\n\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-network-introduction/master/images/graph.png\"\u003e\u003c/p\u003e\n\n\u003cp\u003eNodes represent some object such as people, languages, countries, or tags, to name a few. The relationships between these objects are the edges between them. For example, later in this section you'll investigate the relationship of various technology tags on the popular website \u003ca href=\"stackoverflow.com\"\u003eStackOverflow\u003c/a\u003e. One potential network visualization of this data looks like this:\n\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-network-introduction/master/images/stackoverflow_clusters.png\"\u003e\u003c/p\u003e\n\n\u003ch2\u003ePath Searching\u003c/h2\u003e\n\n\u003cp\u003eAn important concept in network analysis are path searching algorithms. Finding the shortest path between two nodes is a foundational concept for creating a distance metric which can then be used to conduct more advanced analyses. Mapping applications such as Google Maps, Apple Maps, Waze, or Uber are also natural applications for path searching algorithms. In this section, you'll investigate Dijkstra's algorithm for finding the shortest path between two points, coding it from scratch using Python.\u003c/p\u003e\n\n\u003ch2\u003eCentrality\u003c/h2\u003e\n\n\u003cp\u003eOnce you've familiar with the concept of path searching, you'll then go on to investigate properties of nodes and edges. Centrality is a key concept in this, helping to determine which nodes are most influential in a network, or hold pivotal positions in connecting the network.\u003c/p\u003e\n\n\u003ch2\u003eCliques and Clustering\u003c/h2\u003e\n\n\u003cp\u003eMoving from the study of single objects nodes and edges within the network, you'll then start to investigate larger structures. With this, you'll investigate the concept of cliques and clusters in order to subdivide a network into smaller groups. Natural applications of this include sub-setting social networks into groups or categorizing items such as books or languages.\u003c/p\u003e\n\n\u003ch2\u003eRecommendation Systems\u003c/h2\u003e\n\n\u003cp\u003eTo round out this section, you'll investigate how networks can be used to fuel recommendation systems, a popular and exciting topic. With this, you'll work on recommending amazon products to customers.\u003c/p\u003e\n\n\u003ch2\u003eSummary\u003c/h2\u003e\n\n\u003cp\u003eGet ready to dive into the exciting realm of networks! In this section, you'll get to play around with a range of datasets from Twitter, Game of Thrones, and the Amazon Marketplace!\u003c/p\u003e\n\u003cfooter class=\"fis-footer\" style=\"visibility: hidden;\"\u003e\u003cdiv class=\"fis-feedback\"\u003e\n\u003ch5\u003eHow do you feel about this lesson?\u003c/h5\u003e\n\u003cimg id=\"thumbs-up\" data-repository=\"dsc-network-introduction\" title=\"Thumbs up!\" alt=\"thumbs up\"\u003e\u003cimg id=\"thumbs-down\" data-repository=\"dsc-network-introduction\" title=\"Thumbs down!\" alt=\"thumbs down\"\u003e\n\u003c/div\u003e\n\u003ch5\u003eHave specific feedback? \u003ca href=\"https://github.com/learn-co-curriculum/dsc-network-introduction/issues/new\"\u003eTell us here!\u003c/a\u003e\n\u003c/h5\u003e\u003c/footer\u003e","frontPage":false},{"exportId":"parallel-and-distributed-computing-with-mapreduce","title":"Parallel and Distributed Computing with MapReduce","type":"WikiPage","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-parallel-and-distributed-computing-with-mapreduce\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-parallel-and-distributed-computing-with-mapreduce/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eMapReduce is a programming paradigm that enables the ability to scale across hundreds or thousands of servers for big data analytics. The underlying concept can be somewhat difficult to grasp, because this paradigm differs from the traditional programming practices. This lesson aims to present a simple yet intuitive account of MapReduce that we shall put into practice in upcoming labs.\u003c/p\u003e\n\u003cp\u003e\u003cem\u003eIn a nutshell, the term \"MapReduce\" refers to two distinct tasks. The first is the \u003cstrong\u003eMap\u003c/strong\u003e job, which takes one set of data and transforms it into another set of data, where individual elements are broken down into tuples \u003cstrong\u003e(key/value pairs)\u003c/strong\u003e, while the \u003cstrong\u003eReduce\u003c/strong\u003e job takes the output from a map as input and combines those data tuples into a smaller set of tuples.\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003eWe'll see this with help of some simple examples in this lesson.\u003c/p\u003e\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\u003cp\u003eYou will be able to:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eExplain how the MapReduce paradigm works and how it differs from traditional programming approaches\u003c/li\u003e\n\u003cli\u003eExplain what is meant by distributed and parallel processing\u003c/li\u003e\n\u003cli\u003eUse MapReduce with a simple word count example\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003eParallel and Distributed Processing\u003c/h2\u003e\n\u003cp\u003eThe MapReduce programming paradigm is designed to allow \u003cstrong\u003eparallel and distributed processing\u003c/strong\u003e of large sets of data (also known as big data). MapReduce allows us to convert such big datasets into sets of \u003cstrong\u003etuples\u003c/strong\u003e as \u003cstrong\u003ekey:value\u003c/strong\u003e pairs, as we'll see shortly. These pairs are analogous to the data structures we saw with dictionaries and JSON files etc. These tuples are \u003cstrong\u003emapped\u003c/strong\u003e and \u003cstrong\u003ereduced\u003c/strong\u003e in a computational environment to allow distributed execution of complex tasks on a group (cluster) of interconnected computers.\u003c/p\u003e\n\u003cp\u003eSo in simpler terms, \u003cem\u003eMapReduce uses parallel distributed computing to turn big data into regular data.\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003eLet's first see what we mean by parallel and distributed processing below.\u003c/p\u003e\n\u003ch3\u003eDistributed Processing Systems\u003c/h3\u003e\n\u003cblockquote\u003e\n\u003cp\u003eA distributed processing system is a group of computers in a network working in tandem to accomplish a task\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eWhen computers are in a distributed system, they do not share hard drive memory or processing memory; they communicate with one other through messages, which are transferred over a network. The individual computers in this network are referred to as \u003cstrong\u003enodes\u003c/strong\u003e. As you've seen before, computers can send requests as well as packets of data to one another.\u003c/p\u003e\n\u003cp\u003eThe two most common ways of organizing computers into a distributed system are the client-server system and peer-to-peer system.\u003c/p\u003e\n\u003cp\u003eThe client-server architecture has nodes that make requests to a central server. The server will then decide to accept or reject these requests and send additional methods out to the outer nodes.\u003c/p\u003e\n\u003cp\u003ePeer-to-peer systems allow nodes to communicate with one another directly without requiring approval from a server.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-parallel-and-distributed-computing-with-mapreduce/master/images/types_of_network.png\"\u003e\u003c/p\u003e\n\u003ch3\u003eParallel Processing Systems\u003c/h3\u003e\n\u003cp\u003eThese networks are useful for many applications all over the web, but they are generally ill-suited for dealing with the processing power required for very large sets of data and complex problems.\u003c/p\u003e\n\u003cp\u003eJust like in the workplace, whenever there is an extremely complex task, it is best to divide and conquer. In the world of big data, if the data is \"big\" enough, it is generally better to take the approach of splitting up the larger task into smaller pieces.\u003c/p\u003e\n\u003cp\u003eEven though individual processors are getting faster (remember \u003ca href=\"https://en.wikipedia.org/wiki/Moore%27s_law\"\u003eMoore's Law\u003c/a\u003e), they will never have the ability to keep up with the amount of data we are able to produce. The best solution computer scientists have developed has been to use the power of \u003cstrong\u003emultiple processors\u003c/strong\u003e to put them to the same task. When using a well-developed distributed system, multiple processors can accomplish tasks at a fraction of the time it would take for a single processor to accomplish. As noted in the picture below, if you can divide the work between multiple processors, everything will be more efficient.\u003c/p\u003e\n\u003cp\u003eWith parallel computing:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003ea larger problem is broken up into smaller pieces\u003c/li\u003e\n\u003cli\u003eevery part of the problem follows a series of instructions\u003c/li\u003e\n\u003cli\u003eeach one of the instructions is executed simultaneously on different processors\u003c/li\u003e\n\u003cli\u003eall of the answers are collected from the small problems and combined into one final answer\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eIn the image below, you can see a simple example of a process being broken up and completed both sequentially and in parallel.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-parallel-and-distributed-computing-with-mapreduce/master/images/parallel.png\"\u003e\u003c/p\u003e\n\u003cp\u003eOf course, not all problems can be parallelized, but there are some that are formally called \u003ca href=\"https://en.wikipedia.org/wiki/Embarrassingly_parallel\"\u003eembarrassingly parallel\u003c/a\u003e problems that require hardly any effort to ensure that a certain task is able to easily parallelizable. One example of something that would be embarrassingly parallelizable would be password cracking. Another example would be a movie production company trying to calculate the total profit they made from all of the movies they released in a given year. Let's think about all of the components that go into determining whether or not a movie is profitable.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003estory rights\u003c/li\u003e\n\u003cli\u003eproducer\u003c/li\u003e\n\u003cli\u003edirector\u003c/li\u003e\n\u003cli\u003ecast\u003c/li\u003e\n\u003cli\u003eproduction costs\u003c/li\u003e\n\u003cli\u003evisual effects\u003c/li\u003e\n\u003cli\u003emusic\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eand of course\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003ebox office revenue\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eHere is what this would look like if it was calculated sequentially.\u003c/p\u003e\n\u003cp\u003eIf a movie studio was to compute each one it's movie's profits sequentially, it would take far more time than if it calculated each movie's profit and combined them in parallel.\u003c/p\u003e\n\u003cp\u003eHere is a diagram of what parallel processing looks like in action:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-parallel-and-distributed-computing-with-mapreduce/master/images/parallel_movies_.png\"\u003e\u003c/p\u003e\n\u003cp\u003eSo how can we make all these nodes communicate with one another? By using a programming paradigm called MapReduce!\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eMapReduce\u003c/strong\u003e is a software framework developed for processing datasets that qualify as \"Big Data\", in a \u003cstrong\u003edistributed and parallel\u003c/strong\u003e processing environment over several computers/nodes connected to each other as part of a \u003cstrong\u003ecluster\u003c/strong\u003e. It is a specific instance of the generalized split-apply-combine technique used to perform different data analyses.\u003c/p\u003e\n\u003cp\u003eWe will soon look into a simple example that is shown to introduce MapReduce, \u003cstrong\u003eThe Word Count Problem\u003c/strong\u003e. The overall concept of MapReduce is very simple yet very powerful as:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eSomehow, all data can be mapped to \u003cstrong\u003ekey:value\u003c/strong\u003e pairs\u003c/li\u003e\n\u003cli\u003eKeys and values themselves can be of ANY data type\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eFor our example, let's say a national association of zoos wants to determine the total number of species of animals in the country. After receiving responses from every zoo in the country, a data scientist in charge of receives a large file that has a different zoo located on each line with the species at that location.\u003c/p\u003e\n\u003cp\u003eHere are the first five zoos the data scientist reads over in the data document they receive:\u003c/p\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003eAnimals\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd\u003elion tiger bear\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003elion giraffe\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003egiraffe penguin\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003epenguin lion giraffe\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003ekoala giraffe\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003cp\u003eLet's now look at how you would use the MapReduce framework in this simple word count example that could be generalized to much more data.\u003c/p\u003e\n\u003cp\u003eWe'll take a look at an image of this process in action and determine what's actually going on.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-parallel-and-distributed-computing-with-mapreduce/master/images/word_count.png\"\u003e\u003c/p\u003e\n\u003ch3\u003e1. MAP Task (Splitting \u0026amp; Mapping)\u003c/h3\u003e\n\u003cp\u003eThe dataset that needs processing must first be transformed into \u003cstrong\u003ekey:value\u003c/strong\u003e pairs and split into fragments, which are then assigned to map tasks. Each computing cluster is assigned a number of map tasks, which are subsequently distributed among its nodes. In this example, let's assume that we are using 5 nodes (a server with 5 different workers).\u003c/p\u003e\n\u003cp\u003eFirst, split the data from one file or files into however many nodes are being used.\u003c/p\u003e\n\u003cp\u003eWe will then use the map function to create key:value pairs represented by:\u003cbr\u003e\u003cem\u003e{animal}\u003c/em\u003e , \u003cem\u003e{# of animals per zoo}\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003eAfter processing of the original key:value pairs, some \u003cstrong\u003eintermediate\u003c/strong\u003e key:value pairs are generated. The intermediate key:value pairs are \u003cstrong\u003esorted by their key values\u003c/strong\u003e to create a new list of key:value pairs.\u003c/p\u003e\n\u003ch3\u003e2. Shuffling\u003c/h3\u003e\n\u003cp\u003eThis list from the map task is divided into a new set of fragments that sorts and shuffles the mapped objects into an order or grouping that will make it easier to reduce them. \u003cstrong\u003eThe number of these new fragments will be the same as the number of the reduce tasks\u003c/strong\u003e.\u003c/p\u003e\n\u003ch3\u003e3. REDUCE Task (Reducing)\u003c/h3\u003e\n\u003cp\u003eNow, every properly shuffled segment will have a reduce task applied to it. After the task is completed, the final output is written onto a file system. The underlying file system is usually HDFS (Hadoop Distributed File System).\u003c/p\u003e\n\u003cp\u003eIt's important to note that MapReduce will generally only be powerful when dealing with large amounts of data. When working with a small dataset, it will be faster not to perform operations in the MapReduce framework.\u003c/p\u003e\n\u003cp\u003eThere are two groups of entities in this process to ensuring that the MapReduce task gets done properly:\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eJob Tracker\u003c/strong\u003e: a \"master\" node that informs the other nodes which map and reduce jobs to complete\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eTask Tracker\u003c/strong\u003e: the \"worker\" nodes that complete the map and reduce operations\u003c/p\u003e\n\u003cp\u003eThere are different names for these components depending on the technology used, but there will always be a master node that informs worker nodes what tasks to perform.\u003c/p\u003e\n\u003cp\u003eA general pseudocode for a word count map and reduce tasks would look like\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"python\"\u003edef map( doc ) :\n    for word in doc.split( ' ' ) :\n    emit ( word , 1 )\n\ndef reduce( key , values ) :\n    emit ( key , sum( values ) )\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eSimilarly, we can discuss combining several MapReduce jobs in order to complete a given task. This means that once the first MapReduce job is finished, the output will become an input for the second MapReduce job and that output could be the final result (or fed into another MapReduce job).\u003c/p\u003e\n\u003cp\u003eLet's assume that we would like to extend the word count program and we would like to count all words in a given Twitter dataset. The first MapReduce will read our twitter data and extract the tweets' text. The second MapReduce is the word count Map-Reduce which will analyze the Twitter dataset and produce the statistics about it. So it is simply chaining together multiple jobs.\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003eInputFile -\u0026gt; Map-1 -\u0026gt; Reduce-1 -\u0026gt; output-1 -\u0026gt; Map-2 - \u0026gt; Reduce-2 -\u0026gt; output-2 -\u0026gt; ... Map-x -\u0026gt; Reduce-x\u003c/strong\u003e\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eNext, we are going to look at Apache Spark, which adds extra features of security and fault tolerance to its MapReduce offering, making it an industry standard. We will also look at programming for the aforementioned word count problem.\u003c/p\u003e\n\u003ch2\u003eAdditional Resources\u003c/h2\u003e\n\u003cp\u003eVisit following external links to read about the previous descriptions and examples in more detail.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e\u003ca href=\"https://www.tutorialspoint.com/map_reduce/map_reduce_introduction.htm\"\u003eMapReduce Introduction\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003ca href=\"https://www.guru99.com/introduction-to-mapreduce.html\"\u003eWhat is MapReduce? How it Works\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003eSummary\u003c/h2\u003e\n\u003cp\u003eIn this lesson, we looked at how MapReduce allows a programming paradigm quite different than traditional programming practices, yet very powerful and effective towards processing large amounts of data. Next, we will look at the Spark programming environment and some coding exercises to get grips with PySpark programming.\u003c/p\u003e","frontPage":false}],"assignments":[{"exportId":"gca1eaecb686e0a3170787fc030994922","title":"Amazon Recommendation System - Lab","type":"Assignment","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-network-recomendation-systems-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-network-recomendation-systems-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"g550d57825b87cbd1a1ccfd722c39dfee","title":"ARMA Models in statsmodels","type":"Assignment","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-arma-models-statsmodels\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-arma-models-statsmodels/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"gb8e9f73e2c5c06e41e700610fe3b55fc","title":"ARMA Models in statsmodels - Lab","type":"Assignment","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-arma-models-statsmodels-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-arma-models-statsmodels-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"g747e08959248d6f56d169dbde42204c8","title":"Basic Time Series Models","type":"Assignment","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-basic-time-series-models\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-basic-time-series-models/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"g95b09de1227a7a19b301fe2cefa6fad2","title":"Basic Time Series Models - Lab","type":"Assignment","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-basic-time-series-models-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-basic-time-series-models-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"gda1cffb40f24b5c7208326f1c703f24a","title":"Building a CNN from Scratch","type":"Assignment","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-building-a-cnn-from-scratch\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-building-a-cnn-from-scratch/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"gd6ed89fe9dc5314f1389f740f4a3ad88","title":"Building a Recommendation System in PySpark - Lab","type":"Assignment","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/als-recommender-system-pyspark-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/als-recommender-system-pyspark-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"g121f22a4b44ed598b79a72372b7b38a9","title":"Classification with Word Embeddings - Codealong","type":"Assignment","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-classification-with-word-embeddings-codealong\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-classification-with-word-embeddings-codealong/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"g685fa164fdd87e600d0d9c69caade1d3","title":"Collaborative Filtering and Singular Value Decomposition","type":"Assignment","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-collaborative-filtering-singular-value-decomposition\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-collaborative-filtering-singular-value-decomposition/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"g9e1da4e964e6ef1d671b812ed34cfd86","title":"Collaborative Filtering with Surprise","type":"Assignment","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-implementing-recommender-systems\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-implementing-recommender-systems/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"g856ffd862141af76250734dab1429013","title":"Collaborative Filtering with Surprise - Lab","type":"Assignment","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-implementing-recommender-systems-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-implementing-recommender-systems-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"g4bd186e43e87c61c4f6d438f7ac7fbb5","title":"Context-Free Grammars - Codealong","type":"Assignment","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-context-free-grammars-codealong\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-context-free-grammars-codealong/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"gd919e29f628ca3cb56a0eaa1b92c9823","title":"Convolutional Neural Networks - Codealong","type":"Assignment","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-convolutional-neural-networks-codealong\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-convolutional-neural-networks-codealong/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"gd9ca4d7e43d0dd71897c824e143e4b8f","title":"Corpus Statistics - Lab","type":"Assignment","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-corpus-statistics-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-corpus-statistics-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"g13fd6669c9d9a7d16859808cedfc68d8","title":"Correlation and Autocorrelation in Time Series","type":"Assignment","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-corr-autocorr-in-time-series\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-corr-autocorr-in-time-series/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"gfd8c77f82835e9df5254d856f144bea8","title":"Correlation and Autocorrelation in Time Series - Lab","type":"Assignment","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-corr-autocorr-in-time-series-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-corr-autocorr-in-time-series-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"g5653177efd941d689ec4c48330ea0c79","title":"Curse of Dimensionality - Lab","type":"Assignment","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-curse-of-dimensionality-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-curse-of-dimensionality-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"g17c9d8480ba5bdd1a2170d37c73d0873","title":"Deeper Neural Networks","type":"Assignment","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-deeper-neural-networks\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-deeper-neural-networks/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"g0677b15f508cb160637b9b55f9a38db2","title":"Deeper Neural Networks - Lab","type":"Assignment","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-deeper-neural-networks-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-deeper-neural-networks-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"gd0b13827d67ec74091ad20339b8e202f","title":"Feature Engineering for Text Data","type":"Assignment","content":"","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"gfec2e4016c3b1477ed76b57a2fef86d9","title":"Generating Word Embeddings - Lab","type":"Assignment","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-generating-word-embeddings-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-generating-word-embeddings-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"gb241a59232b1afda854cb830e82a967c","title":"Hierarchical Agglomerative Clustering - Codealong","type":"Assignment","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-hierarchical-agglomerative-clustering-codealong\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-hierarchical-agglomerative-clustering-codealong/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"g7f795263473e749df9e93f3c2adf4c5b","title":"Image Classification - Lab","type":"Assignment","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-image-classification-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-image-classification-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"gde61e951e7ac6dd47355b675556c49d7","title":"Image Classification with MLPs - Lab","type":"Assignment","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-image-classification-with-mlps-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-image-classification-with-mlps-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"g7cb9fa827af71c29f528e875448b7abf","title":"Installing and Configuring PySpark with Docker","type":"Assignment","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-spark-docker-installation\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-spark-docker-installation/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"ge86474f485a622c725ebcd07e1f92cac","title":"Integrating PCA in Pipelines - Lab","type":"Assignment","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-pca-and-pipelines-v2-1\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-pca-and-pipelines-v2-1/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"g021829d58a0c8e3807c9059bf191da7f","title":"Introduction to Keras","type":"Assignment","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-introduction-to-keras\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-introduction-to-keras/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"gd5546ee7f0ca0758abec95363ffaf8c5","title":"Introduction to NetworkX","type":"Assignment","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-networkX-intro\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-networkX-intro/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"ga1447a0475f0838fa82e90775a779485","title":"Introduction to NetworkX - Lab","type":"Assignment","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-networkX-intro-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-networkX-intro-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"gffa036196b934e18fcee36cfbcf6fd48","title":"Introduction to Neural Networks ","type":"Assignment","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-introduction-to-neural-networks\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-introduction-to-neural-networks/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"g49d3c582a310e0e78243e0f475f7a4f0","title":"Introduction to Neural Networks - Lab","type":"Assignment","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-introduction-to-neural-networks-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-introduction-to-neural-networks-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"gbaae5e6d3758f4da648edc92d7e4e9dc","title":"Introduction to Regular Expressions","type":"Assignment","content":"","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"g47191c997c95223ee0f5ab16ae5df781","title":"Introduction to Time Series","type":"Assignment","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-introduction-to-time-series\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-introduction-to-time-series/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"g3502c431022e3820cf9cf756efed0090","title":"Keras - Lab","type":"Assignment","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-introduction-to-keras-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-introduction-to-keras-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"g04296a1eec45d2008f3fecb687111576","title":"K-Means Clustering - Lab","type":"Assignment","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-k-means-clustering-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-k-means-clustering-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"g84ed5ca72401403a935312139bcef300","title":"Machine Learning with Spark","type":"Assignment","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-machine-learning-with-spark\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-machine-learning-with-spark/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"g395b717b16a2407d6667a519246a8e78","title":"Machine Learning with Spark -  Lab","type":"Assignment","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-machine-learning-with-spark-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-machine-learning-with-spark-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"gd94d0e7feb4fabba41d06eaea1e409a3","title":"Managing Time Series Data - Lab","type":"Assignment","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-managing-time-series-data-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-managing-time-series-data-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"gb889aa7446b061678725d79be8b38b4c","title":"Market Segmentation with Clustering - Lab","type":"Assignment","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-market-segmentation-clustering-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-market-segmentation-clustering-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"gec228d629172fb18ee30e8df90f2a7d4","title":"Matrix Factorization with Alternating Least Squares","type":"Assignment","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-matrix-factorization-als\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-matrix-factorization-als/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"g7060fdce18ee0fb08e702f7658f24fea","title":"Network Clustering","type":"Assignment","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-network-clustering\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-network-clustering/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"g23001e4161fe27e0d6bb3b07a3e574ab","title":"Network Clustering - Lab","type":"Assignment","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-network-clustering-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-network-clustering-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"ga652e513c124ee3685343bdf8b88938e","title":"Network Connectivity:  Community Detection -Lab","type":"Assignment","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-network-community-detection-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-network-community-detection-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"gf6b89c6ad7f9e67e3acea8ee704d8571","title":"Node Centrality","type":"Assignment","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-node-centrality\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-node-centrality/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"g4a334319ab37f18d3ef05fb1d1986116","title":"Node Centrality - Lab","type":"Assignment","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-node-centrality-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-node-centrality-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"g6b3bee0ce377440b6d13d88cdd056389","title":"Parallel and Distributed Computing with MapReduce","type":"Assignment","content":"","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"ge1abb17e54a9ceb85fe1539297e7e4a1","title":"PCA and Digital Image Processing","type":"Assignment","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-pca-and-digital-image-processing\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-pca-and-digital-image-processing/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"g4e184b24c353e4bea0bababe338be61d","title":"PCA and Digital Image Processing - Lab","type":"Assignment","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-pca-and-digital-image-processing-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-pca-and-digital-image-processing-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"g17b335a023f0ff43e5f2dab37c1cf7b2","title":"PCA Background: Covariance Matrix and Eigendecomposition","type":"Assignment","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-pca-covariance-matrix-eigendecomp\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-pca-covariance-matrix-eigendecomp/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"gef7dcb9d05c4072a9f49d7aee1c49a3e","title":"PCA in scikit-learn","type":"Assignment","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-pca-in-scikitlearn\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-pca-in-scikitlearn/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"g6cf4693bcea5b2ce7d90b156cf14056c","title":"PCA in scikit-learn - Lab","type":"Assignment","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-pca-in-scikitlearn-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-pca-in-scikitlearn-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"g1b186fdf4181788a31260a9296a62568","title":"Performing Principal Component Analysis","type":"Assignment","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-performing-principle-component-analysis\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-performing-principle-component-analysis/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"g791d0e61061e39932162ce861f82e51e","title":"Performing Principal Component Analysis - Lab","type":"Assignment","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-pca-numpy-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-pca-numpy-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"gc9968eeea195184c8693bc492d2e8946","title":"Phase 4 Blog Post","type":"Assignment","content":"\u003cp\u003e\u003cspan\u003ePlease put the URL to your Phase 4 Blog Post here. \u003c/span\u003e\u003cspan\u003eRefer to the \u003c/span\u003e\u003ca title=\"Blogging Overview\" href=\"pages/blogging-overview\" data-api-endpoint=\"https://learning.flatironschool.com/api/v1/courses/347/pages/blogging-overview\" data-api-returntype=\"Page\"\u003eBlogging Overview\u003c/a\u003e\u003cspan\u003e to learn about how to write good blog posts that\u003c/span\u003e\u003cspan style=\"font-family: inherit; font-size: 1rem;\"\u003e meet Flatiron Schools requirements.\u003c/span\u003e\u003c/p\u003e","submissionTypes":"a website url","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"ga5b9102687ae238d813078a99550dec8","title":"Phase 4 Project","type":"Assignment","content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-phase-4-project\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-phase-4-project\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-phase-4-project/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\u003cp\u003eFinal phase down -- you're absolutely crushing it! You've made it all the way through one of the toughest phase of this course. You must have an amazing brain in your head!\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-phase-4-project/main/images/brain.gif\"\u003e\u003c/p\u003e\n\u003ch2\u003eProject Overview\u003c/h2\u003e\n\u003cp\u003eFor this phase, you will choose a project that requires building one of these four models:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eTime Series Modeling\u003c/li\u003e\n\u003cli\u003eRecommendation System\u003c/li\u003e\n\u003cli\u003eImage Classification with Deep Learning\u003c/li\u003e\n\u003cli\u003eNatural Language Processing\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003eThe Data\u003c/h3\u003e\n\u003cp\u003eWe have provided a dataset suitable to each model, but you are also encouraged to source your own dataset. If you choose your own dataset, \u003cstrong\u003erun the dataset and business problem by your instructor for approval\u003c/strong\u003e before starting your project.\u003c/p\u003e\n\u003ch3\u003eHow to Choose a Project\u003c/h3\u003e\n\u003cp\u003eWhen choosing a project, consider:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eDepth:\u003c/strong\u003e Choose a project that similar to what you want to do for your capstone project (Phase 5). This will allow you to practice those methods in a group setting before needing to use it independently. This will help you build a better Capstone project and a portfolio that demonstrates the ability to deeply learn and implement one modeling approach.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eBreadth:\u003c/strong\u003e Choose a problem that you don't necessarily plan to use in your capstone project. This will allow you to develop applied experience with multiple modeling approaches. This will help you refine your areas of interest and build a portfolio that demonstrates the ability to learn and implement multiple modeling approaches.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eIf you are feeling overwhelmed or behind, we recommend you choose Problem #3: Image Classification with Deep Learning.\u003c/p\u003e\n\u003ch3\u003eProblem 1: Time Series Modeling\u003c/h3\u003e\n\u003cp\u003eIf you choose the Time Series option, you will be forecasting real estate prices of various zip codes using data from \u003ca href=\"https://www.zillow.com/research/data/\"\u003eZillow Research\u003c/a\u003e. For this project, you will be acting as a consultant for a fictional real-estate investment firm. The firm has asked you what seems like a simple question:\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eWhat are the top 5 best zip codes for us to invest in?\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eThis may seem like a simple question at first glance, but there's more than a little ambiguity here that you'll have to think through in order to provide a solid recommendation. Should your recommendation be focused on profit margins only? What about risk? What sort of time horizon are you predicting against? Your recommendation will need to detail your rationale and answer any sort of lingering questions like these in order to demonstrate how you define \"best\".\u003c/p\u003e\n\u003cp\u003eThere are many datasets on the \u003ca href=\"https://www.zillow.com/research/data/\"\u003eZillow Research Page\u003c/a\u003e, and making sure you have exactly what you need can be a bit confusing. For simplicity's sake, we have already provided the dataset for you in this repo -- you will find it in the file \u003ccode\u003etime-series/zillow_data.csv\u003c/code\u003e.\u003c/p\u003e\n\u003cp\u003eThe goal of this project is to have you complete a very common real-world task in regard to time series modeling. However, real world problems often come with a significant degree of ambiguity, which requires you to use your knowledge of statistics and data science to think critically about and answer. While the main task in this project is time series modeling, that isn't the overall goal -- it is important to understand that time series modeling is a tool in your toolbox, and the forecasts it provides you are what you'll use to answer important questions.\u003c/p\u003e\n\u003cp\u003eIn short, to pass this project, demonstrating the quality and thoughtfulness of your overall recommendation is at least as important as successfully building a time series model!\u003c/p\u003e\n\u003ch4\u003eStarter Jupyter Notebook\u003c/h4\u003e\n\u003cp\u003eFor this project, you will be provided with a Jupyter notebook, \u003ccode\u003etime-series/starter_notebook.ipynb\u003c/code\u003e, containing some starter code. If you inspect the Zillow dataset file, you'll notice that the datetimes for each sale are the actual column names -- this is a format you probably haven't seen before. To ensure that you're not blocked by preprocessing, we've provided some helper functions to help simplify getting the data into the correct format. You're not required to use this notebook or keep it in its current format, but we strongly recommend you consider making use of the helper functions so you can spend your time working on the parts of the project that matter.\u003c/p\u003e\n\u003ch4\u003eEvaluation\u003c/h4\u003e\n\u003cp\u003eIn addition to deciding which quantitative metric(s) you want to target (e.g. minimizing mean squared error), you need to start with a definition of \"best investment\". Consider additional metrics like risk vs. profitability, or ROI yield.\u003c/p\u003e\n\u003ch3\u003eProblem 2: Recommendation System\u003c/h3\u003e\n\u003cp\u003eIf you choose the Recommendation System option, you will be making movie recommendations based on the \u003ca href=\"https://grouplens.org/datasets/movielens/latest/\"\u003eMovieLens\u003c/a\u003e dataset from the GroupLens research lab at the University of Minnesota. Unless you are planning to run your analysis on a paid cloud platform, we recommend that you use the \"small\" dataset containing 100,000 user ratings (and potentially, only a particular subset of that dataset).\u003c/p\u003e\n\u003cp\u003eYour task is to:\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eBuild a model that provides top 5 movie recommendations to a user, based on their ratings of other movies.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eThe MovieLens dataset is a \"classic\" recommendation system dataset, that is used in numerous academic papers and machine learning proofs-of-concept. You will need to create the specific details about how the user will provide their ratings of other movies, in addition to formulating a more specific business problem within the general context of \"recommending movies\".\u003c/p\u003e\n\u003ch4\u003eCollaborative Filtering\u003c/h4\u003e\n\u003cp\u003eAt minimum, your recommendation system must use collaborative filtering. If you have time, consider implementing a hybrid approach, e.g. using collaborative filtering as the primary mechanism, but using content-based filtering to address the \u003ca href=\"https://en.wikipedia.org/wiki/Cold_start_(computing)\"\u003ecold start problem\u003c/a\u003e.\u003c/p\u003e\n\u003ch4\u003eEvaluation\u003c/h4\u003e\n\u003cp\u003eThe MovieLens dataset has explicit ratings, so achieving some sort of evaluation of your model is simple enough. But you should give some thought to the question of metrics. Since the rankings are ordinal, we know we can treat this like a regression problem. But when it comes to regression metrics there are several choices: RMSE, MAE, etc. \u003ca href=\"http://fastml.com/evaluating-recommender-systems/\"\u003eHere\u003c/a\u003e are some further ideas.\u003c/p\u003e\n\u003ch3\u003eProblem 3: Image Classification with Deep Learning\u003c/h3\u003e\n\u003cp\u003eIf you choose this option, you'll put everything you've learned together to build a deep neural network that trains on a large dataset for classification on a non-trivial task. In this case, using x-ray images of pediatric patients to identify whether or not they have pneumonia. The dataset comes from Kermany et al. on \u003ca href=\"https://data.mendeley.com/datasets/rscbjbr9sj/3\"\u003eMendeley\u003c/a\u003e, although there is also a version on \u003ca href=\"https://www.kaggle.com/paultimothymooney/chest-xray-pneumonia\"\u003eKaggle\u003c/a\u003e that may be easier to use.\u003c/p\u003e\n\u003cp\u003eYour task is to:\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eBuild a model that can classify whether a given patient has pneumonia, given a chest x-ray image.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch4\u003eAim for a Proof of Concept\u003c/h4\u003e\n\u003cp\u003eWith Deep Learning, data is king -- the more of it, the better. However, the goal of this project isn't to build the best model possible -- it's to demonstrate your understanding by building a model that works. You should try to avoid datasets and model architectures that won't run in reasonable time on your own machine. For many problems, this means downsampling your dataset and only training on a portion of it. Once you're absolutely sure that you've found the best possible architecture and other hyperparameters for your model, then consider training your model on your entire dataset overnight (or, as larger portion of the dataset that will still run in a feasible amount of time).\u003c/p\u003e\n\u003cp\u003eAt the end of the day, we want to see your thought process as you iterate and improve on a model. A project that achieves a lower level of accuracy but has clearly iterated on the model and the problem until it found the best possible approach is more impressive than a model with high accuracy that did no iteration. We're not just interested in seeing you finish a model -- we want to see that you understand it, and can use this knowledge to try and make it even better!\u003c/p\u003e\n\u003ch4\u003eEvaluation\u003c/h4\u003e\n\u003cp\u003eEvaluation is fairly straightforward for this project. But you'll still need to think about which metric to use and about how best to cross-validate your results.\u003c/p\u003e\n\u003ch3\u003eProblem 4: Natural Language Processing (NLP)\u003c/h3\u003e\n\u003cp\u003eIf you choose this option, you'll build an NLP model to analyze Twitter sentiment about Apple and Google products. The dataset comes from CrowdFlower via \u003ca href=\"https://data.world/crowdflower/brands-and-product-emotions\"\u003edata.world\u003c/a\u003e. Human raters rated the sentiment in over 9,000 Tweets as positive, negative, or neither.\u003c/p\u003e\n\u003cp\u003eYour task is to:\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eBuild a model that can rate the sentiment of a Tweet based on its content.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch4\u003eAim for a Proof of Concept\u003c/h4\u003e\n\u003cp\u003eThere are many approaches to NLP problems - start with something simple and iterate from there. For example, you could start by limiting your analysis to positive and negative Tweets only, allowing you to build a binary classifier. Then you could add in the neutral Tweets to build out a multiclass classifier. You may also consider using some of the more advanced NLP methods in the Mod 4 Appendix.\u003c/p\u003e\n\u003ch4\u003eEvaluation\u003c/h4\u003e\n\u003cp\u003eEvaluating multiclass classifiers can be trickier than binary classifiers because there are multiple ways to mis-classify an observation, and some errors are more problematic than others. Use the business problem that your NLP project sets out to solve to inform your choice of evaluation metrics.\u003c/p\u003e\n\u003ch3\u003eSourcing Your Own Data\u003c/h3\u003e\n\u003cp\u003eSourcing new data is a valuable skill for data scientists, but it requires a great deal of care. An inappropriate dataset or an unclear business problem can lead you spend a lot of time on a project that delivers underwhelming results. The guidelines below will help you complete a project that demonstrates your ability to engage in the full data science process.\u003c/p\u003e\n\u003cp\u003eYour dataset must be...\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eAppropriate for one of this project's models.\u003c/strong\u003e These are time series, recommendation systems, deep learning, or natural language processing.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eUsable to solve a specific business problem.\u003c/strong\u003e This solution must rely on your model.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eSomewhat complex.\u003c/strong\u003e It should contain thousands of rows and features that require creativity to use.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eUnfamiliar.\u003c/strong\u003e It can't be one we've already worked with during the course or that is commonly used for demonstration purposes (e.g. MNIST).\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eManageable.\u003c/strong\u003e Stick to datasets that you can model using the techniques introduced in Phase 4.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eOnce you've sourced your own dataset and identified the business problem you want to solve with it, you must to \u003cstrong\u003erun them by your instructor for approval\u003c/strong\u003e.\u003c/p\u003e\n\u003ch4\u003eProblem First, or Data First?\u003c/h4\u003e\n\u003cp\u003eThere are two ways that you can source your own dataset: \u003cstrong\u003e\u003cem\u003eProblem First\u003c/em\u003e\u003c/strong\u003e or \u003cstrong\u003e\u003cem\u003eData First\u003c/em\u003e\u003c/strong\u003e. The less time you have to complete the project, the more strongly we recommend a Data First approach to this project.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e\u003cem\u003eProblem First\u003c/em\u003e\u003c/strong\u003e: Start with a problem that you are interested in that you could potentially solve using one of the four project models. Then look for data that you could use to solve that problem. This approach is high-risk, high-reward: Very rewarding if you are able to solve a problem you are invested in, but frustrating if you end up sinking lots of time in without finding appropriate data. To mitigate the risk, set a firm limit for the amount of time you will allow yourself to look for data before moving on to the Data First approach.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e\u003cem\u003eData First\u003c/em\u003e\u003c/strong\u003e: Take a look at some of the most popular internet repositories of cool data sets we've listed below. If you find a data set that's particularly interesting for you, then it's totally okay to build your problem around that data set.\u003c/p\u003e\n\u003cp\u003eThere are plenty of amazing places that you can get your data from. We recommend you start looking at data sets in some of these resources first:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://archive.ics.uci.edu/ml/datasets.php\" target=\"_blank\"\u003eUCI Machine Learning Datasets Repository\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://www.kaggle.com/datasets\"\u003eKaggle Datasets\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/awesomedata/awesome-public-datasets\"\u003eAwesome Datasets Repo on Github\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003eThe Deliverables\u003c/h2\u003e\n\u003cp\u003eThere are three deliverables for this project:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eA \u003cstrong\u003eGitHub repository\u003c/strong\u003e\n\u003c/li\u003e\n\u003cli\u003eA \u003cstrong\u003eJupyter Notebook\u003c/strong\u003e\n\u003c/li\u003e\n\u003cli\u003eA \u003cstrong\u003enon-technical presentation\u003c/strong\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eReview the \"Project Submission \u0026amp; Review\" page in the \"Milestones Instructions\" topic for instructions on creating and submitting your deliverables. Refer to the rubric associated with this assignment for specifications describing high-quality deliverables.\u003c/p\u003e\n\u003ch3\u003eKey Points\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eChoose your project quickly.\u003c/strong\u003e We've given you a lot of choices - don't get stuck spending too much time choosing which project to do. Give yourself a firm time limit for picking a project (e.g. 2 hours) so you can get on with making something great. Don't worry about picking the perfect project - remember that you will get to do a new, larger Capstone project very soon!\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eYour Jupyter Notebook should demonstrate an iterative approach to modeling.\u003c/strong\u003e This means that you begin with a basic model, evaluate it, and then provide justification for and proceed to a new model. This is a great way to add narrative structure to your notebook, especially if you compare model performance across each iteration.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eYou must choose and implement an appropriate validation strategy.\u003c/strong\u003e This is one of the trickiest parts of machine learning models, especially for models that don't lend themselves easily to traditional cross-validation (e.g. time series \u0026amp; recommendation systems).\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003eGetting Started\u003c/h2\u003e\n\u003cp\u003eCreate a new repository for your project to get started. We recommend structuring your project repository similar to the structure in \u003ca href=\"https://github.com/learn-co-curriculum/dsc-project-template\"\u003ethe Phase 1 Project Template\u003c/a\u003e. You can do this either by creating a new fork of that repository to work in or by building a new repository from scratch that mimics that structure.\u003c/p\u003e\n\u003ch2\u003eProject Submission and Review\u003c/h2\u003e\n\u003cp\u003eReview the \"Project Submission \u0026amp; Review\" page in the \"Milestones Instructions\" topic to learn how to submit your project and how it will be reviewed. Your project must pass review for you to progress to the next Phase.\u003c/p\u003e\n\u003ch2\u003eSummary\u003c/h2\u003e\n\u003cp\u003eThis project is your chance to show off your data science prowess with some advanced machine learning algorithms. Now that you've gone through all of the core course content, we're excited to see what you are able to do!\u003c/p\u003e","submissionTypes":"a file upload","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"ge3a2332be06fb7ebdca5e17879290a6a","title":"Phase 4 Project - GitHub Repository URL","type":"Assignment","content":"\u003cp\u003e\u003cspan\u003ePlease put the URL to your Phase 4 Project GitHub Repository here.\u0026nbsp;\u003c/span\u003e\u003c/p\u003e","submissionTypes":"a website url","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"g0f0233ecfbd7bc2cfc18149458f8aec7","title":"Productionizing a Model with Docker and SageMaker","type":"Assignment","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-productionizing-models-with-sagemaker\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-productionizing-models-with-sagemaker/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"g2cc004165ab91dcd7c9452c6b2b0050c","title":"Recommendation Systems","type":"Assignment","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-network-recommendation-systems\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-network-recommendation-systems/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"gcf7f63d29efacf29c4069fd830ae1ce3","title":"Regular Expressions - Codealong","type":"Assignment","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-regular-expressions-codealong\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-regular-expressions-codealong/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"ge609caa4ce192937728849d3e8579757","title":"Removing Trends","type":"Assignment","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-removing-trends\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-removing-trends/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"gf1d75b5dd1effc263db5285e973d0a50","title":"Removing Trends - Lab","type":"Assignment","content":"\u003cheader class=\"fis-header\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-removing-trends-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-removing-trends-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"gb8717a1aec7738ff1870579ddf51b133","title":"Resilient Distributed Datasets (RDDs) - Lab","type":"Assignment","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-resilient-distributed-datasets-rdd-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-resilient-distributed-datasets-rdd-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"gf057ff7aa80100653161461b977f43f1","title":"Simple and Shortest Paths","type":"Assignment","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-graph-theory-shortest-path\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-graph-theory-shortest-path/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"g40b6c15fc2920faf3ff41c3e7cb089af","title":"Simple and Shortest Paths - Lab","type":"Assignment","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-graph-theory-shortest-path-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-graph-theory-shortest-path-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"g81908fe02dffb8e98b7ebb5cc4074411","title":"Testing for Trends - Lab","type":"Assignment","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-testing-for-trends-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-testing-for-trends-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"g581a73de14060f438fe098fbf0dd1bec","title":"Text Classification - Lab","type":"Assignment","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-text-classification-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-text-classification-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"ge39f9b78a9073aac955e8962d85e3ae9","title":"Time Series Decomposition","type":"Assignment","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-time-series-decomposition\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-time-series-decomposition/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"ga70b5b86efba6490cfd5ac5183691cfc","title":"Time Series: Facebook Prophet - Lab","type":"Assignment","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-facebook-prophet-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-facebook-prophet-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"gc90a0be09b0a290f138aa62d4823d44e","title":"Time Series: SARIMA Models - Lab","type":"Assignment","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-sarima-models-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-sarima-models-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"g9e2060c34e3aeb853796494094b66f02","title":"Tuning Neural Networks from Start to Finish - Lab","type":"Assignment","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-tuning-neural-networks-from-start-to-finish-lab-v2-1\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-tuning-neural-networks-from-start-to-finish-lab-v2-1/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"g59df043892200b17758cb18112232fb3","title":"Tuning Neural Networks with Normalization - Lab","type":"Assignment","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-tuning-neural-networks-with-normalization-lab-v2-1\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-tuning-neural-networks-with-normalization-lab-v2-1/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"ga9f534dba2b781f0f5c2ce6bf283b0e4","title":"Tuning Neural Networks with Regularization - Lab","type":"Assignment","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-tuning-neural-networks-with-regularization-lab-v2-1\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-tuning-neural-networks-with-regularization-lab-v2-1/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"g5dee489c6bfeb0ed4ebcabc43c337ac5","title":"Types of Trends","type":"Assignment","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-types-of-trends\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-types-of-trends/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"g968c20c24c5b4f090ed4f1eb53ee98df","title":"Understanding Spark Context - Lab","type":"Assignment","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-sparkcontext-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-sparkcontext-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"gf6786f1c116d1462286609041ecf2943","title":"Using Pretrained Networks","type":"Assignment","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-using-pretrained-networks\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-using-pretrained-networks/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"g8dcb0ed9dda1678da4c9568f5a9024ae","title":"Using Pretrained Networks - Codealong","type":"Assignment","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-using-pretrained-networks-codealong\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-using-pretrained-networks-codealong/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"g330e12dd2f9ba72ecab3b7c17b68b6c7","title":"Using Word2Vec","type":"Assignment","content":"","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"gbe8fd1ecc88b063ae494c9fba34c5e21","title":"Visualizing Activation Functions - Lab","type":"Assignment","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-visualizing-activation-functions-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-visualizing-activation-functions-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"gf24089e714a5280c53ada52bbe6085e5","title":"Visualizing Intermediate Activations","type":"Assignment","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-visualizing-intermediate-activations\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-visualizing-intermediate-activations/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"g9f78f4552bfdc85602573b4d20e581d3","title":"Visualizing Time Series Data - Lab","type":"Assignment","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-visualizing-time-series-data-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-visualizing-time-series-data-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"g0e8c2fc0851dfc42e4d00c386b41ff1a","title":"Word Count with MapReduce - Lab","type":"Assignment","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-word-count-with-map-reduce-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-word-count-with-map-reduce-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"g4af22bf8d80e6133dd9c77767c2ff51b","title":"Word Vectorization - Lab","type":"Assignment","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-word-vectorization-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-word-vectorization-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null}],"discussion_topics":[],"quizzes":[],"files":[{"type":"folder","name":"course_image","size":null,"files":[{"type":"file","name":"phase_4.png","size":6927,"files":null},{"type":"file","name":"Image_8_DataVisualization copy 6.png","size":6642,"files":null}]},{"type":"file","name":"bp-data-science-phase-1-quiz-export.zip","size":3547,"files":null}]}