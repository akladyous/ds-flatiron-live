window.COURSE_DATA = {"language":"en","lastDownload":"2021-06-23T16:10:34Z","title":"Phase 2","modules":[{"id":15069,"name":"Topic 11: Combinatorics and Probability","status":"started","unlockDate":null,"prereqs":[],"requirement":"all","sequential":false,"exportId":"gac3170d74f79b2460cdc3d1c43dec1d0","items":[{"id":141242,"title":"Combinatorics and Probability - Introduction","type":"WikiPage","indent":0,"locked":false,"requirement":"must_mark_done","completed":true,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-probability-section-intro\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-probability-section-intro/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eIn this section, you'll learn about the foundation of statistics: probability!\u003c/p\u003e\n\u003ch2\u003eCombinatorics and Probability\u003c/h2\u003e\n\u003cp\u003eIn this section, we'll take a little time to \"get our math on\" with some basic probability. You're going to start with some basic set theory and look at how to operate on related sets using Python.\u003c/p\u003e\n\u003cp\u003eFrom there, we're going to use what we learned about sets to start to learn and apply some of the basic rules of probability.\u003c/p\u003e\n\u003ch3\u003eFactorials and Permutations\u003c/h3\u003e\n\u003cp\u003eNext we're going to dig into factorials, and how they can be used to calculate various permutations.\u003c/p\u003e\n\u003ch3\u003eCombinations\u003c/h3\u003e\n\u003cp\u003eWe're then going to examine the difference between permutations and combinations. We'll get some practice calculating combinations for everything from drawing letters from a bag to creating soccer teams for a tournament!\u003c/p\u003e\n\u003ch3\u003eConditional Probability\u003c/h3\u003e\n\u003cp\u003eWe start the section off with an introduction to conditional probability. We look at the difference between dependent and independent events, look at how to calculate dependent probabilities, and then introduce some key theorems related to conditional probabilities: the product rule, the chain rule, and Bayes theorem.\u003c/p\u003e\n\u003ch3\u003ePartitioning and the Law of Total Probabilities\u003c/h3\u003e\n\u003cp\u003eFrom there, we introduce the concept of partitioning a sample space, explain the law of total probabilities, and then introduce the idea of conditional independence.\u003c/p\u003e\n\u003ch2\u003eSummary\u003c/h2\u003e\n\u003cp\u003eThis is a mathematics heavy section. Some of the discrete problems you'll solve may not seem to be particularly relevant to machine learning, but we deliberately introduce them so that you have the foundations required to make thoughtful choices as we introduce you to a range of new machine learning models in the later sections.\u003c/p\u003e\n\u003cfooter class=\"fis-footer\" style=\"visibility: hidden;\"\u003e\n\u003cdiv class=\"fis-feedback\"\u003e\n\u003ch5\u003eHow do you feel about this lesson?\u003c/h5\u003e\n\u003cimg id=\"thumbs-up\" title=\"Thumbs up!\" alt=\"thumbs up\" data-repository=\"dsc-probability-section-intro\"\u003e\u003cimg id=\"thumbs-down\" title=\"Thumbs down!\" alt=\"thumbs down\" data-repository=\"dsc-probability-section-intro\"\u003e\n\u003c/div\u003e\n\u003ch5\u003eHave specific feedback? \u003ca href=\"https://github.com/learn-co-curriculum/dsc-probability-section-intro/issues/new\"\u003eTell us here!\u003c/a\u003e\n\u003c/h5\u003e\n\u003c/footer\u003e","exportId":"combinatorics-and-probability-introduction"},{"id":141247,"title":"What is Probability?","type":"WikiPage","indent":0,"locked":false,"requirement":"must_mark_done","completed":true,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-probability-introduction\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-probability-introduction/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eAs an aspiring data scientist, it's important to know the foundations of probability and combinatorics, as these areas form the backbone of many concepts in data science. In the following lessons and labs, you'll get a gentle introduction to several concepts that are related to probability, such as sets, combinations, and permutations.\u003c/p\u003e\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\u003cp\u003eYou will be able to:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eDefine probability\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003eWhat is probability, and how does it relate to data science?\u003c/h2\u003e\n\u003cp\u003eProbability is the chance that a certain event will happen, in other words, how \"likely\" it is that some event will happen.\u003c/p\u003e\n\u003cp\u003eAs data science often uses \u003cem\u003estatistical inference\u003c/em\u003e to analyze or predict certain events or trends, knowing probability and its applications is important because statistical inference uses probability distributions of the data. Although it might take a little more time for you to understand just how important the foundations of probability are for data science, by the end of the first part of the probability section, you'll be able to answer questions like:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eHow likely is it to end up with heads when flipping a coin once? (the answer here is 50% - not very surprising)\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eHow likely is it to end up with exactly 2 x heads and 3 x tails when flipping a coin 5 times?\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eHow likely is it to throw tails first, then heads, then tails, then heads, then tails when flipping a coin 5 times?\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eIf you throw 5 dice, what is the probability of throwing a \u003ca href=\"http://grail.sourceforge.net/demo/yahtzee/rules.html\"\u003e\"full house\"\u003c/a\u003e?\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eWhat is the probability of drawing 2 consecutive aces from a standard deck of cards?\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003eSummary\u003c/h2\u003e\n\u003cp\u003eNow, let's dive deeper into the understanding of sets. Getting these concepts will make calculating your first probabilities much easier!\u003c/p\u003e\n\u003cfooter class=\"fis-footer\" style=\"visibility: hidden;\"\u003e\n\u003cdiv class=\"fis-feedback\"\u003e\n\u003ch5\u003eHow do you feel about this lesson?\u003c/h5\u003e\n\u003cimg id=\"thumbs-up\" title=\"Thumbs up!\" alt=\"thumbs up\" data-repository=\"dsc-probability-introduction\"\u003e\u003cimg id=\"thumbs-down\" title=\"Thumbs down!\" alt=\"thumbs down\" data-repository=\"dsc-probability-introduction\"\u003e\n\u003c/div\u003e\n\u003ch5\u003eHave specific feedback? \u003ca href=\"https://github.com/learn-co-curriculum/dsc-probability-introduction/issues/new\"\u003eTell us here!\u003c/a\u003e\n\u003c/h5\u003e\n\u003c/footer\u003e","exportId":"what-is-probability"},{"id":141249,"title":"Introduction to Sets","type":"WikiPage","indent":0,"locked":false,"requirement":"must_mark_done","completed":true,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-intro-to-sets\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-intro-to-sets/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eYou have definitely heard of sets before. In this section, however, you will learn about the formal definition of sets, which will serve as a foundation for everything related to probability and combinatorics!\u003c/p\u003e\n\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\n\u003cp\u003eYou will be able to: \u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eDefine a set in the context of probability theory\u003c/li\u003e\n\u003cli\u003eDefine a universal set and subsets\u003c/li\u003e\n\u003cli\u003eDescribe the process of making unions, intersections, and complements\u003c/li\u003e\n\u003cli\u003eUse Venn Diagrams to visually demonstrate set operations\u003c/li\u003e\n\u003cli\u003eDescribe the inclusion-exclusion principle\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch2\u003eWhat is a Set?\u003c/h2\u003e\n\n\u003cp\u003eIn probability theory, a set is defined as a \u003cem\u003ewell-defined collection of objects\u003c/em\u003e. \u003c/p\u003e\n\n\u003cp\u003eMathematically, you can denote a set by \u003cimg src=\"https://render.githubusercontent.com/render/math?math=S\"\u003e. If an element \u003cimg src=\"https://render.githubusercontent.com/render/math?math=x\"\u003ebelongs to a set \u003cimg src=\"https://render.githubusercontent.com/render/math?math=S\"\u003e, then you'd write \u003cimg src=\"https://render.githubusercontent.com/render/math?math=x%5Cin%20S\"\u003e. On the other hand, if \u003cimg src=\"https://render.githubusercontent.com/render/math?math=x\"\u003edoes not belong to a set \u003cimg src=\"https://render.githubusercontent.com/render/math?math=S\"\u003e, then you'd write \u003cimg src=\"https://render.githubusercontent.com/render/math?math=x%5Cnotin%20S\"\u003e.\u003c/p\u003e\n\n\u003cp\u003eExample: If \u003cimg src=\"https://render.githubusercontent.com/render/math?math=S\"\u003eis defined as the set of even numbers, then:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003e\u003cp\u003eIf \u003cimg src=\"https://render.githubusercontent.com/render/math?math=x%20=%202\"\u003e, \u003cimg src=\"https://render.githubusercontent.com/render/math?math=x%5Cin%20S\"\u003ebecause \u003cimg src=\"https://render.githubusercontent.com/render/math?math=x\"\u003eis an even number.\u003c/p\u003e\u003c/li\u003e\n\u003cli\u003e\u003cp\u003eIf \u003cimg src=\"https://render.githubusercontent.com/render/math?math=x%20=%209\"\u003e, \u003cimg src=\"https://render.githubusercontent.com/render/math?math=x%5Cnotin%20S\"\u003ebecause \u003cimg src=\"https://render.githubusercontent.com/render/math?math=x\"\u003eis not an even number.\u003c/p\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch2\u003eSubsets\u003c/h2\u003e\n\n\u003cp\u003eSet \u003cimg src=\"https://render.githubusercontent.com/render/math?math=T\"\u003eis a subset of set \u003cimg src=\"https://render.githubusercontent.com/render/math?math=S\"\u003eif \u003cem\u003eevery element\u003c/em\u003e in set \u003cimg src=\"https://render.githubusercontent.com/render/math?math=T\"\u003eis also in set \u003cimg src=\"https://render.githubusercontent.com/render/math?math=S\"\u003e. The mathematical notation for a subset is \u003cimg src=\"https://render.githubusercontent.com/render/math?math=T%20%5Csubseteq%20S\"\u003e. \u003c/p\u003e\n\n\u003cp\u003eTypically, you'll be more interested in \u003cem\u003eproper subsets\u003c/em\u003e. All proper subsets are subsets. The only difference between subsets and proper subsets is that a subset can technically be the entire set. In other words, if A = {1,2,3} and B = {1,2,3} A is subset of B. If C = {1,2} then C is both a subset and proper subset of A. C is also a subset and proper subset of B. The mathematical notation for proper subsets is : \u003cimg src=\"https://render.githubusercontent.com/render/math?math=C%20%5Csubset%20A\"\u003e\u003c/p\u003e\n\n\u003cp\u003e\u003cstrong\u003eExample\u003c/strong\u003e: If S is the set of even numbers, set \u003cimg src=\"https://render.githubusercontent.com/render/math?math=T%20=%5C%7B2,%206,%2022%5C%7D\"\u003eis a proper subset of \u003cimg src=\"https://render.githubusercontent.com/render/math?math=S\"\u003e. Formally, you can write this as \u003cimg src=\"https://render.githubusercontent.com/render/math?math=T%20%5Csubset%20S\"\u003e. \u003cimg src=\"https://render.githubusercontent.com/render/math?math=T%20%5Csubseteq%20S\"\u003eis also correct in this case!\u003c/p\u003e\n\n\u003ch2\u003eUniversal Sets\u003c/h2\u003e\n\n\u003cp\u003eThe collection of all possible outcomes in a certain context or universe is called the \u003cstrong\u003euniversal set\u003c/strong\u003e.\nA universal set is often denoted by \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5COmega\"\u003e.\u003c/p\u003e\n\n\u003cp\u003eExample of a universal set: All the possible outcomes when rolling a dice.\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5COmega%20=%5C%7B1,2,3,4,5,6%5C%7D\"\u003e\u003c/p\u003e\n\n\u003cp\u003eRemember that a universal set is not necessarily all the possible things that have ever existed. Typically, a universal set is just all the possible elements within certain bounds, e.g., the set of all countries in the world, the set of all the animal species in the Bronx Zoo, etc.\u003c/p\u003e\n\n\u003cp\u003eA universal set can have an infinite number of elements, for example, the set of all real numbers!\u003c/p\u003e\n\n\u003ch2\u003eElementary Set Operations\u003c/h2\u003e\n\n\u003cp\u003eNext, let's talk about set operations. Imagine you have two sets of numbers, say the first 4 multiples of 3 in set \u003cimg src=\"https://render.githubusercontent.com/render/math?math=S\"\u003e:\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=S%20=%5C%7B3,6,9,12%5C%7D\"\u003e\u003c/p\u003e\n\n\u003cp\u003eand the first 4 multiples of 2 in set \u003cimg src=\"https://render.githubusercontent.com/render/math?math=T\"\u003e:\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=T%20=%5C%7B2,4,6,8%5C%7D\"\u003e.\u003c/p\u003e\n\n\u003ch3\u003ea) Union of Two Sets\u003c/h3\u003e\n\n\u003cp\u003eThe union of two sets \u003cimg src=\"https://render.githubusercontent.com/render/math?math=S\"\u003eand \u003cimg src=\"https://render.githubusercontent.com/render/math?math=T\"\u003eis the set of elements of either S or T, or in both.  \u003c/p\u003e\n\n\u003cp\u003eApplied to our example, the union of \u003cimg src=\"https://render.githubusercontent.com/render/math?math=S\"\u003eand \u003cimg src=\"https://render.githubusercontent.com/render/math?math=T\"\u003eis given by the elements \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5C%7B2,3,4,6,8,9,12%5C%7D\"\u003e. \u003c/p\u003e\n\n\u003cp\u003eIn mathematical terms, the union of \u003cimg src=\"https://render.githubusercontent.com/render/math?math=S\"\u003eand \u003cimg src=\"https://render.githubusercontent.com/render/math?math=T\"\u003eis denoted as \u003cimg src=\"https://render.githubusercontent.com/render/math?math=S%20%5Ccup%20T\"\u003e.\u003c/p\u003e\n\n\u003cp\u003eA popular way to represent sets and their relationships is through Venn Diagrams, (\u003ca href=\"https://en.wikipedia.org/wiki/Venn_diagram\"\u003ehttps://en.wikipedia.org/wiki/Venn_diagram\u003c/a\u003e), see picture below!\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-intro-to-sets/master/images/new_union.png\" width=\"250\"\u003e\u003c/p\u003e\n\n\u003ch3\u003eb) Intersection of Two Sets\u003c/h3\u003e\n\n\u003cp\u003eThe intersection of two sets \u003cimg src=\"https://render.githubusercontent.com/render/math?math=S\"\u003eand \u003cimg src=\"https://render.githubusercontent.com/render/math?math=T\"\u003eis the set that contains all elements of \u003cimg src=\"https://render.githubusercontent.com/render/math?math=S\"\u003ethat also belong to \u003cimg src=\"https://render.githubusercontent.com/render/math?math=T\"\u003e. \u003c/p\u003e\n\n\u003cp\u003eApplied to our example, the intersection of \u003cimg src=\"https://render.githubusercontent.com/render/math?math=S\"\u003eand \u003cimg src=\"https://render.githubusercontent.com/render/math?math=T\"\u003eis given by {6}, so it contains the elements that are multiples of both 2 AND 3.\u003c/p\u003e\n\n\u003cp\u003eIn mathematical terms, the intersection of \u003cimg src=\"https://render.githubusercontent.com/render/math?math=S\"\u003eand \u003cimg src=\"https://render.githubusercontent.com/render/math?math=T\"\u003eis denoted as \u003cimg src=\"https://render.githubusercontent.com/render/math?math=S%20%5Ccap%20T\"\u003e.\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-intro-to-sets/master/images/new_intersection.png\" width=\"250\"\u003e\u003c/p\u003e\n\n\u003ch3\u003ec) Relative Complement or the Difference\u003c/h3\u003e\n\n\u003cp\u003eIf you have S and T, the relative complement of S contains all the elements of T that are NOT in S. This is also sometimes referred to as the \u003cem\u003edifference\u003c/em\u003e. The difference is denoted by \u003cimg src=\"https://render.githubusercontent.com/render/math?math=T%5Cbackslash%20S\"\u003eor \u003cimg src=\"https://render.githubusercontent.com/render/math?math=T-S\"\u003e.\u003c/p\u003e\n\n\u003cp\u003eIn this case, the relative complement of S (or \u003cimg src=\"https://render.githubusercontent.com/render/math?math=T%5Cbackslash%20S\"\u003e) is \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5C%7B2,4,8%5C%7D\"\u003e. The relative complement of T (or \u003cimg src=\"https://render.githubusercontent.com/render/math?math=S%5Cbackslash%20T\"\u003e) is \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5C%7B3,9,12%5C%7D\"\u003e.\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-intro-to-sets/master/images/new_rel_comp.png\" width=\"250\"\u003e\u003c/p\u003e\n\n\u003ch3\u003ed) Absolute Complement\u003c/h3\u003e\n\n\u003cp\u003eThere is another definition of the complement when considering universal sets \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5COmega\"\u003eas well. In this context, we're talking about the \u003cem\u003eabsolute complement\u003c/em\u003e.\u003c/p\u003e\n\n\u003cp\u003eThe absolute complement of \u003cimg src=\"https://render.githubusercontent.com/render/math?math=S\"\u003e, with respect to the Universal set \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5COmega\"\u003e, is the collection of the objects in \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5COmega\"\u003ethat don't belong to \u003cimg src=\"https://render.githubusercontent.com/render/math?math=S\"\u003e.\u003c/p\u003e\n\n\u003cp\u003eNote how the definition of \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5COmega\"\u003eis very important here. Imagine a set \u003cimg src=\"https://render.githubusercontent.com/render/math?math=S=%5C%7B%5Ctext%7Belephant,%20alligator,%20tiger,%20bear%7D%5C%7D\"\u003e. The complement of this set will depend on how the universal set is defined: Is \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5COmega\"\u003eequal to \u003cem\u003ethe animals in the Bronx Zoo\u003c/em\u003e, or \u003cem\u003ethe 20 most deadly animals in the world\u003c/em\u003e?\u003c/p\u003e\n\n\u003cp\u003eMathematically, the absolute complement of \u003cimg src=\"https://render.githubusercontent.com/render/math?math=S\"\u003eis denoted as \u003cimg src=\"https://render.githubusercontent.com/render/math?math=S'\"\u003eor \u003cimg src=\"https://render.githubusercontent.com/render/math?math=S%5Ec\"\u003e.\u003c/p\u003e\n\n\u003cp\u003eLet's reconsider \u003cimg src=\"https://render.githubusercontent.com/render/math?math=S\"\u003eand \u003cimg src=\"https://render.githubusercontent.com/render/math?math=T\"\u003eas defined previously.\u003c/p\u003e\n\n\u003cp\u003eLet's define \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5COmega\"\u003e, the universal set (denoted by the box around the two Venn diagrams), as the set that contains the multiples of both 2 and 3 until 20. Then the elements of \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5COmega\"\u003eare \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5C%7B2,3,4,6,8,9,10,12,14,15,16,18,20%5C%7D\"\u003e. \u003c/p\u003e\n\n\u003cp\u003eThe absolute complement of \u003cimg src=\"https://render.githubusercontent.com/render/math?math=S\"\u003e(so, \u003cimg src=\"https://render.githubusercontent.com/render/math?math=S'\"\u003eor \u003cimg src=\"https://render.githubusercontent.com/render/math?math=S%5Ec\"\u003e) is then given by \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5C%7B2,4,8,10,14,15,16,18,20%5C%7D\"\u003e.\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-intro-to-sets/master/images/new_abs_comp.png\" width=\"250\"\u003e\u003c/p\u003e\n\n\u003ch2\u003eInclusion-Exclusion Principle\u003c/h2\u003e\n\n\u003cp\u003eNote that if you want to know how many elements are in set \u003cimg src=\"https://render.githubusercontent.com/render/math?math=S\"\u003eversus \u003cimg src=\"https://render.githubusercontent.com/render/math?math=T\"\u003e, you can't simply sum up the elements, because they have elements in common.\u003c/p\u003e\n\n\u003cp\u003eIn combinational mathematics, the inclusion-exclusion principle is a counting technique that solves this problem.\u003c/p\u003e\n\n\u003cp\u003eWhen having two sets, the method for counting the number of elements in the union of two finite sets is given by:\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cmid%20S%20%5Ccup%20T%5Cmid%20=%5Cmid%20S%5Cmid%20%2b%5Cmid%20T%5Cmid%20-%5Cmid%20S%20%5Ccap%20T%5Cmid\"\u003e,\u003c/p\u003e\n\n\u003cp\u003ewhere the horizontal lines denote the \u003cem\u003ecardinality\u003c/em\u003e of a set, which is the number of elements in the set, considering a set with a finite number of elements. \u003c/p\u003e\n\n\u003cp\u003eThe formula expresses the fact that the sum of the sizes of the two sets may be too large since some elements may be counted twice. For the double-counted elements, one is subtracted again.\u003c/p\u003e\n\n\u003cp\u003eThis formula can be extended to three sets, four sets, etc. For example, imagine you have a third set \u003cimg src=\"https://render.githubusercontent.com/render/math?math=R\"\u003e. The number of elements in the union of three finite sets is given by:\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cmid%20S%20%5Ccup%20T%20%5Ccup%20R%5Cmid%20=%5Cmid%20S%5Cmid%20%2b%5Cmid%20T%5Cmid%20%2b%5Cmid%20R%5Cmid%20-%5Cmid%20S%20%5Ccap%20T%5Cmid%20%20-%5Cmid%20S%20%5Ccap%20R%5Cmid%20-%5Cmid%20R%20%5Ccap%20T%5Cmid%20%20%2b%5Cmid%20S%20%5Ccap%20T%20%5Ccap%20R%5Cmid\"\u003e\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-intro-to-sets/master/images/new_venn_diagram.png\" width=\"350\"\u003e\u003c/p\u003e\n\n\u003ch2\u003eEmpty Sets\u003c/h2\u003e\n\n\u003cp\u003eWhen there are no elements in a certain set, this set is \u003cstrong\u003eempty\u003c/strong\u003e, denoted by \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cemptyset\"\u003eor simply \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5C%7B%5C%7D\"\u003e\u003c/p\u003e\n\n\u003ch2\u003eSets in Python\u003c/h2\u003e\n\n\u003cp\u003eSome things to bear in mind when working with sets in Python:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eSets are unordered collections of unique elements.\u003c/li\u003e\n\u003cli\u003eSets are iterable.\u003c/li\u003e\n\u003cli\u003eSets are collections of lower level python objects (just like lists or dictionaries).\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003eDocumentation for sets in Python can be found here: \u003ca href=\"https://docs.python.org/3.6/library/stdtypes.html#set-types-set-frozenset\"\u003eSets\u003c/a\u003e\u003c/p\u003e\n\n\u003ch2\u003eSets and Set Operations: A Summative Example\u003c/h2\u003e\n\n\u003cp\u003eTo put this all together, let's consider an example with restaurants:\u003c/p\u003e\n\n\u003cp\u003eThink about a \u003cem\u003eset A\u003c/em\u003e with all the restaurants that serve Italian food.\nNext, there is a \u003cem\u003eset B\u003c/em\u003e with all the restaurants that serve burgers.\u003c/p\u003e\n\n\u003cp\u003eThe \u003cstrong\u003eunion\u003c/strong\u003e of these sets, \u003cem\u003eset C\u003c/em\u003e, contains the set of restaurants that either serve Italian food, burgers or both.\u003c/p\u003e\n\n\u003cp\u003eYou could say that the \u003cstrong\u003euniversal set\u003c/strong\u003e here, \u003cem\u003eset U\u003c/em\u003e, contains all the restaurants in the world (with any type of food). Then \u003cem\u003eset C\u003c/em\u003e is a \u003cstrong\u003esubset\u003c/strong\u003e of \u003cem\u003eset U\u003c/em\u003e. \u003c/p\u003e\n\n\u003cp\u003eThe \u003cstrong\u003eintersection\u003c/strong\u003e of \u003cem\u003eA\u003c/em\u003e and \u003cem\u003eB\u003c/em\u003e contains the restaurants that serve \u003cem\u003eboth\u003c/em\u003e Italian food and burgers.\u003c/p\u003e\n\n\u003cp\u003eThe \u003cstrong\u003erelative complement\u003c/strong\u003e of \u003cem\u003eset A\u003c/em\u003e contains the restaurants that serve burgers but \u003cem\u003enot\u003c/em\u003e Italian food.\u003c/p\u003e\n\n\u003cp\u003eThe \u003cstrong\u003eabsolute complement\u003c/strong\u003e of \u003cem\u003eset A\u003c/em\u003e contains the restaurants that serve \u003cem\u003eany food\u003c/em\u003e but \u003cem\u003eno\u003c/em\u003e Italian food.\u003c/p\u003e\n\n\u003ch2\u003eSummary\u003c/h2\u003e\n\n\u003cp\u003eIn this section, you learned about sets, subsets, and universal sets. Next, you were introduced to some elementary set operations such as unions, intersections, and complements. After that, all this information was tied together through the inclusion-exclusion principle. Next, you saw how sets translate into Python. You'll start exploring this in further detail in the next lab!\u003c/p\u003e\n\u003cfooter class=\"fis-footer\" style=\"visibility: hidden;\"\u003e\u003cdiv class=\"fis-feedback\"\u003e\n\u003ch5\u003eHow do you feel about this lesson?\u003c/h5\u003e\n\u003cimg id=\"thumbs-up\" data-repository=\"dsc-intro-to-sets\" title=\"Thumbs up!\" alt=\"thumbs up\"\u003e\u003cimg id=\"thumbs-down\" data-repository=\"dsc-intro-to-sets\" title=\"Thumbs down!\" alt=\"thumbs down\"\u003e\n\u003c/div\u003e\n\u003ch5\u003eHave specific feedback? \u003ca href=\"https://github.com/learn-co-curriculum/dsc-intro-to-sets/issues/new\"\u003eTell us here!\u003c/a\u003e\n\u003c/h5\u003e\u003c/footer\u003e","exportId":"introduction-to-sets"},{"id":141250,"title":"Introduction to Sets - Lab","type":"Assignment","indent":1,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_mark_done","completed":true,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-intro-to-sets-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-intro-to-sets-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","exportId":"g3d063dab473e0613a7266642ae03008c"},{"id":141253,"title":"Introduction to Probability","type":"WikiPage","indent":0,"locked":false,"requirement":"must_mark_done","completed":false,"content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-intro-to-probability\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-intro-to-probability\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-intro-to-probability/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eNow that you understand the basics of sets, you'll learn how this knowledge can be used to calculate your first probabilities! In this section, you'll learn how to use sets to create probabilities and you'll learn about the very foundations of probability through the three probability axioms.\u003c/p\u003e\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\u003cp\u003eYou will be able to:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eCompare experiments, outcomes, and the event space\u003c/li\u003e\n\u003cli\u003eCalculate probabilities by using relative frequency of outcomes to event space\u003c/li\u003e\n\u003cli\u003eDescribe the three axioms of probability\u003c/li\u003e\n\u003cli\u003eDescribe the addition law of probability\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003eExperiment and outcomes\u003c/h2\u003e\n\u003cp\u003ePreviously, we defined sets and related concepts. Now let's look at the set\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=S=%20%20%20%5C%7B1,2,3,4,5,6%20%5C%7D\"\u003e , which contains all possible outcomes when throwing a dice.\u003c/p\u003e\n\u003cp\u003eWhen you throw a dice once, you can consider this a \u003cem\u003erandom experiment\u003c/em\u003e. The result of this \"experiment\" is the \u003cem\u003eoutcome\u003c/em\u003e.\u003c/p\u003e\n\u003cp\u003eYou can then say that:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cimg src=\"https://render.githubusercontent.com/render/math?math=S\"\u003e defines all the \u003cstrong\u003epossible outcomes\u003c/strong\u003e when throwing the dice once\u003c/li\u003e\n\u003cli\u003e\n\u003cimg src=\"https://render.githubusercontent.com/render/math?math=S\"\u003e is our Universal set \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5COmega\"\u003e , as seen before\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eWhen conducting experiments, you say that your universal set is your \u003cstrong\u003esample space\u003c/strong\u003e: it is the universe in which your possible outcomes are listed as elements.\u003c/p\u003e\n\u003cp\u003eOther examples of sample spaces: - The number of text messages you send each day: in this case, S is equal to some number x, with x being a \u003cstrong\u003epositive integer\u003c/strong\u003e, or mathematically: \u003cimg src=\"https://render.githubusercontent.com/render/math?math=S%20=%20%20%20%5C%7Bx%20%20%20%5Cmid%20x%20%20%20%5Cin%20%20%20%5Cmathbb%7BZ%7D,%20x%20%20%20%5Cgeq%200%20%5C%7D\"\u003e - The number of hours someone watches TV each day: \u003cimg src=\"https://render.githubusercontent.com/render/math?math=S%20=%20%20%20%5C%7Bx%20%20%20%5Cmid%20x%20%20%20%5Cin%20%20%20%5Cmathbb%7BR%7D,%200%20%20%20%5Cleq%20x%20%20%20%5Cleq%2024%20%20%20%5C%7D\"\u003e\u003c/p\u003e\n\u003ch2\u003eEvent space\u003c/h2\u003e\n\u003cp\u003eNext, let's define event space. The \u003cstrong\u003eevent space\u003c/strong\u003e is a subset of the sample space, \u003cimg src=\"https://render.githubusercontent.com/render/math?math=E%20%20%20%5Csubseteq%20S\"\u003e\u003c/p\u003e\n\u003cp\u003eFor example, the event \"throwing a number higher than 4\" when throwing a dice would result in an event space \u003cimg src=\"https://render.githubusercontent.com/render/math?math=E=%20%20%20%5C%7B5,6%20%5C%7D\"\u003e . Throwing an odd number would lead to an event space \u003cimg src=\"https://render.githubusercontent.com/render/math?math=E=%20%20%20%5C%7B1,3,5%20%5C%7D\"\u003e .\u003c/p\u003e\n\u003cp\u003eSummarized, the event space is a collection of events that we \u003cem\u003ecare\u003c/em\u003e about. We say that event \u003cimg src=\"https://render.githubusercontent.com/render/math?math=E\"\u003e happened if the actual outcome after rolling the dice belongs to the predefined event space \u003cimg src=\"https://render.githubusercontent.com/render/math?math=E\"\u003e .\u003c/p\u003e\n\u003cp\u003eWith \u003cstrong\u003esample space\u003c/strong\u003e and \u003cstrong\u003eevent space\u003c/strong\u003e, you now understand the two foundational concepts of \u003cstrong\u003eprobability\u003c/strong\u003e.\u003c/p\u003e\n\u003cp\u003eOther examples of event spaces based on previously defined sample spaces:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eIf you define that the event \"low daily number of text messages sent\" means 20 or fewer text messages, the event space is defined as: \u003cimg src=\"https://render.githubusercontent.com/render/math?math=E%20=%20%20%20%5C%7Bx%20%20%20%5Cmid%20x%20%20%20%5Cin%20%20%20%5Cmathbb%7BZ%7D,%200%20%20%20%5Cleq%20x%20%20%20%5Cleq%2020%20%20%20%5C%7D\"\u003e\n\u003c/li\u003e\n\u003cli\u003eBinge-watch day: \u003cimg src=\"https://render.githubusercontent.com/render/math?math=E%20=%20%20%20%5C%7Bx%20%20%20%5Cmid%20x%20%20%20%5Cin%20%20%20%5Cmathbb%7BR%7D,%20x%20%20%20%5Cgeq%206%20%20%20%5C%7D\"\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003eIntroduction to probability\u003c/h2\u003e\n\u003ch3\u003eThe law of relative frequency\u003c/h3\u003e\n\u003cp\u003eWhile conducting an endless stream of experiments, the relative frequency by which an event will happen becomes a fixed number.\u003c/p\u003e\n\u003cp\u003eLet's denote an event by \u003cimg src=\"https://render.githubusercontent.com/render/math?math=E\"\u003e , and the \u003cem\u003eprobability\u003c/em\u003e of the event \u003cimg src=\"https://render.githubusercontent.com/render/math?math=E\"\u003e occurring by \u003cimg src=\"https://render.githubusercontent.com/render/math?math=P(E)\"\u003e . Next, let \u003cimg src=\"https://render.githubusercontent.com/render/math?math=n\"\u003e be the number of conducted experiments, and \u003cimg src=\"https://render.githubusercontent.com/render/math?math=S(n)\"\u003e the count of \"successful\" experiments (i.e. the times that event \u003cimg src=\"https://render.githubusercontent.com/render/math?math=E\"\u003e happened). The formal definition of probability as a relative frequency is given by:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=P(E)%20=%20%20%20%5Clim_%7Bn%20%5Crightarrow%20%5Cinfty%7D%20%20%20%5Cdfrac%7BS%7B(n)%7D%7D%7Bn%7D\"\u003e\u003c/p\u003e\n\u003cp\u003eThis is the basis of a frequentist statistical interpretation: an event's probability is the ratio of the positive trials to the total number of trials as we repeat the process infinitely.\u003c/p\u003e\n\u003cp\u003eFor example, the probability of rolling a 5 on a 6 sided dice is the limit of the successes to trials as the number of trials goes to infinity.\u003c/p\u003e\n\u003ch3\u003eProbability axioms\u003c/h3\u003e\n\u003cp\u003eIn the early 20th century, Kolmogorov and Von Mises came up with three axioms that further expand on the idea of probability. The three axioms are:\u003c/p\u003e\n\u003ch4\u003e1. Positivity\u003c/h4\u003e\n\u003cp\u003eA probability is always bigger than or equal to 0, or \u003cimg src=\"https://render.githubusercontent.com/render/math?math=0%20%20%20%5Cleq%20P(E)%20%20%20%5Cleq%201\"\u003e\u003c/p\u003e\n\u003ch4\u003e2. Probability of a certain event\u003c/h4\u003e\n\u003cp\u003eIf the event of interest is the sample space, we say that the outcome is a certain event, or \u003cimg src=\"https://render.githubusercontent.com/render/math?math=P(S)%20=%201\"\u003e\u003c/p\u003e\n\u003ch4\u003e3. Additivity\u003c/h4\u003e\n\u003cp\u003eThe probability of the union of two exclusive events is equal to the sum of the probabilities of the individual events happening.\u003c/p\u003e\n\u003cp\u003eIf \u003cimg src=\"https://render.githubusercontent.com/render/math?math=A%20%20%20%20%5Ccap%20B%20=%20%20%20%5Cemptyset\"\u003e , then \u003cimg src=\"https://render.githubusercontent.com/render/math?math=P(A%20%20%5Ccup%20B)%20=%20P(A)%20%2b%20P(B)\"\u003e\u003c/p\u003e\n\u003ch3\u003eAddition law of probability\u003c/h3\u003e\n\u003cp\u003eThe additivity axiom is great, but most of the time events are not exclusive. A very important property is the \u003cstrong\u003eaddition law or probability\u003c/strong\u003e or the \u003cstrong\u003esum rule\u003c/strong\u003e.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=P(A%20%20%5Ccup%20B)%20=%20P(A)%20%2b%20P(B)%20-%20P(A%20%20%20%20%5Ccap%20B)\"\u003e\u003c/p\u003e\n\u003cp\u003ePut in words, the probability that \u003cimg src=\"https://render.githubusercontent.com/render/math?math=A\"\u003e or \u003cimg src=\"https://render.githubusercontent.com/render/math?math=B\"\u003e will happen is the sum of the probabilities that \u003cimg src=\"https://render.githubusercontent.com/render/math?math=A\"\u003e will happen and that \u003cimg src=\"https://render.githubusercontent.com/render/math?math=B\"\u003e will happen, minus the probability that \u003cem\u003eboth\u003c/em\u003e \u003cimg src=\"https://render.githubusercontent.com/render/math?math=A\"\u003e and \u003cimg src=\"https://render.githubusercontent.com/render/math?math=B\"\u003e will happen.\u003c/p\u003e\n\u003ch2\u003eExamples\u003c/h2\u003e\n\u003cp\u003eLet's reconsider the dice example to explain what was explained before:\u003c/p\u003e\n\u003ch3\u003eAdditivity of exclusive events\u003c/h3\u003e\n\u003cp\u003eLet's consider two events: event \u003cimg src=\"https://render.githubusercontent.com/render/math?math=M\"\u003e means throwing a 6, event \u003cimg src=\"https://render.githubusercontent.com/render/math?math=N\"\u003e means that you throw an odd number \u003cimg src=\"https://render.githubusercontent.com/render/math?math=N=%7B1,3,5%7D\"\u003e . These events are exclusive, and you can use the additivity rule if you want to know the answer to the question:\u003c/p\u003e\n\u003cp\u003e\u003cem\u003e\"what is the probability that your outcome will be a 6, or an odd number?\"\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=P(M%20%20%5Ccup%20N)%20=%20P(M)%20%2b%20P(N)%20=%20%20%20%5Cdfrac%7B1%7D%7B6%7D%2b%20%5Cdfrac%7B3%7D%7B6%7D=%20%5Cdfrac%7B4%7D%7B6%7D\"\u003e\u003c/p\u003e\n\u003ch3\u003eAddition law of probability\u003c/h3\u003e\n\u003cp\u003eNow, let's consider the same event \u003cimg src=\"https://render.githubusercontent.com/render/math?math=N=%7B1,3,5%7D\"\u003e and another event \u003cimg src=\"https://render.githubusercontent.com/render/math?math=Q=%7B4,5%7D\"\u003e . These events are \u003cem\u003enot\u003c/em\u003e mutually exclusive, so if you want to know the probability that \u003cimg src=\"https://render.githubusercontent.com/render/math?math=N\"\u003e or \u003cimg src=\"https://render.githubusercontent.com/render/math?math=Q\"\u003e will happen, you need to use the addition law of probability.\u003c/p\u003e\n\u003cp\u003eNote that \u003cimg src=\"https://render.githubusercontent.com/render/math?math=(N%20%20%20%20%5Ccap%20Q)\"\u003e is equal to getting an outcome of 5, as that is the \"common\" element in the respective event spaces of \u003cimg src=\"https://render.githubusercontent.com/render/math?math=N\"\u003e and \u003cimg src=\"https://render.githubusercontent.com/render/math?math=Q\"\u003e . This means that \u003cimg src=\"https://render.githubusercontent.com/render/math?math=P(N%20%20%20%20%5Ccap%20Q)%20=%20%20%20%5Cdfrac%7B1%7D%7B6%7D\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=P(N%20%20%5Ccup%20Q)%20=%20P(N)%20%2b%20P(Q)%20-%20P(N%20%20%20%20%5Ccap%20Q)%20=%20%20%20%5Cdfrac%7B3%7D%7B6%7D%20%2b%20%20%20%5Cdfrac%7B2%7D%7B6%7D%20-%20%20%20%5Cdfrac%7B1%7D%7B6%7D%20=%20%20%20%5Cdfrac%7B4%7D%7B6%7D\"\u003e\u003c/p\u003e\n\u003ch2\u003eFinal Note\u003c/h2\u003e\n\u003cp\u003eIn the previous examples, you noticed that for our dice example, it is easy to use these fairly straightforward probability formulas to calculate probabilities of certain outcomes.\u003c/p\u003e\n\u003cp\u003eHowever, if you think about our text message example, things are less straightforward, e.g.:\u003c/p\u003e\n\u003cp\u003e\u003cem\u003e\"What is the probability of sending less than 20 text messages in a day?\"\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003eThis is where the probability concepts introduced here fall short. The probability of throwing any number between 1 and 6 with a die is always exactly \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cdfrac%7B1%7D%7B6%7D\"\u003e , but we can't simply count our messages event space. In words, the probability of sending 20 messages is likely different than the probability of sending, say, 5 messages, and will be different for any number of messages sent. You'll learn about tools to solve problems like these later on.\u003c/p\u003e\n\u003ch2\u003eSummary\u003c/h2\u003e\n\u003cp\u003eWell done! In this section, you learned how to use sets to get to probabilities. You learned about experiments, event spaces, and outcomes. Next, you learned about the law of relative frequency and how it can be used to calculate probabilities, along with the three probability axioms.\u003c/p\u003e","exportId":"introduction-to-probability"},{"id":141256,"title":"Introduction to Probability - Lab","type":"Assignment","indent":1,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_mark_done","completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-intro-to-probability-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-intro-to-probability-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","exportId":"g6afe51ee2d4b8c4884a7c2e6046d3f6e"},{"id":141259,"title":"Permutations and Factorials","type":"WikiPage","indent":0,"locked":false,"requirement":"must_mark_done","completed":true,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-permutations-and-factorials-v2-1\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-permutations-and-factorials-v2-1/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eIn the previous lab, you defined a few sample spaces by counting the total number of possible outcomes. This is not very practical when sample spaces grow. In this lab, you'll be introduced to \u003cem\u003epermutations\u003c/em\u003e, which will provide a structured way to help you define sample space sizes!\u003c/p\u003e\n\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\n\u003cp\u003eYou will be able to: \u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eDescribe how factorials are related to permutations\u003c/li\u003e\n\u003cli\u003eMathematically derive how many permutations there are for large sets\u003c/li\u003e\n\u003cli\u003eCalculate permutations of a subset\u003c/li\u003e\n\u003cli\u003eCalculate permutations with repetition and replacement\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch2\u003eDefining the Sample Space by Counting\u003c/h2\u003e\n\n\u003cp\u003eLet's consider the following example.\u003c/p\u003e\n\n\u003cp\u003eThe Beyoncé tribute band \"The Single Ladies\" is playing a free mini gig in your local park next week. They have selected three all-time classics: \"Drunk in Love\", \"Crazy in Love\" and \"Formation\", but still have to decide the order they will play the songs in. Knowing this, how many playlists are possible?\u003c/p\u003e\n\n\u003cp\u003eIt is easy and fairly quick to write down possible orders here:\u003c/p\u003e\n\n\u003cp\u003e\"Drunk in Love\", \"Crazy in Love\", \"Formation\"\u003c/p\u003e\n\n\u003cp\u003e\"Drunk in Love\", \"Formation\", \"Crazy in Love\"\u003c/p\u003e\n\n\u003cp\u003e\"Crazy in Love\", \"Drunk in Love\", \"Formation\"\u003c/p\u003e\n\n\u003cp\u003e\"Crazy in Love\", \"Formation\", \"Drunk in Love\" \u003c/p\u003e\n\n\u003cp\u003e\"Formation\", \"Drunk in Love\", \"Crazy in Love\"\u003c/p\u003e\n\n\u003cp\u003e\"Formation\", \"Crazy in Love\", \"Drunk in Love\"\u003c/p\u003e\n\n\u003cp\u003eThat's it! When we count the possible outcomes, we get to 6 elements in the sample set. Now what if \"The Single Ladies\" plays a setlist of 4 songs? or 5? That's where the notion of \u003cem\u003epermutations\u003c/em\u003e comes in handy.\u003c/p\u003e\n\n\u003ch2\u003ePermutations\u003c/h2\u003e\n\n\u003cp\u003eThe problem setting, in general, is that there are  \u003cimg src=\"https://render.githubusercontent.com/render/math?math=n\"\u003e objects and we want to know how many \u003cem\u003epermutations\u003c/em\u003e are possible.\u003c/p\u003e\n\n\u003cp\u003eThis is a way how you can tackle this. You're the lead singer and have to decide which song to play first. You have 3 songs to choose from, so 3 ways of choosing a first song. Then, you move on to the second song. You've chosen the first one, so you have 2 songs to choose from now, etc. Mathematically, this boils down to:\u003c/p\u003e\n\n\u003cp\u003e# Beyoncé permutations\u003cimg src=\"https://render.githubusercontent.com/render/math?math==%203*2*1%20=%203!%20=%206\"\u003e \u003c/p\u003e\n\n\u003cp\u003eGeneralizing this to  \u003cimg src=\"https://render.githubusercontent.com/render/math?math=n\"\u003e , this means that the number of permutations with  \u003cimg src=\"https://render.githubusercontent.com/render/math?math=n\"\u003e distinct objects is  \u003cimg src=\"https://render.githubusercontent.com/render/math?math=n!\"\u003e , or the factorial of  \u003cimg src=\"https://render.githubusercontent.com/render/math?math=n\"\u003e .\u003c/p\u003e\n\n\u003ch2\u003ePermutations of a Subset\u003c/h2\u003e\n\n\u003cp\u003eNow, let's consider another example. \"The Single Ladies\" are still playing a concert at central park, but they disagree on the final three songs that they will play. They only get a 12 min gig slot, so they really can't play more than 3, yet they have a shortlist of 8 they need to pick from. How many final song selections are possible given this info? As for the first example, the order of the songs played is still important.\u003c/p\u003e\n\n\u003cp\u003eWhen the band members decide on the first song, they have 8 possible songs to choose from. When choosing the second song, they have 7 to choose from. Then for the third song, they have 6 left.\u003c/p\u003e\n\n\u003cp\u003e# Beyoncé k-permutations\u003cimg src=\"https://render.githubusercontent.com/render/math?math==%208*7*6%20=%20336\"\u003e \u003c/p\u003e\n\n\u003cp\u003eformalizing this, the question is how many ways we can select  \u003cimg src=\"https://render.githubusercontent.com/render/math?math=k\"\u003e elements out of a pool of  \u003cimg src=\"https://render.githubusercontent.com/render/math?math=n\"\u003e objects. The answer is \u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=n*(n-1)*...*(n-k%2b1)\"\u003e or in other words,  \u003cimg src=\"https://render.githubusercontent.com/render/math?math=P_%7Bk%7D%5E%7Bn%7D=%20%5Cdfrac%7Bn!%7D%7B(n-k)!%7D\"\u003e \u003c/p\u003e\n\n\u003cp\u003eThis is known as a  \u003cimg src=\"https://render.githubusercontent.com/render/math?math=k\"\u003e -permutation of  \u003cimg src=\"https://render.githubusercontent.com/render/math?math=n\"\u003e .\u003c/p\u003e\n\n\u003cp\u003eThe idea is here that we only \"care\" about the order of the first  \u003cimg src=\"https://render.githubusercontent.com/render/math?math=k\"\u003e objects. The order of the other  \u003cimg src=\"https://render.githubusercontent.com/render/math?math=(n-k)\"\u003e objects doesn't matter, hence they're left out of the equation.\u003c/p\u003e\n\n\u003ch2\u003ePermutations with Replacement\u003c/h2\u003e\n\n\u003cp\u003eWhen talking about setlists, it makes total sense to assume that songs will not be played twice. This is not always how it works though. Imagine a bag with three marbles in it: a green one, a red one, and a blue one. Now we'll draw marbles three times in a row, but each time, we'll write down the marble color and \u003cem\u003eput it back in the bag\u003c/em\u003e before drawing again.\u003c/p\u003e\n\n\u003cp\u003eNow the number of possible outcomes is  \u003cimg src=\"https://render.githubusercontent.com/render/math?math=3%20*%203%20*%203\"\u003e .\u003c/p\u003e\n\n\u003cp\u003eGeneralizing this to  \u003cimg src=\"https://render.githubusercontent.com/render/math?math=n\"\u003e , this means that the number of permutations with replacement when having  \u003cimg src=\"https://render.githubusercontent.com/render/math?math=n\"\u003e distinct objects is equal to  \u003cimg src=\"https://render.githubusercontent.com/render/math?math=n%5Ej\"\u003e where  \u003cimg src=\"https://render.githubusercontent.com/render/math?math=j\"\u003e is the number of \"draws\".\u003c/p\u003e\n\n\u003ch2\u003ePermutations with Repetition\u003c/h2\u003e\n\n\u003cp\u003eWhen using permutations, some elements may be \u003cem\u003erepeated\u003c/em\u003e.\u003c/p\u003e\n\n\u003cp\u003eA classic example is using permutations on words. Let's say you have the letters of the word \"TENNESSEE\". How many different words can you create using these letters?\u003c/p\u003e\n\n\u003cp\u003eSimply saying that there are 9 letters so the answer is  \u003cimg src=\"https://render.githubusercontent.com/render/math?math=9!\"\u003e does not give you the correct answer. Looking at the word TENNESSEE by itself, you can swap the 3rd and the 4th letter and have the same word. So the total number is less than  \u003cimg src=\"https://render.githubusercontent.com/render/math?math=9!\"\u003e .\u003c/p\u003e\n\n\u003cp\u003eThe solution is to divide  \u003cimg src=\"https://render.githubusercontent.com/render/math?math=9!\"\u003e by the factorials for each letter that is repeated!\u003c/p\u003e\n\n\u003cp\u003eThe answer here is then (9 letters, 4 x E, 2 x N, 2 x S)\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cdfrac%7B9!%7D%7B4!2!2!%7D%20=%203780\"\u003e \u003c/p\u003e\n\n\u003cp\u003eThe general formula can be written as:\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cdfrac%7Bn!%7D%7Bn_1!n_2!%5Cldots%20n_k!%7D\"\u003e \u003c/p\u003e\n\n\u003cp\u003ewhere  \u003cimg src=\"https://render.githubusercontent.com/render/math?math=n_j\"\u003e stands for identical objects of type  \u003cimg src=\"https://render.githubusercontent.com/render/math?math=j\"\u003e (the distinct letters in our TENNESSEE example). \u003c/p\u003e\n\n\u003ch2\u003eLevel-Up: Factorials and Recursion\u003c/h2\u003e\n\n\u003cp\u003eAt the start of this lesson, when discussing the number of possible permutations we can obtain for n distinct objects, we mentioned the concept of the factorial of n, denoted by  \u003cimg src=\"https://render.githubusercontent.com/render/math?math=n!\"\u003e . \u003c/p\u003e\n\n\u003cp\u003eIn the example presented to you, we wanted to count all possible ways in which three different Beyoncé songs could be played by the Beyoncé tribute band \"The Single Ladies\". There were 3 possible ways of choosing a first song, 2 possible ways of choosing a second song, and only 1 way of choosing a third and final song, for  \u003cimg src=\"https://render.githubusercontent.com/render/math?math=3%20*%202%20*1%20=%206\"\u003e different ways in which the three different songs could be played. This number, 6, is equal to the factorial of 3,  \u003cimg src=\"https://render.githubusercontent.com/render/math?math=3!\"\u003e , the number of permutations of 3 distinct objects. \u003c/p\u003e\n\n\u003cp\u003eHere,  \u003cimg src=\"https://render.githubusercontent.com/render/math?math=3!%20=%20(3%20*%202%20*%201)%20=%206\"\u003e . Notice that this is the same as writing  \u003cimg src=\"https://render.githubusercontent.com/render/math?math=3%20*%202!%20=%203%20*%20(2%20*%201)\"\u003e and  \u003cimg src=\"https://render.githubusercontent.com/render/math?math=3%20*%202%20*%201!%20=%203%20*%202%20*%20(1)\"\u003e . (By definition, the factorial of 1,  \u003cimg src=\"https://render.githubusercontent.com/render/math?math=1!\"\u003e , is equal to 1. The factorial of 0,  \u003cimg src=\"https://render.githubusercontent.com/render/math?math=0!\"\u003e is also defined to be equal to 1.)\u003c/p\u003e\n\n\u003cp\u003eWe can generalize this to the case of computing the factorial of an integer n,  \u003cimg src=\"https://render.githubusercontent.com/render/math?math=n!\"\u003e . The factorial of n,  \u003cimg src=\"https://render.githubusercontent.com/render/math?math=n!\"\u003e , can be written as  \u003cimg src=\"https://render.githubusercontent.com/render/math?math=n%20*%20(n-1)!\"\u003e , which itself can be written as  \u003cimg src=\"https://render.githubusercontent.com/render/math?math=n%20*%20(n-1)%20*%20(n-2)!\"\u003e . That is, we can define the factorial of n in terms of the product of  \u003cimg src=\"https://render.githubusercontent.com/render/math?math=n\"\u003e and the factorial of  \u003cimg src=\"https://render.githubusercontent.com/render/math?math=(n-1)\"\u003e , and the factorial  \u003cimg src=\"https://render.githubusercontent.com/render/math?math=(n-1)\"\u003e can be defined in terms of the product of  \u003cimg src=\"https://render.githubusercontent.com/render/math?math=(n-1)\"\u003e and the factorial of  \u003cimg src=\"https://render.githubusercontent.com/render/math?math=(n-2)\"\u003e , and so on and so forth, as seen in the equation below, until we get to  \u003cimg src=\"https://render.githubusercontent.com/render/math?math=1!\"\u003e , which is defined to be equal to 1: \u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=n!%20=%20n%20*%20(n-1)!%20=%20n%20*%20(n-1)%20*%20(n-2)!%20=%20...%20=%20n%20*%20(n-1)%20*%20(n-2)%20*%20%5Cldots%20*%202!%20=%20n*%20(n-1)%20*%20(n-2)%20*%20%5Cldots%20*%202%20*%201!\"\u003e \u003c/p\u003e\n\n\u003ch3\u003eRecursion\u003c/h3\u003e\n\n\u003cp\u003eWhen we define a function in terms of itself, in this case, the factorial of n in terms of the factorial of (n-1), we are using \u003cstrong\u003erecursion\u003c/strong\u003e.  Recursive functions are functions that can call themselves in order to loop until a condition is met. In the next lab, you'll get a glimpse on how to write a recursive function in Python, but in the Appendix to this Module, we go over recursive functions in Python in much more detail.\u003c/p\u003e\n\n\u003ch2\u003eSummary\u003c/h2\u003e\n\n\u003cp\u003eNow you're well on your way to calculate all sorts of permutations using factorials - both for understanding the sample space, subsets, etc! Let's move on for some practice!\u003c/p\u003e\n\u003cfooter class=\"fis-footer\" style=\"visibility: hidden;\"\u003e\u003cdiv class=\"fis-feedback\"\u003e\n\u003ch5\u003eHow do you feel about this lesson?\u003c/h5\u003e\n\u003cimg id=\"thumbs-up\" data-repository=\"dsc-permutations-and-factorials-v2-1\" title=\"Thumbs up!\" alt=\"thumbs up\"\u003e\u003cimg id=\"thumbs-down\" data-repository=\"dsc-permutations-and-factorials-v2-1\" title=\"Thumbs down!\" alt=\"thumbs down\"\u003e\n\u003c/div\u003e\n\u003ch5\u003eHave specific feedback? \u003ca href=\"https://github.com/learn-co-curriculum/dsc-permutations-and-factorials-v2-1/issues/new\"\u003eTell us here!\u003c/a\u003e\n\u003c/h5\u003e\u003c/footer\u003e","exportId":"permutations-and-factorials"},{"id":141261,"title":"Permutations and Factorials - Lab","type":"Assignment","indent":1,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_mark_done","completed":true,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-permutations-and-factorials-lab-v2-1\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-permutations-and-factorials-lab-v2-1/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","exportId":"ga5e233748c4f8ff678a69b36af55a7f3"},{"id":141264,"title":"Combinations","type":"WikiPage","indent":0,"locked":false,"requirement":"must_mark_done","completed":true,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-combinations\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-combinations/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eIn the previous section, you learned about how to apply permutations. Permutations come in handy when we want to know how many ways we can order sets. Now, what if order is not important? That's where \u003cem\u003ecombinations\u003c/em\u003e come in.\u003c/p\u003e\n\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\n\u003cp\u003eYou will be able to: \u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eDescribe how combinations are used when order is not important\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch2\u003eWhy combinations?\u003c/h2\u003e\n\n\u003cp\u003eIn some settings, the order of the selection is not important.\u003c/p\u003e\n\n\u003cp\u003eLet's go back to our example of a coverband creating a setlist. Imagine that the band is playing 3 songs out of their 8 song repertoire. How many ways can they select songs, assuming that the \u003cstrong\u003eorder of the chosen songs is not important\u003c/strong\u003e? Here, we just want to know \u003cem\u003ewhich\u003c/em\u003e three songs they play, and not which song goes first, second and last.\u003c/p\u003e\n\n\u003cp\u003eYou can use a backward rationale here. You know that when order \u003cem\u003edid\u003c/em\u003e matter, our answer was  \u003cimg src=\"https://render.githubusercontent.com/render/math?math=8%20*%207%20*%206\"\u003e . When having three elements, there are 6 possible orders (ABC, ACB, CAB, CBA, BAC, BCA), so the answer can be obtained by dividing our previous answer by 6. \u003c/p\u003e\n\n\u003cp\u003eThis type of problem can be solved by using \u003cem\u003ecombinations\u003c/em\u003e.\nIn general, combinations answer the question: \"How many ways can we create a subset  \u003cimg src=\"https://render.githubusercontent.com/render/math?math=k\"\u003e out of  \u003cimg src=\"https://render.githubusercontent.com/render/math?math=n\"\u003e objects?\". The subset is not ordered. \u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cdisplaystyle%5Cbinom%7Bn%7D%7Bk%7D%20=%20%5Cdfrac%7BP_%7Bk%7D%5E%7Bn%7D%7D%7Bk!%7D=%5Cdfrac%7B%20%5Cdfrac%7Bn!%7D%7B(n-k)!%7D%7D%7Bk!%7D%20=%20%5Cdfrac%7Bn!%7D%7B(n-k)!k!%7D\"\u003e \u003c/p\u003e\n\n\u003cp\u003eApplied to our example, this means that there are \u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cdfrac%7B8!%7D%7B(8-3)!3!%7D%20=%20%5Cdfrac%7B8!%7D%7B(8-3)!3!%7D%20=%5Cdfrac%7B%208*7*6%7D%7B6%7D%20=%2056\"\u003e \u003c/p\u003e\n\n\u003cp\u003eso there are 56 ways to choose 3 songs out of an 8 song repertoire.\u003c/p\u003e\n\n\u003ch2\u003eSummary\u003c/h2\u003e\n\n\u003cp\u003eIn this section, you learned what combinations are and how to use them. Let's put this knowledge into practice!\u003c/p\u003e\n\u003cfooter class=\"fis-footer\" style=\"visibility: hidden;\"\u003e\u003cdiv class=\"fis-feedback\"\u003e\n\u003ch5\u003eHow do you feel about this lesson?\u003c/h5\u003e\n\u003cimg id=\"thumbs-up\" data-repository=\"dsc-combinations\" title=\"Thumbs up!\" alt=\"thumbs up\"\u003e\u003cimg id=\"thumbs-down\" data-repository=\"dsc-combinations\" title=\"Thumbs down!\" alt=\"thumbs down\"\u003e\n\u003c/div\u003e\n\u003ch5\u003eHave specific feedback? \u003ca href=\"https://github.com/learn-co-curriculum/dsc-combinations/issues/new\"\u003eTell us here!\u003c/a\u003e\n\u003c/h5\u003e\u003c/footer\u003e","exportId":"combinations"},{"id":141267,"title":"Combinations - Lab","type":"Assignment","indent":1,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_mark_done","completed":true,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-combinations-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-combinations-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","exportId":"g387b78b9456a87e090681645f7516dd3"},{"id":141269,"title":"Conditional Probability","type":"WikiPage","indent":0,"locked":false,"requirement":"must_mark_done","completed":false,"content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-conditional-probability\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-conditional-probability\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-conditional-probability/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eIn the previous lessons and labs, you learned about some fundamentals of probability theory, along with basic combinatorics such as permutations and combinations. You'll now extend your knowledge of probability by learning about \u003cstrong\u003eConditional Probability\u003c/strong\u003e. You'll see how Conditional Probability is extremely important in Statistics, and the foundation of many applications. Understanding conditional probability is essential when exploring fields in Machine Learning and Artificial Intelligence.\u003c/p\u003e\n\u003cp\u003eIn this lesson, you'll learn about conditional probability, what it is, and how and when to use it. Later on, you'll see how this simple idea becomes a key component in most statistical machine learning algorithms.\u003c/p\u003e\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\u003cp\u003eYou will be able to:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eDifferentiate between independent and dependent events\u003c/li\u003e\n\u003cli\u003eUse the multiplication rule to find the probability of the intersection of two events\u003c/li\u003e\n\u003cli\u003eUse conditional probability to explain the Product Rule, Chain Rule, and Bayes Theorem\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003eEvents and Sample Space\u003c/h2\u003e\n\u003cp\u003eBefore introducing you to specific event types, let's do a quick recap of the notion of event and sample space.\u003c/p\u003e\n\u003cp\u003eAn \u003cstrong\u003eevent\u003c/strong\u003e is the outcome of an experiment, for example, obtaining heads when tossing of a coin or getting 3 after a dice roll. Note: an event can also be a collection of different events grouped together (or a so-called \u003cstrong\u003ecompound\u003c/strong\u003e event), e.g. getting a 3 twice when rolling a dice twice.\u003c/p\u003e\n\u003cp\u003eA \u003cstrong\u003esample space\u003c/strong\u003e is a collection of every single possible outcome in a trial, generally represented by \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5COmega\"\u003e . The sample space for 1 random dice throw is {1,2,3,4,5,6}.\u003c/p\u003e\n\u003cp\u003eAs you remember for the previous lessons, we can combine events and sample space to compute event probability.\u003c/p\u003e\n\u003cp\u003eYou'll learn about 3 important event types: \u003cstrong\u003eindependent\u003c/strong\u003e, \u003cstrong\u003edisjoint\u003c/strong\u003e, and \u003cstrong\u003edependent\u003c/strong\u003e events.\u003c/p\u003e\n\u003ch3\u003eIndependent Events\u003c/h3\u003e\n\u003cp\u003e\u003cstrong\u003eEvents \u003cimg src=\"https://render.githubusercontent.com/render/math?math=A\"\u003e and \u003cimg src=\"https://render.githubusercontent.com/render/math?math=B\"\u003e are independent when the occurrence of \u003cimg src=\"https://render.githubusercontent.com/render/math?math=A\"\u003e has no effect on whether \u003cimg src=\"https://render.githubusercontent.com/render/math?math=B\"\u003e will occur (or not).\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eConsider the following independent events\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eGetting heads after flipping a coin \u003cstrong\u003eand\u003c/strong\u003e getting a 5 after throwing a fair dice\u003c/li\u003e\n\u003cli\u003eChoosing a marble from a container \u003cstrong\u003eand\u003c/strong\u003e getting heads after flipping a coin\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch4\u003eTwo independent events\u003c/h4\u003e\n\u003cp\u003eFormally, events A and B are independent if:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=P(A%20%20%5Ccap%20B)%20=%20P(A)P(B)\"\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThe probability of A or B occurring, \u003cimg src=\"https://render.githubusercontent.com/render/math?math=P%20(A%20%20%5Ccup%20B)\"\u003e , is given by the addition rule of probability:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=P%20(A%20%20%5Ccup%20B)%20=%20P(A)%20%2b%20P(B)%20-%20P(A%20%20%5Ccap%20B)\"\u003e\u003c/p\u003e\n\u003cp\u003eWe subtract the intersection of the two events to avoid over-counting. See the diagram below for some intuition:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-conditional-probability/master/images/Image_67_independent.png\" width=\"500\"\u003e\u003c/p\u003e\n\u003cp\u003eThus, in the case of two independent events, by substitution,\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=P%20(A%20%20%5Ccup%20B)%20=%20P(A)%20%2b%20P(B)%20-%20P(A)P(B).\"\u003e\u003c/p\u003e\n\u003ch4\u003eThree independent events\u003c/h4\u003e\n\u003cp\u003eThree events, A, B and C, are independent if:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=P(A%20%20%5Ccap%20B)%20=%20P(A)P(B)\"\u003e\u003c/li\u003e\n\u003cli\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=P(A%20%20%5Ccap%20C)%20=%20P(A)P(C)\"\u003e\u003c/li\u003e\n\u003cli\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=P(B%20%20%5Ccap%20C)%20=%20P(B)P(C)\"\u003e\u003c/li\u003e\n\u003cli\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=P(A%20%20%5Ccap%20B%20%20%5Ccap%20C)%20=%20P(A)P(B)P(C)\"\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eSo you need both \u003cem\u003epairwise independence\u003c/em\u003e and \u003cem\u003ethree-way independence\u003c/em\u003e\u003c/p\u003e\n\u003ch3\u003eDisjoint Events\u003c/h3\u003e\n\u003cp\u003e\u003cstrong\u003eEvents \u003cimg src=\"https://render.githubusercontent.com/render/math?math=A\"\u003e and \u003cimg src=\"https://render.githubusercontent.com/render/math?math=B\"\u003e are disjoint if \u003cimg src=\"https://render.githubusercontent.com/render/math?math=A\"\u003e occurring means that \u003cimg src=\"https://render.githubusercontent.com/render/math?math=B\"\u003e cannot occur.\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eDisjoint events are \u003cstrong\u003emutually exclusive\u003c/strong\u003e. \u003cimg src=\"https://render.githubusercontent.com/render/math?math=P%20(A%20%20%5Ccap%20B)\"\u003e is \u003cstrong\u003eempty\u003c/strong\u003e.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-conditional-probability/master/images/Image_68Disjoint.png\" width=\"500\"\u003e\u003c/p\u003e\n\u003ch3\u003eDependent Events\u003c/h3\u003e\n\u003cp\u003e\u003cstrong\u003eEvents \u003cimg src=\"https://render.githubusercontent.com/render/math?math=A\"\u003e and \u003cimg src=\"https://render.githubusercontent.com/render/math?math=B\"\u003e are dependent when the occurrence of \u003cimg src=\"https://render.githubusercontent.com/render/math?math=A\"\u003e somehow has an effect on whether \u003cimg src=\"https://render.githubusercontent.com/render/math?math=B\"\u003e will occur (or not).\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eNow things start getting a bit more interesting.\u003c/p\u003e\n\u003cp\u003eLet's look at an example. Let's say event \u003cimg src=\"https://render.githubusercontent.com/render/math?math=A\"\u003e is taking an orange or purple marble out of a jar. The jar contains 3 orange and 2 purple marbles.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-conditional-probability/master/images/Image_69_Marb.png\" width=\"300\"\u003e\u003c/p\u003e\n\u003cp\u003eThe probability of getting a purple marble is \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cdfrac%7B2%7D%7B5%7D\"\u003e and getting an orange marble is \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cdfrac%7B3%7D%7B5%7D\"\u003e .\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-conditional-probability/master/images/Image_70_Cond3.png\" width=\"300\"\u003e\u003c/p\u003e\n\u003cp\u003eAt that point, one marble is taken out and we now take another marble from the jar (event \u003cimg src=\"https://render.githubusercontent.com/render/math?math=B\"\u003e ).\u003c/p\u003e\n\u003cp\u003eHere you can see that our second event is dependent on the outcome of the first draw.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eIf we drew an orange marble first, the probability of getting a purple marble for event B is \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cdfrac%7B2%7D%7B4%7D\"\u003e .\u003c/li\u003e\n\u003cli\u003eIf we saw a purple marble first, however, the probability of seeing a purple in the second trial is \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cdfrac%7B1%7D%7B4%7D\"\u003e .\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eIn simple terms, the probability of seeing an event \u003cimg src=\"https://render.githubusercontent.com/render/math?math=B\"\u003e in the second trial depends on the outcome \u003cimg src=\"https://render.githubusercontent.com/render/math?math=A\"\u003e of the first trial. We say that \u003cimg src=\"https://render.githubusercontent.com/render/math?math=P(B)\"\u003e is \u003cstrong\u003econditional\u003c/strong\u003e on \u003cimg src=\"https://render.githubusercontent.com/render/math?math=P(A)\"\u003e .\u003c/p\u003e\n\u003cp\u003eA \u003cstrong\u003etree diagram\u003c/strong\u003e can be used to explore all possible events.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-conditional-probability/master/images/Image_71_TreeDiag.png\" width=\"500\"\u003e\u003c/p\u003e\n\u003ch2\u003eConditional Probability\u003c/h2\u003e\n\u003cp\u003e\u003cstrong\u003eConditional probability emerges when the outcome a trial may influence the results of the upcoming trials.\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eWhile calculating the probability of the second event (event \u003cimg src=\"https://render.githubusercontent.com/render/math?math=B\"\u003e ) given that the primary event (event \u003cimg src=\"https://render.githubusercontent.com/render/math?math=A\"\u003e ) has just happened, we say that the probability of event \u003cimg src=\"https://render.githubusercontent.com/render/math?math=B\"\u003e relies on the occurrence of event \u003cimg src=\"https://render.githubusercontent.com/render/math?math=A\"\u003e .\u003c/p\u003e\n\u003cp\u003eHere are some more examples:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eDrawing a 2nd Ace from a deck of cards given that the first card you drew was an Ace.\u003c/li\u003e\n\u003cli\u003eFinding the probability of liking \"The Matrix\" given that you know this person likes science fiction.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eLet's say that \u003cimg src=\"https://render.githubusercontent.com/render/math?math=P(A)\"\u003e is the event we are interested in, and this event depends on a certain event \u003cimg src=\"https://render.githubusercontent.com/render/math?math=B\"\u003e that has happened.\u003c/p\u003e\n\u003cp\u003eThe conditional probability (Probability of \u003cimg src=\"https://render.githubusercontent.com/render/math?math=A\"\u003e \u003cstrong\u003egiven\u003c/strong\u003e \u003cimg src=\"https://render.githubusercontent.com/render/math?math=B\"\u003e ) can be written as:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=P%20(A%20%5Cmid%20B)%20=%20%5Cdfrac%7BP(A%20%20%5Ccap%20B)%7D%7BP(B)%7D\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=P(A%7CB)\"\u003e , is the probability A \u003cstrong\u003egiven\u003c/strong\u003e that \u003cimg src=\"https://render.githubusercontent.com/render/math?math=B\"\u003e has just happened.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-conditional-probability/master/images/Image_72_Cond4.png\" width=\"300\"\u003e\u003c/p\u003e\n\u003cp\u003eUnderstanding this formula may be easier if you look at two simple Venn Diagrams and use the multiplication rule. Here's how to derive this formula:\u003c/p\u003e\n\u003cp\u003eStep 1: Write out the multiplication rule:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=P(A%20%20%5Ccap%20B)=%20P(B)*P(A%5Cmid%20B)\"\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eStep 2: Divide both sides of the equation by P(B):\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cdfrac%7BP(A%20%20%5Ccap%20B)%7D%7B%20P(B)%7D%20=%20%5Cdfrac%7BP(B)*P(A%5Cmid%20B)%7D%7BP(B)%7D\"\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eStep 3: Cancel P(B) on the right side of the equation:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cdfrac%7BP(A%20%20%5Ccap%20B)%7D%7BP(B)%7D%20=%20P(A%20%5Cmid%20B)\"\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eStep 4: This is of course equal to:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=P(A%20%5Cmid%20B)=%5Cdfrac%7BP(A%20%20%5Ccap%20B)%7D%7BP(B)%7D\"\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eAnd this is our conditional probability formula.\u003c/p\u003e\n\u003cp\u003eThere are a few variations and theorems that are related to and/or results of this conditional probability formula. The most important ones are: the \u003cstrong\u003eproduct rule\u003c/strong\u003e, the \u003cstrong\u003echain rule\u003c/strong\u003e and \u003cstrong\u003eBayes Theorem\u003c/strong\u003e\u003c/p\u003e\n\u003ch3\u003eTheorem 1 - Product Rule\u003c/h3\u003e\n\u003cp\u003eThe \u003cstrong\u003eproduct rule\u003c/strong\u003e was used to derive the conditional probability formula above, but is often used in situations where the conditional probability is easy to compute, but the probability of intersections of events isn't.\u003c/p\u003e\n\u003cp\u003eThe intersection of events \u003cimg src=\"https://render.githubusercontent.com/render/math?math=A\"\u003e and \u003cimg src=\"https://render.githubusercontent.com/render/math?math=B\"\u003e can be given by:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=P(A%20%5Ccap%20B)%20=%20P(B)%20P(A%20%5Cmid%20B)%20=%20P(A)%20P(B%20%5Cmid%20A)\"\u003e\u003c/p\u003e\n\u003cp\u003eRemember that if \u003cimg src=\"https://render.githubusercontent.com/render/math?math=A\"\u003e and \u003cimg src=\"https://render.githubusercontent.com/render/math?math=B\"\u003e are independent, then conditioning on \u003cimg src=\"https://render.githubusercontent.com/render/math?math=B\"\u003e means nothing (and vice-versa) so \u003cimg src=\"https://render.githubusercontent.com/render/math?math=P(A%7CB)%20=%20P(A)\"\u003e , and \u003cimg src=\"https://render.githubusercontent.com/render/math?math=P(A%20%20%5Ccap%20B)%20=%20P(A)%20P(B)\"\u003e .\u003c/p\u003e\n\u003ch3\u003eTheorem 2 - Chain Rule\u003c/h3\u003e\n\u003cp\u003eThe \u003cstrong\u003echain rule\u003c/strong\u003e (also called the \u003cstrong\u003egeneral product rule\u003c/strong\u003e) permits the calculation of any member of the joint distribution of a set of random variables using only conditional probabilities.\u003c/p\u003e\n\u003cp\u003eRecall the product rule:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=P(A%20%20%5Ccap%20B)%20=%20P(A%20%5Cmid%20B)%20P(B)\"\u003e\u003c/p\u003e\n\u003cp\u003eWhen you extend this for three variables:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=P(A%20%5Ccap%20B%20%20%5Ccap%20C)%20=%20P(A%20%5Ccap(%20B%20%20%5Ccap%20C))%20=%20P(A%5Cmid%20B%20%20%5Ccap%20C)%20P(B%20%20%5Ccap%20C)%20=%20P(A%20%5Cmid%20B%20%20%5Ccap%20C)%20P(B%20%5Cmid%20C)%20P(C)\"\u003e\u003c/p\u003e\n\u003cp\u003eAnd you can keep extending this to \u003cimg src=\"https://render.githubusercontent.com/render/math?math=n\"\u003e variables:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=P(A_1%20%20%5Ccap%20A_2%20%20%5Ccap%20%5Cldots%20%20%5Ccap%20A_n)%20=%20P(A_1%20%5Cmid%20A_2%20%20%5Ccap%20%5Cldots%20%5Ccap%20A_n)%20P(A_2%20%5Cmid%20A_3%20%20%20%5Ccap%20%5Cldots%20%20%5Ccap%20%5C%20A_n)%20P(A_%7Bn-1%7D%7CA_n)%20P(A_n)\"\u003e\u003c/p\u003e\n\u003cp\u003eThis idea is known as the \u003cstrong\u003echain rule\u003c/strong\u003e.\u003c/p\u003e\n\u003cp\u003eIf on the other hand you have disjoint events \u003cimg src=\"https://render.githubusercontent.com/render/math?math=C_1,%20C_2,...,C_m\"\u003e such that \u003cimg src=\"https://render.githubusercontent.com/render/math?math=C_1%20%5Ccup%20C_2%20%5Ccup%20%C2%B7%C2%B7%C2%B7%20%20%5Ccup%20%20C_m%20=%20%5COmega\"\u003e , the probability of any event can be decomposed as:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=P(A)%20=%20P(A%20%5Cmid%20C_1)P(C_1)%20+%20P(A%20%5Cmid%20C_2)P(C_2)%20+%20%5Cldots%20+%20P(A%20%5Cmid%20C_m)P(C_m)\"\u003e\u003c/p\u003e\n\u003ch3\u003eTheorem 3 - Bayes Theorem\u003c/h3\u003e\n\u003cp\u003eThe \u003cstrong\u003eBayes theorem\u003c/strong\u003e, which is the outcome of this section. Below is the formula that we will dig deeper into in upcoming lessons.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=P(A%7CB)%20=%20%5Cfrac%20%7BP(B%7CA)P(A)%7D%7BP(B)%7D%20%5Ctext%7B%20%C2%A0%C2%A0%C2%A0%20-this%20follows%20from%20Theorem%201%7D\"\u003e\u003c/p\u003e\n\u003ch3\u003eAdditional note: the complement of an event\u003c/h3\u003e\n\u003cp\u003eYou learned about (absolute and relative) complements before, but the complement of an event is also applicable to conditional probabilities.\u003c/p\u003e\n\u003cp\u003eThe basic rule is:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=P(A)%20%2b%20P(A')%20=%201\"\u003e\u003c/p\u003e\n\u003cp\u003ewith A' being the complement of A.\u003c/p\u003e\n\u003cp\u003eSimilarly, extending this to conditional probabilities:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=P(A%7CB)%20%2b%20P(A'%7CB)%20=%201\"\u003e\u003c/p\u003e\n\u003ch2\u003eExample: An Aspiring Data Scientist's Dilemma\u003c/h2\u003e\n\u003cp\u003eLet's see a very simple use of the conditional probability formula. A data scientist comes across the following infographic:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-conditional-probability/master/images/Image_73_Mood.png\" width=\"600\"\u003e\u003c/p\u003e\n\u003cp\u003eCurious as data scientists are, he starts collecting data about weather conditions and his own mood.\u003c/p\u003e\n\u003cp\u003eConsider the data in the following table, recorded over a month with 50 days by our data scientist. On each day he recorded whether it was sunny or Cloudy, and whether his mood was good or not.\u003c/p\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003e\u0026nbsp;\u003c/th\u003e\n\u003cth\u003eSunny weather\u003c/th\u003e\n\u003cth\u003eCloudy weather\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd\u003eGood mood\u003c/td\u003e\n\u003ctd\u003e14\u003c/td\u003e\n\u003ctd\u003e11\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eBad mood\u003c/td\u003e\n\u003ctd\u003e2\u003c/td\u003e\n\u003ctd\u003e23\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003cp\u003eHe wants to now know if his mood had anything to do with the weather on a particular day and how he can calculate the probability of having a good mood given the weather conditions.\u003c/p\u003e\n\u003ch3\u003eIf he picked a day at random from the 50 days on record, what is the probability that he was in a good mood on that day, \u003cimg src=\"https://render.githubusercontent.com/render/math?math=P(G)\"\u003e ?\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eThe sample space is 50 days here\u003c/li\u003e\n\u003cli\u003eThe event space is \"good mood\", so \u003cimg src=\"https://render.githubusercontent.com/render/math?math=14%20%2b%2011%20=%2025\"\u003e .\u003c/li\u003e\n\u003cli\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=P(G)%20=%20%5Cdfrac%7B25%7D%7B50%7D%20=%200.5\"\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003eWhat is the probability that the day chosen was a Sunny day, \u003cimg src=\"https://render.githubusercontent.com/render/math?math=P(S)\"\u003e ?\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eThe sample space is still 50 days\u003c/li\u003e\n\u003cli\u003eIt was sunny on \u003cimg src=\"https://render.githubusercontent.com/render/math?math=14%20%2b%202%20=%2016\"\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=P(S)%20=%20%5Cdfrac%7B16%7D%7B50%7D%20=%200.32\"\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003eWhat is the probability of having a good mood given it's a sunny day \u003cimg src=\"https://render.githubusercontent.com/render/math?math=P(G%20%5Cmid%20S)\"\u003e ?\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cimg src=\"https://render.githubusercontent.com/render/math?math=P(G%20%5Cmid%20S)%20=%20%5Cdfrac%7BP(G%20%20%5Ccap%20S)%7D%7B%20P(S)%7D\"\u003e , so we need to calculate \u003cimg src=\"https://render.githubusercontent.com/render/math?math=P(G%20%20%5Ccap%20S)\"\u003e first.\u003c/li\u003e\n\u003cli\u003e\n\u003cimg src=\"https://render.githubusercontent.com/render/math?math=P(G%20%20%5Ccap%20S)\"\u003e consists of sunny days in which he is in a good mood. There were 14 of them, so \u003cimg src=\"https://render.githubusercontent.com/render/math?math=P(G%20%20%5Ccap%20S)%20=%20%5Cdfrac%7B14%7D%7B50%7D\"\u003e\n\u003c/li\u003e\n\u003cli\u003eTherefore \u003cimg src=\"https://render.githubusercontent.com/render/math?math=P(G%20%5Cmid%20S)%20=%5Cdfrac%7B%5Cfrac%7B14%7D%7B50%7D%7D%7B%5Cfrac%7B16%7D%7B50%7D%7D%20=%200.875\"\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThe infographic had some truth in it indeed. There's a 87.5% chance that our curious data scientist would be in good mood on a sunny day.\u003c/p\u003e\n\u003cp\u003eThe data scientist is satisfied and thinks the outcome is comforting.\u003c/p\u003e\n\u003cp\u003eSurfing the Internet, however, he comes across a Garth Stein quote. Although not very scientific, this raises his curiosity further. The quote goes as follows:\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\"That which is around me does not affect my mood; my mood affects that which is around me\"\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eWhat if...?\u003c/p\u003e\n\u003ch3\u003eNow the data scientist wants to know if his mood had any impact on the weather. What is \u003cimg src=\"https://render.githubusercontent.com/render/math?math=P(S%20%5Cmid%20G)\"\u003e ?\u003c/h3\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=P(S%20%5Cmid%20G)%20=%5Cdfrac%7BP(G%20%20%5Ccap%20S)%7D%7B%20P(G)%7D%20=%20%5Cdfrac%7B%5Cfrac%7B14%7D%7B50%7D%7D%7B%5Cfrac%7B25%7D%7B50%7D%7D%20=%200.56\"\u003e\u003c/p\u003e\n\u003cp\u003eHe finds that the probability is slightly higher than random chance (50%). In other words, there's a 56% chance that it will be all nice and sunny given that he is in a good mood.\u003c/p\u003e\n\u003cp\u003eHe also realizes that \u003cimg src=\"https://render.githubusercontent.com/render/math?math=P(G%20%5Cmid%20S)\"\u003e is not equal to \u003cimg src=\"https://render.githubusercontent.com/render/math?math=P(S%20%5Cmid%20G)\"\u003e . So does this mean that weather has a higher impact on his mood than his mood has on the weather...?\u003c/p\u003e\n\u003cp\u003eThis doesn't really make sense. Our mood doesn't \u003cem\u003ecause\u003c/em\u003e the weather, so there is no cause-effect relationship. In the example above, the weather and other such external conditions can have a positive effect on human mood and behavior, and this can be said with reference to literature. However, it is unlikely that mood has any effect on weather. There is no scientific evidence to support this notion (and it's very unlikely that there will ever be). What is clear, however, is that there is a relationship between weather and mood.\u003c/p\u003e\n\u003ch3\u003eSay Hello to Reverend Thomas Bayes\u003c/h3\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-conditional-probability/master/images/Thomas_Bayes.gif\" width=\"300\"\u003e\u003c/p\u003e\n\u003cp\u003eBayes theorem is a very foundational theorem that uses the fact that \u003cimg src=\"https://render.githubusercontent.com/render/math?math=P(A%20%20%5Ccap%20B)%20=%20P(B)%20P(A%20%5Cmid%20B)%20=%20P(A)%20P(B%20%5Cmid%20A)\"\u003e . Note that, using Bayes theorem, you can compute conditional probabilities without explicitly needing to know \u003cimg src=\"https://render.githubusercontent.com/render/math?math=P(A%20%20%5Ccap%20B)\"\u003e !\u003c/p\u003e\n\u003cp\u003eThis theorem is extremely important in many machine learning algorithms.\u003c/p\u003e\n\u003cp\u003eOur data scientist realizes that he needs to learn a bit of Bayesian reasoning in order to get more meaningful results. And that is exactly what we will discuss further. First, we need to cover a few topics to fully understand how this simple equation lets you do some serious predictive analysis.\u003c/p\u003e\n\u003cp\u003eYou'll do a few exercises next to get a good grip on conditional probability calculations.\u003c/p\u003e\n\u003ch2\u003eAdditional Resources\u003c/h2\u003e\n\u003cp\u003eYou are strongly advised to visit the following links to get an in-depth understanding with examples and proofs for explaining the formulas highlighted in this lesson.\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"http://faculty.arts.ubc.ca/vmarmer/econ327/327_02_cond_probability.pdf\"\u003eConditional probability, Independence and Bayes rule\u003c/a\u003e - A deeper mathematical explanation around Independence and theorems we have seen above (and some we shall cover in upcoming lessons)\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://www.mathsisfun.com/data/probability-tree-diagrams.html\"\u003eTree Diagrams\u003c/a\u003e - Drawing tree diagrams to calculate conditional probability\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://www.mathgoodies.com/lessons/vol6/conditional\"\u003eConditional Probability, Examples and simple exercises\u003c/a\u003e - Practice with probability calculations\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"http://setosa.io/conditional/\"\u003eConditional probability: A visual explanation\u003c/a\u003e - A great little interactive animation to explain how conditional probability works\u003c/p\u003e\n\u003ch2\u003eSummary\u003c/h2\u003e\n\u003cp\u003eIn this lesson, you learned about disjoint, independent, and dependent events, and how to use the addition and multiplication rule to find the probability of the union and intersection of two events, respectively. You also learned how to compute conditional probabilities in case you have dependent events in your sample space, with a step-by-step derivation of the formula used to compute conditional probabilities. You also worked through an example to see this concept in action. Finally, Bayes' theorem was discussed. Later in the course, you'll build further on these ideas towards having a clear understanding of Bayesian Logic and its role in machine learning. Next up, you'll practice solving problems with conditional probability!\u003c/p\u003e","exportId":"conditional-probability"},{"id":141271,"title":"Conditional Probability - Lab","type":"Assignment","indent":1,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_mark_done","completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-conditional-probability-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-conditional-probability-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","exportId":"g47df50e52a73ae6dd373534fff104399"},{"id":141273,"title":"Partitioning and the Law of Total Probability","type":"WikiPage","indent":0,"locked":false,"requirement":"must_mark_done","completed":false,"content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-law-of-total-probability\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-law-of-total-probability\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-law-of-total-probability/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eIn this lesson, we'll look at the law of total probability. In probability theory, the law (or formula) of total probability is a fundamental rule relating \u003cstrong\u003emarginal probabilities\u003c/strong\u003e to conditional probabilities. It expresses the total probability of an outcome that can be realized via several distinct events.\u003c/p\u003e\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\u003cp\u003eYou will be able to:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eState the law of total probabilities based on a partitioned event space\u003c/li\u003e\n\u003cli\u003eExplain the concept of event space and partitioning\u003c/li\u003e\n\u003cli\u003eDescribe conditional independence\u003c/li\u003e\n\u003cli\u003ePerform partitioning based on known and unknown probabilities to solve a problem\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003ePartitioning a Sample Space\u003c/h2\u003e\n\u003cp\u003ethe Law of Total Probability can be used to calculate \u003cimg src=\"https://render.githubusercontent.com/render/math?math=P(B)\"\u003e . The law requires that you have a set of disjoint events \u003cimg src=\"https://render.githubusercontent.com/render/math?math=A_i\"\u003e that collectively \"cover\" the event \u003cimg src=\"https://render.githubusercontent.com/render/math?math=B\"\u003e . Then, instead of calculating \u003cimg src=\"https://render.githubusercontent.com/render/math?math=P(B)\"\u003e directly, you add up the intersection of \u003cimg src=\"https://render.githubusercontent.com/render/math?math=B\"\u003e with each of the events \u003cimg src=\"https://render.githubusercontent.com/render/math?math=A_i\"\u003e . Let's see this graphically below:\u003c/p\u003e\n\u003cp\u003eLet \u003cimg src=\"https://render.githubusercontent.com/render/math?math=A_1,%20A_2,%20%5Cdots,%20A_n\"\u003e partition sample space \u003cimg src=\"https://render.githubusercontent.com/render/math?math=S\"\u003e into disjoint regions that sum up to \u003cimg src=\"https://render.githubusercontent.com/render/math?math=S\"\u003e . In the example, the four regions \u003cimg src=\"https://render.githubusercontent.com/render/math?math=A_1,%20A_2,%20A_3\"\u003e and \u003cimg src=\"https://render.githubusercontent.com/render/math?math=A_4\"\u003e sum up to sample space \u003cimg src=\"https://render.githubusercontent.com/render/math?math=S\"\u003e .\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-law-of-total-probability/master/images/Image_55_TotProb.png\" width=\"500\"\u003e\u003c/p\u003e\n\u003cp\u003eThe probability of a random event \u003cimg src=\"https://render.githubusercontent.com/render/math?math=B\"\u003e (orange area) can be written down as:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=P(B)\"\u003e = \u003cimg src=\"https://render.githubusercontent.com/render/math?math=P(B%20%5Ccap%20A1)%20%2b%20P(B%20%5Ccap%20A2)%20%2b%20P(B%20%5Ccap%20A3)%20%2b%20P(B%20%5Ccap%20A4)\"\u003e = \u003cimg src=\"https://render.githubusercontent.com/render/math?math=P(B%20%5Cmid%20A1)P(A1)%20%2b%20P(B%20%5Cmid%20A2)P(A2)%20%2b%20P(B%20%5Cmid%20A3)P(A3)%20%2b%20P(B%20%5Cmid%20A4)P(A4)\"\u003e\u003c/p\u003e\n\u003cp\u003eHere we use the first theorem mentioned in the previous lesson to find the combined probabilities.\u003c/p\u003e\n\u003ch3\u003eExample\u003c/h3\u003e\n\u003cp\u003eLet's use a simple example to clarify the image above! The example is created to match the image.\u003c/p\u003e\n\u003cp\u003eIn a certain country, there are four provinces (eg. disjoint regions) \u003cimg src=\"https://render.githubusercontent.com/render/math?math=A_1,%20A_2\"\u003e , \u003cimg src=\"https://render.githubusercontent.com/render/math?math=A_3\"\u003e and \u003cimg src=\"https://render.githubusercontent.com/render/math?math=A_4\"\u003e .\u003c/p\u003e\n\u003cp\u003eYou are interested in the total forest area, \u003cimg src=\"https://render.githubusercontent.com/render/math?math=B\"\u003e , in the country.\u003c/p\u003e\n\u003cp\u003eSuppose that you know that the forest area in \u003cimg src=\"https://render.githubusercontent.com/render/math?math=A_1\"\u003e , \u003cimg src=\"https://render.githubusercontent.com/render/math?math=A_2\"\u003e , and \u003cimg src=\"https://render.githubusercontent.com/render/math?math=A_3\"\u003e are 100\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Ctext%7Bkm%7D%5E2\"\u003e , 50\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Ctext%7Bkm%7D%5E2\"\u003e , and 150\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Ctext%7Bkm%7D%5E2\"\u003e , and 0\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Ctext%7Bkm%7D%5E2\"\u003e respectively. What is the total forest area in the country?\u003c/p\u003e\n\u003cp\u003e100\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Ctext%7Bkm%7D%5E2\"\u003e + 50\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Ctext%7Bkm%7D%5E2\"\u003e + 150\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Ctext%7Bkm%7D%5E2\"\u003e + 0\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Ctext%7Bkm%7D%5E2\"\u003e = 300\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Ctext%7Bkm%7D%5E2\"\u003e\u003c/p\u003e\n\u003cp\u003eWe can simply add forest areas in each province to obtain the forest area in the whole country.\u003c/p\u003e\n\u003cp\u003eThis is the idea behind the law of total probability, in which the area of forest is replaced by probability of an event \u003cimg src=\"https://render.githubusercontent.com/render/math?math=B\"\u003e . In particular, if you want to find \u003cimg src=\"https://render.githubusercontent.com/render/math?math=P(B)\"\u003e , you can look at a partition of \u003cimg src=\"https://render.githubusercontent.com/render/math?math=S\"\u003e (our sample space composed of \u003cimg src=\"https://render.githubusercontent.com/render/math?math=A_1,%5Cldots,%20A_4\"\u003e ), and add the amount of probability of \u003cimg src=\"https://render.githubusercontent.com/render/math?math=A\"\u003e that falls in each partition.\u003c/p\u003e\n\u003ch3\u003eTwo Events\u003c/h3\u003e\n\u003cp\u003eIn general, we can say that for any two events \u003cimg src=\"https://render.githubusercontent.com/render/math?math=A\"\u003e and \u003cimg src=\"https://render.githubusercontent.com/render/math?math=B\"\u003e :\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=P(A)=P(A%20%20%5Ccap%20B)%2bP(A%20%20%5Ccap%20B')\"\u003e\u003c/p\u003e\n\u003cp\u003eand using the definition of conditional probability, \u003cimg src=\"https://render.githubusercontent.com/render/math?math=P(A%20%20%5Ccap%20B)=P(A%20%5Cmid%20B)P(B)\"\u003e , we can write\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=P(A)=P(A%20%5Cmid%20B)P(B)%2bP(A%20%5Cmid%20B')P(B')\"\u003e\u003c/p\u003e\n\u003cp\u003eThe law of total probability is basically a general version of this.\u003c/p\u003e\n\u003ch2\u003eLaw of Total Probability\u003c/h2\u003e\n\u003cp\u003eIf \u003cimg src=\"https://render.githubusercontent.com/render/math?math=B_1\"\u003e , \u003cimg src=\"https://render.githubusercontent.com/render/math?math=B_2\"\u003e , \u003cimg src=\"https://render.githubusercontent.com/render/math?math=B_3\"\u003e , \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cdots\"\u003e is a partition of the sample space S, then for any event A we have\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=P(A)=%20%5Csum_i%20P(A%20%20%5Ccap%20B_i)=%20%5Csum_i%20P(A%20%5Cmid%20B_i)P(B_i)\"\u003e\u003c/p\u003e\n\u003cp\u003eUsing a Venn diagram, we can pictorially see the idea behind the law of total probability. In the figure below, we have\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=A_1%20=%20A%20%20%5Ccap%20B_1\"\u003e\u003c/li\u003e\n\u003cli\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=A_2%20=%20A%20%20%5Ccap%20B_2\"\u003e\u003c/li\u003e\n\u003cli\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=A_3%20=%20A%20%20%5Ccap%20B_3\"\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-law-of-total-probability/master/images/Image_56_vent.png\" width=\"400\"\u003e\u003c/p\u003e\n\u003cp\u003eAs it can be seen from the figure, \u003cimg src=\"https://render.githubusercontent.com/render/math?math=A_1\"\u003e , \u003cimg src=\"https://render.githubusercontent.com/render/math?math=A_2\"\u003e , and \u003cimg src=\"https://render.githubusercontent.com/render/math?math=A_3\"\u003e form a partition of the set A, and thus\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=P(A)=P(A_1)%2bP(A_2)%2bP(A_3)\"\u003e\u003c/p\u003e\n\u003cp\u003eHere is a typical scenario in which we use the law of total probability. We are interested in finding the probability of an event \u003cimg src=\"https://render.githubusercontent.com/render/math?math=A\"\u003e , but we don't know how to find P(A) directly. Instead, we know the conditional probability of \u003cimg src=\"https://render.githubusercontent.com/render/math?math=A\"\u003e given some events \u003cimg src=\"https://render.githubusercontent.com/render/math?math=B_i\"\u003e , where the \u003cimg src=\"https://render.githubusercontent.com/render/math?math=B_i\"\u003e 's form a partition of the sample space. This way, you can use \u003cimg src=\"https://render.githubusercontent.com/render/math?math=P(A)\"\u003e using the law of total probability\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=P(A)=%5Csum_i%20P(A%20%5Cmid%20B_i)P(B_i)\"\u003e\u003c/p\u003e\n\u003ch2\u003eMore on Partitions\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eThe natural numbers \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cmathbb%7BN%7D\"\u003e can be partitioned into even and odd numbers.\u003c/li\u003e\n\u003cli\u003eThe set of animal species in the world can be partitioned into subsets where a subset reflects a continent and each species is positioned in a subset depending on which continent they originated from.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eIn statistics, choosing the right partitioning is key as bad choices of partitions may results in many sub-problems that are even more difficult to solve.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-law-of-total-probability/master/images/Image_57_TotProb_2.png\" width=\"500\"\u003e\u003c/p\u003e\n\u003cp\u003eThe probability of \u003cimg src=\"https://render.githubusercontent.com/render/math?math=A\"\u003e can be written as sums of event \u003cimg src=\"https://render.githubusercontent.com/render/math?math=B\"\u003e (note that \u003cimg src=\"https://render.githubusercontent.com/render/math?math=B%5Ec\"\u003e is another way of writing \u003cimg src=\"https://render.githubusercontent.com/render/math?math=B'\"\u003e) The total probability rule is:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=P(A)%20=%20P(A%20%20%5Ccap%20B)%20%2b%20P(A%20%20%5Ccap%20B%5Ec)\"\u003e\u003c/p\u003e\n\u003cp\u003eAn alternate version of the total probability rule (found with the multiplication rule) can be used when the necessary probabilities are known:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=P(A)%20=%20P(A%20%5Cmid%20B)%20%20P(B)%20%2b%20P(A%20%5Cmid%20B%5Ec)P(B%5Ec)\"\u003e\u003c/p\u003e\n\u003cp\u003eYou need to be careful when dealing with conditional probabilities and conditioning. Let's look at a few examples to see this idea in action.\u003c/p\u003e\n\u003ch3\u003eExample 1\u003c/h3\u003e\n\u003cp\u003eIn a certain county in the United States, 60% of registered voters are Republicans, 30% are Democrats and 10% are Independents.\u003c/p\u003e\n\u003cp\u003eWhen those voters were asked about increasing military spending.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e40% of Republicans opposed it.\u003c/li\u003e\n\u003cli\u003e65% of the Democrats opposed it.\u003c/li\u003e\n\u003cli\u003e55% of the Independents opposed it.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eWhat is the probability that a randomly selected voter in this county opposes increased military spending?\u003c/p\u003e\n\u003cp\u003eYou know that:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5COmega\"\u003e = {registered voters in the county}\u003c/li\u003e\n\u003cli\u003e\n\u003cimg src=\"https://render.githubusercontent.com/render/math?math=R\"\u003e = {registered republicans}, \u003cimg src=\"https://render.githubusercontent.com/render/math?math=P(R)\"\u003e = 0.6\u003c/li\u003e\n\u003cli\u003e\n\u003cimg src=\"https://render.githubusercontent.com/render/math?math=D\"\u003e = {registered democrats}, \u003cimg src=\"https://render.githubusercontent.com/render/math?math=P(D)\"\u003e = 0.3\u003c/li\u003e\n\u003cli\u003e\n\u003cimg src=\"https://render.githubusercontent.com/render/math?math=I\"\u003e = {registered independents}, \u003cimg src=\"https://render.githubusercontent.com/render/math?math=P(I)\"\u003e = 0.1\u003c/li\u003e\n\u003cli\u003e\n\u003cimg src=\"https://render.githubusercontent.com/render/math?math=B\"\u003e = {registered voters opposing increased military spending}\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eYou also know that:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=P(B%20%5Cmid%20R)%20=%200.4\"\u003e\u003c/li\u003e\n\u003cli\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=P(B%20%5Cmid%20D)%20=%200.65\"\u003e\u003c/li\u003e\n\u003cli\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=P(B%20%5Cmid%20I)%20=%200.55\"\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eBy the total probability theorem:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=Pr(B)%20=%20Pr(B%20%5Cmid%20R)%20Pr(R)%20%2b%20Pr(B%20%5Cmid%20D)%20Pr(D)%20%2b%20Pr(B%20%5Cmid%20I)%20Pr(I)\"\u003e \u003cimg src=\"https://render.githubusercontent.com/render/math?math==%20(0.4%20*%200.6)%20%2b%20(0.65%20*%200.3)%20%2b%20(0.55%20*%200.1)%20=%200.49\"\u003e\u003c/p\u003e\n\u003ch3\u003eExample 2\u003c/h3\u003e\n\u003cp\u003eLet's consider a 2-card hand drawn from a standard playing deck. What is the probability of drawing 2 aces, given that we know one of the cards is an ace?\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=P(%5Ctext%7Bboth%20are%20aces%20%7C%20one%20is%20ace%7D)%20=%20%5Cdfrac%7BP(%5Ctext%7Bboth%20are%20aces%7D)%7D%7BP(%5Ctext%7Bone%20is%20ace%7D)%7D%20=%20%5Cdfrac%7BP(%5Ctext%7Bboth%20are%20aces%7D)%7D%7B1%20-%20P(%5Ctext%7Bneither%20is%20ace%7D)%7D%20=%5Cdfrac%7B%5Cbinom%7B4%7D%7B2%7D/%5Cbinom%7B52%7D%7B2%7D%7D%7B1%20-%20%5Cbinom%7B48%7D%7B2%7D/%5Cbinom%7B52%7D%7B2%7D%7D=%5Cdfrac%7B1%7D%7B33%7D\"\u003e\u003c/p\u003e\n\u003cp\u003eBut now think about this: What is the probability of drawing 2 aces, knowing that one of the cards \u003cstrong\u003eis the ace of spades\u003c/strong\u003e?\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=P(%5Ctext%7Bboth%20are%20aces%20%7C%20ace%20of%20spades%7D)%20=%20P(%5Ctext%7Bother%20card%20is%20also%20an%20ace%7D)%20=%20%5Cdfrac%7B3%7D%7B51%7D=%20%5Cdfrac%7B1%7D%7B17%7D\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cem\u003eNotice how the fact that we know we have the ace of spades nearly doubles the probability of having 2 aces\u003c/em\u003e\u003c/p\u003e\n\u003ch3\u003eExample 3\u003c/h3\u003e\n\u003cp\u003eSuppose there is a test for a disease, and this test is said to be \"95% accurate\". The disease in question afflicts 1% of the population. Now say that there is a patient who tests positive for this disease under this test.\u003c/p\u003e\n\u003cp\u003eFirst, we define the events in question:\u003c/p\u003e\n\u003cp\u003eLet \u003cimg src=\"https://render.githubusercontent.com/render/math?math=D\"\u003e be the event that the patient actually has the disease.\u003c/p\u003e\n\u003cp\u003eLet \u003cimg src=\"https://render.githubusercontent.com/render/math?math=T\"\u003e be the event that the patient tests positive.\u003c/p\u003e\n\u003cp\u003eSince that phrase \"95% accurate\" is ambiguous, we need to clarify that.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=P(T%7CD)%20=%20P(T%5Ec%7CD%5Ec)%20=%200.95\"\u003e\u003c/p\u003e\n\u003cp\u003eIn other words, \u003cstrong\u003econditioning on whether or not the patient has the disease\u003c/strong\u003e, we will assume that the test is 95% accurate.\u003c/p\u003e\n\u003cp\u003e\u003cem\u003eWhat exactly are we trying to find?\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003eWhat the patient really wants to know is not \u003cimg src=\"https://render.githubusercontent.com/render/math?math=P(T%7CD)\"\u003e , which is the accuracy of the test; but rather \u003cimg src=\"https://render.githubusercontent.com/render/math?math=P(D%7CT)\"\u003e , or the probability she has the disease given that the test returns positive. Fortunately, we know how \u003cimg src=\"https://render.githubusercontent.com/render/math?math=P(T%7CD)\"\u003e relates to \u003cimg src=\"https://render.githubusercontent.com/render/math?math=P(D%7CT)\"\u003e .\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=P(D%7CT)\"\u003e = \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cfrac%7BP(T%7CD)P(D)%7D%7BP(T)%7D\"\u003e (Bayes Rule)\u003c/p\u003e\n\u003cp\u003e= \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cfrac%7BP(T%7CD)P(D)%7D%7BP(T%7CD)P(D)%20%2b%20P(T%7CD%5Ec)P(D%5Ec)%7D\"\u003e (by the Law of Total Probability)\u003c/p\u003e\n\u003cp\u003e= \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cfrac%7B(0.95)(0.01)%7D%7B(0.95)(0.01)%20%2b%20(0.05)(0.99)%7D\"\u003e (the rarity of the disease competes with the rarity of true negatives)\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Capprox%200.16\"\u003e\u003c/p\u003e\n\u003ch2\u003eCommon Pitfalls\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eMistaking \u003cimg src=\"https://render.githubusercontent.com/render/math?math=P(A%7CB)\"\u003e for \u003cimg src=\"https://render.githubusercontent.com/render/math?math=P(B%7CA)\"\u003e\u003c/p\u003e\n\u003cp\u003eThis is also known as the \u003ca href=\"https://en.wikipedia.org/wiki/Prosecutor%27s_fallacy\"\u003eProsecutor's Fallacy\u003c/a\u003e, where instead of asking about the \u003cem\u003eprobability of guilt (or innocence) given all the evidence\u003c/em\u003e, we make the mistake of concerning ourselves with the \u003cem\u003eprobability of the evidence given guilt\u003c/em\u003e.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eConfusing \u003cem\u003eprior\u003c/em\u003e \u003cimg src=\"https://render.githubusercontent.com/render/math?math=P(A)\"\u003e with \u003cem\u003eposterior\u003c/em\u003e \u003cimg src=\"https://render.githubusercontent.com/render/math?math=P(A%20%5Cmid%20B)\"\u003e\u003c/p\u003e\n\u003cp\u003eObserving that event \u003cimg src=\"https://render.githubusercontent.com/render/math?math=A\"\u003e occurred does \u003cstrong\u003enot\u003c/strong\u003e mean that \u003cimg src=\"https://render.githubusercontent.com/render/math?math=P(A)%20=%201\"\u003e . But \u003cimg src=\"https://render.githubusercontent.com/render/math?math=P(A%20%5Cmid%20A)%20=%201\"\u003e and \u003cimg src=\"https://render.githubusercontent.com/render/math?math=P(A)%20%5Cneq%201\"\u003e .\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eConfusing \u003cem\u003eindependence\u003c/em\u003e with \u003cstrong\u003econditional independence\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eThis is more subtle than the other two. Let's look at this in a bit more detail\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003eConditional Independence\u003c/h2\u003e\n\u003cp\u003eEvents \u003cimg src=\"https://render.githubusercontent.com/render/math?math=A\"\u003e and \u003cimg src=\"https://render.githubusercontent.com/render/math?math=B\"\u003e are \u003cstrong\u003econditionally independent\u003c/strong\u003e given event \u003cimg src=\"https://render.githubusercontent.com/render/math?math=C\"\u003e , if:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=P(A%20%20%5Ccap%20B%20%5Cmid%20C)%20=%20P(A%20%5Cmid%20C)P(B%20%5Cmid%20C)\"\u003e\u003c/p\u003e\n\u003cp\u003ei.e. conditioning on event \u003cimg src=\"https://render.githubusercontent.com/render/math?math=C\"\u003e does not give us any additional information on \u003cimg src=\"https://render.githubusercontent.com/render/math?math=A\"\u003e or \u003cimg src=\"https://render.githubusercontent.com/render/math?math=B\"\u003e .\u003c/p\u003e\n\u003ch3\u003eConditional independence given \u003cimg src=\"https://render.githubusercontent.com/render/math?math=C\"\u003e DOES NOT imply unconditional independence\u003c/h3\u003e\n\u003cp\u003eConsider playing a series of 5 games against a chess opponent of unknown strength. Winning all five games would give you a good idea that you are a better player. So winning each successive game is actually providing us with information about the strength of our opponent. If you have prior knowledge about the strength of your opponent, you condition on the strength of our opponent i.e. winning one game would not provide us with any additional information on the probability of winning the next. Having no prior knowledge of your opponent and winning a string a games will give you information about the probability of winning the next game.\u003c/p\u003e\n\u003cp\u003eThe games are conditionally independent given the strength of our opponent, but \u003cstrong\u003enot\u003c/strong\u003e independent unconditionally.\u003c/p\u003e\n\u003ch3\u003eUnconditional independence DOES NOT imply conditional independence given \u003cimg src=\"https://render.githubusercontent.com/render/math?math=C\"\u003e\n\u003c/h3\u003e\n\u003cp\u003eFor example, let \u003cimg src=\"https://render.githubusercontent.com/render/math?math=A\"\u003e be the event of the fire alarm going off, \u003cimg src=\"https://render.githubusercontent.com/render/math?math=F\"\u003e be the event of a fire, and \u003cimg src=\"https://render.githubusercontent.com/render/math?math=C\"\u003e be the event of someone making popcorn. Suppose that either \u003cimg src=\"https://render.githubusercontent.com/render/math?math=F\"\u003e or \u003cimg src=\"https://render.githubusercontent.com/render/math?math=C\"\u003e will result in \u003cimg src=\"https://render.githubusercontent.com/render/math?math=A\"\u003e and the fire alarm going off. Now if \u003cimg src=\"https://render.githubusercontent.com/render/math?math=F\"\u003e and \u003cimg src=\"https://render.githubusercontent.com/render/math?math=C\"\u003e are independent: knowing that there's a fire \u003cimg src=\"https://render.githubusercontent.com/render/math?math=F\"\u003e doesn't tell you anything about anyone making popcorn \u003cimg src=\"https://render.githubusercontent.com/render/math?math=C\"\u003e , and vice versa. But the probability of a fire given that the alarm goes off \u003cstrong\u003eand\u003c/strong\u003e no one is making any popcorn is given by \u003cimg src=\"https://render.githubusercontent.com/render/math?math=P(F%20%5Cmid%20A,C%5Ec)%20=%201\"\u003e . After all, if the fire alarm goes off and no one is making popcorn, there can only be one explanation: \u003cem\u003ethere must be a fire\u003c/em\u003e.\u003c/p\u003e\n\u003cp\u003eSo \u003cimg src=\"https://render.githubusercontent.com/render/math?math=F\"\u003e and \u003cimg src=\"https://render.githubusercontent.com/render/math?math=C\"\u003e may be independent, but they are not \u003cem\u003econditionally independent\u003c/em\u003e when we condition on event \u003cimg src=\"https://render.githubusercontent.com/render/math?math=A\"\u003e . Knowing that nobody is making any popcorn when the alarm goes off can only mean that there is a fire.\u003c/p\u003e\n\u003ch2\u003eAdditional Resources\u003c/h2\u003e\n\u003cp\u003eYou are strongly advised to visit following links to get an indepth understanding with examples and proofs for formulas highlighted in this lesson.\u003c/p\u003e\n\u003cp\u003e\u003ca class=\"\" href=\"https://www.youtube.com/watch?v=J7Evcn4lfhc\"\u003eThe law of total probability - concept and proof\u003c/a\u003e - Excellent YouTube video by Phil Chan.\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://jeremykun.com/2013/03/28/conditional-partitioned-probability-a-primer/\"\u003eConditional (Partitioned) Probability — A Primer\u003c/a\u003e - Deep dive into partitions (A Must Read)\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://www.sangakoo.com/en/unit/law-of-total-probability\"\u003eLaw of Total Probability\u003c/a\u003e - More examples for a deeper understanding around partitioning\u003c/p\u003e\n\u003ch2\u003eSummary\u003c/h2\u003e\n\u003cp\u003eIn this lesson, you further learned about the ideas of conditional probability covered in the previous lessons to explain the law of total probability using partitioning of the sample space. You learned how you can partition probabilities with respect to some other event, when the direct probabilities are not known. Let's move on to some practice!\u003c/p\u003e","exportId":"partitioning-and-the-law-of-total-probability"},{"id":141277,"title":"Partitioning and Law of Total Probability - Lab","type":"Assignment","indent":1,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_mark_done","completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-law-of-total-probability-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-law-of-total-probability-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","exportId":"g70e33c5b44b83638374c02ed86c54bab"},{"id":141280,"title":"Simulations with Conditional and Total Probability - Lab","type":"Assignment","indent":1,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_mark_done","completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-probability-simulations-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-probability-simulations-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","exportId":"gd2e2784c49861970728fbbe6b0ff3e0f"},{"id":141283,"title":"Combinatorics and Probability - Recap","type":"WikiPage","indent":0,"locked":false,"requirement":"must_mark_done","completed":false,"content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-probability-section-recap\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-probability-section-recap\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-probability-section-recap/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eIn this section, you learned about the concepts of combinatorics and probability.\u003c/p\u003e\n\u003ch2\u003eKey Takeaways\u003c/h2\u003e\n\u003cp\u003eIn this section, we dug into a number of foundational concepts:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eProbability is \"how likely\" it is that an event will happen\u003c/li\u003e\n\u003cli\u003eSets in Python are unordered collections of unique elements\u003c/li\u003e\n\u003cli\u003eA sample space is a collection of every single possible outcome in a trial\u003c/li\u003e\n\u003cli\u003eThe inclusion exclusion principle is a counting technique used to calculate the number of elements in a collection of sets with overlapping elements.\u003c/li\u003e\n\u003cli\u003eFactorials provide the basis for calculating permutations\u003c/li\u003e\n\u003cli\u003eThe difference between permutations and combinations is that with combinations, order is not important\u003c/li\u003e\n\u003cli\u003eThe \"sum rule\" of probability states that \u003cimg src=\"https://render.githubusercontent.com/render/math?math=P(A%20%5Ccup%20B)%20=%20P(A)%20%2b%20P(B)%20-%20P(A%20%20%5Ccap%20B)\"\u003e\n\u003c/li\u003e\n\u003cli\u003eIndependent events don't affect each other - e.g. consecutive coin tosses\u003c/li\u003e\n\u003cli\u003eDependent events do affect each other - e.g. picking consecutive colored marbles from a bag\u003c/li\u003e\n\u003cli\u003eThe product rule is useful when the conditional probability is easy to compute, but the probability of intersections of events is not\u003c/li\u003e\n\u003cli\u003eThe chain rule (also called the general product rule) permits the calculation of any member of the joint distribution of a set of random variables using only conditional probabilities.\u003c/li\u003e\n\u003cli\u003eBayes theorem describes the probability of an event based on prior knowledge of conditions that might be related to the event\u003c/li\u003e\n\u003cli\u003eThe law of total probability states that the probability for a sample space is the sum of the probabilities for partitions of that sample space\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eIn this section, we introduced the ideas of combinatorics and probability. In the next section, you'll use this knowledge and take it a step further by learning about statistical distributions and their applications!\u003c/p\u003e","exportId":"combinatorics-and-probability-recap"}]},{"id":15118,"name":"APPENDIX: Combinatorics","status":"completed","unlockDate":null,"prereqs":[],"requirement":null,"sequential":false,"exportId":"g4faa7ffbb988e237cb7820b61e20e392","items":[{"id":141744,"title":"Combinatorics: Recursive Functions","type":"Assignment","indent":0,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":null,"completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-recursive-functions\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-recursive-functions/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","exportId":"gfe5927eda314970e86914220b484ccb5"},{"id":141746,"title":"Combinatorics: Recursive Functions - Lab","type":"Assignment","indent":1,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":null,"completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-recursive-functions-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-recursive-functions-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","exportId":"g094337fc83281fdb9db56acd323423d3"}]},{"id":15074,"name":"Topic 12: Statistical Distributions","status":"started","unlockDate":null,"prereqs":[],"requirement":"all","sequential":false,"exportId":"g196664afd34b637202721c1114e38b83","items":[{"id":141289,"title":"Statistical Distributions - Introduction","type":"WikiPage","indent":0,"locked":false,"requirement":"must_mark_done","completed":true,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-distributions-section-intro-v2-1\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-distributions-section-intro-v2-1/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eIn this section, you'll learn about different probability distributions!\u003c/p\u003e\n\u003ch2\u003eIt's all Stats!\u003c/h2\u003e\n\u003cp\u003eYou've already seen the value of descriptive statistics when doing exploratory data analysis. In this section, we're going to dive deeper into a range of statistical concepts. We're going to start by looking at discrete and continuous distributions and how you can use stem and leaf plots for visualizing distributions.\u003c/p\u003e\n\u003cp\u003eWe're then going to look at a range of techniques for representing distributions - the Probability Mass Function, the Cumulative Distribution Function, and the Probability Density Function.\u003c/p\u003e\n\u003cp\u003eWe're then going to dig a little deeper into the Normal/Gaussian distribution and the Standard Normal Distribution, and we'll introduce the use cases for z-tables and p-values for describing statistical significance. We'll discuss the \"one-sample z test\", the most basic type of hypothesis test, before introducing the concepts of skewness and kurtosis that can be used to quantify how \"un-normal\" a given distribution is.\u003c/p\u003e\n\u003cp\u003eIn the Appendix to this Module, we'll introduce some additional distribution functions, like the uniform, Poisson, and exponential distributions, and use them to solve practical problems.\u003c/p\u003e\n\u003ch2\u003eSummary\u003c/h2\u003e\n\u003cp\u003eIn this section, we're going to take a deeper dive into a range of foundational statistical concepts that we'll need as we start to dig into machine learning later in the course.\u003c/p\u003e\n\u003cfooter class=\"fis-footer\" style=\"visibility: hidden;\"\u003e\n\u003cdiv class=\"fis-feedback\"\u003e\n\u003ch5\u003eHow do you feel about this lesson?\u003c/h5\u003e\n\u003cimg id=\"thumbs-up\" title=\"Thumbs up!\" alt=\"thumbs up\" data-repository=\"dsc-distributions-section-intro-v2-1\"\u003e\u003cimg id=\"thumbs-down\" title=\"Thumbs down!\" alt=\"thumbs down\" data-repository=\"dsc-distributions-section-intro-v2-1\"\u003e\n\u003c/div\u003e\n\u003ch5\u003eHave specific feedback? \u003ca href=\"https://github.com/learn-co-curriculum/dsc-distributions-section-intro-v2-1/issues/new\"\u003eTell us here!\u003c/a\u003e\n\u003c/h5\u003e\n\u003c/footer\u003e","exportId":"statistical-distributions-introduction"},{"id":141292,"title":"Statistical Distributions and Their Use Cases","type":"WikiPage","indent":0,"locked":false,"requirement":"must_mark_done","completed":true,"content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-stat-distributions-use-cases\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-stat-distributions-use-cases\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-stat-distributions-use-cases/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eAs a data scientist, you'll often have to work with statistical distributions. This includes selecting which distribution is most representative of a given set of data. A typical use case includes A/B testing, where understanding the process that generated the data is important. You can think of distributions in relation to statistical analysis as to data structures to computer programming.\u003c/p\u003e\n\u003cp\u003eThere are an enormous amount of distributions out there, but you'll see about a handful of distributions that can represent the vast majority of situations you'll come across. In the upcoming series of lessons, you'll look at ways to analyze common distributions you will encounter most frequently.\u003c/p\u003e\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\u003cp\u003eYou will be able to:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eDefine statistical distributions\u003c/li\u003e\n\u003cli\u003eDifferentiate between discrete and continuous distributions\u003c/li\u003e\n\u003cli\u003eList the common distributions and their use cases\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003eWhat is a Statistical Distribution?\u003c/h2\u003e\n\u003cp\u003eA statistical distribution is a representation of the frequencies of potential events or the percentage of time each event occurs.\u003c/p\u003e\n\u003cp\u003eThis may feel pretty vague which is why we'll use two examples to clarify this concept.\u003c/p\u003e\n\u003ch3\u003eRolling a Dice Distribution\u003c/h3\u003e\n\u003cp\u003eLet's think back about our example rolling a dice. You know that when rolling dice once, you will obtain a number between 1 and 6, with each outcome to be as likely, as denoted in this table:\u003c/p\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003eoutcome\u003c/th\u003e\n\u003cth\u003e1\u003c/th\u003e\n\u003cth\u003e2\u003c/th\u003e\n\u003cth\u003e3\u003c/th\u003e\n\u003cth\u003e4\u003c/th\u003e\n\u003cth\u003e5\u003c/th\u003e\n\u003cth\u003e6\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd\u003eprobability\u003c/td\u003e\n\u003ctd\u003e1/6\u003c/td\u003e\n\u003ctd\u003e1/6\u003c/td\u003e\n\u003ctd\u003e1/6\u003c/td\u003e\n\u003ctd\u003e1/6\u003c/td\u003e\n\u003ctd\u003e1/6\u003c/td\u003e\n\u003ctd\u003e1/6\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003cp\u003eYou can also represent this graphically as follows:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-stat-distributions-use-cases/master/images/dice_roll_pmf.png\" width=\"350\"\u003e\u003c/p\u003e\n\u003cp\u003eNote how, with a fair die, the chance of throwing each number is \u003cem\u003eexactly\u003c/em\u003e 1/6 (or 0.1666). The number of outcomes is finite and the outcome is a set of values. In this case, you are dealing with a \u003cstrong\u003ediscrete distribution\u003c/strong\u003e.\u003c/p\u003e\n\u003ch3\u003eWeather Distribution\u003c/h3\u003e\n\u003cp\u003eLet's look at another situation. Imagine we want to think of the distribution of the temperature in New York on June 1st. Thinking about this, you could say that the temperature would generally range between 65 and 95 Degrees (more extreme values would be exceptional), with the average around 80 Degrees Fahrenheit.\u003c/p\u003e\n\u003cp\u003eA potential distribution looks like this:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-stat-distributions-use-cases/master/images/weather_pdf.png\" width=\"400\"\u003e\u003c/p\u003e\n\u003cp\u003eNote that instead of bars, which we had for the dice example, we have \u003cem\u003econtinuous\u003c/em\u003e lines here. Our distribution is a \u003cstrong\u003econtinuous distribution\u003c/strong\u003e because temperature is a continuous value (we can have a temperature of 80 degrees, of 80.5 degrees, of 80.0034 degrees, etc.).\u003c/p\u003e\n\u003ch3\u003eDiscrete vs Continuous Distributions\u003c/h3\u003e\n\u003cp\u003eWhen dealing with \u003cstrong\u003ediscrete\u003c/strong\u003e data you use a \u003cstrong\u003eProbability Mass Function (PMF)\u003c/strong\u003e (as in our dice example). When dealing with \u003cstrong\u003econtinuous\u003c/strong\u003e data, you use a \u003cstrong\u003eProbability Density Function (PDF)\u003c/strong\u003e (see our weather example).\u003c/p\u003e\n\u003cp\u003eBased on the variation of their attributes, data distributions can take many shapes and forms. In the next few lessons, you'll learn how to describe data distributions. Very often, distributions are described using their statistical mean (or \u003cstrong\u003eexpected value\u003c/strong\u003e) and variance of the data, but this is not always the case. You'll see more on this in the next few lessons.\u003c/p\u003e\n\u003ch2\u003eCommon Distributions\u003c/h2\u003e\n\u003cp\u003eIn this image, you can see the general shapes of some common distributions. The horizontal axis in each chart represents the set of possible numeric outcomes. The vertical axis describes the probability of the respective outcomes.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-stat-distributions-use-cases/master/images/dists.png\" width=\"600\"\u003e\u003c/p\u003e\n\u003cp\u003eYou'll get a more in-depth overview of some important distributions in the next few lessons, but to give you an initial idea of some applications, we'll give you a quick overview below. Let's quickly talk about some common distributions and their use cases below.\u003c/p\u003e\n\u003ch2\u003eExamples of Discrete Distributions\u003c/h2\u003e\n\u003ch3\u003eThe Bernoulli Distribution\u003c/h3\u003e\n\u003cp\u003eThe Bernoulli distribution represents the probability of success for a certain experiment (the outcome being \"success or not\", so there are two possible outcomes). A coin toss is a classic example of a Bernoulli experiment with a probability of success 0.5 or 50%, but a Bernoulli experiment can have any probability of success between 0 and 1.\u003c/p\u003e\n\u003ch3\u003eThe Poisson Distribution\u003c/h3\u003e\n\u003cp\u003eThe Poisson distribution represents the probability of \u003cimg src=\"https://render.githubusercontent.com/render/math?math=n\"\u003e events in a given time period when the overall rate of occurrence is constant. A typical example is pieces of mail. If your overall mail received is constant, the number of items received on a single day (or month) follows a Poisson distribution. Other examples might include visitors to a website, or customers arriving at a store, or clients waiting to be served in a queue.\u003c/p\u003e\n\u003ch3\u003eThe Uniform Distribution\u003c/h3\u003e\n\u003cp\u003eThe uniform distribution occurs when all possible outcomes are equally likely. The dice example shown before follows a uniform distribution with equal probabilities for throwing values from 1 to 6. The dice example follows a discrete uniform distribution, but continuous uniform distributions exist as well.\u003c/p\u003e\n\u003ch2\u003eExamples of Continuous Distributions\u003c/h2\u003e\n\u003ch3\u003eThe Normal or Gaussian Distribution\u003c/h3\u003e\n\u003cp\u003eA normal distribution is the single most important distribution, you'll basically come across it very often. The normal distribution follows a bell shape and is a foundational distribution for many models and theories in statistics and data science. A normal distribution turns up very often when dealing with real-world data including heights, weights of different people, errors in some measurement or grades on a test. Our temperature example above follows a normal distribution as well!\u003c/p\u003e\n\u003ch2\u003eSummary\u003c/h2\u003e\n\u003cp\u003eIn this lesson, you learned about the concept of (discrete and continuous) statistical distributions, as well as some common ones. You'll learn more about distributions and their properties in the next few lessons!\u003c/p\u003e","exportId":"statistical-distributions-and-their-use-cases"},{"id":141296,"title":"The Probability Mass Function","type":"Assignment","indent":0,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_mark_done","completed":true,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-probability-mass-function\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-probability-mass-function/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","exportId":"g51a8033327123f706ed2c1cd895fdb6f"},{"id":141299,"title":"The Probability Mass Function - Lab","type":"Assignment","indent":1,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_mark_done","completed":true,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-probability-mass-function-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-probability-mass-function-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","exportId":"gdc0dbd661ca4318278fdc48bdbf533c2"},{"id":141303,"title":"The Probability Density Function","type":"Assignment","indent":0,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_mark_done","completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-probability-density-function\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-probability-density-function/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","exportId":"gbeaed92c2bba3b0fc174fb0883710cc6"},{"id":141307,"title":"The Probability Density Function - Lab","type":"Assignment","indent":1,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_mark_done","completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/probability-density-functions-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/probability-density-functions-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","exportId":"g1c2cb0d718cb1c863428626c9535eb85"},{"id":141310,"title":"The Cumulative Distribution Function","type":"WikiPage","indent":0,"locked":false,"requirement":"must_mark_done","completed":true,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-cumulative-distribution-function\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-cumulative-distribution-function/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eThe PMF function that we saw before works great for inspecting discrete random variables and calculating their expected values. However, we did see that when moving towards continuous random variables, obtaining probabilities for observing a specific outcome is not possible (or, simply put, the probabilities were 0). We also noted that when working with PDFs, you can't really read the y-axis and have to be careful with interpretation. In this lesson, you'll learn about the cumulative distribution function (CDF) and how it is useful to overcome these issues.\u003c/p\u003e\n\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\n\u003cp\u003eYou will be able to:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eDifferentiate between a PMF, PDF, and a CDF in terms of cumulative probabilities \u003c/li\u003e\n\u003cli\u003eCalculate CDF in Python for a given discrete variable with limited set of possible values\u003c/li\u003e\n\u003cli\u003eVisualize and inspect a given CDF in order to make assumptions about the underlying data \u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch2\u003eLimitations of PMFs and PDFs\u003c/h2\u003e\n\n\u003cp\u003eTo illustrate the use of Cumulative Distribution Functions, let's have another look at the PMF and PDF of our dice and temperature example:\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-cumulative-distribution-function/master/images/dice_and_temp.png\" width=\"900\"\u003e\u003c/p\u003e\n\n\u003cp\u003eRecall how we could easily read probabilities from the dice PMF plot (\"The probability of throwing a 4 is 16.66%\"), but it is much harder to interpret the temperature PDF. What is the probability that the temperature is exactly 80 degrees? We learned in the previous lesson that all these so-called \"point probabilities\" are 0, so the bottom line is that it is very hard to \"read\" any interesting information from a PDF. The PDF is mainly there to get a sense of the data density, but you cannot readily read the y-axis to get to probabilities.\u003c/p\u003e\n\n\u003cp\u003eWe did see last that when you want to calculate probabilities, you need to take integrals and look at ranges of values of your continuous random variables. For example, you can ask yourself the question: \"What is the probability the temperature in NYC is between 82 and 85 degrees on June 1?\" The answer is the surface of the red shaded area!\u003c/p\u003e\n\n\u003cp\u003eFrom the last lesson, you learned that you can use the integral to get this \"area under the curve\" value by taking the integral as follows:\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-cumulative-distribution-function/master/images/section_temp.png\" width=\"650\"\u003e\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Clarge%20P(82%20%5Cleq%20X%20%5Cleq%2085)%20=%20%5Cint_%7B82%7D%5E%7B85%7D%20f_x(x)%20dx%20%5Cgeq%200\"\u003e \u003c/p\u003e\n\n\u003cp\u003eThis is the rationale that is being used when working with Cumulative Density Functions, which will be introduced next.\u003c/p\u003e\n\n\u003ch2\u003eHow does a Cumulative Density Function (CDF) work?\u003c/h2\u003e\n\n\u003cp\u003eThe CDF is a function of \u003cimg src=\"https://render.githubusercontent.com/render/math?math=x\"\u003ejust like a PMF or a PDF, where \u003cimg src=\"https://render.githubusercontent.com/render/math?math=x\"\u003eis any value that can possibly appear in a given distribution. To calculate the \u003cimg src=\"https://render.githubusercontent.com/render/math?math=CDF(x)\"\u003efor any value of \u003cimg src=\"https://render.githubusercontent.com/render/math?math=x\"\u003e, we compute the proportion of values in the distribution less than or equal to \u003cimg src=\"https://render.githubusercontent.com/render/math?math=x\"\u003eas follows:\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Clarge%20F(x)%20=%20P(X%20%5Cleq%20x)\"\u003e\u003c/p\u003e\n\n\u003cblockquote\u003e\n\u003cp\u003eThe Cumulative Distribution Function, CDF, gives the probability that the variable \u003cimg src=\"https://render.githubusercontent.com/render/math?math=X\"\u003eis less than or equal to a certain possible value \u003cimg src=\"https://render.githubusercontent.com/render/math?math=x\"\u003e. \u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003cp\u003eThe cumulative distribution functions for a dice roll and the weather in NYC are plotted below.\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-cumulative-distribution-function/master/images/cdfs_dice_nyc_2.png\" width=\"950\"\u003e\u003c/p\u003e\n\n\u003cp\u003eThis is also what \"cumulative\" means - you're simply adding up probabilities.\u003c/p\u003e\n\n\u003cp\u003eYou'll notice that in general, CDFs are smooth curves for continuous random variables, where they are \"step functions\" when looking at discrete random variables. Looking at these curves, we can answer questions by looking at the y-axis.\u003c/p\u003e\n\n\u003cp\u003eWhat is the probability that you throw a value \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cleq4\"\u003e when throwing a dice? 0.6667 or 66.67. For this discrete example it is pretty straightforward, as this is the probability of throwing a 1 OR 2 OR 3 OR 4, so \u003cimg src=\"https://render.githubusercontent.com/render/math?math=0.1666%20*%204\"\u003e.\u003c/p\u003e\n\n\u003cp\u003eWhat is the probability that the temperature in NYC is \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cleq79\"\u003e? Looking at the associated y-value when looking at an \u003cimg src=\"https://render.githubusercontent.com/render/math?math=x\"\u003e-value of 79, this probability is around 40% or 0.4.\u003c/p\u003e\n\n\u003ch2\u003eCalculating more probabilities using the CDF\u003c/h2\u003e\n\n\u003cp\u003eLet's go back to our weather example introduced before. An additional advantage of CDFs is that you can use them to easily calculate things like:\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-cumulative-distribution-function/master/images/section_temp.png\" width=\"650\"\u003e\u003c/p\u003e\n\n\u003cp\u003eThe idea is that\n\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Clarge%20P(82%20%5Cleq%20X%20%5Cleq%2085)%20=%20P(X%20%5Cleq%2085)%20-%20P(X%20%5Cleq%2082)=%20F_X(85)-%20F_X(82)\"\u003e\u003c/p\u003e\n\n\u003cp\u003eThis means that you can look at the y-value of your cumulative density function to get the answer to this question.\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Clarge%20F_X(85)-%20F_X(82)%20%5Capprox%200.95-0.6%20=%200.35\"\u003e\u003c/p\u003e\n\n\u003ch2\u003eSummary\u003c/h2\u003e\n\n\u003cp\u003eIn this lesson, we looked at a CDF as a so-called \"percentile probability function\" of discrete or continuous random variables. You learned how to calculate and visualize a CDF and how to use them to calculate certain probabilities.\u003c/p\u003e\n\u003cfooter class=\"fis-footer\" style=\"visibility: hidden;\"\u003e\u003cdiv class=\"fis-feedback\"\u003e\n\u003ch5\u003eHow do you feel about this lesson?\u003c/h5\u003e\n\u003cimg id=\"thumbs-up\" data-repository=\"dsc-cumulative-distribution-function\" title=\"Thumbs up!\" alt=\"thumbs up\"\u003e\u003cimg id=\"thumbs-down\" data-repository=\"dsc-cumulative-distribution-function\" title=\"Thumbs down!\" alt=\"thumbs down\"\u003e\n\u003c/div\u003e\n\u003ch5\u003eHave specific feedback? \u003ca href=\"https://github.com/learn-co-curriculum/dsc-cumulative-distribution-function/issues/new\"\u003eTell us here!\u003c/a\u003e\n\u003c/h5\u003e\u003c/footer\u003e","exportId":"the-cumulative-distribution-function"},{"id":141312,"title":"The Cumulative Distribution Function - Lab","type":"Assignment","indent":1,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_mark_done","completed":true,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-cumulative-distribution-function-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-cumulative-distribution-function-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","exportId":"ga16ad827e78e8d7ff5dd65bce2bd7079"},{"id":141315,"title":"The Bernoulli and Binomial Distribution","type":"Assignment","indent":0,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_mark_done","completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-bernoulli-and-binomial-distribution\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-bernoulli-and-binomial-distribution/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","exportId":"gff37f5d2e456c53e4445862ab4bfae30"},{"id":141317,"title":"The Bernoulli and Binomial Distribution - Lab","type":"Assignment","indent":1,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_mark_done","completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-bernoulli-and-binomial-distribution-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-bernoulli-and-binomial-distribution-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","exportId":"g472db7170ed5b59b47fccb9c6e72cb77"},{"id":141320,"title":"The Normal Distribution","type":"Assignment","indent":0,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_mark_done","completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-normal-distribution\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-normal-distribution/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","exportId":"g53fed3bbd7b332c89a39edbfeaff06b2"},{"id":141324,"title":"The Normal Distribution - Lab","type":"Assignment","indent":1,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_mark_done","completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-normal-distribution-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-normal-distribution-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","exportId":"g4a9042ed5eaea96c3b4f394e59cba58e"},{"id":141327,"title":"The Standard Normal Distribution","type":"Assignment","indent":0,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_mark_done","completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-standard-normal-distribution\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-standard-normal-distribution/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","exportId":"g100cabc871d1dd3cccf782278b282cd0"},{"id":141330,"title":"The Standard Normal Distribution - Lab","type":"Assignment","indent":1,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_mark_done","completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-standard-normal-distribution-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-standard-normal-distribution-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","exportId":"g80e4421181c9727c6d13f6eacf53ff6d"},{"id":141335,"title":"Statistical Testing with z-score and p-value","type":"Assignment","indent":0,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_mark_done","completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-z-score-p-value\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-z-score-p-value/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","exportId":"gd01d4550bbe8406ccadd2473523f73cc"},{"id":141338,"title":"One-Sample z-test","type":"Assignment","indent":0,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_mark_done","completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-one-sample-z-test\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-one-sample-z-test/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","exportId":"g74555cddbb298dd685adf8df41b13252"},{"id":141342,"title":"One-Sample z-test - Lab","type":"Assignment","indent":1,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_mark_done","completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-one-sample-z-test-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-one-sample-z-test-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","exportId":"gc78e74e59178dffd9fe23bcd5aaed136"},{"id":141345,"title":"Skewness and Kurtosis","type":"WikiPage","indent":0,"locked":false,"requirement":"must_mark_done","completed":false,"content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-skewness-and-kurtosis\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-skewness-and-kurtosis\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-skewness-and-kurtosis/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eWe have previously identified a normal distribution to be symmetrical in shape. But when you're dealing with real-world data you'll often come across asymmetric distributions as well. In this lesson, you'll learn how to measure asymmetry (or skewness) in a distribution. Additionally, you'll learn about kurtosis. Kurtosis defines whether a distribution is truly \"normal\" or whether it may have so-called \"fatter\" or \"thinner\" tails than you would observe when data are normally distributed.\u003c/p\u003e\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\u003cp\u003eYou will be able to:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eDefine skewness and kurtosis and their relationship to symmetric distributions\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003eSymmetric Distributions\u003c/h2\u003e\n\u003cp\u003eA distribution is symmetric if the relative frequency or probability of certain values are equal at equal distances from the point of symmetry. The point of symmetry for normal distributions is the mean (and at the same time median and mode!)\u003c/p\u003e\n\u003cp\u003eHave a look at following histogram:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-skewness-and-kurtosis/master/images/retirement.png\" width=\"450\"\u003e\u003c/p\u003e\n\u003cp\u003eThis distribution meets all of the conditions of being symmetrical.\u003c/p\u003e\n\u003cp\u003eThe most common symmetric distribution is the normal distribution, however, there are a number of other distributions that are symmetric. \u003ca href=\"https://www.statisticshowto.datasciencecentral.com/symmetric-distribution-2/\"\u003eHere is a good article\u003c/a\u003e that looks into all sorts of symmetrical distributions. We'll focus on normal distributions (by far the most common group) here, and see how these can lose symmetry!\u003c/p\u003e\n\u003ch2\u003eSkewness\u003c/h2\u003e\n\u003cp\u003eSkewness is the degree of distortion or deviation from the symmetrical normal distribution. Skewness can be seen as a measure to calculate the lack of symmetry in the data distribution.\u003c/p\u003e\n\u003cp\u003eSkewness helps you identify extreme values in one of the tails. Symmetrical distributions have a skewness of 0.\u003c/p\u003e\n\u003cp\u003eDistributions can be \u003cstrong\u003epositively\u003c/strong\u003e or \u003cstrong\u003enegatively\u003c/strong\u003e skewed.\u003c/p\u003e\n\u003ch3\u003ePositive Skewness\u003c/h3\u003e\n\u003cp\u003eA distribution is \u003cstrong\u003epositively skewed\u003c/strong\u003e when the tail on the right side of the distribution is longer (also often called \"fatter\"). When there is positive skewness, the mean and median are bigger than the mode.\u003c/p\u003e\n\u003ch3\u003eNegative Skewness\u003c/h3\u003e\n\u003cp\u003eDistributions are \u003cstrong\u003enegatively skewed\u003c/strong\u003e when the tail on the left side of the distribution is longer or fatter than the tail on the right side. When there is negative skewness, the mean and median are smaller than the mode.\u003c/p\u003e\n\u003cp\u003eThis behavior is shown in the images below:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-skewness-and-kurtosis/master/images/skewness.png\" width=\"700\"\u003e\u003c/p\u003e\n\u003cp\u003eSkewness can have implications for data analysis and the usage of certain models. The \"normality assumption\" seen before does not hold when data is skewed. When data is skewed, you'll need to transform the data first.\u003c/p\u003e\n\u003ch3\u003eMeasuring Skewness\u003c/h3\u003e\n\u003cp\u003eFor univariate data \u003cimg src=\"https://render.githubusercontent.com/render/math?math=Y_1,%20Y_2,%20...,%20Y_n\"\u003e the formula for skewness is:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cdfrac%7B%5Cdfrac%7B%5Cdisplaystyle%5Csum%5En_%7Bi=1%7D(Y_i-Y)%5E3%7D%7Bn%7D%7D%7Bs%5E3%7D\"\u003e\u003c/p\u003e\n\u003cp\u003ewhere \u003cimg src=\"https://render.githubusercontent.com/render/math?math=Y\"\u003e is the mean, \u003cimg src=\"https://render.githubusercontent.com/render/math?math=s\"\u003e is the standard deviation, and \u003cimg src=\"https://render.githubusercontent.com/render/math?math=n\"\u003e is the number of data points. This formula for skewness is referred to as the \u003cstrong\u003eFisher-Pearson coefficient of skewness\u003c/strong\u003e. There are also other ways to calculate skewness, yet this one is the one that is used most commonly.\u003c/p\u003e\n\u003ch3\u003eUsing this formula, when is data skewed?\u003c/h3\u003e\n\u003cp\u003eThe rule of thumb seems to be:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eA skewness between -0.5 and 0.5 means that the data are pretty symmetrical\u003c/li\u003e\n\u003cli\u003eA skewness between -1 and -0.5 (negatively skewed) or between 0.5 and 1 (positively skewed) means that the data are moderately skewed.\u003c/li\u003e\n\u003cli\u003eA skewness smaller than -1 (negatively skewed) or bigger than 1 (positively skewed) means that the data are highly skewed.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003eExample\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eImagine you have house values ranging from 200,000 USD to 1,500,000 USD with an average of 800,000 USD.\u003c/p\u003e\n\u003cp\u003eIf the peak of the distribution is left of the average value, the house prices are positively skewed. This means that more than half of the houses were sold for less than the average value 800,000 USD, and that there are a limited number of houses that were sold for a \u003cem\u003emuch\u003c/em\u003e higher value than 800,000 USD, leading to a long tail in the higher price ranges.\u003c/p\u003e\n\u003cp\u003eIf the peak of the distributed data is on the right-hand side of the average value, this means there is negative skewness, meaning that more than half of the houses were sold for more than the average value of 800,000 USD. Additionally, this means that there is a long tail in the lower price ranges.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-skewness-and-kurtosis/master/images/homeskewed.png\" width=\"500\"\u003e\u003c/p\u003e\n\u003ch2\u003eKurtosis\u003c/h2\u003e\n\u003cp\u003eKurtosis deals with the lengths of tails in the distribution.\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003eWhere skewness talks about extreme values in one tail versus the other, kurtosis aims at identifying extreme values in both tails at the same time!\u003c/strong\u003e\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eYou can think of Kurtosis as a \u003cstrong\u003emeasure of outliers\u003c/strong\u003e present in the distribution.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-skewness-and-kurtosis/master/images/kurtosis.png\" width=\"550\"\u003e\u003c/p\u003e\n\u003cp\u003eThe distribution denoted in the image above has relatively more observations around the mean, then a steep decline and longer tails compared to the normal distribution.\u003c/p\u003e\n\u003ch3\u003eMeasuring Kurtosis\u003c/h3\u003e\n\u003cp\u003eFor univariate data \u003cimg src=\"https://render.githubusercontent.com/render/math?math=Y_1,%20Y_2,%20%5Cdots,%20Y_n\"\u003e the formula for kurtosis is:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cdfrac%7B%5Cdfrac%7B%5Cdisplaystyle%5Csum%5En_%7Bi=1%7D(Y_i-Y)%5E4%7D%7Bn%7D%7D%7Bs%5E4%7D\"\u003e\u003c/p\u003e\n\u003cp\u003eIf there is a high kurtosis, then you may want to investigate why there are so many outliers. The presence of outliers could be indications of errors on the one hand, but they could also be some interesting observations that may need to be explored further. For banking transactions, for example, an outlier may signify fraudulent activity. How we deal with outliers mainly depends on the domain.\u003c/p\u003e\n\u003cp\u003eLow kurtosis in a data set is an indication that data has light tails or lacks outliers. If we get low kurtosis, then also we need to investigate and trim the dataset of unwanted results.\u003c/p\u003e\n\u003ch3\u003eHow much kurtosis is bad kurtosis?\u003c/h3\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-skewness-and-kurtosis/master/images/mesokurtosis.png\" width=\"550\"\u003e\u003c/p\u003e\n\u003ch4\u003eMesokurtic ( \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Ctext%7Bkurtosis%7D%20%5Capprox%203\"\u003e ):\u003c/h4\u003e\n\u003cp\u003eA mesokurtic distribution has kurtosis statistics that lie close to the ones of a normal distribution. Mesokurtic distributions have a kurtosis of around 3. According to this definition, the standard normal distribution has a kurtosis of 3.\u003c/p\u003e\n\u003ch4\u003ePlatykurtic ( \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Ctext%7Bkurtosis%7D%20\u003c%203\"\u003e ):\u003c/h4\u003e\n\u003cp\u003eWhen a distribution is platykurtic, the distribution is shorter and tails are thinner than the normal distribution. The peak is lower and broader than Mesokurtic, which means that the tails are light and that there are fewer outliers than in a normal distribution.\u003c/p\u003e\n\u003ch4\u003eLeptokurtic ( \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Ctext%7Bkurtosis%7D%20\u003e%203\"\u003e ):\u003c/h4\u003e\n\u003cp\u003eWhen you have a leptokurtic distribution, you have a distribution with longer and fatter tails. The peak is higher and sharper than the peak of a normal distribution, which means that data have heavy tails and that there are more outliers.\u003c/p\u003e\n\u003cp\u003eOutliers stretch your horizontal axis of the distribution, which means that the majority of the data appear in a narrower vertical range. This is why the leptokurtic distribution looks \"skinny\".\u003c/p\u003e\n\u003ch2\u003eSummary\u003c/h2\u003e\n\u003cp\u003eIn this lesson, you learned about skewness and kurtosis. In the next lab, you'll learn how to measure skewness and kurtosis in Python.\u003c/p\u003e","exportId":"skewness-and-kurtosis"},{"id":141350,"title":"Skewness and Kurtosis - Lab","type":"Assignment","indent":1,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_mark_done","completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-skewness-and-kurtosis-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-skewness-and-kurtosis-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","exportId":"g2f104a36fbe644d161e0a1f64478943c"},{"id":141351,"title":"Statistical Distributions - Recap","type":"WikiPage","indent":0,"locked":false,"requirement":"must_mark_done","completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-distributions-section-recap-v2-1\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-distributions-section-recap-v2-1/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eThis short lesson summarizes the topics we covered in this section and why they'll be important to you as a data scientist.\u003c/p\u003e\n\n\u003ch2\u003eKey Takeaways\u003c/h2\u003e\n\n\u003cp\u003eIn this section, we really dug into statistical distributions. \u003c/p\u003e\n\n\u003cp\u003eKey takeaways include:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eThere are two types of distributions - continuous, where (subject to measurement and/or storage precision) there are effectively an infinite number of possible values, and discrete, where there are a distinct, non-infinite number of options. For example, a person's height is continuous - assuming a suitably precise tape measure - whereas the number of bedrooms in a house is discrete\u003c/li\u003e\n\u003cli\u003eHow to describe the distribution of data sets using Probability Mass Functions, Cumulative Distribution Functions, and Probability Density Functions\u003c/li\u003e\n\u003cli\u003eOne type of discrete distribution deals with a series of boolean events or trials - often called Bernoulli Trials\u003c/li\u003e\n\u003cli\u003eA Normal distribution is the classic \"bell curve\" with 68% of the probability mass within 1 SD of the mean, 95% within 2 SDs and 99.7% within 3 SDs\u003c/li\u003e\n\u003cli\u003eDifferences between the normal and the standard normal distribution\u003c/li\u003e\n\u003cli\u003eThe uses of \u003cimg src=\"https://render.githubusercontent.com/render/math?math=z\"\u003e-scores and p-values for describing a distribution\u003c/li\u003e\n\u003cli\u003eHow a one sample \u003cimg src=\"https://render.githubusercontent.com/render/math?math=z\"\u003e-test is a very simple form of hypothesis testing.\u003c/li\u003e\n\u003cli\u003eHow skewness and kurtosis can be used to measure how different a given distribution is from a normal distribution\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003eIn the Appendix to this Module, you'll have the opportunity to learn about:  \u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eThe uniform distribution, which represents processes where each outcome is equally likely, like rolling a dice.\u003cbr\u003e\n\u003c/li\u003e\n\u003cli\u003eThe Poisson distribution, which can be used to display the likelihood of a given number of successes over a given time period.\u003cbr\u003e\n\u003c/li\u003e\n\u003cli\u003eThe exponential distribution, which can be used to describe the probability distribution of the amount of time it may take before a given event occurs.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cfooter class=\"fis-footer\" style=\"visibility: hidden;\"\u003e\u003cdiv class=\"fis-feedback\"\u003e\n\u003ch5\u003eHow do you feel about this lesson?\u003c/h5\u003e\n\u003cimg id=\"thumbs-up\" data-repository=\"dsc-distributions-section-recap-v2-1\" title=\"Thumbs up!\" alt=\"thumbs up\"\u003e\u003cimg id=\"thumbs-down\" data-repository=\"dsc-distributions-section-recap-v2-1\" title=\"Thumbs down!\" alt=\"thumbs down\"\u003e\n\u003c/div\u003e\n\u003ch5\u003eHave specific feedback? \u003ca href=\"https://github.com/learn-co-curriculum/dsc-distributions-section-recap-v2-1/issues/new\"\u003eTell us here!\u003c/a\u003e\n\u003c/h5\u003e\u003c/footer\u003e","exportId":"statistical-distributions-recap"}]},{"id":15121,"name":"APPENDIX: More Distributions","status":"completed","unlockDate":null,"prereqs":[],"requirement":null,"sequential":false,"exportId":"g8e471a03d1483e973460eca50f69b344","items":[{"id":141755,"title":"The Uniform Distribution","type":"WikiPage","indent":0,"locked":false,"requirement":null,"completed":false,"content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-uniform-distribution\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-uniform-distribution\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-uniform-distribution/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eNow that you've been introduced to both discrete (Binomial) and continuous (Normal) distributions, and know how to perform a simple test, let's move on with some more distributions. The uniform distribution is a special one, as there is a discrete as well as a continuous version of this distribution!\u003c/p\u003e\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\u003cp\u003eYou will be able to:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eDefine a uniform distribution\u003c/li\u003e\n\u003cli\u003eCalculate mean and variance of uniform distribution\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003eUniform Distribution\u003c/h2\u003e\n\u003cp\u003eThe \u003cstrong\u003eUniform Distribution\u003c/strong\u003e describes an event where every possible outcome is equally likely. No single outcome carries any more or less probability of happening than any other possible outcome. \u003cstrong\u003eThe Uniform Distribution can be discrete or continuous\u003c/strong\u003e.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-uniform-distribution/master/images/new_uniform.png\" width=\"500\"\u003e\u003c/p\u003e\n\u003ch2\u003eThe Discrete Uniform Distribution\u003c/h2\u003e\n\u003cp\u003eYou've seen an example of a Discrete Uniform Distribution before: rolling a 6-sided dice. This idea can be extended to an \u003cimg src=\"https://render.githubusercontent.com/render/math?math=n\"\u003e-sided dice. No matter how many sides the dice has, with a fair dice, you'd be equally likely to roll every side.\u003c/p\u003e\n\u003cp\u003eIf \u003cimg src=\"https://render.githubusercontent.com/render/math?math=a\"\u003e is the smallest number and \u003cimg src=\"https://render.githubusercontent.com/render/math?math=b\"\u003e is the biggest number that can be observed (for a fair dice, \u003cimg src=\"https://render.githubusercontent.com/render/math?math=a=1\"\u003e, \u003cimg src=\"https://render.githubusercontent.com/render/math?math=b=6\"\u003e )\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eProbability Mass Function:\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=p(x)%20=%20%5Cdfrac%7B1%7D%7B(b%20-%20a%2b1)%7D%5Ctext%7B%20when%20%7D%20a%20%5Cleq%20x%20%5Cleq%20b\"\u003e\u003c/p\u003e\n\u003cp\u003eand\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=p(x)%20=%200%5Ctext%7B%20when%20%7Dx%20%5Cnotin%20%5Ba,%20b%5D\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eUniform Distribution Mean:\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=E(X)=%20%5Cfrac%7Ba%20%2b%20b%7D%7B2%7D\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eUniform Distribution Variance\u003c/strong\u003e:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=Var(X)%20=%20%5Csqrt%7B%5Cfrac%7B(b-a)(b-a%2b2)%7D%7B12%7D%7D\"\u003e\u003c/p\u003e\n\u003ch2\u003eThe Continuous Uniform Distribution\u003c/h2\u003e\n\u003cp\u003eAn example of a \u003cstrong\u003eContinuous Uniform\u003c/strong\u003e distributed variable would be the waiting time for an elevator that could be on any floor in the building when you call it and can take between 0 and 40 seconds to arrive at your floor. Since the elevator is equally likely to be at any given floor, you can assume every amount of time between 0 and 40 seconds before the elevator arrives (decimals and fractions allowed, to an infinite amount of precision). The formulas for a continuous uniform distribution diverge slightly!\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eProbability Density Function:\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=f(x)%20=%20%5Cdfrac%7B1%7D%7B(b%20-%20a)%7D%5Ctext%7B%20when%20%7D%20a%20%5Cleq%20x%20\u003c%20b\"\u003e\u003c/p\u003e\n\u003cp\u003eand\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=f(x)%20=%200%5Ctext%7B%20when%20%7Dx%20%5Cnotin%20%5Ba,%20b%5D\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eUniform Distribution Mean:\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=E(X)=%20%5Cfrac%7Ba%20%2b%20b%7D%7B2%7D\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eUniform Distribution Variance\u003c/strong\u003e:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=Var(X)%20=%20%5Cfrac%7B(b%20-%20a)%5E2%7D%7B12%7D\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e\u003cem\u003eNOTE:\u003c/em\u003e\u003c/strong\u003e If you're confused why there is a 12 in the denominator of the formula of the variance for a Uniform Distribution, you're not alone. The short answer is that it involves calculus. As a data scientist, you don't need to understand the derivation of this formula and where this 12 comes from. However, if you're interested, this \u003ca href=\"https://www.quora.com/Why-is-there-a-12-in-the-variance-of-uniform-distribution\"\u003eQuora answer gives an excellent explanation\u003c/a\u003e!\u003c/p\u003e\n\u003ch2\u003eSummary\u003c/h2\u003e\n\u003cp\u003eIn this lesson, you learned about the uniform distribution, its applications, and the equations used to compute the mean and variance of a uniformly distributed random variable.\u003c/p\u003e","exportId":"the-uniform-distribution"},{"id":141758,"title":"Poisson Distribution","type":"WikiPage","indent":0,"locked":false,"requirement":null,"completed":false,"content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-poisson-distribution\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-poisson-distribution\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-poisson-distribution/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eIn this lesson, you'll learn about the \u003cstrong\u003ePoisson Distribution\u003c/strong\u003e and explore some practical ways you can use it.\u003c/p\u003e\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\u003cp\u003eYou will be able to:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eExplain the parameters of the Poisson distribution and its use cases\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003eWhat is the Poisson Distribution?\u003c/h2\u003e\n\u003cp\u003eThe \u003cstrong\u003ePoisson Distribution\u003c/strong\u003e is yet another statistical distribution you can use to answer questions about the probability of a given number of successes, the probability of success, and a series of independent trials. Specifically, the Poisson Distribution allows you to calculate the probability of a given event happening by examining the mean number of events that happen in a given time period. Given a set time period, we can use the Poisson Distribution to predict how many times a given event will happen over that time period. To help you better understand this, let's examine a few sample questions that we can answer using the Poisson Distribution.\u003c/p\u003e\n\u003ch3\u003eSample Question 1\u003c/h3\u003e\n\u003cp\u003eAn average of 20 customers walk into a store in a given hour. What is the probability that 25 customers walk into a store in the next hour?\u003c/p\u003e\n\u003ch3\u003eSample Question 2\u003c/h3\u003e\n\u003cp\u003eA police officer pulls over an average of 3 people for speeding violations per shift. What is the probability that the officer will pull over two people for speeding violations during their next shift?\u003c/p\u003e\n\u003ch2\u003eUnderstanding the Parameters\u003c/h2\u003e\n\u003cp\u003eIn order to use the Poisson Distribution, we only need to know a few parameters:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cmu\"\u003e : The average number of successes over a given time period. For instance, the average number of customers that walk into a store in a given hour, or the average number of speeding violations a police officer sees in a shift.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=x\"\u003e : Our random variable - the number of successes we want to find the probability mass of given our knowledge of \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cmu\"\u003e .\u003c/p\u003e\n\u003ch3\u003eRelationship to the Binomial Distribution\u003c/h3\u003e\n\u003cp\u003eThe Poisson distribution has a special relation to the binomial distribution. The theoretical underpinnings are as follows. Imagine that we take a time period and break it into \u003cimg src=\"https://render.githubusercontent.com/render/math?math=n\"\u003e subintervals that are so small that at most one successful event could occur. We can then imagine that for any of these subintervals, a binomial distribution could apply where there is some probability of the event occurring, \u003cimg src=\"https://render.githubusercontent.com/render/math?math=p\"\u003e , a probability \u003cimg src=\"https://render.githubusercontent.com/render/math?math=q=1-p\"\u003e that the event does not occur, and a probability of 0 that more than one event occurs. We assume that as we cut time into smaller and smaller intervals, the chance of success should go down. If we take the limit of the binomial distribution as \u003cimg src=\"https://render.githubusercontent.com/render/math?math=n\"\u003e goes to infinity (more and more subintervals that are progressively smaller and smaller), the result is the Poisson distribution.\u003c/p\u003e\n\u003cp\u003eBinomial Probability Distribution: \u003cimg src=\"https://render.githubusercontent.com/render/math?math=p(x)%20=%20%5Cbinom%7Bn%7D%7Bx%7Dp%5Ex(1-p)%5E%7Bn-x%7D\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Clambda%20=%20n*p\"\u003e\u003c/p\u003e\n\u003cp\u003ePoisson Probability Distribution: \u003cimg src=\"https://render.githubusercontent.com/render/math?math=p(x)%20=%20%5Cfrac%7B%5Clambda%5Exe%5E%7B-%5Clambda%7D%7D%7Bx!%7D\"\u003e\u003c/p\u003e\n\u003cp\u003eAlso note that lambda \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Clambda\"\u003e is the now the average number of successes that we anticipate in a given interval: the probability \u003cimg src=\"https://render.githubusercontent.com/render/math?math=p\"\u003e of success, times the number of intervals \u003cimg src=\"https://render.githubusercontent.com/render/math?math=n\"\u003e . This is then exactly how the Poisson is used in practice--if we know the average number of occurrences in a given interval, what is the probability that the actual number of occurrences is slightly more, slightly less, far more or far less?\u003c/p\u003e\n\u003ch3\u003eUnderstanding the Formula\u003c/h3\u003e\n\u003cp\u003eLet's take another look at the formula for the Poisson Probability Distribution:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=p(x)%20=%20%5Cfrac%7B%5Clambda%5Exe%5E%7B-%5Clambda%7D%7D%7Bx!%7D\"\u003e\u003c/p\u003e\n\u003cp\u003eIn the other statistical distributions we've explored, we were explicitly given the probability of success or failure as one of our parameters. In this example, we are not given this probability--however, we know how likely an event is to occur the mean number of times over a given time period, which means that we actually \u003cstrong\u003edo\u003c/strong\u003e know the probability--we just need to do some basic calculations to uncover this probability.\u003c/p\u003e\n\u003cp\u003eFor instance, if we know that 6 customers walk into a store per hour, we also know enough to calculate the probability that a customer walks in during a given minute. We do this by just dividing the mean number of customers by the length of our interval!\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=p%20=%20%5Cfrac%7B6%7D%7B60%7D%20=%200.1\"\u003e\u003c/p\u003e\n\u003cp\u003eThere is no expectation that customers will walk into a store in evenly spaced intervals--a customer may walk in every 10 minutes on the dot--however, we may also see 3 customers walk in during the first 5 minutes, 3 more customers 10 minutes later, and no other customers for the rest of the hour. Remember, these events are independent, and this is also the mean number per hour. This doesn't mean that we have 6 customers every hour - it's possible that we do, but it's also possible that we have 12 customers one hour and no customers the next hour. It's also possible that in a 10-hour day, 60 customers enter the store during the first hour, and then none for the rest of the day. If your intuition is telling you that this is possible, but not \u003cstrong\u003e\u003cem\u003eplausible\u003c/em\u003e\u003c/strong\u003e because it has a very low probability of happening, you're right--and the probability of this happening is exactly what the Poisson Distribution allows us to calculate!\u003c/p\u003e\n\u003cp\u003eIn light of this, it makes sense for us to calculate the probability that a customer will walk in during \u003cstrong\u003e\u003cem\u003eany given minute\u003c/em\u003e\u003c/strong\u003e, which we discovered by just dividing our mean number of customers per hour by the number of minutes in our interval, showing us that the probability of a customer walking in during any given minute is 0.1, or 10%. This number is our \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Clambda\"\u003e parameter.\u003c/p\u003e\n\u003cp\u003eTake a look at the following graph - note the relationship of each line to its given \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Clambda\"\u003e parameter:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-poisson-distribution/master/images/new_poisson.png\" width=\"400\"\u003e\u003c/p\u003e\n\u003ch3\u003eThe Rest of the Formula\u003c/h3\u003e\n\u003cp\u003eDon't let the other terms in that equation scare you - you've seen them before, and even if you haven't they're quite easy to work with:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=e\"\u003e : Euler's Constant, which is \u003cimg src=\"https://render.githubusercontent.com/render/math?math=e%20%5Capprox%202.71828\"\u003e . You may also know this as the base of the natural logarithm. On calculators, this is the \u003ccode\u003eexp\u003c/code\u003e button. In Python, we can access it by using NumPy's \u003ccode\u003enp.exp()\u003c/code\u003e function.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=x!\"\u003e : The factorial of \u003cimg src=\"https://render.githubusercontent.com/render/math?math=x\"\u003e . For example, \u003cimg src=\"https://render.githubusercontent.com/render/math?math=3!%20=%203%20*%202%20*%201%20=%206\"\u003e\u003c/p\u003e\n\u003ch2\u003eSummary\u003c/h2\u003e\n\u003cp\u003eIn this lesson, you learned about the Poisson Distribution, the Poisson Probability Formula, and how you can use this distribution to solve real-world problems!\u003c/p\u003e","exportId":"poisson-distribution"},{"id":141760,"title":"The Poisson Distribution - Lab","type":"Assignment","indent":1,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":null,"completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-poisson-distribution-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-poisson-distribution-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","exportId":"g6f09a05dcb3184c71a23b39ff743efd0"},{"id":141763,"title":"The Exponential Distribution","type":"Assignment","indent":0,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":null,"completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-exponential-distribution\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-exponential-distribution/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","exportId":"gefa254c77133f56abb94724c82340ff2"},{"id":141767,"title":"The Exponential Distribution - Lab","type":"Assignment","indent":1,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":null,"completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-exponential-distribution-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-exponential-distribution-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","exportId":"g2e8f25bd5db6c49088c8d3cf1f632b63"}]},{"id":15080,"name":"Topic 13: Central Limit Theorem and Confidence Intervals","status":"started","unlockDate":null,"prereqs":[],"requirement":"all","sequential":false,"exportId":"g0ac60a949cbafa19ed7ccbd7ac5942d7","items":[{"id":141362,"title":"Central Limit Theorem and Confidence Intervals - Introduction","type":"WikiPage","indent":0,"locked":false,"requirement":"must_mark_done","completed":true,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-inferential-statistics-section-intro\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-inferential-statistics-section-intro/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eIn this section, you'll be introduced to inferential statistics. You'll learn about sampling, the central limit theorem, and the T-distribution.\u003c/p\u003e\n\u003ch2\u003eDistributions and Sampling\u003c/h2\u003e\n\u003cp\u003eIn this section, we're returning to statistics to broaden and deepen our understanding of distributions and sampling.\u003c/p\u003e\n\u003ch3\u003eSampling\u003c/h3\u003e\n\u003cp\u003eWe'll start by providing an introduction to the idea of \u003cstrong\u003e\u003cem\u003eSampling\u003c/em\u003e\u003c/strong\u003e - selecting a subset of a population to survey. We'll then start to introduce some statistics related to sampling by explaining and showing how to calculate the standard error.\u003c/p\u003e\n\u003ch3\u003eThe Central Limit Theorem\u003c/h3\u003e\n\u003cp\u003eOnce we understand a bit about sampling, we'll explore how we can use it by digging deep into one of the coolest and most important concepts in inferential statistics--the \u003cstrong\u003e\u003cem\u003eCentral Limit Theorem\u003c/em\u003e\u003c/strong\u003e! We'll start by learning about how the Central Limit Theorem works, and explore how we can use it in a way that allows us to treat non-normal distributions as normal distributions, and provides a way for us to estimate parameters about a population.\u003c/p\u003e\n\u003ch3\u003eThe T-Distribution\u003c/h3\u003e\n\u003cp\u003eFinally, we'll end this section by learning about how we can use the \u003cstrong\u003e\u003cem\u003eT-Distribution\u003c/em\u003e\u003c/strong\u003e for dealing with samples that are smaller, and that have an unknown standard deviation. We'll explore how the T-Distribution works, learn about \u003cem\u003edegrees of freedom\u003c/em\u003e, and then see how we can calculate confidence intervals using our newfound knowledge of the T-Distribution.\u003c/p\u003e\n\u003ch2\u003eSummary\u003c/h2\u003e\n\u003cp\u003eWhile some of this material may seem a little dry, a deep understanding of and intuition for distributions and sampling will be important in your career as a data scientist. This knowledge will help you avoid making mistakes in your EDA (exploratory data analysis), feature selection, and modeling work, which could lead to faulty predictions from your models.\u003c/p\u003e\n\u003cfooter class=\"fis-footer\" style=\"visibility: hidden;\"\u003e\n\u003cdiv class=\"fis-feedback\"\u003e\n\u003ch5\u003eHow do you feel about this lesson?\u003c/h5\u003e\n\u003cimg id=\"thumbs-up\" title=\"Thumbs up!\" alt=\"thumbs up\" data-repository=\"dsc-inferential-statistics-section-intro\"\u003e\u003cimg id=\"thumbs-down\" title=\"Thumbs down!\" alt=\"thumbs down\" data-repository=\"dsc-inferential-statistics-section-intro\"\u003e\n\u003c/div\u003e\n\u003ch5\u003eHave specific feedback? \u003ca href=\"https://github.com/learn-co-curriculum/dsc-inferential-statistics-section-intro/issues/new\"\u003eTell us here!\u003c/a\u003e\n\u003c/h5\u003e\n\u003c/footer\u003e","exportId":"central-limit-theorem-and-confidence-intervals-introduction"},{"id":141366,"title":"Introduction to Sampling","type":"Assignment","indent":0,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_mark_done","completed":true,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-introduction-to-sampling\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-introduction-to-sampling/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","exportId":"gb20db705208d830723544a65c459ce36"},{"id":141369,"title":"Central Limit Theorem","type":"WikiPage","indent":0,"locked":false,"requirement":"must_mark_done","completed":false,"content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-central-limit-theorem\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-central-limit-theorem\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-central-limit-theorem/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eIn this lesson, we'll start to investigate a \u003cem\u003ecentral\u003c/em\u003e statistical concept; the central limit theorem! (And how to write a good dry math pun.)\u003c/p\u003e\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\u003cp\u003eYou will be able to:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eDescribe how the central limit theorem is related to sampling\u003c/li\u003e\n\u003cli\u003eDescribe how the central limit theorem can be used for parameter estimation\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003eWhat does the Central Limit Theorem stand for?\u003c/h2\u003e\n\u003cp\u003eThe central limit theorem states that, under many conditions, independent random variables summed together will converge to a normal distribution as the number of variables increases. This becomes very useful for applying statistical logic to sample statistics in order to estimate population parameters. For example, as we saw in the previous lecture, the averages of samples will form a normal distribution. We can then use this information to put further bounds on our estimates of the population. We can also use this information to estimate the probability of samples taking on extreme values that deviate from the population mean.\u003c/p\u003e\n\u003cp\u003eFor example, let's say that we know the mean and standard deviation of asthma rates in the United States. If we then take a sample from a specific city and find that the mean of this sample is substantially lower than that of the overall population, we may be interested in questions such as \"what is the probability that this was just caused by random chance in sampling?\" If the probability is exceedingly low, we have further reason to believe that this city has higher rates of asthma and that its population is statistically different then that of the general population.\u003c/p\u003e\n\u003cp\u003eThe computation would be something like this: we know the mean population, and by the central limit theorem, the average of various samples takes on a normal distribution. From that normal distribution of sample means, we can then compare the mean of our actual sample and compare it to the distribution of means. It should be quite rare that our sample mean falls outside 2 or 3 standard deviations from the mean of sample means, (roughly 2.35% and .15% respectively for each tail). As such, having a sample mean that falls outside of these scopes is worthy of further investigation.\u003c/p\u003e\n\u003cp\u003eFor reference, here's is a rough empirical rule for percentiles within a normal distribution. (And again, by the central limit theorem, we expect our sample means to take on a normal distribution!)\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-central-limit-theorem/master/images/new_CentralLimitTheorem.png\" width=\"600\"\u003e\u003c/p\u003e\n\u003ch2\u003eAdditional Resources\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"http://sphweb.bumc.bu.edu/otlt/MPH-Modules/BS/BS704_Probability/BS704_Probability13.html\"\u003eApplication of the Central Limit Theorem page, by the Boston University School of Public Health\u003c/a\u003e\u003c/p\u003e\n\u003ch2\u003eSummary\u003c/h2\u003e\n\u003cp\u003eIn this brief lesson, we continued to discuss the central limit theorem and its application for sampling statistics and confidence intervals.\u003c/p\u003e","exportId":"central-limit-theorem"},{"id":141374,"title":"Central Limit Theorem - Lab","type":"Assignment","indent":1,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_mark_done","completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-central-limit-theorem-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-central-limit-theorem-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","exportId":"g772df31476f2ba5011a59cdd20744c3e"},{"id":141379,"title":"Sampling Statistics - Lab","type":"Assignment","indent":1,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_mark_done","completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-sampling-statistics-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-sampling-statistics-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","exportId":"gf1cd3018d0cc0e9bf45afe0606c00209"},{"id":141381,"title":"Confidence Intervals - Lab","type":"Assignment","indent":1,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_mark_done","completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-confidence-intervals-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-confidence-intervals-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","exportId":"gcbaf0fc2f6b7b95100b633a509ccc15d"},{"id":141384,"title":"Confidence Intervals with T Distribution","type":"Assignment","indent":0,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_mark_done","completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-intervals-with-t-distribution\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-intervals-with-t-distribution/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","exportId":"g4adbc7b98a3576855368b2984d52ff29"},{"id":141388,"title":"Confidence Intervals with T Distribution - Lab","type":"Assignment","indent":1,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_mark_done","completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-intervals-with-t-distribution-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-intervals-with-t-distribution-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","exportId":"g66374fdc758e947e88c4136dd7ac5783"},{"id":141391,"title":"Central Limit Theorem and Confidence Intervals - Recap","type":"WikiPage","indent":0,"locked":false,"requirement":"must_mark_done","completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-inferential-statistics-section-recap\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-inferential-statistics-section-recap/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eThis short lesson summarizes the topics we covered in this section and why they'll be important to you as a data scientist.\u003c/p\u003e\n\u003ch2\u003eKey Takeaways\u003c/h2\u003e\n\u003cp\u003eThis section was all about building further on your statistics foundations by introducing the Central Limit Theorem and confidence intervals. Some of the key takeaways include:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eThe Central Limit Theorem states that often, independent random variables summed together will converge to a normal distribution as the number of variables increases\u003c/li\u003e\n\u003cli\u003eUsing the Central Limit Theorem, we can work with non-normally distributed data sets as if they were normally distributed\u003c/li\u003e\n\u003cli\u003eThe Standard Error is a measure of spread - it is the standard deviation of samples from the sample mean\u003c/li\u003e\n\u003cli\u003eIf you take repeated samples and compute the 95% confidence interval for a given parameter for each sample, 95% of the intervals would contain the population parameter.\u003c/li\u003e\n\u003cli\u003eThe z-critical value is the number of standard deviations you'd have to go from the mean of the normal distribution to capture the proportion of the data associated with the desired confidence level.\u003c/li\u003e\n\u003cli\u003eIf you don't know the standard deviation for a population, you need to use t-distributions to compute the margin of error for calculating a confidence interval.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cfooter class=\"fis-footer\" style=\"visibility: hidden;\"\u003e\n\u003cdiv class=\"fis-feedback\"\u003e\n\u003ch5\u003eHow do you feel about this lesson?\u003c/h5\u003e\n\u003cimg id=\"thumbs-up\" title=\"Thumbs up!\" alt=\"thumbs up\" data-repository=\"dsc-inferential-statistics-section-recap\"\u003e\u003cimg id=\"thumbs-down\" title=\"Thumbs down!\" alt=\"thumbs down\" data-repository=\"dsc-inferential-statistics-section-recap\"\u003e\n\u003c/div\u003e\n\u003ch5\u003eHave specific feedback? \u003ca href=\"https://github.com/learn-co-curriculum/dsc-inferential-statistics-section-recap/issues/new\"\u003eTell us here!\u003c/a\u003e\n\u003c/h5\u003e\n\u003c/footer\u003e","exportId":"central-limit-theorem-and-confidence-intervals-recap"}]},{"id":15081,"name":"Topic 14: Hypothesis Testing","status":"started","unlockDate":null,"prereqs":[],"requirement":"all","sequential":false,"exportId":"g86de2853f1c97ed93d8aa38ae07de7d2","items":[{"id":141402,"title":"Hypothesis Testing - Introduction","type":"WikiPage","indent":0,"locked":false,"requirement":"must_mark_done","completed":true,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-hypothesis-testing-intro-v2-1\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-hypothesis-testing-intro-v2-1/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eIn this section, you'll learn about experimental design and hypothesis testing. All scientific research that comes out of universities uses hypothesis testing to determine if the results of an experiment are significant or not. As a data scientist, you might be tasked with designing, performing, and analyzing the results of an experiment. Finally, you'll also learn about resampling methods, which are modern statistical techniques that involve taking repeated subsamples from a sample and help better estimate the precision of your sample statistics.\u003c/p\u003e\n\u003ch2\u003eHypothesis Testing\u003c/h2\u003e\n\u003cp\u003eIn this section, you'll be looking at experimental design, effect size, T-tests, Type 1 and Type 2 errors, and resampling techniques like the jackknife, bootstrap, and permutation tests.\u003c/p\u003e\n\u003ch3\u003eExperimental Design\u003c/h3\u003e\n\u003cp\u003eWithout good experimental design, it's very easy to draw the wrong conclusions from your experiments. Because of that, you'll kick this section off by looking at the scientific method and the key elements of good experimental design - forming \u003cstrong\u003ealternative\u003c/strong\u003e and \u003cstrong\u003enull hypotheses\u003c/strong\u003e, conducting an experiment, analyzing the results for statistical significance and drawing conclusions.\u003c/p\u003e\n\u003ch3\u003eEffect Size\u003c/h3\u003e\n\u003cp\u003eWe then look at how to calculate and interpret the size of the difference between control and test groups. We'll see how the \"Effect Size\" can be used to communicate the practical significance of experimental results, to perform meta-analyses of multiple studies, and to perform power analysis to determine the number of participants that a study would require to achieve a certain probability of finding a true effect.\u003c/p\u003e\n\u003ch3\u003eOne and Two Sample T-tests\u003c/h3\u003e\n\u003cp\u003eNext, you'll also look at t-tests and how they can be used to compare two averages to see how significant the differences are between one or two samples once we have defined the experimental design.\u003c/p\u003e\n\u003ch3\u003eType 1 and Type 2 Errors\u003c/h3\u003e\n\u003cp\u003eFrom there, you'll learn about \u003cstrong\u003etype 1 (false positive)\u003c/strong\u003e and \u003cstrong\u003etype 2 (false negative) errors\u003c/strong\u003e and the inherent tradeoff between them.\u003c/p\u003e\n\u003ch3\u003eJackknife, Bootstrap, and Permutation Tests\u003c/h3\u003e\n\u003cp\u003eWe'll look at techniques for taking repeated subsamples from a sample using bootstrapping, jackknife and permutation tests to better estimate the precision of your sample statistics or validate models by using random subsets.\u003c/p\u003e\n\u003ch2\u003eSummary\u003c/h2\u003e\n\u003cp\u003eWithout a good understanding of experimental design, it's easy to end up confusing spurious correlations for meaningful results or placing too much (or too little) weight on the results of any given test. In this section, we cover a range of tools and techniques to ensure that you design your experiments rigorously and interpret them thoughtfully.\u003c/p\u003e\n\u003cfooter class=\"fis-footer\" style=\"visibility: hidden;\"\u003e\n\u003cdiv class=\"fis-feedback\"\u003e\n\u003ch5\u003eHow do you feel about this lesson?\u003c/h5\u003e\n\u003cimg id=\"thumbs-up\" title=\"Thumbs up!\" alt=\"thumbs up\" data-repository=\"dsc-hypothesis-testing-intro-v2-1\"\u003e\u003cimg id=\"thumbs-down\" title=\"Thumbs down!\" alt=\"thumbs down\" data-repository=\"dsc-hypothesis-testing-intro-v2-1\"\u003e\n\u003c/div\u003e\n\u003ch5\u003eHave specific feedback? \u003ca href=\"https://github.com/learn-co-curriculum/dsc-hypothesis-testing-intro-v2-1/issues/new\"\u003eTell us here!\u003c/a\u003e\n\u003c/h5\u003e\n\u003c/footer\u003e","exportId":"hypothesis-testing-introduction"},{"id":141406,"title":"Introduction to Experimental Design","type":"WikiPage","indent":0,"locked":false,"requirement":"must_mark_done","completed":false,"content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-experimental-design\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-experimental-design\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-experimental-design/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eIn this lesson, you'll learn about the importance of sound experimental design, and how it underpins every decision you will make as a Data Scientist!\u003c/p\u003e\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\u003cp\u003eYou will be able to:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eList the steps of the scientific method\u003c/li\u003e\n\u003cli\u003eExplain the purpose of control/experimental groups\u003c/li\u003e\n\u003cli\u003eList four assumptions for appropriate sampling techniques and sample size\u003c/li\u003e\n\u003cli\u003eCompare and explain the importance of different kinds of randomized control trials\u003c/li\u003e\n\u003cli\u003eSet up null and alternative hypotheses\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003eThe Scientific Method\u003c/h2\u003e\n\u003cp\u003eYou probably remember at least a little bit about the \u003cstrong\u003e\u003cem\u003eScientific Method\u003c/em\u003e\u003c/strong\u003e from your time in school. This lesson will focus on the thing that makes it work--sound experimental design! The scientific method has been responsible for all the great progress humanity has seen in everything from medicine to physics to electronics, all because scientists working on problems knew how to design experiments in a way that helped them answer important questions with as little ambiguity as possible. If the scientific method was a car, then experimental design would be the engine that allows that car to move. This is especially important to Data Scientists because it allows them to examine any problem through the lens of the \u003cstrong\u003e\u003cem\u003eNull Hypothesis\u003c/em\u003e\u003c/strong\u003e!\u003c/p\u003e\n\u003cp\u003eThe general structure of an experiment is as follows:\u003c/p\u003e\n\u003ch3\u003e1. Make an Observation\u003c/h3\u003e\n\u003cp\u003eThe first step of the scientific method is to observe something that you want to test. During this step, you must observe phenomena to help refine the question that you want to answer. This might be anything from \"does this drug have an effect on headaches?\" to \"does the color of this button affect the number of sales a website makes in a day?\". Before testing these ideas, you need to observe that there might be some phenomena occurring and then come up with a specific question to answer.\u003c/p\u003e\n\u003ch3\u003e2. Examine the Research\u003c/h3\u003e\n\u003cp\u003eGood data scientists work smart before they work hard. In the case of the scientific method, this means seeing what research already exists that may help you answer your question, directly or indirectly. It could be that someone else has already done an experiment that answers your question--if that's the case, you should be aware of that experiment before starting your own, as it could inform your approach to structuring your experiment, or maybe even answer your question outright!\u003c/p\u003e\n\u003ch3\u003e3. Form a Hypothesis\u003c/h3\u003e\n\u003cp\u003eThis is the stage that most people remember from learning the scientific method in grade school. In school, you learned that a hypothesis is just an educated guess that you will try to prove by conducting an experiment. In reality, it's a bit more complicated than that. During this stage, you'll formulate 2 hypotheses to test--your educated guess about the outcome is called the \u003cstrong\u003e\u003cem\u003eAlternative Hypothesis\u003c/em\u003e\u003c/strong\u003e, while the opposite of it is called the \u003cstrong\u003e\u003cem\u003eNull Hypothesis\u003c/em\u003e\u003c/strong\u003e. This is where the language behind experimental design (and the idea of what you can actually \u003cstrong\u003e\u003cem\u003eprove\u003c/em\u003e\u003c/strong\u003e using an experiment) gets a bit complicated--more on this below.\u003c/p\u003e\n\u003ch3\u003e4. Conduct an Experiment\u003c/h3\u003e\n\u003cp\u003eThis step is the part of the scientific method that will be the focus of this section. You can only test a hypothesis by gathering data from a well-structured experiment. A well-structured experiment is one that accounts for all of the mistakes and randomness that could give you false signals relating to the effect of an intervention. Just because you're running an experiment doesn't prove that A causes B, or that there's even a relationship between A and B! A poorly designed experiment will lead to false conclusions that you haven't considered or controlled for. A well-designed experiment leaves you no choice but to acknowledge that the effects seen in a dependent variable are related to an independent variable. The world is messy and random. You have to account for this messiness and randomness in experiments so that you can filter it out and be left only with the things you're actively trying to measure.\u003c/p\u003e\n\u003ch3\u003e5. Analyze Experimental Results\u003c/h3\u003e\n\u003cp\u003eWhether you realize it or not, you've already gotten pretty good at this step! All the work you've done with statistics is usually in service of this goal--looking at the data and understanding what happened. During this step, you will tease out relationships, filter out noise, and try to determine if something that happened is \u003cstrong\u003e\u003cem\u003estatistically significant\u003c/em\u003e\u003c/strong\u003e or not.\u003c/p\u003e\n\u003ch3\u003e6. Draw Conclusions\u003c/h3\u003e\n\u003cp\u003eThis step is the logical endpoint for an experiment. You've asked a question, looked at experimental results from others that could be related to your question, made an educated guess, designed an experiment, collected data, and analyzed the results. All that is left is to use the results of the analysis step to evaluate whether you believe the hypothesis was correct or not! While the public generally oversimplifies this step for determining causal relationships (e.g. \"my experiment showed that {x} causes {y}\"), true scientists rarely make claims so bold. The reality of this step is that you use your analysis of the data to do one of two things: either \u003cstrong\u003e\u003cem\u003ereject the null hypothesis or fail to reject the null hypothesis\u003c/em\u003e\u003c/strong\u003e. This is a tricky concept, so you'll explore it in much more detail in a future lesson.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-experimental-design/master/images/new_the-scientific-method.png\" width=\"650\"\u003e\u003c/p\u003e\n\u003ch2\u003eThe Foundations of a Sound Experiment\u003c/h2\u003e\n\u003cp\u003eAll experiments are not created equal--simply following the steps outlined above does not guarantee that the results of any experiment will be meaningful. For instance, there's nothing stopping a person from testing the hypothesis that \"wearing a green shirt will make it rain tomorrow!\", seeing rain the next day, and rejecting the null hypothesis, thereby incorrectly \"proving\" that their choice of wardrobe affected the weather. Good experiments demonstrate that independent variables {X} have an effect on the dependent variables {Y} because you control for all the other things that could be affecting {Y}, until you are forced to conclude that the only thing that explains what happened to {Y} is {X}!\u003c/p\u003e\n\u003cp\u003eAlthough there are many different kinds of experiments, there are some fundamental aspects of experimental design that all experiments have:\u003c/p\u003e\n\u003ch3\u003e1. A Control Group/Random Controlled Trials\u003c/h3\u003e\n\u003cp\u003eOne of the most important aspects of a sound experiment is the use of a \u003cstrong\u003e\u003cem\u003eControl Group\u003c/em\u003e\u003c/strong\u003e. A Control Group is a cohort that receives no treatment or intervention--for them, it's just business as usual. In a medical test, this might be a \u003cstrong\u003e\u003cem\u003eplacebo\u003c/em\u003e\u003c/strong\u003e, such as a sugar pill. In the example of testing the color of a button on a website, this would be customers that are shown a version of the website with the button color unchanged. Using a control group allows researchers to compare the results of doing nothing (the control) with the effects of doing something (the \u003cstrong\u003e\u003cem\u003eintervention\u003c/em\u003e\u003c/strong\u003e). Without a control group, you have no way of knowing how much of the results you see can be attributed to the intervention, and how much would have happened anyways.\u003c/p\u003e\n\u003cp\u003eTo make this more obvious, consider what you can actually know with confidence after an experiment that doesn't use a control. Assume that a pharmaceutical company decides to test a new drug that is supposed to reduce the amount of time someone has the flu. The company gives the drug to all participants in the study. After analyzing the data, you find that the average length of time a person had the flu was 12 days. Was the drug effective, or not? Without a control, you don't know how long this flu would have lasted if these people were never given a drug. It could be that your drug reduced the time of infection down to 12 days. Then again, it could be that these people would have gotten better on their own after 12 days, and the drug didn't really do anything--or maybe they would have gotten better in 10 days, and the drug made it worse! By using a control group that gets no drugs and recovers naturally, you can compare the results of the treatment (people that received the experimental flu drug) to your control group (people that recovered naturally).\u003c/p\u003e\n\u003cp\u003eNote that a control group is only a control group if they are sampled from the same population as the treatment groups! If they aren't the same, then there's no way of knowing how much the difference in recovery time should be attributed to the flu drug, and how much should be attributed to the way(s) in which the control group is different. For instance, the experiment would not be very effective if the average age of one group was much higher or lower than another. If that was the case, how would you know the age difference isn't actually causing the difference in results (or lack thereof) between the control and treatment groups, instead of the drug intervention?\u003c/p\u003e\n\u003cp\u003eThe main way scientists deal with this is through \u003cstrong\u003e\u003cem\u003eRandom Controlled Trials\u003c/em\u003e\u003c/strong\u003e. In a Random Controlled Trial, there is a control group and an intervention (also called treatment) group, where subjects are \u003cstrong\u003e\u003cem\u003erandomly assigned to each\u003c/em\u003e\u003c/strong\u003e. You may have heard the term \u003cstrong\u003e\u003cem\u003eSingle-Blind\u003c/em\u003e\u003c/strong\u003e and \u003cstrong\u003e\u003cem\u003eDouble-Blind\u003c/em\u003e\u003c/strong\u003e studies--these refer to people knowing which groups they are in. In a sound experiment, people should not know if they are in the treatment group or the control group, as that could potentially affect the outcome of the trial!\u003c/p\u003e\n\u003cp\u003eA \u003cstrong\u003e\u003cem\u003eSingle-Blind\u003c/em\u003e\u003c/strong\u003e or \u003cstrong\u003e\u003cem\u003eBlind Trial\u003c/em\u003e\u003c/strong\u003e is one where the participant does not know if they are receiving the treatment or a placebo.\u003c/p\u003e\n\u003cp\u003eA \u003cstrong\u003e\u003cem\u003eDouble-Blind Trial\u003c/em\u003e\u003c/strong\u003e is one where the participant does not know if they are receiving the treatment or a placebo, and neither does the person administering the experiment (because their bias could affect the outcomes, too!). Instead, knowing whether someone received the treatment or a placebo is kept hidden from everyone until after the experiment is over (obviously, \u003cem\u003esomeone\u003c/em\u003e has to know for recordkeeping purposes, but that person stays away from the actual experiment to avoid contaminating it with bias from that knowledge).\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-experimental-design/master/images/new_double_blind.png\" width=\"650\"\u003e\u003c/p\u003e\n\u003ch3\u003e2. Appropriate Sampling Techniques and Sample Size\u003c/h3\u003e\n\u003cp\u003eWhen data scientists are performing experiments, they rarely have the opportunity to work with an entire population of data. Rather, they must obtain a sample that is representative of the population. In order to get a high quality sample, you should follow these four assumptions related to sampling techniques and sample size.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eSample is independent\u003c/strong\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eIndependence means the value of one observation does not influence or affect the value of other observations. Independent data items are not connected with one another in any way (unless you account for it in your model). This includes the observations in both the “between” and “within” groups of your sample. Non-independent observations introduce bias and can make your statistical test give too many false positives.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eSample is collected randomly\u003c/strong\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eA sample is random when each data point in your population has an equal chance of being included in the sample; therefore, the selection of any individual observation happens by chance, rather than by choice. This reduces the chance that differences in materials or conditions strongly bias results. Random samples are more likely to be representative of the population; therefore, you can be more confident with your statistical inferences with a random sample.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eThe sample is approximately normally distributed\u003c/strong\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThe normal distribution assumption is that the sampling distribution of the mean is normal. That is, if you took a sample, calculated its mean, and then you took another (independent) sample (from the same population) and got its mean (and repeated this an infinite number of times), then the distribution of the values that you wrote down would always be a perfect bell curve. This is the principle behind the Central Limit Theorem, and it is this idea that allows us to perform hypothesis tests. While maybe surprising, this assumption turns out to be relatively uncontroversial, at least when each of the samples is large, such as N ≥ 30.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eAppropriate Sample Size\u003c/strong\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eRandomness is a big problem in experiments. It can lead you to false conclusions by making you think that something doesn't matter when it does, or vice versa. Small sample sizes make experiments susceptible to the problem of randomness; whereas, large sample sizes protect experiments from it. The following scenario illustrates this point:\u003c/p\u003e\n\u003cp\u003eA person tells you that they can predict the outcome of a fair coin flip. You flip a coin, they call \"tails\", and they are correct. Is this enough evidence to accept or reject this person's statement? What if they got it right 2 times in a row? 5 times in a row? 55 times out of 100?\u003c/p\u003e\n\u003cp\u003eThis situation illustrates two things that are important for us to understand and acknowledge:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003eNo matter how large your sample size, there's always a chance that your results can be attributed to randomness or luck.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eAt some point, you would cross a threshold where random chance is small enough that you'd say \"this probably isn't random\", and are okay with accepting the results as the result of something other than randomness or luck.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eWith the situation above, you probably wouldn't assume that this person can predict coin flips after only seeing them get 1 correct. However, if this person got 970 out of 1000 correct, you would probably believe very strongly that this person \u003cem\u003ecan\u003c/em\u003e predict coin flips because the odds of guessing randomly and getting 970/1000 correct are very, very small--but not 0!\u003c/p\u003e\n\u003cp\u003eLarge sample sizes protect us from randomness and variance. A more realistic example would be testing a treatment for HIV. Less than 1% of the global population carries a protective mutation that makes them resistant to HIV infection. If you took a randomly selected sample of 1 person from the population, there is a ~1% chance that you may mistakenly attribute successful prevention to the drug you're testing, when the results really happened because you randomly selected a person with this mutation. However, if your sample size was 100 people per sample, your odds of randomly selecting 100 people with that mutation are \u003cimg src=\"https://render.githubusercontent.com/render/math?math=0.01%5E%7B100%7D\"\u003e . The larger your sample size, the more unlikely it is that you randomly draw people that happen to affect your study in a way that is not reflected by the general population.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-experimental-design/master/images/new_sample_size.png\" width=\"650\"\u003e\u003c/p\u003e\n\u003ch3\u003e3. Reproducibility\u003c/h3\u003e\n\u003cp\u003eThis one is a big one, and it represents a bit of a crisis in some parts of the scientific community right now. Good scientific experiments have \u003cstrong\u003e\u003cem\u003eReproducible Results\u003c/em\u003e\u003c/strong\u003e! This means that if someone else follows the steps you outline for your experiment and performs it themselves, they should get pretty much the same results as you did (allowing for natural variance and randomness). If many different people try reproducing your experiment and don't get the same results, this might suggest that your results are due to randomness, or to a \u003cstrong\u003e\u003cem\u003elurking variable\u003c/em\u003e\u003c/strong\u003e that was present in your samples that wasn't present in others. Either way, a lack of reproducibility often casts serious doubts on the results of a study or experiment.\u003c/p\u003e\n\u003cp\u003eThis is less of a problem for data scientists, since reproducibility usually just means providing the dataset you worked with and the corresponding Jupyter notebook. However, this isn't always the case! Luckily, you can use code to easily run your experiments multiple times and show reproducibility. When planning experiments, consider running them multiple times to ensure to really help show that your results are sound, and not due to randomness!\u003c/p\u003e\n\u003ch2\u003eSummary\u003c/h2\u003e\n\u003cp\u003eGreat, you now know about experimental design and the fundamental aspects of experiments!\u003c/p\u003e","exportId":"introduction-to-experimental-design"},{"id":141409,"title":"P-Values and the Null Hypothesis","type":"WikiPage","indent":0,"locked":false,"requirement":"must_mark_done","completed":true,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-p-values-and-null-hypothesis\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-p-values-and-null-hypothesis/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eIn this lesson, you'll learn about the relationship between p-values and the Null Hypothesis, and their role in designing an experiment. \u003c/p\u003e\n\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\n\u003cp\u003eYou will be able to:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eDescribe what it means to \"reject the null hypothesis\" and how it is related to p-value\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch2\u003eUnderstanding  The Null Hypothesis\u003c/h2\u003e\n\n\u003cp\u003eAs stated previously, scientific experiments actually have 2 hypotheses:\u003c/p\u003e\n\n\u003cp\u003e\u003cstrong\u003e\u003cem\u003eNull Hypothesis\u003c/em\u003e\u003c/strong\u003e: There is no relationship between A and B\u003cbr\u003e\nExample: \"There is no relationship between this flu medication and a reduced recovery time from the flu\".\u003c/p\u003e\n\n\u003cp\u003eThe \u003cem\u003eNull Hypothesis\u003c/em\u003e is usually denoted as  \u003cimg src=\"https://render.githubusercontent.com/render/math?math=H_%7B0%7D\"\u003e \u003c/p\u003e\n\n\u003cp\u003e\u003cstrong\u003e\u003cem\u003eAlternative Hypothesis\u003c/em\u003e\u003c/strong\u003e: The hypothesis traditionally thought of when creating a hypothesis for an experiment\u003cbr\u003e\nExample: \"This flu medication reduces recovery time for the flu.\"\u003c/p\u003e\n\n\u003cp\u003eThe \u003cem\u003eAlternative Hypothesis\u003c/em\u003e is usually denoted as  \u003cimg src=\"https://render.githubusercontent.com/render/math?math=H_%7B1%7D\"\u003e \u003c/p\u003e\n\n\u003cp\u003eAn easy way to differentiate between the Null Hypothesis and the Alternative Hypothesis is that the Null Hypothesis is the more conservative choice. It always assumes that there is no difference between two different population means, and when it is represented mathematically, it should always contain an equals sign. \u003c/p\u003e\n\n\u003cp\u003eThe Alternative Hypothesis is whatever claim you are trying to prove with an experiment.\u003c/p\u003e\n\n\u003ch3\u003eP-Values and Alpha Values\u003c/h3\u003e\n\n\u003cp\u003eNo matter what you're experimenting on, good experiments come down to one question: Is your p-value less than your alpha value? Let's dive into what each of these values represents, and why they're so important to experimental design. \u003c/p\u003e\n\n\u003cp\u003e\u003cstrong\u003e\u003cem\u003ep-value\u003c/em\u003e\u003c/strong\u003e: The probability of observing a test statistic at least as large as the one observed, by random chance, assuming that the null hypothesis is true.\u003c/p\u003e\n\n\u003cp\u003eIf you calculate a p-value and it comes out to 0.03, you can interpret this as saying \"There is a 3% chance of obtaining the results I'm seeing when the null hypothesis is true.\"  \u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Calpha\"\u003e \u003cstrong\u003e\u003cem\u003e(alpha value)\u003c/em\u003e\u003c/strong\u003e: The marginal threshold at which you're okay with rejecting the null hypothesis. \u003c/p\u003e\n\n\u003cp\u003eAn alpha value can be any value set between 0 and 1. However, the most common alpha value in science is 0.05 (although this is somewhat of a controversial topic in the scientific community, currently).  \u003c/p\u003e\n\n\u003cp\u003eIf you set an alpha value of  \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Calpha%20=%200.05\"\u003e , you're essentially saying \"I'm okay with accepting my alternative hypothesis as true if there is less than a 5% chance that the results that I'm seeing are actually due to randomness.\"\u003c/p\u003e\n\n\u003cp\u003eWhen you conduct an experiment, your goal is to calculate a p-value and compare it to the alpha value. If  \u003cimg src=\"https://render.githubusercontent.com/render/math?math=p%20\u003c%20%5Calpha\"\u003e , then you \u003cstrong\u003e\u003cem\u003ereject the null hypothesis\u003c/em\u003e\u003c/strong\u003e and accept that there is not \"no relationship\" between the dependent and independent variables.  Note that any good scientist will admit that this doesn't prove that there is a \u003cem\u003edirect relationship\u003c/em\u003e between the dependent and independent variables--just that they now have enough evidence to the contrary to show that they can no longer believe that there is no relationship between them. \u003c/p\u003e\n\n\u003cp\u003eIn simple terms:\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=p%20\u003c%20%5Calpha\"\u003e : Reject the \u003cem\u003eNull Hypothesis\u003c/em\u003e and accept the \u003cem\u003eAlternative Hypothesis\u003c/em\u003e\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=p%20\u003e=%20%5Calpha\"\u003e : Fail to reject the \u003cem\u003eNull Hypothesis\u003c/em\u003e.  \u003c/p\u003e\n\n\u003cp\u003eThere are many different ways that you can structure a hypothesis statement, but they always come down to this comparison in the end.  In normally distributed data, you calculate p-values from t-statistics or ( \u003cimg src=\"https://render.githubusercontent.com/render/math?math=z\"\u003e -scores if the population parameters are known). This is done a bit differently with discrete data. You may also have \u003cstrong\u003e\u003cem\u003eOne-Tail\u003c/em\u003e\u003c/strong\u003e and \u003cstrong\u003e\u003cem\u003eTwo-Tail\u003c/em\u003e\u003c/strong\u003e tests.  \u003c/p\u003e\n\n\u003cp\u003eA \u003cstrong\u003e\u003cem\u003eOne-Tail Test\u003c/em\u003e\u003c/strong\u003e is when you want to know if a parameter from the treatment group is greater than (or less than) a corresponding parameter from the control group.\u003c/p\u003e\n\n\u003cp\u003e\u003cstrong\u003e\u003cem\u003eExample One-Tail Hypothesis\u003c/em\u003e\u003c/strong\u003e\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=H_%7B1%7D%20:%20%5Cmu_1%20\u003c%20%5Cmu_2\"\u003e The treatment group given this weight loss drug will lose more weight on average than the control group that was given a competitor's weight loss drug \u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=H_%7B0%7D%20:%20%5Cmu1%20\u003e=%20%5Cmu_2\"\u003e  The treatment group given this weight loss drug will not lose more weight on average than the control group that was given a competitor's weight loss drug\". \u003c/p\u003e\n\n\u003cp\u003eA \u003cstrong\u003e\u003cem\u003eTwo-Tail Test\u003c/em\u003e\u003c/strong\u003e is for when you want to test if a parameter falls between (or outside of) a range of two given values. \u003c/p\u003e\n\n\u003cp\u003e\u003cstrong\u003e\u003cem\u003eExample Two-Tail Hypothesis\u003c/em\u003e\u003c/strong\u003e\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=H_%7B1%7D%20:%20%5Cmu_1%20%5Cneq%20%5Cmu_2\"\u003e \"People in the experimental group that are administered this drug will not lose the same amount of weight as the people in the control group.  They will be heavier or lighter\". \u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=H_%7B0%7D%20:%20%5Cmu_1%20=%20%5Cmu_2\"\u003e \"People in the experimental group that are administered this drug will lose the same amount of weight as the people in the control group.\" \u003c/p\u003e\n\n\u003ch4\u003eWhat Does an Experiment Really Prove?\u003c/h4\u003e\n\n\u003cp\u003eYou may be wondering why you need a \u003cstrong\u003e\u003cem\u003eNull Hypothesis\u003c/em\u003e\u003c/strong\u003e at all. This is a good question. It has to do with being honest about what an experiment actually proves.\u003c/p\u003e\n\n\u003cp\u003eScientists use the \u003cstrong\u003e\u003cem\u003eNull Hypothesis\u003c/em\u003e\u003c/strong\u003e so that they can be very specific in their findings. This is because a successful experiment doesn't actually \u003cem\u003eprove a relationship\u003c/em\u003e between a dependent and independent variable.  Instead, it just proves that there is not enough evidence to convincingly believe there is \u003cem\u003eno relationship\u003c/em\u003e between the dependent and the independent variable. There can always be a lurking variable behind the scenes that is actually responsible for the relationship between two variables--it's almost impossible to cover every possible angle. However, a successful experiment where a p-value is less than an alpha value (typically,  \u003cimg src=\"https://render.githubusercontent.com/render/math?math=p%20\u003c%200.05\"\u003e ) does give enough information to confidently allow someone to say that it's statistically unlikely that there is \u003cem\u003eno relationship\u003c/em\u003e between the two, which is what would have to be true in order for the null hypothesis to be correct!\u003c/p\u003e\n\n\u003ch2\u003eThe Null Hypothesis Loves You and Wants You To Be Happy\u003c/h2\u003e\n\n\u003cp\u003eYou've covered a lot about the null hypothesis and how it's used in experiments in this lesson, but there's a lot more to learn about it! \u003c/p\u003e\n\n\u003cp\u003eRead the following article, \u003ca href=\"https://byrslf.co/the-null-hypothesis-loves-you-and-wants-you-to-be-happy-3189413d8cd0\"\u003eThe Null Hypothesis Loves You and Wants You To Be Happy\u003c/a\u003e.  This does an excellent job of explaining why the concept of the \u003cem\u003eNull Hypothesis\u003c/em\u003e is crucial to good science.  \u003c/p\u003e\n\n\u003ch2\u003eSummary\u003c/h2\u003e\n\n\u003cp\u003eIn this lesson, you learned about the relationship between p-values and the Null Hypothesis. Now you'll see how effect sizes affect your tests!\u003c/p\u003e\n\u003cfooter class=\"fis-footer\" style=\"visibility: hidden;\"\u003e\u003cdiv class=\"fis-feedback\"\u003e\n\u003ch5\u003eHow do you feel about this lesson?\u003c/h5\u003e\n\u003cimg id=\"thumbs-up\" data-repository=\"dsc-p-values-and-null-hypothesis\" title=\"Thumbs up!\" alt=\"thumbs up\"\u003e\u003cimg id=\"thumbs-down\" data-repository=\"dsc-p-values-and-null-hypothesis\" title=\"Thumbs down!\" alt=\"thumbs down\"\u003e\n\u003c/div\u003e\n\u003ch5\u003eHave specific feedback? \u003ca href=\"https://github.com/learn-co-curriculum/dsc-p-values-and-null-hypothesis/issues/new\"\u003eTell us here!\u003c/a\u003e\n\u003c/h5\u003e\u003c/footer\u003e","exportId":"p-values-and-the-null-hypothesis"},{"id":141412,"title":"Effect Sizes","type":"Assignment","indent":0,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_mark_done","completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-effect-sizes\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-effect-sizes/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","exportId":"g21c730d27e96f5b0eb549e0b6c960a93"},{"id":141415,"title":"Conducting T-Tests","type":"Assignment","indent":0,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_mark_done","completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-t-tests\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-t-tests/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","exportId":"g3298b4089d96872f286c0b225a498364"},{"id":141419,"title":"One Sample T-Test - Lab","type":"Assignment","indent":1,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_mark_done","completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-one-sample-t-tests-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-one-sample-t-tests-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","exportId":"ge45fca72ae6e9e8c0c5e9a78de3c967e"},{"id":141423,"title":"Two Sample T-Test - Lab","type":"Assignment","indent":1,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_mark_done","completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-two-sample-t-tests-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-two-sample-t-tests-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","exportId":"gaf3b5206bf114d937bf087ec04fb392c"},{"id":141426,"title":"Type 1 and Type 2 Errors","type":"Assignment","indent":0,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_mark_done","completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-type-1-and-2-error\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-type-1-and-2-error/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","exportId":"gfc3449d1f87be596feb9b0393c970364"},{"id":141430,"title":"Type 1 and Type 2 Errors - Lab","type":"Assignment","indent":1,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_mark_done","completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-type-1-and-2-error-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-type-1-and-2-error-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","exportId":"gd7e932904b8d0a121a5d7fd3e8edaa31"},{"id":141432,"title":"Resampling Methods","type":"WikiPage","indent":0,"locked":false,"requirement":"must_mark_done","completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-resampling-methods\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-resampling-methods/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eResampling techniques are modern statistical techniques that involve taking repeated subsamples from a sample. These procedures tend to be computationally intensive, since they involve computing statistics of a subsample, creating new subsamples and repeating the process thousands or perhaps millions of times. This can allow for additional analysis of the subsamples leading to increased confidence and knowledge of the larger population. The three main techniques we will discuss here are bootstrapping, jackknife, and permutation tests.\u003c/p\u003e\n\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\n\u003cp\u003eYou will be able to:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eIdentify when resampling is used \u003c/li\u003e\n\u003cli\u003eDescribe the process of bootstrapping \u003c/li\u003e\n\u003cli\u003eDescribe permutation testing \u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch2\u003eJackknife and Bootstrapping\u003c/h2\u003e\n\n\u003cp\u003eLet's start by defining the sampling methodology for these techniques. The bootstrap method works by taking random samples with replacement from the original sample of size n. In contrast, the jackknife, the older of the two methods, works by taking samples by removing one, or more, observations at a time. Each one of these (n-1) sized sub-samples is aggregated to create the new jackknife sample. The purpose of these resampling methods is to be able to increase the size of our samples without having to actually go out and obtain more samples. Resampling methods attempt to estimate the variability of point estimators derived from the original samples.\u003c/p\u003e\n\n\u003cp\u003eThe motivating principle behind both is that by analyzing the variance of parameter estimates from these synthetic samples, we can also gauge the variance of our point estimate for the population itself. For example, we might take an original sample from our population and then use the jackknife or bootstrapping method to generate additional synthetic samples. By calculating the point estimate of interest for these synthetic samples, we can better gauge the confidence interval and variability of our original point estimator.\u003c/p\u003e\n\n\u003ch2\u003ePermutation Tests\u003c/h2\u003e\n\n\u003cp\u003eAnother related methodology is permutation tests. Permutation tests can be used in lieu of assumed parameter distributions for any statistical test. For example, we discussed the central limit theorem: that when taking the mean of a repeated sample from a population, the means of these samples will form a normal distribution. From this, we were then able to extrapolate confidence intervals surrounding our estimate for the mean of the entire population by assuming that our sample mean was from a normal distribution. This allowed us to define our confidence bands associated with various levels of type I errors which we set with  \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Calpha\"\u003e . In a hypothesis test, we used the same procedure to calculate the probability of a given sample, and based on alpha, rejected or confirmed the null hypothesis. In a permutation test, rather then assume the distribution itself and calculate p-values, we would calculate all permutations of our relabeling our data and compute the parameter statistic in question for these permutations.\u003c/p\u003e\n\n\u003cp\u003eFor example, let's say we had two samples, one with 37 observations and the other with 45 observations. We calculate the mean of both samples and wish to perform a hypothesis test with a 5% confidence interval for whether the two samples belong to the same overall population. In our previous work, we would use a t-test to perform this comparison. The permutation test alternative would be to compare the difference in these sample means to the difference in sample means of all possible permutations of 37-45 splits between our 82 data points. In other words, we compare the difference between our actual sample means to the difference in sample means between all variations of all those 82 points in order to calculate our p-values and determine whether we accept or reject the null-hypothesis.\u003c/p\u003e\n\n\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003eNote\u003c/strong\u003e: While it's called a permutation test, calculating all of the possible combinations of the observations into two groups is a more pragmatic approach. After all, you are comparing the sample means of the groups and as such the order of group members is irrelevant. When you implement permutation tests in the upcoming lab, you'll use combinations to make the problem computationally feasible. Even so, as you will see, the size of possible variations can quickly explode leading to other estimations of the permutation test, which you'll investigate towards the end of the section. \u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003ch2\u003eAdditional Resources\u003c/h2\u003e\n\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"http://hydrodictyon.eeb.uconn.edu/eebedia/images/9/9d/FelsensteinChap20.pdf\"\u003ehttp://hydrodictyon.eeb.uconn.edu/eebedia/images/9/9d/FelsensteinChap20.pdf\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://www.scss.tcd.ie/Rozenn.Dahyot/453Bootstrap/05_Permutation.pdf\"\u003ehttps://www.scss.tcd.ie/Rozenn.Dahyot/453Bootstrap/05_Permutation.pdf\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch2\u003eSummary\u003c/h2\u003e\n\n\u003cp\u003eIn this lesson, we continued discussing non-parametric statistics and investigated resampling techniques. This included bootstrapping, jackknife, and permutation tests. In the upcoming lab, you'll define functions that implement these techniques and then use them to conduct statistical simulations and tests.\u003c/p\u003e\n\u003cfooter class=\"fis-footer\" style=\"visibility: hidden;\"\u003e\u003cdiv class=\"fis-feedback\"\u003e\n\u003ch5\u003eHow do you feel about this lesson?\u003c/h5\u003e\n\u003cimg id=\"thumbs-up\" data-repository=\"dsc-resampling-methods\" title=\"Thumbs up!\" alt=\"thumbs up\"\u003e\u003cimg id=\"thumbs-down\" data-repository=\"dsc-resampling-methods\" title=\"Thumbs down!\" alt=\"thumbs down\"\u003e\n\u003c/div\u003e\n\u003ch5\u003eHave specific feedback? \u003ca href=\"https://github.com/learn-co-curriculum/dsc-resampling-methods/issues/new\"\u003eTell us here!\u003c/a\u003e\n\u003c/h5\u003e\u003c/footer\u003e","exportId":"resampling-methods"},{"id":141436,"title":"Resampling Methods - Lab","type":"Assignment","indent":1,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_mark_done","completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-resampling-methods-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-resampling-methods-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","exportId":"g748e82eafe8c9e20fcc845d0c3d2ca89"},{"id":141441,"title":"Hypothesis Testing - Recap","type":"WikiPage","indent":0,"locked":false,"requirement":"must_mark_done","completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-hypothesis-testing-section-recap-v2-1\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-hypothesis-testing-section-recap-v2-1/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eYou just learned how to create an experiment and interpret the results! Let's review some of the specific things you have learned.\u003c/p\u003e\n\n\u003ch2\u003eKey Takeaways\u003c/h2\u003e\n\n\u003cp\u003eSome of the key takeaways from this section include:  \u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eIt's important to have a sound approach to experimental design to be able to determine the significance of your findings. \u003c/li\u003e\n\u003cli\u003eStart by examining any existing research to see if it can shed light on the problem you're studying. \u003c/li\u003e\n\u003cli\u003eStart with a clear alternative and null hypothesis for your experiment to \"prove\". \u003c/li\u003e\n\u003cli\u003eIt's important to have a thoughtfully selected control group from the same population for your trial to distinguish effect from variations based on population, time or other factors. \u003c/li\u003e\n\u003cli\u003eYour sample size needs to be selected carefully to ensure your results have a good chance of being statistically significant. \u003c/li\u003e\n\u003cli\u003eYour results should be reproducible by other people and using different samples from the population. \u003c/li\u003e\n\u003cli\u003eThe p-value for an outcome determines how likely it is that the outcome could occur under the null hypothesis. \u003c/li\u003e\n\u003cli\u003e \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Calpha\"\u003e is the marginal threshold at which we're comfortable rejecting the null hypothesis. \u003c/li\u003e\n\u003cli\u003eAn  \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Calpha\"\u003e value of 0.05 is a common choice for many experiments. \u003c/li\u003e\n\u003cli\u003eEffect size measures just the size of the difference between two groups under observation, whereas statistical significance combines effect size with sample size. \u003c/li\u003e\n\u003cli\u003eA one sample t-test is used to determine whether a sample comes from a population with a specific mean\u003cbr\u003e\n\u003c/li\u003e\n\u003cli\u003eA two sample t-test is used to determine if two population means are equal. \u003c/li\u003e\n\u003cli\u003eType 1 errors (false positives) are when we accept an alternative hypothesis which is actually false. \u003c/li\u003e\n\u003cli\u003eThe  \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Calpha\"\u003e that we pick is the likelihood that we will get a type 1 error due to random chance. \u003c/li\u003e\n\u003cli\u003eType 2 errors (false negatives) are when we reject an alternative hypothesis which is actually true. \u003c/li\u003e\n\u003cli\u003eResampling methods allow for improved precision in estimating sample statistics and validating models by using random subsets. \u003c/li\u003e\n\u003cli\u003eCommon resampling techniques include bootstrapping, jackknifing and permutation tests.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cfooter class=\"fis-footer\" style=\"visibility: hidden;\"\u003e\u003cdiv class=\"fis-feedback\"\u003e\n\u003ch5\u003eHow do you feel about this lesson?\u003c/h5\u003e\n\u003cimg id=\"thumbs-up\" data-repository=\"dsc-hypothesis-testing-section-recap-v2-1\" title=\"Thumbs up!\" alt=\"thumbs up\"\u003e\u003cimg id=\"thumbs-down\" data-repository=\"dsc-hypothesis-testing-section-recap-v2-1\" title=\"Thumbs down!\" alt=\"thumbs down\"\u003e\n\u003c/div\u003e\n\u003ch5\u003eHave specific feedback? \u003ca href=\"https://github.com/learn-co-curriculum/dsc-hypothesis-testing-section-recap-v2-1/issues/new\"\u003eTell us here!\u003c/a\u003e\n\u003c/h5\u003e\u003c/footer\u003e","exportId":"hypothesis-testing-recap"}]},{"id":15085,"name":"Topic 15: Statistical Power and ANOVA","status":"unlocked","unlockDate":null,"prereqs":[],"requirement":"all","sequential":false,"exportId":"g92374f3a4f64a7237bc84e243557094a","items":[{"id":141450,"title":"Statistical Power and ANOVA -  Introduction","type":"WikiPage","indent":0,"locked":false,"requirement":"must_mark_done","completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-statistical-power-anova-introduction\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-statistical-power-anova-introduction/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eIn this section you'll continue to deepen your knowledge of hypothesis testing and t-tests by examining the concept of power; an idea closely related to type II errors. With that, you'll see how the rate of type I errors, power, sample size, and effect size are intrinsically related to one another. You will then move on to ANOVA - Analysis of Variance, which allows you to test for the influence of multiple factors all at once.\u003c/p\u003e\n\n\u003ch2\u003eStatistical power\u003c/h2\u003e\n\n\u003cp\u003eStatistical power is equal to  \u003cimg src=\"https://render.githubusercontent.com/render/math?math=1%20-%20%5Cbeta\"\u003e where  \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cbeta\"\u003e is the rate of type II errors. As you will see, power is related to  \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Calpha\"\u003e , sample size, and effect size. Typically a researcher will select an acceptable alpha value and then examine required sample sizes to achieve the desired power such as 0.8 (or higher). \u003c/p\u003e\n\n\u003ch2\u003eWelch's t-test\u003c/h2\u003e\n\n\u003cp\u003eAfter an initial exploration of statistical power, you'll take a look at Welch's t-test. This is an adaptation of the unpaired student's t-test you've seen previously which allows for different sample sizes or different variances between the two groups.\u003c/p\u003e\n\n\u003ch2\u003eMultiple comparisons\u003c/h2\u003e\n\n\u003cp\u003eFrom there, you'll look at some of the issues that arise when trying to perform multiple comparisons - from the risks of spurious correlations to the importance of corrections such as the Bonferroni correction to deal with the cumulative risks of type I errors inherent in multiple comparisons.\u003c/p\u003e\n\n\u003ch2\u003eANOVA\u003c/h2\u003e\n\n\u003cp\u003eFinally, you'll take a look at the more generalized procedure for conducting multiple comparisons: Analysis of Variance or ANOVA. You'll see that ANOVA of only two groups is statistically equivalent to a two sided t-test. That said, ANOVA fully supports comparing multiple factors simultaneously.\u003c/p\u003e\n\n\u003ch2\u003eSummary\u003c/h2\u003e\n\n\u003cp\u003eWithout a good understanding of experimental design, it's easy to end up drawing false conclusions. In this section, you'll cover a range of tools and techniques to deepen your understanding of hypothesis testing and ensure that you design experiments rigorously and interpret them thoughtfully.\u003c/p\u003e\n\u003cfooter class=\"fis-footer\" style=\"visibility: hidden;\"\u003e\u003cdiv class=\"fis-feedback\"\u003e\n\u003ch5\u003eHow do you feel about this lesson?\u003c/h5\u003e\n\u003cimg id=\"thumbs-up\" data-repository=\"dsc-statistical-power-anova-introduction\" title=\"Thumbs up!\" alt=\"thumbs up\"\u003e\u003cimg id=\"thumbs-down\" data-repository=\"dsc-statistical-power-anova-introduction\" title=\"Thumbs down!\" alt=\"thumbs down\"\u003e\n\u003c/div\u003e\n\u003ch5\u003eHave specific feedback? \u003ca href=\"https://github.com/learn-co-curriculum/dsc-statistical-power-anova-introduction/issues/new\"\u003eTell us here!\u003c/a\u003e\n\u003c/h5\u003e\u003c/footer\u003e","exportId":"statistical-power-and-anova-introduction"},{"id":141454,"title":"Statistical Power","type":"Assignment","indent":0,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_mark_done","completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-statistical-power\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-statistical-power/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","exportId":"g1ccea933627c2a879f1ff80b34214092"},{"id":141458,"title":"Statistical Power - Lab","type":"Assignment","indent":1,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_mark_done","completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-statistical-power-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-statistical-power-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","exportId":"ga57a09de707fa13679dc18747e1d3659"},{"id":141463,"title":"Welch's T-Test","type":"WikiPage","indent":0,"locked":false,"requirement":"must_mark_done","completed":false,"content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-welchs-ttest\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-welchs-ttest\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-welchs-ttest/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eThus far, you've seen the traditional Student's t-test for hypothesis testing between two sample means. Recall that z-tests are also appropriate for statistics, such as the mean, which can be assumed to be normally distributed. However, when sample sizes are low (n_observations \u0026lt; 30), the t-test is more appropriate, as the t-distribution has heavier tails. Even with this modification, remember that there are still several assumptions to the model. Most notably, traditional t-tests assume that sample sizes and sample variances between the two groups are equal. When these assumptions are not met, Welch's t-test is generally a more reliable test.\u003c/p\u003e\n\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\n\u003cp\u003eYou will be able to: \u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eList the conditions needed to require a Welch's t-test \u003c/li\u003e\n\u003cli\u003eCalculate the degrees of freedom for a Welch's t-test \u003c/li\u003e\n\u003cli\u003eCalculate p-values using Welch's  t-test \u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch2\u003eT-test review\u003c/h2\u003e\n\n\u003cp\u003eRecall that t-tests are a useful method for determining whether the mean of two small samples indicate different underlying population parameters. The reasoning behind this begins with the use of z-tests to calculate the likelihood of sampling a particular value from a normal distribution. Furthermore, by the central limit theorem, the mean of a sample is a normally distributed variable centered around the actual underlying population mean. That said, t-tests are more appropriate for small samples (n_observations \u0026lt; 30), due to disproportionate tails. Finally, recall that the t-distribution actually converges to a normal distribution as the degrees of freedom continues to increase.  \u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-welchs-ttest/master/images/new_t_vs_norm_dist.png\"\u003e\u003c/p\u003e\n\n\u003cblockquote\u003e\n\u003cp\u003eA normal distribution vs. t-distributions with varying degrees of freedom. Note how the t-distribution approaches the normal distribution as the degrees of freedom increases. Recall that when performing a two-sample t-test, assuming that sample variances are equal, the degrees of freedom equals the total number of observations in the samples minus two.\u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003ch2\u003eWelch's t-test\u003c/h2\u003e\n\n\u003cp\u003eJust as Student's t-test is a useful adaptation of the normal distribution which can lead to better likelihood estimates under certain conditions, the Welch's t-test is a further adaptation that accounts for additional perturbations in the underlying assumptions of the model. Specifically, the Student's t-test assumes that the samples are of equal size and equal variance. When these assumptions are not met, then Welch's t-test provides a more accurate p-value.\u003c/p\u003e\n\n\u003cp\u003eHere is how you calculate it: \u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5CLarge%20t%20=%20%5Cfrac%7B%5Cbar%7BX_1%7D-%5Cbar%7BX_2%7D%7D%7B%5Csqrt%7B%5Cfrac%7Bs_1%5E2%7D%7BN_1%7D%20%2b%20%5Cfrac%7Bs_2%5E2%7D%7BN_2%7D%7D%7D%20=%20%5Cfrac%7B%5Cbar%7BX_1%7D-%5Cbar%7BX_2%7D%7D%7B%5Csqrt%7Bse_1%5E2%2bse_2%5E2%7D%7D\"\u003e \u003c/p\u003e\n\n\u003cp\u003ewhere\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003e \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cbar%7BX_i%7D\"\u003e - mean of sample i\u003c/li\u003e\n\u003cli\u003e \u003cimg src=\"https://render.githubusercontent.com/render/math?math=s_i%5E2\"\u003e - variance of sample i\u003c/li\u003e\n\u003cli\u003e \u003cimg src=\"https://render.githubusercontent.com/render/math?math=N_i\"\u003e - sample size of sample i\u003cbr\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003eThe modification is related to the \u003cstrong\u003edegrees of freedom\u003c/strong\u003e in the t-test, which tends to increase the test power for samples with unequal variance. When two groups have equal sample sizes and variances, Welch’s t-test tends to give the same result as the Student’s t-test. However, when sample sizes and variances are unequal, Student’s t-test is quite unreliable, whereas Welch’s tends perform better.\u003c/p\u003e\n\n\u003ch2\u003eCalculate the degrees of freedom\u003c/h2\u003e\n\n\u003cp\u003eOnce the t-score has been calculated for the experiment using the above formula, you then must calculate the degrees of freedom for the t-distribution. Under the two-sample Student's t-test, this is simply the total number of observations in the samples size minus two, but given that the sample sizes may vary using the Welch's t-test, the calculation is a bit more complex:  \u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5CLarge%20v%20%5Capprox%20%5Cfrac%7B%5Cleft(%20%5Cfrac%7Bs_1%5E2%7D%7BN_1%7D%20%2b%20%5Cfrac%7Bs_2%5E2%7D%7BN_2%7D%5Cright)%5E2%7D%7B%5Cfrac%7Bs_1%5E4%7D%7BN_1%5E2v_1%7D%20%2b%20%5Cfrac%7Bs_2%5E4%7D%7BN_2%5E2v_2%7D%7D\"\u003e \u003c/p\u003e\n\n\u003ch2\u003eCalculate p-values\u003c/h2\u003e\n\n\u003cp\u003eFinally, as with the Student's t-test (or a z-test for that matter), you convert the calculated score into a p-value in order to confirm or reject the null-hypothesis of your statistical experiment. For example, you might be using a one-sided t-test to determine whether a new drug had a positive effect on patient outcomes. The p-value for the experiment is equivalent to the area under the t-distribution with the degrees of freedom, as calculated above, and the corresponding t-score.\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-welchs-ttest/master/images/new_AUC.png\" width=\"500\"\u003e\u003c/p\u003e\n\n\u003cp\u003eThe easiest method for determining said p-values is to use the \u003ccode\u003e.cdf()\u003c/code\u003e method from \u003ccode\u003escipy.stats\u003c/code\u003e to find the complement and subtracting this from 1.\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-welchs-ttest/master/images/new_CdfAndPdf.png\" width=\"500\"\u003e\u003c/p\u003e\n\n\u003cp\u003eHere's the relevant code snippet:\u003c/p\u003e\n\n\u003cpre\u003e\u003ccode class=\"python\"\u003eimport scipy.stats as stats\n\np = 1 - stats.t.cdf(t, df)\n\u003c/code\u003e\u003c/pre\u003e\n\n\u003ch2\u003eSummary\u003c/h2\u003e\n\n\u003cp\u003eThis lesson briefly introduced you to another statistical test for comparing the means of two samples: Welch's t-test. Remember that when your samples are not of equal size or do not have equal variances, it is a more appropriate statistical test than the Student's t-test!\u003c/p\u003e","exportId":"welchs-t-test"},{"id":141466,"title":"Welch's T-Test - Lab","type":"Assignment","indent":1,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_mark_done","completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-welchs-ttest-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-welchs-ttest-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","exportId":"g38a0b69cae8a3d7ca7a04ffaab15eb71"},{"id":141467,"title":"Effect Sizes, P-Values and Power - Lab","type":"Assignment","indent":1,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_mark_done","completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-effect-sizes-pvalues-and-power-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-effect-sizes-pvalues-and-power-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","exportId":"g781cad4217d3a5a0addc52ebfce79fa8"},{"id":141471,"title":"The Multiple Comparisons Problem","type":"WikiPage","indent":0,"locked":false,"requirement":"must_mark_done","completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-multiple-comparisons-problem\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-multiple-comparisons-problem/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eIn this lesson, you'll learn about the problems that can arise from doing multiple comparisons in a single experiment.\u003c/p\u003e\n\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\n\u003cp\u003eYou will be able to:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eExplain why multiple comparisons increases the likelihood of misleading results \u003c/li\u003e\n\u003cli\u003eExplain the concept of spurious correlation \u003c/li\u003e\n\u003cli\u003eUse corrections to deal with multiple comparison problems \u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch2\u003eWhat is the multiple comparisons problem?\u003c/h2\u003e\n\n\u003cp\u003eObtaining an incredibly low p-value does not guarantee that the null-hypothesis is incorrect. For example, a p-value of 0.001 states that there is still a 1 in 1000 chance that the null hypothesis is true. Yet, as you've seen, p-values alone can be misleading. For example, if you perform repeated experiments, at some point you're apt to stumble upon a small p-value, whether or not the null hypothesis is valid.\u003c/p\u003e\n\n\u003cp\u003eTo restate this, imagine we take 100 scientific studies with a p-value of 0.03. Are all of these conclusions valid? Sadly, probably not. Remember, for any experiment with a p-value of 0.03, there is still a 3% chance that the null-hypothesis is actually true. So collectively, the probability that \u003cstrong\u003eall\u003c/strong\u003e of these null hypotheses are false is actually quite small. You can be fairly confident in each study, but there is also apt to be a false-conclusion drawn somewhere. (In fact, the p-value itself implies that, on average, 3 of these 100 conclusions will be false.)\u003c/p\u003e\n\n\u003cpre\u003e\u003ccode class=\"python\"\u003e0.97**100 # Probability all 100 experiments with p=0.03 are all true \n\u003c/code\u003e\u003c/pre\u003e\n\n\u003cpre\u003e\u003ccode\u003e0.04755250792540563\n\u003c/code\u003e\u003c/pre\u003e\n\n\u003cp\u003eSimilarly, if you are testing multiple metrics simultaneously in an experiment, the chances that one of these will satisfy your alpha threshold increases. A fun similar phenomenon is spurious correlation. If we start comparing a multitude of quantities, we are bound to find some quantities that are highly correlated, whether or not an actual relationship exists. Tyler Vigen set out to find such relationships; here are a few entertaining ones (of many):  \u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-multiple-comparisons-problem/master/images/nicolas_cage_vs_drowning.svg\"\u003e\n\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-multiple-comparisons-problem/master/images/chicken_vs_oil.svg\"\u003e\n\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-multiple-comparisons-problem/master/images/math_phds_vs_uranium.svg\"\u003e\n\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-multiple-comparisons-problem/master/images/spelling_vs_spiders.svg\"\u003e\u003c/p\u003e\n\n\u003cp\u003eAs we can see, although these graphs show that each pair of quantities is strongly correlated, it seems unreasonable to expect that any of them have any causal relationships. Regardless of what the statistics tell us, there is no relationship through which the length of spelling bee word affects the number of people killed by venomous spiders.\u003c/p\u003e\n\n\u003ch2\u003eHow do multiple comparisons increase the chances of finding spurious correlations?\u003c/h2\u003e\n\n\u003cp\u003eSpurious correlation is a \u003cstrong\u003e\u003cem\u003eType 1 Error\u003c/em\u003e\u003c/strong\u003e, meaning that it's a type of \u003cstrong\u003e\u003cem\u003eFalse Positive\u003c/em\u003e\u003c/strong\u003e. We think we've found something important when really there isn't any.  With each comparison we make in an experiment, we try to set a really low p-value to limit our exposure to type 1 errors.  When we only reject the null hypothesis when p \u0026lt; 0.05, for example, we are effectively saying \"I'm only going to accept these results as true if there is less than a 5% chance that I didn't actually find anything important, and my data only looks like this due to randomness\".  However, when we make \u003cstrong\u003e\u003cem\u003emultiple comparisons\u003c/em\u003e\u003c/strong\u003e by checking for many things at once, each of the small risks of a Type 1 error becomes cumulative! \u003c/p\u003e\n\n\u003cp\u003eHere's another easy to way to phrase this -- a p-value threshold of less than 0.05 means that we will only make a Type 1 error 1 in every 20 times. This means that statistically, if we have 20 findings where the p-value is less than 0.05 at the same time, 1 of them is almost guaranteed to be a Type 1 error (False Positive) -- but we have no idea of which one!\u003c/p\u003e\n\n\u003ch2\u003eThe Bonferroni correction\u003c/h2\u003e\n\n\u003cp\u003eBack to the problem of multiple comparisons. Due to the cumulative risk of drawing false conclusions when statistically testing multiple quantities simultaneously, statisticians have devised methods to minimize the chance of type 1 errors. One of these is the \u003cstrong\u003e\u003cem\u003eBonferroni correction\u003c/em\u003e\u003c/strong\u003e.  With the Bonferroni correction, you divide  \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Calpha\"\u003e by the number of comparisons you are making to set a new, adjusted threshold rejecting the null hypothesis.\u003c/p\u003e\n\n\u003cp\u003eFor example, if you desire  \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Calpha%20=%200.05\"\u003e , but are making 10 comparisons simultaneously, the Bonferroni Correction would advise you set our adjusted p-value threshold to  \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cfrac%7B0.05%7D%7B10%7D%20=%200.005\"\u003e !  The stricter p-value threshold helps control for Type 1 errors.  This doesn't mean that you are immune to them -- it just helps reduce the cumulative chance that one occurs. That said, the effective power of these tests is therefore reduced (and in turn type 2 errors are more likely).\u003c/p\u003e\n\n\u003ch2\u003eAdditional Resources\u003c/h2\u003e\n\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"http://tylervigen.com/spurious-correlations\"\u003eTyle Vigen - Spurious correlations\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://www.nationalgeographic.com/science/phenomena/2015/09/11/nick-cage-movies-vs-drownings-and-more-strange-but-spurious-correlations/\"\u003eNick Cage movies vs. drownings, and more strange (but spurious) correlations\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch2\u003eSummary\u003c/h2\u003e\n\n\u003cp\u003eIn this lesson, you learned about the problems that can arise from doing multiple comparisons in a single experiment, as well as some entertaining spurious correlations that exist with real-world data.\u003c/p\u003e\n\u003cfooter class=\"fis-footer\" style=\"visibility: hidden;\"\u003e\u003cdiv class=\"fis-feedback\"\u003e\n\u003ch5\u003eHow do you feel about this lesson?\u003c/h5\u003e\n\u003cimg id=\"thumbs-up\" data-repository=\"dsc-multiple-comparisons-problem\" title=\"Thumbs up!\" alt=\"thumbs up\"\u003e\u003cimg id=\"thumbs-down\" data-repository=\"dsc-multiple-comparisons-problem\" title=\"Thumbs down!\" alt=\"thumbs down\"\u003e\n\u003c/div\u003e\n\u003ch5\u003eHave specific feedback? \u003ca href=\"https://github.com/learn-co-curriculum/dsc-multiple-comparisons-problem/issues/new\"\u003eTell us here!\u003c/a\u003e\n\u003c/h5\u003e\u003c/footer\u003e","exportId":"the-multiple-comparisons-problem"},{"id":141475,"title":"Goodhart's Law and Metric Tracking","type":"WikiPage","indent":0,"locked":false,"requirement":"must_mark_done","completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-goodharts-law-and-metric-tracking\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-goodharts-law-and-metric-tracking/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eIn this lesson, you'll learn about \u003cstrong\u003e\u003cem\u003eGoodhart's law\u003c/em\u003e\u003c/strong\u003e and why you should be cautious and thoughtful when making policy recommendations based on data.\u003c/p\u003e\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\u003cp\u003eYou will be able to:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eDefine Goodhart's law and its relationship to hypothesis testing\u003c/li\u003e\n\u003cli\u003eIdentify real-world examples of Goodhart's law in action\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003eWhat is Goodhart's law?\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"https://en.wikipedia.org/wiki/Goodhart%27s_law\"\u003eGoodhart's law\u003c/a\u003e is an observation made by the British economist Charles Goodhart in 1975. Charles Goodhart famously said:\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\"Any observed statistical regularity will tend to collapse once pressure is placed upon it for control purposes.\" -- Charles Goodhart\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eIn plain English, this translates to:\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\"Any measure which becomes a target ceases to be an effective measure!\"\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch3\u003eSo what does that mean?\u003c/h3\u003e\n\u003cp\u003eGoodhart's law succinctly explains a cardinal sin that many data scientists, project managers, and CEOs make all the time without realizing it -- they make policy or set goals based on statistical metrics without considering the unintended consequences and effects these policies might have!\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-goodharts-law-and-metric-tracking/master/images/goodhart.jpg\" width=\"500\"\u003e\u003c/p\u003e\n\u003cp\u003eImage from: \u003ca href=\"https://www.sketchplanations.com/post/167369765942/goodharts-law-when-a-measure-becomes-a-target\"\u003eSketchplantations\u003c/a\u003e\u003c/p\u003e\n\u003ch2\u003eExample 1: Cobra skins\u003c/h2\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-goodharts-law-and-metric-tracking/master/images/new_cobra.png\" width=\"300\"\u003e\u003c/p\u003e\n\u003cp\u003eThe \u003ca href=\"https://en.wikipedia.org/wiki/Cobra_effect\"\u003eCobra effect\u003c/a\u003e refers to an anecdote that demonstrates an example of Goodhart's law in effect during the time of British rule of colonial India. As the story goes, a high-ranking officer in the British military was concerned about the number of highly venomous cobras that could be found in Delhi. He had the bright idea of offering a bounty for every cobra skin brought to him! Initially, this seemed to work -- people hunted cobras, sold the skins to the British government for their bounty, and the cobra population dipped slightly in the city. However, this soon backfired spectacularly, when citizens started breeding cobras! As a result, the cobra population stopped declining and even repopulated a bit. After a while, the officer caught onto the breeding, as he realized they were paying out many bounties but the cobra problem in the city was still prevalent as ever. After realizing this, he canceled the bounty. Ironically, this meant that all the cobra breeders now had no reason to keep the cobras they were breeding, so they dumped them in the street -- causing the city to have even more cobras than before the bounty program had been implemented in the first place!\u003c/p\u003e\n\u003cp\u003e(Fun fact: The French military made the same mistake when Hanoi, Vietnam was under their colonial rule with a rat bounty, and there is solid evidence to prove that this actually happened!)\u003c/p\u003e\n\u003ch3\u003eThe problem with proxy metrics\u003c/h3\u003e\n\u003cp\u003eThe first mistake by this British commander was using a \u003cstrong\u003e\u003cem\u003eproxy metric\u003c/em\u003e\u003c/strong\u003e in the form of \"cobra skins collected\". He mistakenly assumed that there was an inverse relationship between the number of skins turned in for a bounty and the number of wild cobras in the city of Delhi! Although this may have been the case at first, as hunting cobras was pretty much the only way to obtain skins to turn in for the bounty, he failed to realize that there were other possible sources for cobra skins that he hadn't accounted for. He wanted to reduce one metric, \u003cem\u003eCobra population\u003c/em\u003e, but he wasn't actually tracking that metric -- he was tracking a proxy for that metric which he assumed he could use to gauge what was happening to his target metric. The system he implemented had no way of determining if the cobra skins turned in for bounties were skins from cobras on the streets of Delhi -- with no way to tell, he had no way of knowing as his proxy metric became less and less relevant.\u003c/p\u003e\n\u003ch3\u003ePolicies can change things you didn't plan for\u003c/h3\u003e\n\u003cp\u003eThis leads to his other mistake -- he failed to account for how his policies might change things. Policies do not happen in a vacuum. They have a tendency to change things in unexpected ways, if not crafted thoughtfully and carefully! At first glance, introducing a monetary incentive for cobra skins seems like a good way to reduce the cobra population. However, he failed to account for the way this new incentive might change people's behaviors. By making cobra skins highly valuable, he inadvertently caused people to realize that breeding cobras was much safer, easier, and more lucrative than hunting them. Although his policy may have caused the change he wanted, in the beginning, he had no way of knowing what other sorts of behaviors this new policy might create or encourage.\u003c/p\u003e\n\u003ch2\u003eExample 2: Standardized testing in US schools\u003c/h2\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-goodharts-law-and-metric-tracking/master/images/new_test.png\" width=\"400\"\u003e\u003c/p\u003e\n\u003cp\u003eA more depressing real-world example of Goodhart's law in action is the prevalence of standardized testing in the American public school system. These tests were originally designed as a way to measure both individual student performance and overall teacher and school effectiveness. However, school funding is tied directly to test scores. This incentivizes schools to \"teach to the test\", spending a disproportionate amount of class time each year focusing on test preparation. By having incentives for schools to focus heavily on preparing students for these tests, the system has created ripple effects including reorientating student's focus on preparing for tests rather than other learning goals that might be more characteristic of real-world applications such as project orientated tasks. In this case, policymakers started out with a harmless, positive intention -- measure student and school performance -- but failing to account for Goodhart's law and offering strong incentives in relation to these metrics has degraded the usefulness of these test scores by altering behaviors.\u003c/p\u003e\n\u003ch2\u003eWhy does this matter for Data Scientists?\u003c/h2\u003e\n\u003cp\u003eGoodhart's law is something that matters much to Data Scientists because it is our findings and experiments that often drive the policies and decisions made by a company. Data Science is complex, and often, project managers, CEOs, and other decision makers don't want to know about experimental methodologies or confidence intervals -- they just want to know what the best decision they can make is, based on what the data says! It's quite common for decision makers to not realize that setting a target for one metric can negatively affect other metrics in ways that aren't immediately obvious. For instance, pushing employees at a call center to reduce call times might reduce customer satisfaction; it seems reasonable to imagine employees hustling to get off the phone based on this shorter call time \"target\" handed down from management.\u003c/p\u003e\n\u003cp\u003eAs a data scientist, it is important to communicate your results clearly to stakeholders -- but it is also important to be the voice of reason at times. This is why communication with stakeholders is important throughout the process of any data science project. The sooner you know how they plan on using your results, the more you can help them avoid ugly unforeseen problems that come from Goodhart's law -- always remember that massive amounts of data are no substitute for \u003cem\u003ecritical thinking\u003c/em\u003e! At the very least, you should get a bit nervous when you see targets being set for certain metrics. Note that this doesn't necessarily mean \"don't set targets\" -- instead, seek to encourage decision makers to think critically about any unintended consequences these targets could have, and track changes in metrics early and often when new policies or targets are put in place to ensure that unintended consequences are caught early!\u003c/p\u003e\n\u003ch2\u003eSummary\u003c/h2\u003e\n\u003cp\u003eIn this lesson, you learned about Goodhart's law and why you should be cautious and thoughtful when making policy recommendations based on data.\u003c/p\u003e","exportId":"goodharts-law-and-metric-tracking"},{"id":141477,"title":"The Kolmogorov-Smirnov Test","type":"WikiPage","indent":0,"locked":false,"requirement":"must_mark_done","completed":false,"content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-kolmogorov-smirnov-test\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-kolmogorov-smirnov-test\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-kolmogorov-smirnov-test/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eDuring data analysis, you have to satisfy a number of assumptions for the underlying dataset. One of the most common assumptions that you will come across is the \"Normality Assumption\", i.e., the underlying data roughly follows a normal distribution.\u003c/p\u003e\n\n\u003cp\u003eIf the data is not found to be normally distributed (i.e. data with kurtosis and skew while doing linear regression), you may first answer a question like: “Given my data … if there is a deviation from normality, will there be a material impact on my results?”\u003c/p\u003e\n\n\u003cp\u003eIn this lesson, we'll look at a popular statistical test for satisfying the normality assumption, the Kolmogorov-Smirnov test, or simply, the K-S test.\u003c/p\u003e\n\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\n\u003cp\u003eYou will be able to:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eExplain the role of the normality assumption in statistical tests \u003c/li\u003e\n\u003cli\u003eCalculate a one-and two-sample Kolmogorov-Smirnov test\u003c/li\u003e\n\u003cli\u003eInterpret the results of a one- and two-sample Kolmogorov-Smirnov test\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch2\u003eNormality assumption\u003c/h2\u003e\n\n\u003cp\u003eFormal normality tests always reject the huge sample sizes we work with today. When n (our sample size) gets large, even the smallest deviation from perfect normality will lead to a significant result. And as every dataset has some degree of random noise, no single dataset will be a \u003cstrong\u003eperfectly\u003c/strong\u003e normally distributed sample. \u003c/p\u003e\n\n\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003eIn applied statistics, the question is not whether the data/residuals are perfectly normal, but normal enough for the assumptions to hold\u003c/strong\u003e.\u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003cp\u003eThis question is answered through visualization techniques like qqplots, boxplots, or more advanced statistical tests including:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eThe Shapiro-Wilk test;\u003c/li\u003e\n\u003cli\u003eThe Anderson-Darling test, and;\u003c/li\u003e\n\u003cli\u003eThe Kolmogorov-Smirnov test \u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003eIn this lesson, we'll focus on the Kolmogorov-Smirnov test (K-S test) which will give you a strong foundation to help you understand and implement other tests when needed. \u003c/p\u003e\n\n\u003ch2\u003eKolmogorov-Smirnov Test\u003c/h2\u003e\n\n\u003cp\u003eA K-S test provides a way of comparing distributions, whether two sample distributions or a sample distribution with a theoretical distribution - comparable to what we've already seen when we learned about one sample or two-sample t-tests. The distributions are compared in their cumulative form as \u003cstrong\u003eEmpirical Cumulative Distribution Functions\u003c/strong\u003e. The test statistic in K-S test used to compare distributions is simply the maximum vertical distance between the two functions. Essentially, we are testing the sample data against another sample, to compare their distributions for similarities.\u003c/p\u003e\n\n\u003ch3\u003eThe Empirical Cumulative Distribution Function (ECDF)\u003c/h3\u003e\n\n\u003cblockquote\u003e\n\u003cp\u003eAn empirical cumulative distribution function (CDF) is a non-parametric estimator of the underlying CDF of a random variable. It assigns a probability to each data point, orders the data from smallest to largest in value, and calculates the sum of the assigned probabilities up to and including each data point.\u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003cp\u003eThe most intuitive way to think about the empirical distribution function is that it relates to the cumulative distribution function (CDF) in a similar way to how a histogram relates to a probability density function. Let's look at the following figures to get this idea:\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-kolmogorov-smirnov-test/master/images/rnorm.png\" width=\"800\"\u003e\u003c/p\u003e\n\n\u003cp\u003eThe left figure shows a regular histogram with samples looking like a normal distribution. The right figure shows the same samples except each bin in the histogram contains the cumulative count of samples up to that bin, which approximates the shape of the CDF for this random variable. Now the right figure doesn't exactly represent an empirical distribution function because the Y-axis is not normalized to 1 and the samples are binned instead of just plotted cumulatively. Nonetheless, the idea remains the same. An example of an empirical CDF is given below:\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-kolmogorov-smirnov-test/master/images/cumul_prob.png\" width=\"400\"\u003e\u003c/p\u003e\n\n\u003cp\u003eThis image sums up the intuition for empirical distribution function. The blue line is our empirical CDF whereas the  grey one is our theoretical CDF (i.e. plotted using parameters and fitting a probability function).\u003c/p\u003e\n\n\u003cp\u003eIf X is a random variable with CDF \u003cimg src=\"https://render.githubusercontent.com/render/math?math=F(x)=P(X%E2%89%A4x)\"\u003e, and \u003cimg src=\"https://render.githubusercontent.com/render/math?math=x1,%5Cldots,xn\"\u003e are i.i.d. random variables sampled from X. Then, the empirical distribution function, \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Chat%7BF%7D(x)\"\u003e , is a CDF:\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Chat+F%28x%29+%3D+%5Cdfrac%7B%28%5Ctext%7B%23+of+elements+in+sample%7D%29+%5Cleq+x%7D%7Bn%7D+%3D+%5Cdfrac%7B1%7D%7Bn%7D+%5CSigma_%7Bi%3D1%7D%5En+I%28x_i+%5Cleq+x%29+%5Ctag%7B1%7D\"\u003e\u003c/p\u003e\n\n\u003ch3\u003eOne-Sample K-S test\u003c/h3\u003e\n\n\u003cp\u003eThis is also known as the \u003cstrong\u003eKolmogorov-Smirnov Goodness of Fit test\u003c/strong\u003e. It calculates the similarity between an observed (empirical) distribution and a completely specified theoretical continuous distribution. It is sensitive to all attributes of a distribution including mean, variance, and shape.\u003c/p\u003e\n\n\u003cp\u003eThe key assumption of the one-sample test is that the theoretical distribution is fully defined continuous distribution, in terms of its parameters. This obviously means that its most common use case is that of testing normality. The test statistic,  \u003cimg src=\"https://render.githubusercontent.com/render/math?math=d\"\u003e ,  is simply the largest deviation between the observed cumulative function and the expected theoretical cumulative frequency distribution, i.e. \u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=d=max(abs%5BF_0(X)-F_r(X)%5D)\"\u003e \u003c/p\u003e\n\n\u003cp\u003ewhere\n- \u003cstrong\u003ed\u003c/strong\u003e is the maximum deviation Kolmogorov statistic \n- \u003cstrong\u003eF\u003csub\u003e0\u003c/sub\u003e(X)\u003c/strong\u003e = (No.of observations ≤ X)/(Total no.of observations) i.e. the non parametric empirical distribution\n- \u003cstrong\u003eF\u003csub\u003er\u003c/sub\u003e(X)\u003c/strong\u003e = The theoretical frequency distribution of X - parametric (e.g. based on mean value) \u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-kolmogorov-smirnov-test/master/./images/new_d.png\" width=\"600\"\u003e\u003c/p\u003e\n\n\u003cp\u003e\u003cstrong\u003eNull Hypothesis:\u003c/strong\u003e There is no difference between the distribution of our sample and a normal distribution. \u003c/p\u003e\n\n\u003cp\u003e\u003cstrong\u003eAcceptance Criteria:\u003c/strong\u003e If the calculated value is less than the critical value, accept the null hypothesis.\u003c/p\u003e\n\n\u003cp\u003e\u003cstrong\u003eRejection Criteria:\u003c/strong\u003e If the calculated value is greater than the critical value, reject the null hypothesis.\u003c/p\u003e\n\n\u003ch3\u003eExample\u003c/h3\u003e\n\n\u003ch4\u003eProblem Statement:\u003c/h4\u003e\n\n\u003cp\u003eIn a study done from various modules of a data science course with 60 students, equal number of students are samples from each module. These students are interviewed and their intention to join the advanced machine learning module was noted. Following shows how many students showed a positive intention\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003ePython (5) \u003c/li\u003e\n\u003cli\u003eData Visualizations (9)\u003c/li\u003e\n\u003cli\u003eSQL (11)\u003c/li\u003e\n\u003cli\u003eStatistics (16)\u003c/li\u003e\n\u003cli\u003eNLP (19)\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003eIt was expected that 12 students from each module would join advanced ML. \u003c/p\u003e\n\n\u003cp\u003eLet's use K-S test to find if there is any difference among student classes with regard to their intention of joining the advanced machine learning module.\u003c/p\u003e\n\n\u003cp\u003eFirst, we need to set up our null hypothesis. \u003c/p\u003e\n\n\u003cblockquote\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=H_%7B0%7D\"\u003e : There is no difference among students of different modules with respect to their intention of joining advanced ML. \u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003cpre\u003e\u003ccode\u003eStreams    No. of students interested in joining    FO(X)   Fr(X)   |FO(X)−FT(X)|\n           Observed(O)  Theoretical(T)           \nPython     5            12                          5/60    12/60   7/60\nViz.       9            12                          14/60   24/60   10/60\nSQL        11           12                          25/60   36/60   11/60\nStats      16           12                          41/60   48/60   7/60\nNLP        19           12                          60/40   60/60   60/60\n\nTotal      n=60     \n\u003c/code\u003e\u003c/pre\u003e\n\n\u003cp\u003eAccording to the formula above, \n \u003cimg src=\"https://render.githubusercontent.com/render/math?math=d=max(abs%5BF_0(X)-F_r(X)%5D)\"\u003e \u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=d=11/60%20=%200.183\"\u003e \u003c/p\u003e\n\n\u003cp\u003eHere's the Smirnov d-statistic for reference: \n\u003cimg src=\"images/1samp.png\" alt=\"\"\u003e\u003c/p\u003e\n\n\u003cp\u003eThe table value of d at 5% significance level is given by\n \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%7Bd(0.05)=%5Cfrac%7B1.36%7D%7B%5Csqrt%7Bn%7D%7D%7D%5C%5C%5B7pt%5D%5C,=%5Cfrac%7B1.36%7D%7B%5Csqrt%7B60%7D%7D%5C%5C%5B7pt%5D%5C,=0.175\"\u003e \u003c/p\u003e\n\n\u003cp\u003eSince the calculated d value (0.183) is greater than the critical value (0.175), hence we reject the null hypothesis and conclude that there is a difference among students of different modules in their intention of joining the advanced ML course. \u003c/p\u003e\n\n\u003ch3\u003eTwo-Sample K-S Test\u003c/h3\u003e\n\n\u003cp\u003eThe two-sample K-S test checks if two \u003cstrong\u003eindependent\u003c/strong\u003e samples have been drawn from the same population, or, equivalently, from two identical populations (X = Y).\u003c/p\u003e\n\n\u003cp\u003eAs with the one-sample test, it is moderately sensitive to all parameters of the distribution. The one-tailed version of this test has a specific purpose i.e .to test whether values of one population are larger than values of another population. Similar to one-sample test, cumulative distributions are compared, but here two sample distributions are compared instead of a sample distribution and a theoretical distribution as we saw above. For the two-tailed version of the test, the test statistic (d) is the largest absolute deviation between the two observed cumulative step functions, irrespective of the direction of the difference.\u003c/p\u003e\n\n\u003cblockquote\u003e\n\u003cp\u003eThe null hypothesis states for this test that there is no difference between the two distributions. The d-statistic is calculated in the same manner as we saw above.\u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=d%20=%20max%5Babs%5B%7BF_%7Bn1%7D(X)-F_%7Bn2%7D(X)%7D%5D%5D\"\u003e \u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003e\u003cp\u003en1 = Observations from first sample.\u003c/p\u003e\u003c/li\u003e\n\u003cli\u003e\u003cp\u003en2 = Observations from second sample.\u003c/p\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003eWhen the cumulative distribution shows large maximum deviation d, it is a reflection of the difference between the two sample distributions.\u003c/p\u003e\n\n\u003cp\u003eThe critical value of d for samples where n1=n2 and is ≤ 40, the K-S table for two sample case is used. When n1 and/or n2 \u0026gt; 40 then the K-S table for large samples of two-sample test should be used. The null hypothesis is accepted if the calculated value is less than the table value and vice-versa.\u003c/p\u003e\n\n\u003cp\u003eThus, the use of any of these nonparametric tests helps a researcher to test the significance of his results when the characteristics of the target population are unknown or no assumptions had been made about them.\u003c/p\u003e\n\n\u003ch3\u003eExample\u003c/h3\u003e\n\n\u003cp\u003eGiven two samples, test if their distributions are the same.\u003c/p\u003e\n\n\u003cp\u003eCompute the observed cumulative distribution functions of the two samples and compute their maximum difference.\n\u003ccode\u003e\nX : 1.2, 1.4, 1.9, 3.7, 4.4, 4.8, 9.7, 17.3, 21.1, 28.4\nY : 5.6, 6.5, 6.6, 6.9, 9.2, 10.4, 10.6, 19.3\n\u003c/code\u003e\u003c/p\u003e\n\n\u003cp\u003eWe sort the combined sample, in order to compute the empirical cdfs:\u003c/p\u003e\n\n\u003cp\u003ethe combined sample, in order to compute the\nempirical cdf’s:\n\u003ccode\u003e\n1.2 1.4 1.9 3.7 4.4 4.8 5.6 6.5 6.6 6.9 9.2 9.7 10.4 10.6 17.3 19.3 21.1 28.4\nFx 0.1 0.2 0.3 0.4 0.5 0.6 0.6 0.6 0.6 0.6 0.6 0.7 0.7  0.7  0.8  0.8  0.9  1.0\nFy 0.0 0.0 0.0 0.0 0.0 0.0 0.1 0.2 0.4 0.5 0.6 0.6 0.8  0.9  0.9  1.0  1.0  1.0\n\u003c/code\u003e  \u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-kolmogorov-smirnov-test/master/images/dist_2.png\" width=\"600\"\u003e\u003c/p\u003e\n\n\u003cp\u003eThe Kolmogorov-Smirnov statistic is again the maximum absolute difference of the two observed distribution functions. From the above image, and also by feeding above values in the given formula, we get \u003cstrong\u003ed = 0.6\u003c/strong\u003e.\u003c/p\u003e\n\n\u003cp\u003eFor two samples, the 95% critical value can be approximated by\nthe formula:\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=d(0.05)%20=%201.36%5Csqrt%7B1/n_1%20%2b%201/n_2%7D%20=%200.645\"\u003e \u003c/p\u003e\n\n\u003cp\u003eSince 0.6 \u0026lt; 0.645, we retain the null hypothesis in this case. \u003c/p\u003e\n\n\u003chr\u003e\n\n\u003cp\u003eKolmogorov-Smirnov tests have the advantages that:\n- the distribution of statistic does not depend on cumulative distribution function being tested and\u003cbr\u003e\n- the test is exact  \u003c/p\u003e\n\n\u003cp\u003eThey have the disadvantage that they are more sensitive to deviations near the center of the distribution than at the tails.\u003c/p\u003e\n\n\u003ch2\u003eSummary\u003c/h2\u003e\n\n\u003cp\u003eIn this lesson, we looked at K-S test and how this test can be used to test for normality assumptions. We also looked at a one-sample K-S test and a two-sample K-S test with simple examples. Next, we'll see how to implement these tests in Python. \u003c/p\u003e","exportId":"the-kolmogorov-smirnov-test"},{"id":141480,"title":"The Kolmogorov-Smirnov Test - Lab","type":"Assignment","indent":1,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_mark_done","completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-komogorov-smirnov-test-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-komogorov-smirnov-test-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","exportId":"g7703a422072f9b8d04c9bb2dd5c72fcb"},{"id":141483,"title":"ANOVA","type":"Assignment","indent":0,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_mark_done","completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-anova\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-anova/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","exportId":"g8ed2ab5bdad3f72d51c8431e73364c75"},{"id":141486,"title":"ANOVA - Lab","type":"Assignment","indent":1,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_mark_done","completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-anova-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-anova-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","exportId":"gc23e46ac9f4a13cccc155fa867e94bfd"},{"id":141489,"title":"Statistical Power and ANOVA - Recap","type":"WikiPage","indent":0,"locked":false,"requirement":"must_mark_done","completed":false,"content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-statistical-power-anova-recap\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-statistical-power-anova-recap\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-statistical-power-anova-recap/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eYou've covered quite a bit in this section and should be gearing up to start conducting your own hypothesis testing! Before moving on to that exciting realm, take a minute to review some of the key takeaways.\u003c/p\u003e\n\u003ch2\u003eKey Takeaways\u003c/h2\u003e\n\u003cp\u003eRemember that the section began where the last left off, examining the relationship between \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Calpha\"\u003e , power, effect size, and sample size. As you saw, these 4 quantities form a deterministic relationship; know any 3, and you can caulculate the fourth. While a lower alpha value will lead to fewer type I errors, and a higher power will lead to fewer type II errors, in practice these are often set to common default standards due to exploding sample sizes required to detect various effect sizes. Some common thresholds used are:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eSetting alpha equal to 0.05 (or 0.01)\u003c/li\u003e\n\u003cli\u003eRequiring power values of 0.8 or greater\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eAfter a thorough investigation of this relationship, you then also saw an alternative t-test, Welch's t-test which can be used for comparing samples of different sizes or different variances. While the formula was a bit complicated, the most important piece to remember is that when the assumptions that sample size and sample variance are equal for the two samples is violated, use Welch's t-test rather than the Student's t-test.\u003c/p\u003e\n\u003cp\u003eAside from ensuring that the assumptions of a t-test are met, it's also important to know how type I errors are compounded if you perform multiple tests. This is known as the multiple comparison problem and you saw that type I errors compound under multiple tests. So while the probability of a type I error is equal to \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Calpha\"\u003e for any one test, the collective probability that there is at least 1 type I error continues to increase as you perform more tests, further detracting from the confidence that you have uncovered a meaningful relationship. In order to account for this, you can use stricter criteria when defining \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Calpha\"\u003e such as the Bonferroni correction. Alternatively, ANOVA is equivalent to a 2-sided t-test when comparing two groups, but also generalizes appropriately to multiple group comparisons.\u003c/p\u003e\n\u003ch2\u003eSummary\u003c/h2\u003e\n\u003cp\u003eRemember that simply observing a low p-value is not meaningful in and of itself. There are a number of factors to take into consideration when interpreting the results of a statistical test, from alpha, power, sample size, effect size, and the formulation of the problem itself. Good hypothesis testing requires careful thought and design.\u003c/p\u003e","exportId":"statistical-power-and-anova-recap"}]},{"id":15090,"name":"Topic 16: AB Testing","status":"unlocked","unlockDate":null,"prereqs":[],"requirement":"all","sequential":false,"exportId":"g33de6a96f4479124d868f9de590327c9","items":[{"id":141501,"title":"AB Testing - Introduction","type":"WikiPage","indent":0,"locked":false,"requirement":"must_mark_done","completed":false,"content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-ab-testing-introduction\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-ab-testing-introduction\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-ab-testing-introduction/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eIn this section, you'll get to develop your skills regarding AB testing. Before diving in, take a minute to note some key points which you should keep in mind when completing the various labs and conducting your own hypothesis tests in practice.\u003c/p\u003e\n\u003ch2\u003eExperimental Design\u003c/h2\u003e\n\u003cp\u003eYou've seen that a lot goes into the proper design of statistical tests. You've learned about Goodheart's law as well as the multiple comparisons problem. Additionally, you've also seen that a p-value by itself is prone to misinterpretation if not presented with other relevant design parameters such as effect size, sample size, and \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Calpha\"\u003e . With that, here are three overarching considerations to keep in mind.\u003c/p\u003e\n\u003ch3\u003eWell Formulated Questions\u003c/h3\u003e\n\u003cp\u003eA well-formulated question is essential to a good statistical experiment. This includes careful thought of unintended consequences, as you saw in the discussion of Goodheart's law. Additionally, the question should also be specific and measurable.\u003c/p\u003e\n\u003ch3\u003eChoosing Appropriate Parameters\u003c/h3\u003e\n\u003cp\u003eIt cannot be stressed enough how important the relationship between \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Calpha\"\u003e , power, sample size and effect size is. While larger sample sizes provide more powerful tests, one should also realize that tiny effects can produce significant p-values with large samples. While this may be interesting, such small practical differences might have little to no applicable value. Furthermore, avoiding pitfalls such as the multiple comparisons problem is also important. Recall that if you perform multiple t-tests, The probability of encountering a type I error will continue to increase with additional tests. (Each test will still have the corresponding alpha value set, but collectively, the chance that a false positive type I error exists in your conclusions increases.)\u003c/p\u003e\n\u003ch3\u003ePreprocessing, Data Anomalies and Outliers\u003c/h3\u003e\n\u003cp\u003eOn the other end of problem formulation is formatting the data to actually answer said question. You'll encounter this most explicitly in the final lab of this section. There, you'll have to transform your data into an appropriate format before conducting the statistical test. Furthermore, it's important to note how idiosyncrasies in your data can impact results. For example, monumental outliers can drastically impact the outcome of statistical tests. Whether or not to remove these data points can be a source of contention and will vary upon the circumstance. Similarly, it should go without saying that erroneous data or faulty data will clearly degrade statistical tests. All in all, it's always important to get familiar with the structure of the data and the context of the question being asked before diving into the statistics themselves.\u003c/p\u003e\n\u003ch2\u003eSummary\u003c/h2\u003e\n\u003cp\u003eTime to have at it! Dive in and start practicing some hypothesis testing!\u003c/p\u003e","exportId":"ab-testing-introduction"},{"id":141506,"title":"A/B Testing","type":"WikiPage","indent":0,"locked":false,"requirement":"must_mark_done","completed":false,"content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-ab-testing\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-ab-testing\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-ab-testing/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eYou've now seen all of the statistical techniques and background to design and conduct your own A/B tests in practice! To do this, you'll go through the process of stating the null hypothesis and the alternative hypothesis which will include some test statistic for comparison. For example, you might compare the average purchase price between customers on two different versions of your online store, or a pharmaceutical researcher might compare the blood pressure of patients before and after taking a prescription. You've also seen that good test design requires multiple decisions. Recall both the multiple comparisons problem and Goodheart's law: you can't effectively measure everything (without increasing the risk of mistakes), and incentivizing measurements can lead to unforeseen consequences. With that, this section will give you a chance to put your new statistical techniques into practical applications.\u003c/p\u003e\n\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\n\u003cp\u003eYou will be able to:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eList the steps required to design, structure, and run an A/B test\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch2\u003eChoosing a Metric\u003c/h2\u003e\n\n\u003cp\u003eAny hypothesis testing will start with a given scenario. This will determine everything from what metrics are deemed important to realistically obtainable sample sizes. Goodheart's law can also be an important consideration when performing ongoing tests in attempts to optimize performance metrics.  \u003c/p\u003e\n\n\u003ch2\u003eDefining the Null Hypothesis:  \u003cimg src=\"https://render.githubusercontent.com/render/math?math=H_0\"\u003e\n\u003c/h2\u003e\n\n\u003cp\u003eOnce an appropriate metric has been selected, it's time to formally define the experiment with a null hypothesis. Typically, the null hypothesis is the claim that a researcher is hoping to refute. For example, a medical researcher might hope to show that a new drug is more effective than a previous treatment option. A common practice is then to define the null hypothesis as the contrary: there is no difference between the two drugs. The researcher hopes to refute the null hypothesis thereby proving their claim by contradiction. \u003c/p\u003e\n\n\u003cp\u003eYou might start with something like \" \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Ctext%7Bdrug%7D_a\"\u003e is more effective than  \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Ctext%7Bdrug%7D_b\"\u003e \".\u003c/p\u003e\n\n\u003cp\u003eWhile this is a good start, proper formulation of the null hypothesis should ensure that it is written with a quantitative measurement. Perhaps the drugs are for high blood pressure and so the statement becomes, \" \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Ctext%7Bdrug%7D_a\"\u003e at  \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Ctext%7Bdose%7D_a\"\u003e lowers blood pressure more than  \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Ctext%7Bdrug%7D_b\"\u003e at  \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Ctext%7Bdose%7D_b\"\u003e \".\u003c/p\u003e\n\n\u003cp\u003eFormulating the null-hypothesis like this is apt to lead you into conducting a paired t-test for the mean blood pressure of two groups: one representing  \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Ctext%7Bdrug%7D_a\"\u003e , and another representing  \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Ctext%7Bdrug%7D_b\"\u003e . \u003c/p\u003e\n\n\u003cp\u003eAlternatively, if one of these two medications were more heavily researched, one might wish to compare the effectiveness of the new medication with a predetermined metric such as the average drop in blood pressure from medication. This would lead to a 1-sample t-test, as opposed to a two-sample t-test.\u003c/p\u003e\n\n\u003ch2\u003eInvestigating  \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Calpha\"\u003e , power, effect size, and sample size\u003c/h2\u003e\n\n\u003cp\u003eFinally, one must formulate the various surrounding parameters required to conduct the test. You've seen that there is an intimate relationship between  \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Calpha\"\u003e , power, sample size, and effect size. With that, questions such as \"How costly is sample size?\" are instrumental in experiment design. For example, in an online scenario, it might be quite easy to conduct experiments at scale. On the other hand, in medical research, larger sample sizes are apt to be extremely costly. Investigating the relationships between  \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Calpha\"\u003e , power, effect size, and sample size is important for all experiments, and a suitable combination will depend on these contextual factors regarding implementation.\u003c/p\u003e\n\n\u003ch2\u003eSummary\u003c/h2\u003e\n\n\u003cp\u003eWhen researching, you are often presented with two choices for stating a question. One is to estimate a parameter in question, such as the procedures previously examined for estimating the mean of a population. Alternatively, you may wish to test the validity of a claim—whether you can refute that claim, or whether you should withhold judgment. In practice, it is up to the practitioner to determine the appropriate alpha, beta, and sample size that is determined to be both satisfactory confidence and a viable sample size to attain. In the upcoming labs, you'll get to practice this setup and design work-flow for a few scenarios.\u003c/p\u003e","exportId":"a-slash-b-testing"},{"id":141509,"title":"AB Testing - Lab","type":"Assignment","indent":1,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_mark_done","completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-ab-testing-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-ab-testing-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","exportId":"g28be896744baf3d0b8bbf77023ac9489"},{"id":141513,"title":"In-Depth AB Testing - Lab","type":"Assignment","indent":1,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_mark_done","completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-in-depth-ab-testing-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-in-depth-ab-testing-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","exportId":"ge931231c25dfbfc69c2f3ce1900b6d1f"},{"id":141520,"title":"Website AB Testing - Lab","type":"Assignment","indent":1,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_mark_done","completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-website-ab-testing-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-website-ab-testing-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","exportId":"g7472e73bf23577eb778e4c03765a1aef"},{"id":141523,"title":"AB Testing - Recap","type":"WikiPage","indent":0,"locked":false,"requirement":"must_mark_done","completed":false,"content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-ab-testing-recap\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-ab-testing-recap\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-ab-testing-recap/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eIn this section, you got a chance to further practice hypothesis testing with some real-world scenarios. Here's a brief recap of some of the key takeaways from the section.\u003c/p\u003e\n\n\u003ch2\u003eA Brief Review\u003c/h2\u003e\n\n\u003cp\u003eWhen conducting statistical tests, there's simply no substitute for critical thinking. As you've seen, there are numerous considerations from formulating appropriate hypotheses to thinking about the context of the problem itself. While the techniques presented are extremely powerful if properly employed, you've also seen how each technique comes with its own assumptions, and ignoring these can invalidate results. Similarly, how the test is structured can lead to widely different results. \u003c/p\u003e\n\n\u003cp\u003eThere are also additional statistical tests not touched upon here. To date, your primary method has been conducting t-tests. This the most common statistical test used in practice and is very effective at comparing averages (of any metric) between groups. You've also seen how ANOVA can generalize this process to multiple groups. If you wish to continue to strengthen your knowledge of other statistical tests, take a look at some of the resources below.\u003c/p\u003e\n\n\u003ch2\u003eAdditional Resources\u003c/h2\u003e\n\n\u003cul\u003e\n\u003cli\u003e\u003cp\u003e\u003ca href=\"https://stats.idre.ucla.edu/other/mult-pkg/whatstat/\"\u003eChoosing Statistical Tests\u003c/a\u003e\u003c/p\u003e\u003c/li\u003e\n\u003cli\u003e\u003cp\u003e\u003ca href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3116565/\"\u003eHow to choose the right statistical test?\u003c/a\u003e\u003c/p\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch2\u003eSummary\u003c/h2\u003e\n\n\u003cp\u003eWell done! You've not only learned about a variety of statistical tests and the theory behind them, but you've now also put these techniques into practice in order to carry out hypothesis testing!\u003c/p\u003e","exportId":"ab-testing-recap"}]},{"id":15095,"name":"Topic 17: Bayesian Statistics","status":"unlocked","unlockDate":null,"prereqs":[],"requirement":"all","sequential":false,"exportId":"g0b888d99112060de4ae462813f3be6bf","items":[{"id":141531,"title":"Bayesian Statistics - Introduction","type":"WikiPage","indent":0,"locked":false,"requirement":"must_mark_done","completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-bayesian-stats-introduction\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-bayesian-stats-introduction/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eIn this section, you'll investigate the Bayesian statistical framework. Bayesian statistics are an alternative perspective to classical Frequentist approaches which you've seen thus far. Bayesian statistics applies reasoning to unknown probabilities in a manner in which the Frequentist approach does not allow. \u003c/p\u003e\n\n\u003ch2\u003eThomas Bayes\u003c/h2\u003e\n\n\u003cp\u003eBayesian statistics owes its name to the famous mathematician Thomas Bayes. Born (sometime) in the early 1700s, he bucked many academic traditions of his time due to his families religious beliefs. Cambridge and Oxford were known for the most prestigious mathematics of the time, but Bayes was a Presbytarien barring him from these universities which had ties to the Church of England. \u003c/p\u003e\n\n\u003ch2\u003eBayes' theorem\u003c/h2\u003e\n\n\u003cp\u003eBayes' theorem is a method for rewriting conditional probabilities. The formula is:\u003c/p\u003e\n\n\u003ch3\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=P(A%7CB)%20=%5Cdfrac%7BP(B%7CA)P(A)%7D%7BP(B)%7D\"\u003e\u003c/h3\u003e\n\n\u003cp\u003eIn the following lessons, you'll learn more about two traditional interpretations of this formula. The first provides an intuitive understanding, viewing the numerator as the probability of both A and B occuring:  \u003c/p\u003e\n\n\u003ch3\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=P(A%7CB)%20=%5Cdfrac%7BP(A%20%5Ccap%20B)%7D%7BP(B)%7D\"\u003e\u003c/h3\u003e\n\n\u003cp\u003eThis should make perfect sense: the probability that A is true, given B is true, is the probability that A and B are both true, divided by the probability that B was true in the first place. \u003c/p\u003e\n\n\u003cp\u003eThe second interpretation of Bayes theorem leads straight into the Bayesian statistical framework itself, bringing about discussions of priors, likelihoods, and posterior probabilities.\u003c/p\u003e\n\n\u003ch2\u003eSummary\u003c/h2\u003e\n\n\u003cp\u003eGet ready to jump in! This section provides an exciting introduction to Bayes theorem and Bayesian statistics, further rounding out your statistical toolbox!\u003c/p\u003e\n\u003cfooter class=\"fis-footer\" style=\"visibility: hidden;\"\u003e\u003cdiv class=\"fis-feedback\"\u003e\n\u003ch5\u003eHow do you feel about this lesson?\u003c/h5\u003e\n\u003cimg id=\"thumbs-up\" data-repository=\"dsc-bayesian-stats-introduction\" title=\"Thumbs up!\" alt=\"thumbs up\"\u003e\u003cimg id=\"thumbs-down\" data-repository=\"dsc-bayesian-stats-introduction\" title=\"Thumbs down!\" alt=\"thumbs down\"\u003e\n\u003c/div\u003e\n\u003ch5\u003eHave specific feedback? \u003ca href=\"https://github.com/learn-co-curriculum/dsc-bayesian-stats-introduction/issues/new\"\u003eTell us here!\u003c/a\u003e\n\u003c/h5\u003e\u003c/footer\u003e","exportId":"bayesian-statistics-introduction"},{"id":141535,"title":"Bayesians vs Frequentists","type":"WikiPage","indent":0,"locked":false,"requirement":"must_mark_done","completed":false,"content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-bayesians-vs-frequentists\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-bayesians-vs-frequentists\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-bayesians-vs-frequentists/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eUp until now, all of the statistical theory you have encountered has been through the lens of a Frequentist. This has included discussions of z-tests, t-tests, p-values, and ANOVA; all are from the Frequentist perspective. In this lesson, you'll start to explore an alternative perspective donned by Bayesians.\u003c/p\u003e\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\u003cp\u003eYou will be able to:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eCompare the Bayesian v. Frequentist statistical frameworks\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003ePhilosophical Interpretations\u003c/h2\u003e\n\u003cp\u003eA natural place to start when outlining the differences between Bayesians and Frequentists is to talk of their interpretation of probability itself. For Frequentists, the probability of an event is the limit of the rate of occurrences of the event if the same scenario including context and assumptions were repeated ad infinitum. In contrast, Bayesians interpret probability as the level of confidence, or belief, in a particular event occurring. In many ways, this makes a more natural interpretation for rare events that cannot possibly reoccur in the same context and circumstances.\u003c/p\u003e\n\u003ch2\u003ePractical Implications\u003c/h2\u003e\n\u003cp\u003eThe practical implications of Bayesians versus Frequentists rest upon making assumptions about unknown quantities. In the Bayesian framework, you make assumptions about unknown variables which you are attempting to estimate. For example, you might assume that the number of individuals who will buy a product can be represented by a binomial variable with parameter \u003cimg src=\"https://render.githubusercontent.com/render/math?math=p\"\u003e . In contrast, the Frequentist perspective does not allow embedding of prior beliefs such as this into statistical experiments and analyses.\u003c/p\u003e\n\u003ch2\u003eSummary\u003c/h2\u003e\n\u003cp\u003eIn this lesson, you started to learn about the differences in Bayesian versus Frequentist statistical perspectives. Keep in mind that there are not always rigid lines between these modes of thought. Nonetheless, the discussion is a worthwhile one in considering which approach you wish to take in your research and analysis.\u003c/p\u003e","exportId":"bayesians-vs-frequentists"},{"id":141540,"title":"Bayes' Theorem","type":"WikiPage","indent":0,"locked":false,"requirement":"must_mark_done","completed":false,"content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-bayes-theorem\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-bayes-theorem\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-bayes-theorem/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eBayes theorem is an indispensable law of probability, allowing you to deductively quantify unknown probabilities. The theory rests upon conditional probability. Let's take a look at it in practice.\u003c/p\u003e\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\u003cp\u003eYou will be able to:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eDefine Bayes' theorem in relation to conditional probabilities\u003c/li\u003e\n\u003cli\u003eIdentify examples of applications of Bayes' theorem\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003eBayes' formula\u003c/h2\u003e\n\u003ch3\u003eBreaking the formula apart\u003c/h3\u003e\n\u003cp\u003eBayes' theorem is quite intuitive, decomposing the conditional probability of 'A given B' in terms of the probability that both events are true divided by the probability that B is true. Bayes theorem takes this natural idea a step further, expressing the probability that both events are true as a conditional probability multiplied by the condition itself.\u003c/p\u003e\n\u003cp\u003eTo recap:\u003c/p\u003e\n\u003cp\u003eBayes' Theorem takes the definition of the conditional likelihood:\u003c/p\u003e\n\u003ch3\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=P(A%7CB)%20=%20%5Cdfrac%7BP(A%20%20%5Ccap%20B)%7D%7BP(B)%7D\"\u003e\u003c/h3\u003e\n\u003cp\u003eand rewrites the \u003cimg src=\"https://render.githubusercontent.com/render/math?math=P(A%20%20%5Ccap%20B)\"\u003e as \u003cimg src=\"https://render.githubusercontent.com/render/math?math=P(B%7CA)P(A)\"\u003e , which makes perfect sense; the probability of B given A is true, multiplied by the probability that A is true, gives us the probability that both are true.\u003c/p\u003e\n\u003cp\u003eMaking this substitution, you have Bayes' Theorem:\u003c/p\u003e\n\u003ch3\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=P(A%7CB)%20=%20%5Cdfrac%7BP(B%7CA)P(A)%7D%7BP(B)%7D\"\u003e\u003c/h3\u003e\n\u003ch2\u003eA simple example\u003c/h2\u003e\n\u003cp\u003eLet's take a simple theoretical example to demonstrate. Imagine there are two fish tanks at the local pet store. The small tank holds 10 Betta fish. The large tank has 200 goldfish and 35 Betta fish. Given that a fish is a Betta fish, what's the probability it comes from the small tank?\u003c/p\u003e\n\u003cp\u003eOn the one hand, it seems that if you were to select a fish from the large tank, you'd probably end up with a goldfish. However, because these tanks are of such vastly different sizes, the probability that the fish came from the larger tank is actually more probable.\u003c/p\u003e\n\u003cp\u003eUsing Bayes' theorem, you are looking to find the probability that the fish came from the small tank, given that it is a Betta fish:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=P(%5Ctext%7Bsmall%20tank%20%7C%20Betta%20fish%7D)%20=%20%5Cdfrac%7BP(%5Ctext%7BBetta%20fish%20%7C%20small%20tank%7D)P(%5Ctext%7Bsmall%20tank%7D)%7D%7BP(%5Ctext%7BBetta%20fish%7D)%7D\"\u003e\u003c/p\u003e\n\u003cp\u003eFurthermore, you know:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=P(%5Ctext%7BBetta%20fish%20%7C%20small%20tank%7D)%20=%201\"\u003e\u003cbr\u003e\u003cbr\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=P(%5Ctext%7Bsmall%20tank%7D)%20=%20%5Cdfrac%7B%5Ctext%7Bnumber%20of%20fish%20in%20small%20tank%7D%7D%7B%5Ctext%7Bnumber%20of%20all%20fish%7D%7D%20=%20%5Cdfrac%7B10%7D%7B245%7D\"\u003e\u003cbr\u003e\u003cbr\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=P(%5Ctext%7BBetta%20fish%7D)%20=%20%5Cdfrac%7B45%7D%7B245%7D\"\u003e\u003c/p\u003e\n\u003cp\u003eGiving you:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=P(%5Ctext%7Bsmall%20tank%20%7C%20Betta%20fish%7D)%20=%20%5Cdfrac%7B1%20%5Ccdot%20%5Cdfrac%7B10%7D%7B245%7D%7D%7B%5Cdfrac%7B45%7D%7B245%7D%7D\"\u003e \u003cbr\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=P(%5Ctext%7Bsmall%20tank%20%7C%20Betta%20fish%7D)%20=%20%5Cdfrac%7B10%7D%7B45%7D\"\u003e\u003c/p\u003e\n\u003cp\u003eWhile concrete, this example fails to demonstrate the full power of Bayes' theorem since you had all of the underlying information, so you don't even need to use Bayes' theorem. You could have simply looked at the number of Betta fish in the small tank versus the number of Betta fish overall:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cdfrac%7B10%7D%7B45%7D\"\u003e\u003c/p\u003e\n\u003cp\u003egiving you exactly the same result.\u003c/p\u003e\n\u003ch2\u003eAn NLP example\u003c/h2\u003e\n\u003cp\u003eWith this simple example out of the way, let's examine a more practical example from the field of Natural Language Processing.\u003c/p\u003e\n\u003cp\u003eA common introductory example to Natural Language Processing or classification is detecting spam. While you may enjoy spam in a can, you probably don't enjoy getting spam in your inbox. Bayes' theorem can serve as a natural classification method in these scenarios. Assume that the word \"offer\" (as in Special Offer, We Have an Offer for You, or Don't Miss This Offer!) occurs in 73% of the spam messages you receive. In comparison, only 10% of your desired mail contains the word \"offer\". If 20% of the messages you receive are spam, and you receive another message with the word \"offer\", what is the probability that it is spam?\u003c/p\u003e\n\u003cp\u003eAs you might have guessed, you can solve this using the Bayes' theorem!\u003c/p\u003e\n\u003cp\u003eFirst, set up the problem:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=P(%5Ctext%7BSpam%20%7C%20Offer%7D)%20=%20%5Cdfrac%7BP(%5Ctext%7BOffer%20%7C%20Spam%7D)P(%5Ctext%7BSpam%7D)%7D%7BP(%5Ctext%7BOffer%7D)%7D\"\u003e\u003c/p\u003e\n\u003cp\u003eThen substituting some of the immediate knowledge we have from the scenario:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=P(%5Ctext%7BSpam%20%7C%20Offer%7D)%20=%20%5Cdfrac%7B.73%20%5Ccdot%20.20%7D%7BP(%5Ctext%7BOffer%7D)%7D\"\u003e\u003c/p\u003e\n\u003cp\u003eFinally, the probability of receiving an email with the word \"offer\", \u003cimg src=\"https://render.githubusercontent.com/render/math?math=P(%5Ctext%7BOffer%7D)\"\u003e , can be evaluated by decomposing it into the two subsets spam and not spam:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=P(%5Ctext%7BOffer%7D)%20=%20P(%5Ctext%7BSpam%7D)%5Ccdot%20P(%5Ctext%7BOffer%20%7C%20Spam%7D)%20%2b%20P(%5Ctext%7B~Spam)%7D%20%5Ccdot%20P(%5Ctext%7BOffer%20%7C%20~Spam%7D)\"\u003e\u003cbr\u003e\u003cbr\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=P(%5Ctext%7BOffer%7D)%20=%20.20%20%5Ccdot%20.73%20%2b%20.8%20%5Ccdot%20.10\"\u003e\u003cbr\u003e\u003cbr\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=P(%5Ctext%7BOffer%7D)%20=%20.146%20%2b%20.08\"\u003e\u003cbr\u003e\u003cbr\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=P(%5Ctext%7BOffer%7D)%20=%20.226\"\u003e\u003c/p\u003e\n\u003cp\u003eFinally, substituting this into the original Bayes formula you have:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=P(%5Ctext%7BSpam%20%7C%20Offer%7D)%20=%20%5Cdfrac%7B.73%20%5Ccdot%20.20%7D%7BP(%5Ctext%7BOffer%7D)%7D\"\u003e\u003cbr\u003e\u003cbr\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=P(%5Ctext%7BSpam%20%7C%20Offer%7D)%20=%20%5Cdfrac%7B.73%20%5Ccdot%20.20%7D%7B.226%7D\"\u003e\u003cbr\u003e\u003cbr\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=P(%5Ctext%7BSpam%20%7C%20Offer%7D)%20=%20.6460\"\u003e\u003c/p\u003e\n\u003cp\u003eAs you can see, while spam has a much higher occurrence of the word \"offer\", the presence of the word alone does not provide strong confidence that the message is spam. To provide more statistical power, you will eventually extend Bayes' theorem to multiple observations simultaneously using the relative probabilities of multiple words.\u003c/p\u003e\n\u003ch2\u003eSummary\u003c/h2\u003e\n\u003cp\u003eIn this lesson, you were introduced to the Bayes' theorem, and saw how it can be used to quantify conditional probabilities. With that, let's turn to some more simple examples for you to practice and deepen your understanding.\u003c/p\u003e","exportId":"bayes-theorem"},{"id":141545,"title":"Bayes' Theorem - Lab","type":"Assignment","indent":1,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_mark_done","completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-bayes-theorem-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-bayes-theorem-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","exportId":"g44807f40116a2dac1709e9858fd2b0e8"},{"id":141550,"title":"The Monty Hall Problem - Lab","type":"Assignment","indent":1,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_mark_done","completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-monty-hall-problem-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-monty-hall-problem-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","exportId":"g8f9ca8f2dd8d6d35eb5444425cda3263"},{"id":141555,"title":"Maximum Likelihood Estimation (MLE)","type":"Assignment","indent":0,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_mark_done","completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-mle\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-mle/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","exportId":"gaff47b4cc6ce1b5b1ce7c5cc86fbce5f"},{"id":141559,"title":"Maximum A Posteriori Estimation (MAP) and Multinomial Bayes","type":"WikiPage","indent":0,"locked":false,"requirement":"must_mark_done","completed":false,"content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-map-multinomial-bayes\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-map-multinomial-bayes\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-map-multinomial-bayes/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eMaximum A Posteriori provides a means for estimating a parameter given some prior knowledge about a variable. In it, one assumes a given distribution for the variable and then estimates the parameter itself given additional information. In this lesson, you'll see how Bayes' theorem can be applied in this manner and then extended to multivariate cases.\u003c/p\u003e\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\u003cp\u003eYou will be able to\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eIdentify how Maximum A Posteriori Estimation is related to MLE\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003eMaximum A Posteriori Estimation\u003c/h2\u003e\n\u003cp\u003eMaximum A Posteriori Estimation (MAP) is similar to Maximum Likelihood Estimation but extends this concept by allowing one to also account for prior beliefs regarding the distribution of the variable in question. Recall Bayes' theorem:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Clarge%20P(A%7CB)%20=%20%5Cdfrac%7BP(B%7CA)(A)%7D%7BP(B)%7D\"\u003e\u003c/p\u003e\n\u003cp\u003eThe Bayesian interpretation of this formula is\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Clarge%20%5Ctext%7BPosterior%7D%20=%20%5Cdfrac%7B%5Ctext%7BLikelihood%7D%20%5Ccdot%20%5Ctext%7BPrior%7D%7D%7B%5Ctext%7BEvidence%7D%7D\"\u003e\u003c/p\u003e\n\u003cp\u003eWith MAP, you then attempt to optimize a parameter \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Ctheta\"\u003e for the assumed distribution in order to maximize the posterior probability.\u003c/p\u003e\n\u003ch2\u003eMultinomial Bayes\u003c/h2\u003e\n\u003cp\u003eMultinomial Bayes also extends the notions within Bayes' theorem, allowing one to chain inferences. The primary assumption for this is assuming that your variables are independent of one another. Recall that if you assume two events A and B are independent of one another, then \u003cimg src=\"https://render.githubusercontent.com/render/math?math=P(A%20%20%5Ccap%20B)%20=%20P(A)%5Ccdot%20P(B)\"\u003e . Similarly, if independence is assumed when extending Bayes theorem to a multivariate case, one can multiply the successive probability estimates. Mathematically, this can be summarized as:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Clarge%20P(Y%7CX_1,%20X_2,...,X_n)%20=%20%5Cdfrac%7BP(X_1%7CY)%5Ccdot%20P(X_2%7CY)%20%5Ccdot%20...%20%5Ccdot%20P(X_n%7CY)%7D%7BP(X_1,%20X_2,...,X_n)%7DP(Y)\"\u003e\u003c/p\u003e\n\u003ch2\u003eSummary\u003c/h2\u003e\n\u003cp\u003eThis lesson briefly introduced the concept of Maximum A Posteriori Estimation and extending Bayes' theorem to multivariate cases. In later sections, you'll investigate these ideas in practice, working with practical examples and coding your own implementations to gain a full understanding.\u003c/p\u003e","exportId":"maximum-a-posteriori-estimation-map-and-multinomial-bayes"},{"id":141562,"title":"Bayesian Statistics - Recap","type":"WikiPage","indent":0,"locked":false,"requirement":"must_mark_done","completed":false,"content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-bayesian-stats-recap\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-bayesian-stats-recap\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-bayesian-stats-recap/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eWell done! You covered a lot of ground in this section. From Bayes' theorem to Maximum Likelihood Estimation (MLE), you now have the foundation to dive into the world of Bayesians!\u003c/p\u003e\n\u003ch2\u003eBayes' Theorem\u003c/h2\u003e\n\u003cp\u003eTo start, you investigated Bayes' theorem and some hypothetical examples.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Clarge%20P(A%7CB)%20=%20%5Cdfrac%7BP(B%7CA)P(A)%7D%7BP(B)%7D\"\u003e\u003c/p\u003e\n\u003ch2\u003eBayesian Statistics\u003c/h2\u003e\n\u003cp\u003eFrom there, you then went on to read more about some of the philosophical differences between Bayesians and Frequentists. Bayesians interpret probability as the level of confidence or belief in an event. In contrast, Frequentists view probability as the limit as the number of trials goes to infinity of successes versus trials.\u003c/p\u003e\n\u003ch2\u003eMLE and MAP\u003c/h2\u003e\n\u003cp\u003eIn outlining the discussion of Bayesian techniques, you got an introduction to Maximum Likelihood Estimation and Maximum A Posteriori Estimation. In both, you saw methods for optimizing one's beliefs given certain information. This was used to estimate parameters for assumed distributions.\u003c/p\u003e\n\u003ch2\u003eSummary\u003c/h2\u003e\n\u003cp\u003eAgain, quite a bit was covered in this section. There are certainly plenty of additional resources available if you wish to further dive into MLE, MAP, or other Bayesian techniques. Bayesian inference can provide a powerful framework for quantifying and reasoning with uncertainty that has continued to gain popularity with additional computing resources.\u003c/p\u003e","exportId":"bayesian-statistics-recap"}]},{"id":15099,"name":"Topic 18: Introduction to Linear Regression","status":"started","unlockDate":null,"prereqs":[],"requirement":"all","sequential":false,"exportId":"gc8629b4e2884ba7e5e97d32f691c5016","items":[{"id":141575,"title":"Introduction to Linear Regression - Introduction","type":"WikiPage","indent":0,"locked":false,"requirement":"must_mark_done","completed":true,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-linear-regression-section-intro-v2-1\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-linear-regression-section-intro-v2-1/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eIn this section, you're going to learn about one of the most basic machine learning models, linear regression! Many of the ideas you learn in this section will be foundational knowledge for more complex machine learning models.\u003c/p\u003e\n\u003ch2\u003eStatistical Learning Theory\u003c/h2\u003e\n\u003cp\u003eWe'll start this section by exploring Statistical Learning Theory and how dependent and independent variables relate to it. Statistical Learning Theory provides an important framework for understanding machine learning.\u003c/p\u003e\n\u003ch2\u003eLinear Regression\u003c/h2\u003e\n\u003cp\u003eIn this section, we'll introduce our first machine learning model - linear regression. It's really just a fancy way of saying \"(straight) line of best fit\", but it will introduce a number of concepts that will be important as you continue to learn about more sophisticated models.\u003c/p\u003e\n\u003ch2\u003eCoefficient of Determination\u003c/h2\u003e\n\u003cp\u003eWe're then going to introduce the idea of \"R squared\" as the coefficient of determination to quantify how well a particular line fits a particular data set.\u003c/p\u003e\n\u003ch2\u003eA Complete Regression\u003c/h2\u003e\n\u003cp\u003eFrom there we look at calculating a complete linear regression, just using code. We'll cover some of the assumptions that must be held for a \"least squares regression\", introduce Ordinary Least Squares in Statsmodels and introduce some tools for diagnosing your linear regression such as Q-Q plots, the Jarque-Bera test for normal distribution of residuals and the Goldfield-Quandt test for heteroscedasticity. We then look at the interpretation of significance and p-value and finish up by doing a regression model of the Boston Housing data set.\u003c/p\u003e\n\u003ch2\u003eSummary\u003c/h2\u003e\n\u003cp\u003eCongratulations! You've made it through much of the introductory data and we've finally got enough context to take a look at our first machine learning model, while broadening our experience of both coding and math so we'll be able to introduce more sophisticated machine learning models as the course progresses.\u003c/p\u003e\n\u003cfooter class=\"fis-footer\" style=\"visibility: hidden;\"\u003e\n\u003cdiv class=\"fis-feedback\"\u003e\n\u003ch5\u003eHow do you feel about this lesson?\u003c/h5\u003e\n\u003cimg id=\"thumbs-up\" title=\"Thumbs up!\" alt=\"thumbs up\" data-repository=\"dsc-linear-regression-section-intro-v2-1\"\u003e\u003cimg id=\"thumbs-down\" title=\"Thumbs down!\" alt=\"thumbs down\" data-repository=\"dsc-linear-regression-section-intro-v2-1\"\u003e\n\u003c/div\u003e\n\u003ch5\u003eHave specific feedback? \u003ca href=\"https://github.com/learn-co-curriculum/dsc-linear-regression-section-intro-v2-1/issues/new\"\u003eTell us here!\u003c/a\u003e\n\u003c/h5\u003e\n\u003c/footer\u003e","exportId":"introduction-to-linear-regression-introduction"},{"id":141579,"title":"Statistical Learning Theory","type":"WikiPage","indent":0,"locked":false,"requirement":"must_mark_done","completed":true,"content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-stat-learning-theory\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-stat-learning-theory\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-stat-learning-theory/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eIn this lesson, you'll be introduced to Statistical Learning Theory and some key components in the framework of this theory. This is a particularly important theory as it encompasses the majority of statistical inference and functional analyses approaches. Statistical Learning Theory has applications in a wide variety of fields such as image and speech recognition, bioinformatics, sports, etc.\u003c/p\u003e\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\u003cp\u003eYou will be able to:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eIdentify independent and dependent variables in a statistical model\u003c/li\u003e\n\u003cli\u003eDescribe loss and its importance in relation to model creation\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003eStatistical Learning Theory\u003c/h2\u003e\n\u003cblockquote\u003e\n\u003cp\u003eStatistical Learning Theory is based on the idea of using data along with statistics to provide a framework for learning.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eIn Statistical Learning Theory, the main idea is to \u003cstrong\u003econstruct a model\u003c/strong\u003e to draw certain conclusions from data, and next, to \u003cstrong\u003euse this model\u003c/strong\u003e to make predictions.\u003c/p\u003e\n\u003ch2\u003eTypes of Data in Statistical Learning\u003c/h2\u003e\n\u003cp\u003eIn the context of Statistical learning, there are two main types of data:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cstrong\u003eDependent variables\u003c/strong\u003e: data that can be controlled directly (other names: outcome variables, target variables, response variables)\u003c/li\u003e\n\u003cli\u003e\n\u003cstrong\u003eIndependent variables\u003c/strong\u003e: data that cannot be controlled directly (other names: predictor variables, input variables, explanatory variables, features)\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eIn models, the independent variable(s) are the variables that will affect (or will lead to a change in) the dependent variable(s).\u003c/p\u003e\n\u003cp\u003eTwo examples of common \u003cstrong\u003eindependent variables\u003c/strong\u003e are age and time. There is nothing you can do to speed up or slow down time or increase or decrease age. They are independent of everything else.\u003c/p\u003e\n\u003cp\u003eAn example of a \u003cstrong\u003edependent variable\u003c/strong\u003e is how much you weigh at different ages. Here, the dependent variable (weight) depends on the independent variable (age). As someone's weight fluctuates over time, you can observe and record your weight as a dependent variable on your age.\u003c/p\u003e\n\u003cp\u003eIndependent and dependent variables are normally shown on a graph under a standardized approach. This makes it easy for you to quickly see which variable is independent and which is dependent when looking at a graph or chart.\u003c/p\u003e\n\u003cp\u003eConventionally, the independent variable goes on the x-axis, or the horizontal axis. Let's consider another example, one where we look at someone's income depending on their age. Below, you see a scatter plot where age is the independent variable, and income is the dependent variable. In this setting, \u003cstrong\u003ewe want to study if age has some effect on annual income\u003c/strong\u003e.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-stat-learning-theory/master/images/scatter_age_income.png\" width=\"600\"\u003e\u003c/p\u003e\n\u003ch2\u003eStatistical Model\u003c/h2\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003eA statistical model can be thought of as some kind of a transformation that helps us express dependent variables as a function of one or more independent variables\u003c/strong\u003e.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eA statistical model defines a \u003cstrong\u003erelationship\u003c/strong\u003e between a dependent and an independent variable.\u003c/p\u003e\n\u003cp\u003eFor the plot we see above, the relationship between age and income can be shown using a \u003cstrong\u003estraight line\u003c/strong\u003e connecting all the individual observations in the data. So this line here would be our \u003cstrong\u003emodel\u003c/strong\u003e as shown in the image below.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-stat-learning-theory/master/images/scatter_line_age_income.png\" width=\"600\"\u003e\u003c/p\u003e\n\u003cp\u003eWe can define and \u003cstrong\u003efit\u003c/strong\u003e such a straight line to our data following a straight line equation:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=y%20=%20m%20%20x%20%2b%20c\"\u003e\u003c/p\u003e\n\u003cp\u003eYou'll often come across greek letters talking about models like this. Another common way of writing a linear equation is ( \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cbeta\"\u003e is the Greek letter \"beta\"):\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=y%20=%20%5Cbeta_0%20%2b%20%5Cbeta_1%20%20x\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cbeta_0\"\u003e has the same role as \u003cimg src=\"https://render.githubusercontent.com/render/math?math=c\"\u003e in the first expression and denotes the \u003cem\u003eintercept with the y-axis\u003c/em\u003e. \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cbeta_1\"\u003e has the same role as \u003cimg src=\"https://render.githubusercontent.com/render/math?math=m\"\u003e in the first expression and denotes the \u003cem\u003eslope of the line\u003c/em\u003e. More on this below.\u003c/p\u003e\n\u003cp\u003eSuch a simple model would describe a person's height has \u003cstrong\u003ealmost\u003c/strong\u003e a linear relationship with weight i.e. weight increases with height.\u003c/p\u003e\n\u003cp\u003eSo this is our simple model for the relationship. Of course, we can use more sophisticated models like quadratic equations or polynomial equations for a \u003cstrong\u003ebetter fit\u003c/strong\u003e, and you may see this later on if you dig into more advanced modeling.\u003c/p\u003e\n\u003cp\u003eLooking at this line above, we can define it as \u003cstrong\u003eIncome = 1500 + 1000 * Age\u003c/strong\u003e, based on slope ( \u003cimg src=\"https://render.githubusercontent.com/render/math?math=m\"\u003e or \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cbeta_1\"\u003e ) and intercept (c or \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cbeta_0\"\u003e ) values.\u003c/p\u003e\n\u003cp\u003eThis would be our \u003cstrong\u003elinear model\u003c/strong\u003e (Linear refers to a model consisting of a straight line, or \"linear regression\"), which can help us work out a weight value for a given height. In summary,\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eA model is expressed as a mathematical equation showing the relationship between dependent and independent variables.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch2\u003eStatistical Model Parameters\u003c/h2\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003eModel Parameters are the coefficients of the model equation for estimating the output\u003c/strong\u003e.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eStatistical Learning is all about learning these parameters. A statistical learning approach would help us \u003cstrong\u003elearn\u003c/strong\u003e these parameters so we have a clear description of their relationship which we can replicate and analyze under different circumstances.\u003c/p\u003e\n\u003cp\u003eFor the straight line above, we need to learn the \u003cstrong\u003eslope\u003c/strong\u003e and \u003cstrong\u003eintercept\u003c/strong\u003e for the line that best describes the relationship between the data elements in the dataset. We gave you the two values here, but in general, you'll have to \u003cstrong\u003elearn these values\u003c/strong\u003e. These values are denoted by \u003cstrong\u003eparameters\u003c/strong\u003e.\u003c/p\u003e\n\u003cp\u003eOnce we have learned the \u003cimg src=\"https://render.githubusercontent.com/render/math?math=m\"\u003e (or \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cbeta_1\"\u003e ) and \u003cimg src=\"https://render.githubusercontent.com/render/math?math=c\"\u003e (or \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cbeta_0\"\u003e ) values, we can predict a value of \u003cimg src=\"https://render.githubusercontent.com/render/math?math=y\"\u003e (income in our example) for a given value of \u003cimg src=\"https://render.githubusercontent.com/render/math?math=x\"\u003e (age). In our next lab, you'll learn how to calculate these for a given dataset. Let's have a look at another example:\u003c/p\u003e\n\u003ch3\u003eWhat Else Determines an Individual's Income?\u003c/h3\u003e\n\u003cp\u003eIf we suppose that income is a function of not only age, but also education level. A model that estimates the income could look like:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=income%20=%20%5Cbeta_0%20%2b%20%5Cbeta_1%20*%20%20%5Ctext%7Bage%7D%20%2b%20%5Cbeta_2%20*%20%5Ctext%7Beducation%20level%7D\"\u003e\u003c/p\u003e\n\u003cp\u003eHere we have two independent variables i.e. age and education level, with the same dependent variable, income. \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cbeta_0\"\u003e , \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cbeta_1\"\u003e and \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cbeta_2\"\u003e are model parameters.\u003c/p\u003e\n\u003ch2\u003eModel Generalization\u003c/h2\u003e\n\u003cp\u003eAs the data which is available to us for modeling is finite, the available data needs to be used very effectively to build and \u003cstrong\u003evalidate\u003c/strong\u003e a model. Validation of the model usually makes the model more \u003cstrong\u003egeneralizable\u003c/strong\u003e for unseen situations.\u003c/p\u003e\n\u003cp\u003eTraining the model is like the infancy stage for humans. Examples are presented to the model and the model tweaks its parameters to better understand the data. Once the training is over, the model is unleashed upon new data and then uses what it has learned to explain that data. This is where problems can emerge. If we \u003cstrong\u003eover-train\u003c/strong\u003e the model on the training data i.e. make the model every detail of shown data, it will be able to identify all the relevant information in the training data, but will fail miserably when presented with the new data.\u003c/p\u003e\n\u003cp\u003eWe then say that the \u003cstrong\u003emodel is not capable of generalizing\u003c/strong\u003e, or that \u003cstrong\u003emodel is over-fitting the training data\u003c/strong\u003e.\u003c/p\u003e\n\u003cp\u003eHere's a great example of the phenomenon: modeling happiness as a function of wealth.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-stat-learning-theory/master/images/new_happy.png\" width=\"600\"\u003e\u003c/p\u003e\n\u003cp\u003eIn the top three diagrams, we have data and models (dashed curves). From left to right the models have been trained longer and longer on the training data. The training error curve in the bottom box shows that the training error gets better and better as we train longer (increasing model complexity). You may think that if we train longer we'll get better! Well, yes, but \u003cstrong\u003eonly better at describing the training data\u003c/strong\u003e. The top right box shows a very complex model that hits all the data points. This model does great on the training data, but when presented with new data (examine the Prediction error curve in the bottom box) then it does worse!\u003c/p\u003e\n\u003cp\u003eIn order to create good predictive models in machine learning that are capable of generalizing, one needs to know when to stop training the model so that it doesn't over-fit.\u003c/p\u003e\n\u003ch3\u003eModel Validation\u003c/h3\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003eModel validation is a process of controlling overfitting and allows a higher degree of generalizability.\u003c/strong\u003e\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eHere is how we perform validation, in its simplest form:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eSplit the data into two parts with a 70/30, 80/20 or a similar split\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eUse the larger part for training so the model learns from it. This set of data is normally called the \u003cstrong\u003eTraining Data\u003c/strong\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eUse the smaller part for testing the model. This is data is not being used during the model learning process and used only for testing the performance of a learned model. This dataset is called as the \u003cstrong\u003eTesting Data\u003c/strong\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThis setup looks like as shown below:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-stat-learning-theory/master/images/new_train_test_sets.png\" width=\"600\"\u003e\u003c/p\u003e\n\u003cp\u003eIn statistical learning, if the model has learned well from the training data, it will perform well on both training data \u003cstrong\u003eand\u003c/strong\u003e test data. You can then use the test data to calculate the \u003cstrong\u003eaccuracy\u003c/strong\u003e, which is assessed based on how close it has estimated the output to the actual value.\u003c/p\u003e\n\u003ch2\u003eModel Loss\u003c/h2\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003eA loss function evaluates how well your model represents the relationship between data variables\u003c/strong\u003e.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eIf the model is unable to identify the underlying relationship between the independent and dependent variable(s), the loss function will output a very high number. Consider the age vs. income example above. You can see that the linear model is not exactly touching each data point because these points do not exist in a line. the individual distance of each point from the line is the \u003cstrong\u003eloss\u003c/strong\u003e that the model exhibits.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-stat-learning-theory/master/images/new_loss.png\" width=\"400\"\u003e\u003c/p\u003e\n\u003cp\u003eThese individual losses, which is essentially the \u003cstrong\u003evertical distance between the individual data points and the line\u003c/strong\u003e are taken into account to calculate the overall model loss.\u003c/p\u003e\n\u003cp\u003eIf the relationship is well modeled, the loss will be low. As we change the parameters of our model to try and improve results, our loss function is our best friend, telling us if we are on the right track.\u003c/p\u003e\n\u003cp\u003eYou'll learn about loss in further detail in upcoming lessons.\u003c/p\u003e\n\u003ch2\u003eAdditional Resources\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e\u003ca class=\"\" href=\"https://www.youtube.com/watch?v=rqJ8SrnmWu0\" target=\"_blank\"\u003eYoutube: Introduction to Statistical Learning Theory\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003ca href=\"https://www.princeton.edu/~harman/Papers/SLT-tutorial.pdf\" target=\"_blank\"\u003eAn Overview of Statistical Learning Theory with examples\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003eSummary\u003c/h2\u003e\n\u003cp\u003eIn this lesson, you briefly looked at statistical learning theory and its main components. You looked at what a statistical model is and what the model parameters represent. You also got a feel for the differences between independent and dependent variables plus learned about loss and its role in model creation. You looked at all of this in the context of a simple model, a straight line. Next, you‘ll see the \"learning\" part of statistical learning theory by learning slope and intercept parameters of a straight line.\u003c/p\u003e","exportId":"statistical-learning-theory"},{"id":141581,"title":"Simple Linear Regression","type":"WikiPage","indent":0,"locked":false,"requirement":"must_mark_done","completed":true,"content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-simple-linear-regression\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-simple-linear-regression\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-simple-linear-regression/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eRegression analysis is often the first real learning application that aspiring data scientists will come across. It is one of the simplest techniques to master, but it still requires some mathematical and statistical understanding of the underlying process. This lesson will introduce you to the regression process based on the statistical ideas we have discovered so far.\u003c/p\u003e\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\u003cp\u003eYou will be able to:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003ePerform a linear regression using self-constructed functions\u003c/li\u003e\n\u003cli\u003eInterpret the parameters of a simple linear regression model in relation to what they signify for specific data\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003eLinear Regression\u003c/h2\u003e\n\u003cp\u003eRegression analysis is one of the most important statistical techniques for business applications. It’s a statistical methodology that helps estimate the strength and direction of the relationship between two (or more) variables. Regression results show whether the relationship is valid or not. It also helps to \u003cem\u003epredict\u003c/em\u003e an unknown value based on the derived relationship.\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eRegression Analysis is a \u003cstrong\u003eparametric\u003c/strong\u003e technique meaning a set of parameters are used to \u003cstrong\u003epredict\u003c/strong\u003e the value of an unknown target variable (or dependent variable) \u003cimg src=\"https://render.githubusercontent.com/render/math?math=y\"\u003e based on one or more of known input features (or independent variables, predictors), often denoted by \u003cimg src=\"https://render.githubusercontent.com/render/math?math=x\"\u003e .\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eLet's consider another example. Someone's height and foot size are generally considered to be related. Generally speaking, taller people tend to have bigger feet (and, obviously, shoe size).\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-simple-linear-regression/master/images/heightfoot1.png\" width=\"450\"\u003e\u003c/p\u003e\n\u003cp\u003eWe can use a linear regression analysis here to predict foot size (dependent variable), given height (independent variable) of an individual. Regression is proven to give credible results if the data follows some assumptions which will be covered in upcoming lessons in detail. In general, regression analysis helps us in the following ways:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eFinding an \u003cstrong\u003eassociation\u003c/strong\u003e or relationship between certain phenomena or variables\u003c/li\u003e\n\u003cli\u003eIdentifying \u003cstrong\u003ewhich variables contribute\u003c/strong\u003e more towards the outcomes\u003c/li\u003e\n\u003cli\u003e\n\u003cstrong\u003ePrediction\u003c/strong\u003e of future observations\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003eWhy \"linear\" regression?\u003c/h3\u003e\n\u003cp\u003eThe term \u003cstrong\u003elinear\u003c/strong\u003e implies that the model functions along with a straight (or nearly straight) line. \u003cstrong\u003eLinearity\u003c/strong\u003e, one of the assumptions of this approach, suggests that the relationship between dependent and independent variables can be expressed as a straight line.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eSimple Linear Regression\u003c/strong\u003e uses a single feature (one independent variable) to model a linear relationship with a target (the dependent variable) by fitting an optimal model (i.e. the best straight line) to describe this relationship.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eMultiple Linear Regression\u003c/strong\u003e uses more than one feature to predict a target variable by fitting the best linear relationship.\u003c/p\u003e\n\u003cp\u003eIn this section, we will mainly focus on simple regression to build a sound understanding. For the example shown above i.e. height vs foot size, a simple linear regression model would fit a line to the data points as follows:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-simple-linear-regression/master/images/heightfoot2.png\" width=\"450\"\u003e\u003c/p\u003e\n\u003cp\u003eThis line can then be used to describe the data and conduct further experiments using this fitted model. So let's move on and see how to calculate this \"best-fit line\" in a simple linear regression context.\u003c/p\u003e\n\u003ch2\u003eCalculating Regression Coefficients: Slope and Intercepts\u003c/h2\u003e\n\u003cp\u003eA straight line can be written as :\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=y=mx%2bc\"\u003e\u003c/p\u003e\n\u003cp\u003eor, alternatively\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=y%20=%20%20%5Cbeta_0%2b%20%5Cbeta_1%20x\"\u003e\u003c/p\u003e\n\u003cp\u003eYou may come across other ways of expressing this straight line equation for simple linear regression. Yet there are \u003cstrong\u003efour key components\u003c/strong\u003e you'll want to keep in mind:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-simple-linear-regression/master/images/linreg.png\" width=\"650\"\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eA \u003cstrong\u003edependent variable\u003c/strong\u003e that needs to estimated and predicted (here: \u003cimg src=\"https://render.githubusercontent.com/render/math?math=y\"\u003e )\u003c/li\u003e\n\u003cli\u003eAn \u003cstrong\u003eindependent variable\u003c/strong\u003e, the input variable (here: \u003cimg src=\"https://render.githubusercontent.com/render/math?math=x\"\u003e )\u003c/li\u003e\n\u003cli\u003eThe \u003cstrong\u003eslope\u003c/strong\u003e which determines the angle of the line. Here, the slope is denoted as \u003cimg src=\"https://render.githubusercontent.com/render/math?math=m\"\u003e , or \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cbeta_1\"\u003e .\u003c/li\u003e\n\u003cli\u003eThe \u003cstrong\u003eintercept\u003c/strong\u003e which is the constant determining the value of \u003cimg src=\"https://render.githubusercontent.com/render/math?math=y\"\u003e when \u003cimg src=\"https://render.githubusercontent.com/render/math?math=x\"\u003e is 0. We denoted the intercept here as \u003cimg src=\"https://render.githubusercontent.com/render/math?math=c\"\u003e or \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cbeta_0\"\u003e .\u003c/li\u003e\n\u003c/ul\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cem\u003eSlope\u003c/em\u003e and \u003cem\u003eIntercept\u003c/em\u003e are the \u003cstrong\u003ecoefficients\u003c/strong\u003e or the \u003cstrong\u003eparameters\u003c/strong\u003e of a linear regression model. Calculating the regression model simply involves the calculation of these two values.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003e\u003cstrong\u003eLinear regression is simply a manifestation of this simple equation!\u003c/strong\u003e So this is as complicated as our linear regression model gets. The equation here is the same one used to find a line in algebra, but in statistics, the actual data points don't necessarily lie on a line!\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eThe real challenge for regression analysis is to fit a line, out of an infinite number of lines that best describes the data.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eConsider the line below to see how we calculate slope and intercept.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-simple-linear-regression/master/images/linregall.png\" width=\"650\"\u003e\u003c/p\u003e\n\u003cp\u003eIn our example:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=c\"\u003e is equal to 15, which is where our line intersects with the y-axis.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=m\"\u003e is equal to 3, which is our slope.\u003c/p\u003e\n\u003cp\u003eYou can find a slope by taking an arbitrary part of the line, looking at the differences for the x-value and the y-value for that part of the line, and dividing \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5CDelta%20y\"\u003e by \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5CDelta%20x\"\u003e . In other words, you can look at the \u003cstrong\u003echange in y over the change in x\u003c/strong\u003e to find the slope!\u003c/p\u003e\n\u003ch3\u003eImportant note on notation\u003c/h3\u003e\n\u003cp\u003eNow that you know how the slope and intercept define the line, it's time for some more notation.\u003c/p\u003e\n\u003cp\u003eLooking at the above plots, you know that you have the green dots that are our observations associated with x- and y-values.\u003c/p\u003e\n\u003cp\u003eNow, when we draw our regression line based on these few green dots, we use the following notations:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Chat%7By%7D=%5Chat%20m%20x%2b%20%5Chat%7Bc%7D\"\u003e or \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Chat%20y%20=%20%20%5Chat%20%5Cbeta_0%2b%20%5Chat%20%5Cbeta_1%20x\"\u003e\u003c/p\u003e\n\u003cp\u003eAs you can see, you're using a \"hat\" notation which stands for the fact that we are working with \u003cstrong\u003eestimations\u003c/strong\u003e. - When trying to draw a \"best fit line\", you're \u003cstrong\u003eestimating\u003c/strong\u003e the most appropriate value possible for your intercept and your slope, hence \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Chat%7Bc%7D\"\u003e / \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Chat%20%5Cbeta_0\"\u003e and \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Chat%7Bm%7D\"\u003e / \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Chat%20%5Cbeta_1\"\u003e . - Next, when we use our line to predict new values \u003cimg src=\"https://render.githubusercontent.com/render/math?math=y\"\u003e given \u003cimg src=\"https://render.githubusercontent.com/render/math?math=x\"\u003e , your estimate is an \u003cstrong\u003eapproximation\u003c/strong\u003e based on our estimated parameter values. Hence we use \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Chat%20y\"\u003e instead of \u003cimg src=\"https://render.githubusercontent.com/render/math?math=y\"\u003e . \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Chat%20y\"\u003e lies \u003cem\u003eON\u003c/em\u003e your regression line, \u003cimg src=\"https://render.githubusercontent.com/render/math?math=y\"\u003e is the associated y-value for each of the green dots in the plot above. The \u003cstrong\u003eerror\u003c/strong\u003e or the \u003cstrong\u003evertical offset\u003c/strong\u003e between the line and the actual observation values is denoted by the red vertical lines in the plot above. Mathematically, the vertical offset can be written as \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cmid%20%5Chat%20y%20-%20y%5Cmid\"\u003e .\u003c/p\u003e\n\u003cp\u003eSo how do you find the line with the best fit? You may think that you have to try lots and lots of different lines to see which one fits best. Fortunately, this task is not as complicated as in may seem. Given some data points, the best-fit line always has a distinct slope and y-intercept that can be calculated using simple linear algebraic approaches. Let's quickly visit the required formulas.\u003c/p\u003e\n\u003ch3\u003eBest-Fit Line Ingredients\u003c/h3\u003e\n\u003cp\u003eBefore we calculate the best-fit line, we have to make sure that we have calculated the following measures for variables X and Y:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eThe mean of the X \u003cimg src=\"https://render.githubusercontent.com/render/math?math=(%5Cbar%7BX%7D)\"\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eThe mean of the Y \u003cimg src=\"https://render.githubusercontent.com/render/math?math=(%5Cbar%7BY%7D)\"\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eThe standard deviation of the X values \u003cimg src=\"https://render.githubusercontent.com/render/math?math=(S_X)\"\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eThe standard deviation of the y values \u003cimg src=\"https://render.githubusercontent.com/render/math?math=(S_Y)\"\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eThe correlation between X and Y ( often denoted by the Greek letter \"Rho\" or \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Crho\"\u003e - Pearson Correlation)\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003eCalculating Slope\u003c/h2\u003e\n\u003cp\u003eWith the above ingredients in hand, we can calculate the slope (shown as \u003cimg src=\"https://render.githubusercontent.com/render/math?math=b\"\u003e below) of the best-fit line, using the formula:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Chat%20m%20=%20%5Crho%20%5Cfrac%7BS_Y%7D%7BS_X%7D\"\u003e\u003c/p\u003e\n\u003cp\u003eThis formula is also known as the \u003cstrong\u003eleast-squares method\u003c/strong\u003e.\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://en.wikipedia.org/wiki/Simple_linear_regression#Fitting_the_regression_line\"\u003eYou can visit this Wikipedia link\u003c/a\u003e to get take a look into the math behind the derivation of this formula.\u003c/p\u003e\n\u003cp\u003eThe slope of the best-fit line can be a negative number following a negative correlation. For example, if an increase in police officers is related to a decrease in the number of crimes in a linear fashion, the correlation and hence the slope of the best-fitting line in this particular setting is negative.\u003c/p\u003e\n\u003ch2\u003eCalculating Intercept\u003c/h2\u003e\n\u003cp\u003eSo now that we have the slope value (\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Chat%20m\"\u003e), we can put it back into our formula \u003cimg src=\"https://render.githubusercontent.com/render/math?math=(%5Chat%20y%20=%20%5Chat%20m%20x%2b%20%5Chat%20c)\"\u003e to calculate intercept. The idea is that\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cbar%7BY%7D%20=%20%5Chat%20c%20%2b%20%5Chat%20m%20%5Cbar%7BX%7D\"\u003e \u003cbr\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Chat%20c%20=%20%5Cbar%7BY%7D%20-%20%5Chat%20m%5Cbar%7BX%7D\"\u003e\u003c/p\u003e\n\u003cp\u003eRecall that \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cbar%7BX%7D\"\u003e and \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cbar%7BY%7D\"\u003e are the mean values for variables X and Y. So, in order to calculate the \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Chat%20y\"\u003e -intercept of the best-fit line, we start by finding the slope of the best-fit line using the above formula. Then to find the \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Chat%20y\"\u003e -intercept, we multiply the slope value by the mean of x and subtract the result from the mean of y.\u003c/p\u003e\n\u003ch2\u003ePredicting from the model\u003c/h2\u003e\n\u003cp\u003eAs mentioned before, when you have a regression line with defined parameters for slope and intercept as calculated above, you can easily predict the \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Chat%7By%7D\"\u003e (target) value for a new \u003cimg src=\"https://render.githubusercontent.com/render/math?math=x\"\u003e (feature) value using the estimated parameter values:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Chat%7By%7D%20=%20%5Chat%20mx%20%2b%20%5Chat%20c\"\u003e\u003c/p\u003e\n\u003cp\u003eRemember that the difference between y and \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Chat%7By%7D\"\u003e is that \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Chat%7By%7D\"\u003e is the value predicted by the fitted model, whereas \u003cimg src=\"https://render.githubusercontent.com/render/math?math=y\"\u003e carries actual values of the variable (called the truth values) that were used to calculate the best fit.\u003c/p\u003e\n\u003cp\u003eNext, let's move on and try to code these equations to fit a regression line to a simple dataset to see all of this in action.\u003c/p\u003e\n\u003ch2\u003eAdditional Reading\u003c/h2\u003e\n\u003cp\u003eVisit the following series of blogs by Bernadette Low for details on topics covered in this lesson.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://towardsdatascience.com/super-simple-machine-learning-by-me-simple-linear-regression-part-1-concept-and-r-4b5b39bbdb5d\"\u003eSuper Simple Machine Learning — Simple Linear Regression Part 1\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://towardsdatascience.com/super-simple-machine-learning-simple-linear-regression-part-2-math-and-python-1137acb4c352\"\u003eSuper Simple Machine Learning — Simple Linear Regression Part 2\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003eSummary\u003c/h2\u003e\n\u003cp\u003eIn this lesson, you learned the basics of a simple linear regression. Specifically, you learned some details about performing the actual technique and got some practice interpreting regression parameters. Finally, you saw how the parameters can be used to make predictions!\u003c/p\u003e","exportId":"simple-linear-regression"},{"id":141584,"title":"Simple Linear Regression - Lab","type":"Assignment","indent":1,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_mark_done","completed":true,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-simple-linear-regression-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-simple-linear-regression-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","exportId":"gb3589801b82b91898ef6a347827897ed"},{"id":141588,"title":"Coefficient of Determination","type":"WikiPage","indent":0,"locked":false,"requirement":"must_mark_done","completed":true,"content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-coefficient-of-determination\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-coefficient-of-determination\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-coefficient-of-determination/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eFor linear regression analysis, as we saw earlier, the straight line does not \u003cstrong\u003efully\u003c/strong\u003e describe the relationship between variables and there is always some error. In general, you'll want to determine a \"goodness of fit\"-measure of the fitted line. In this lesson, you'll learn about the \"R-Squared\"( \u003cimg src=\"https://render.githubusercontent.com/render/math?math=R%5E2\"\u003e ) measure, also known as the Coefficient of Determination.\u003c/p\u003e\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\u003cp\u003eYou will be able to:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eCalculate the coefficient of determination using self-constructed functions\u003c/li\u003e\n\u003cli\u003eUse the coefficient of determination to determine model performance\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003eR-Squared\u003c/h2\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003eThe \u003cimg src=\"https://render.githubusercontent.com/render/math?math=R%5E2\"\u003e or Coefficient of determination is a statistical measure that is used to assess the goodness of fit of a regression model\u003c/strong\u003e\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eHere is how it works.\u003c/p\u003e\n\u003cp\u003eR-Squared uses a so-called \"baseline\" model which is a very simple, naive model. This baseline model does not make use of any independent variables to predict the value of dependent variable Y. Instead, it uses the \u003cstrong\u003emean\u003c/strong\u003e of the observed responses of the dependent variable \u003cimg src=\"https://render.githubusercontent.com/render/math?math=y\"\u003e and always predicts this mean as the value of \u003cimg src=\"https://render.githubusercontent.com/render/math?math=y\"\u003e for any value of \u003cimg src=\"https://render.githubusercontent.com/render/math?math=x\"\u003e . In the image below, this model is given by the straight orange line.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-coefficient-of-determination/master/images/linreg_rsq.png\" width=\"600\"\u003e\u003c/p\u003e\n\u003cp\u003eYou can see that, in this plot, the baseline model always predicts the mean of \u003cimg src=\"https://render.githubusercontent.com/render/math?math=y\"\u003e \u003cstrong\u003eirrespective\u003c/strong\u003e of the value of the \u003cimg src=\"https://render.githubusercontent.com/render/math?math=x\"\u003e . The gray line, however, is our fitted regression line which makes use of \u003cimg src=\"https://render.githubusercontent.com/render/math?math=x\"\u003e values to predict the values of \u003cimg src=\"https://render.githubusercontent.com/render/math?math=y\"\u003e . Looking at the plot above, R-Squared simply asks the question:\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003eIs our fitted regression line better than our baseline model?\u003c/strong\u003e\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eAny regression model that we fit is compared to this baseline model to understand its \u003cstrong\u003egoodness of fit\u003c/strong\u003e. Simply put, R-Squared just explains how good your model is when compared to the baseline model. That's about it.\u003c/p\u003e\n\u003ch3\u003eGreat, so how do you calculate R-Squared?\u003c/h3\u003e\n\u003cp\u003eThe mathematical formula to calculate R-Squared for a linear regression line is in terms of \u003cstrong\u003esquared errors\u003c/strong\u003e for the fitted model and the baseline model. It's calculated as :\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Clarge%20R%5E2%20=%201-%20%5Cdfrac%7BSS_%7BRES%7D%7D%7BSS_%7BTOT%7D%7D%20=%201%20-%20%5Cdfrac%7B%5Csum_i(y_i%20-%20%5Chat%20y_i)%5E2%7D%7B%5Csum_i(y_i%20-%20%5Coverline%20y_i)%5E2%7D\"\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=SS_%7BRES%7D\"\u003e (also called RSS) is the \u003cstrong\u003eResidual\u003c/strong\u003e sum of squared errors of our regression model, also known as \u003cimg src=\"https://render.githubusercontent.com/render/math?math=SSE\"\u003e (Sum of Squared Errors). \u003cimg src=\"https://render.githubusercontent.com/render/math?math=SS_%7BRES%7D\"\u003e is the squared difference between \u003cimg src=\"https://render.githubusercontent.com/render/math?math=y\"\u003e and \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Chat%20y\"\u003e . For the one highlighted observation in our graph above, the \u003cimg src=\"https://render.githubusercontent.com/render/math?math=SS_%7BRES%7D\"\u003e is denoted by the red arrow. This part of the error is not explained by our model.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=SS_%7BTOT%7D\"\u003e (also called TSS) is the \u003cstrong\u003eTotal\u003c/strong\u003e sum of squared error. \u003cimg src=\"https://render.githubusercontent.com/render/math?math=SS_%7BTOT%7D\"\u003e is the squared difference between \u003cimg src=\"https://render.githubusercontent.com/render/math?math=y\"\u003e and \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Coverline%20y\"\u003e . For the one highlighted observation in our graph above, the \u003cimg src=\"https://render.githubusercontent.com/render/math?math=SS_%7BTOT%7D\"\u003e is denoted by the orange arrow.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eLooking at this, you'll understand that you can interpret R-Squared as \"1 - the proportion of the variance \u003cem\u003enot\u003c/em\u003e explained by the model\", which means as much as \"the variation explained by the model\". As a result, you'll want to maximize the R-Squared.\u003c/p\u003e\n\u003cp\u003eFor completion,\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cimg src=\"https://render.githubusercontent.com/render/math?math=SS_%7BEXP%7D\"\u003e (also called ESS) is the \u003cstrong\u003eExplained\u003c/strong\u003e sum of squared error. \u003cimg src=\"https://render.githubusercontent.com/render/math?math=SS_%7BEXP%7D\"\u003e is the squared difference between \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Chat%20y\"\u003e and \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Coverline%20y\"\u003e . For the one highlighted observation in our graph above, the \u003cimg src=\"https://render.githubusercontent.com/render/math?math=SS_%7BEXP%7D\"\u003e is denoted by the gray arrow.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003eLet's interpret the outcome of \u003cimg src=\"https://render.githubusercontent.com/render/math?math=R%5E2\"\u003e\n\u003c/h3\u003e\n\u003cp\u003eOur worst possible regression model could be the baseline model itself. In that case, the RSS is equal to TSS looking at the graph above. Then your R_squared value is 0. Look at the plot below, where you notice that \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Chat%20y\"\u003e is simply a straight line.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-coefficient-of-determination/master/images/rs5.png\" width=\"500\"\u003e\u003c/p\u003e\n\u003cp\u003eDue to this particular relationship between \u003cimg src=\"https://render.githubusercontent.com/render/math?math=x\"\u003e and \u003cimg src=\"https://render.githubusercontent.com/render/math?math=y\"\u003e , the regression line \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Chat%20y\"\u003e is the same as the mean line for \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Coverline%20y\"\u003e . The R-Squared for this model is 0. It's clear that a straight line is probably not the right fit for this data.\u003c/p\u003e\n\u003cp\u003eOn the other extreme, the best model could also be one that fits all the data points perfectly. Because the unexplained part of the variation is 0, R-Squared is 1–0, so 1 in this case, which indicates a perfect model. Below is an example of this (know that this will rarely happen with real world data).\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-coefficient-of-determination/master/images/rs6.png\" width=\"500\"\u003e\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003eR-Squared can take a value between 0 and 1 where values closer to 0 represent a poor fit and values closer to 1 represent an (almost) perfect fit\u003c/strong\u003e\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch3\u003ePhrasing R-Squared values\u003c/h3\u003e\n\u003cp\u003eAn R-squared value of say 0.85 can be described conceptually as:\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003e\u003cem\u003e85% of the variations in dependent variable \u003cimg src=\"https://render.githubusercontent.com/render/math?math=y\"\u003e are explained by the independent variable in our model.\u003c/em\u003e\u003c/strong\u003e\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch2\u003eSummary\u003c/h2\u003e\n\u003cp\u003eIn this lesson, you looked at the R-Squared, or the Coefficient of Determination to evaluate the goodness of fit for a regression line. You saw how R-Squared is calculated by comparing a given model to a baseline model and learned that it must be a value between 0 and 1. In the next lab, you'll move on to calculating R-Squared in Python.\u003c/p\u003e","exportId":"coefficient-of-determination"},{"id":141593,"title":"Coefficient of Determination - Lab","type":"Assignment","indent":1,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_mark_done","completed":true,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-coefficient-of-determination-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-coefficient-of-determination-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","exportId":"g73ff9c9089ebfa445f0c575d8bc968df"},{"id":141598,"title":"Regression - Lab","type":"Assignment","indent":1,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_mark_done","completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-complete-regression-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-complete-regression-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","exportId":"g9a20f43ba3b9ff0723e1dad01d996ec8"},{"id":141601,"title":"Assumptions for Linear Regression","type":"WikiPage","indent":0,"locked":false,"requirement":"must_mark_done","completed":true,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-regression-assumptions\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-regression-assumptions/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eLeast Squares is one of the most common regression techniques for linear models. As long as our model satisfies the least squares regression assumptions, we can get the best possible estimates. In this lesson, you will learn about these assumptions.\u003c/p\u003e\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\u003cp\u003eYou will be able to:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eList the assumptions of linear regression\u003c/li\u003e\n\u003cli\u003eDetermine if a particular set of data exhibits the assumptions of linear regression\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003eAbout Regression Assumptions\u003c/h2\u003e\n\u003cp\u003eRegression is a powerful analysis technique that is routinely used to answer complex analytical questions. However, if some of the necessary assumptions are not satisfied, you may not be able to get good and trustworthy results!\u003c/p\u003e\n\u003cp\u003eIn this lesson, you'll dig deeper into the topic of ordinary least squares (OLS) regression assumptions. Additionally, you'll learn about their importance as well as some techniques to help us determine whether your model satisfies the assumptions.\u003c/p\u003e\n\u003ch2\u003eRegression is \"Parametric\"\u003c/h2\u003e\n\u003cp\u003eRegression is a parametric technique, which means that it uses parameters learned from the data. Because of that, certain assumptions must be made. These assumptions define the complete scope of regression analysis and it is \u003cstrong\u003emandatory\u003c/strong\u003e that the underlying data fulfills these assumptions. If violated, regression makes biased and unreliable predictions. Luckily, we have measures to check for these assumptions.\u003c/p\u003e\n\u003ch2\u003e1. Linearity\u003c/h2\u003e\n\u003cblockquote\u003e\n\u003cp\u003eThe linearity assumptions requires that there is a \u003cstrong\u003elinear relationship\u003c/strong\u003e between the response variable (Y) and predictor (X). Linear means that the change in Y by 1-unit change in X, is constant.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-regression-assumptions/master/images/lin_2.png\" width=\"800\"\u003e\u003c/p\u003e\n\u003cp\u003eAs shown above, If we try to fit a linear model to a non-linear data set, OLS will fail to capture the trend mathematically, resulting in an inaccurate relationship. This will also result in erroneous predictions on an unseen data set.\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eThe linearity assumption can best be tested with scatter plots\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eFor non-linear relationships, you can use non-linear mathematical functions to fit the data e.g. polynomial and exponential functions. You'll come across these later.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eNote: As an extra measure, it is also important to check for outliers as the presence of outliers in the data can have a major impact on the model.\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-regression-assumptions/master/images/outliers.png\" width=\"600\"\u003e\u003c/p\u003e\n\u003cp\u003eIn the above example, we can see that an outlier prohibits the model to estimate the true relationship between variables by introducing bias.\u003c/p\u003e\n\u003ch2\u003e2. Normality\u003c/h2\u003e\n\u003cblockquote\u003e\n\u003cp\u003eThe normality assumption states that the \u003cstrong\u003emodel residuals\u003c/strong\u003e should follow a normal distribution\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eNote that the normality assumption talks about the \u003cstrong\u003emodel residuals\u003c/strong\u003e and \u003cem\u003enot\u003c/em\u003e about the distributions of the \u003cstrong\u003evariables\u003c/strong\u003e! In general, data scientists will often check the distributions of the variables as well. Keep in mind that the normality assumption is mandatory for the residuals, and it is useful to check normality of your variables to check for weirdness (more on data distributions later), but OLS works fine for non-normal data distributions in the context of prediction.\u003c/p\u003e\n\u003cp\u003eThe easiest way to check for the normality assumption is with histograms or a Q-Q-Plots.\u003c/p\u003e\n\u003ch3\u003eHistograms\u003c/h3\u003e\n\u003cp\u003eWe have already seen quite a few histograms and also know how to build them. You can use histograms to check the errors generated by the model and see if the plot shows a so-called \"normal distribution\" (bell curve shape). As the error term follows a normal distribution, we can develop better confidence in the results and calculate the statistical significance. An example of a regression error histogram is shown below:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-regression-assumptions/master/images/inhouse_histo.png\" width=\"800\"\u003e\u003c/p\u003e\n\u003ch3\u003eQ-Q Plots\u003c/h3\u003e\n\u003cblockquote\u003e\n\u003cp\u003eIn statistics, a Q–Q (quantile-quantile) plot is a probability plot, which is a graphical method for comparing two probability distributions by plotting their quantiles against each other.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eThe Q-Q plot (quantile-quantile plot) is used to help assess if a sample comes from a known distribution such as a normal distribution. For regression, when checking if the data in this sample is normally distributed, we can use a Normal Q-Q plot to test that assumption. Remember that this is just a visual check, so the interpretation remains subjective. However, it is a good first check to see the overall shape of your data against the required distribution. If you can reject normality through Q-Q plots, you have saved yourself from a lot of statistical testing. You have to be careful, however, when deciding that data is totally normal just by looking at a Q-Q plot.\u003c/p\u003e\n\u003cp\u003eBelow, you can find a few examples of comparing histograms and corresponding plots. You can see how the quantiles of normal data appear as a straight line along the diagonal when plotted against a standard normal distribution's quantiles. The skewness and kurtosis of data can also be inspected this way\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-regression-assumptions/master/images/inhouse_qq_plots.png\" width=\"600\"\u003e\u003c/p\u003e\n\u003cp\u003eIn the context of normality of residuals, Q-Q plots can help you validate the assumption of normally distributed residuals. It uses standardized values of residuals to determine the normal distribution of errors. Ideally, this plot should show a straight line. A curved, distorted line suggests residuals have a non-normal distribution.\u003ca href=\"https://data.library.virginia.edu/understanding-q-q-plots/\"\u003eHere is a good article\u003c/a\u003e explaining the interpretation of Q-Q plots in detail.\u003c/p\u003e\n\u003cp\u003eNormality can also be checked with goodness of fit tests such as the Kolmogorov-Smirnov test. When the data is not normally distributed, there are some ways to fix that, such as a non-linear transformation (e.g., log-transformation).\u003c/p\u003e\n\u003ch2\u003e3. Homoscedasticity\u003c/h2\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cem\u003eHeteroscedasticity\u003c/em\u003e (also spelled heteroskedasticity) refers to the circumstance in which the dependent variable is unequal across the range of values of the predictor(s).\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eWhen there is heteroscedasticity in the data, a scatterplot of these variables will often create a cone-like shape. The scatter of the dependent variable widens or narrows as the value of the independent variable increases.\u003c/p\u003e\n\u003cp\u003eThe inverse of heteroscedasticity is \u003cem\u003ehomoscedasticity\u003c/em\u003e, which indicates that a dependent variable's variability is equal across values of the independent variable. \u003cstrong\u003eHomoscedasticity is the third assumption necessary when creating a linear regression model.\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-regression-assumptions/master/images/homo_2.png\" width=\"700\"\u003e\u003c/p\u003e\n\u003cp\u003eA scatter plot is good way to check whether the data are homoscedastic (meaning the residuals are equal across the regression line). The scatter plots shown here are examples of data that are heteroscedastic (except the plot far right). You can also use significance tests like Breusch-Pagan / Cook-Weisberg test or White general test to detect this phenomenon. You will learn about p-values later, but for now, you can remember that, if these tests give you a p-value \u0026lt; 0.05, the null hypothesis can rejected, and you can assume the data is heteroscedastic.\u003c/p\u003e\n\u003ch2\u003eWhat Else?\u003c/h2\u003e\n\u003cp\u003eThere are other assumptions for linear regression that apply to more complicated cases, but for now these three assumptions are sufficient.\u003c/p\u003e\n\u003cp\u003eAs a first check, always looks at plots of the residuals. If you see anything similar to what is shown below, you are violating one or more assumptions and the results will not be reliable.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-regression-assumptions/master/images/prob_2.png\" width=\"700\"\u003e\u003c/p\u003e\n\u003ch2\u003eSummary\u003c/h2\u003e\n\u003cp\u003eIn this lesson, you learned about some assumptions for a simple linear regression that must be held in order to interpret the results reliably. As mentioned earlier, once these assumptions are confirmed, you can run your regression model. Next, you'll be exposed to some examples!\u003c/p\u003e\n\u003cfooter class=\"fis-footer\" style=\"visibility: hidden;\"\u003e\n\u003cdiv class=\"fis-feedback\"\u003e\n\u003ch5\u003eHow do you feel about this lesson?\u003c/h5\u003e\n\u003cimg id=\"thumbs-up\" title=\"Thumbs up!\" alt=\"thumbs up\" data-repository=\"dsc-regression-assumptions\"\u003e\u003cimg id=\"thumbs-down\" title=\"Thumbs down!\" alt=\"thumbs down\" data-repository=\"dsc-regression-assumptions\"\u003e\n\u003c/div\u003e\n\u003ch5\u003eHave specific feedback? \u003ca href=\"https://github.com/learn-co-curriculum/dsc-regression-assumptions/issues/new\"\u003eTell us here!\u003c/a\u003e\n\u003c/h5\u003e\n\u003c/footer\u003e","exportId":"assumptions-for-linear-regression"},{"id":141606,"title":"Ordinary Least Squares in Statsmodels (OLS)","type":"Assignment","indent":0,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_mark_done","completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-ols-statsmodels\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-ols-statsmodels/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","exportId":"g3774a4758e51b40c8feea81adcc91186"},{"id":141611,"title":"Ordinary Least Squares in Statsmodels (OLS) - Lab","type":"Assignment","indent":1,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_mark_done","completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-ols-statsmodels-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-ols-statsmodels-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","exportId":"g028613cbe5404c155b858f6515fff60f"},{"id":141614,"title":"Regression Diagnostics in Statsmodels","type":"Assignment","indent":0,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_mark_done","completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-ols-regression-diagnostics\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-ols-regression-diagnostics/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","exportId":"geae254d203d24cfab6b3671af2eabc10"},{"id":141619,"title":"Interpreting Significance and P-value","type":"Assignment","indent":0,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_mark_done","completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-significance-p-value\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-significance-p-value/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","exportId":"ga10f4e8c856a1c594cfff0c208b01c0d"},{"id":141622,"title":"Project - Regression Modeling with the Ames Housing Dataset","type":"Assignment","indent":0,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_mark_done","completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-regression-boston-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-regression-boston-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","exportId":"g61b7bfd28c078fbc8422990ab3741476"},{"id":141632,"title":"Introduction to Linear Regression - Recap","type":"WikiPage","indent":0,"locked":false,"requirement":"must_mark_done","completed":false,"content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-linear-regression-section-recap-v2-1\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-linear-regression-section-recap-v2-1\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-linear-regression-section-recap-v2-1/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eThis short lesson summarizes the topics we covered in this section and why they'll be important to you as a data scientist.\u003c/p\u003e\n\u003ch2\u003eKey Takeaways\u003c/h2\u003e\n\u003cp\u003eIn this section, the nominal focus was on how to perform a linear regression, but the real value was learning how to think about the application of machine learning models to data sets.\u003c/p\u003e\n\u003cp\u003eKey takeaways include:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eStatistical learning theory deals with the problem of finding a predictive function based on data\u003c/li\u003e\n\u003cli\u003eA loss function calculates how well a given model represents the relationship between data values\u003c/li\u003e\n\u003cli\u003eA linear regression is simply a (straight) line of best fit for predicting a continuous value (y = mx + c)\u003c/li\u003e\n\u003cli\u003eThe Coefficient of Determination (R Squared) can be used to determine how well a given line fits a given data set\u003c/li\u003e\n\u003cli\u003eCertain assumptions must hold true for a least squares linear regression to be useful - linearity, normality and heteroscedasticity\u003c/li\u003e\n\u003cli\u003eQ-Q plots can check for normality in residual errors\u003c/li\u003e\n\u003cli\u003eThe Jarque-Bera test can be used to test for normality - especially when the number of data points is large\u003c/li\u003e\n\u003cli\u003eThe Goldfeld-Quant test can be used to check for homoscedasticity\u003c/li\u003e\n\u003c/ul\u003e","exportId":"introduction-to-linear-regression-recap"}]},{"id":15108,"name":"Topic 19: Multiple Regression and Model Validation","status":"unlocked","unlockDate":null,"prereqs":[],"requirement":"all","sequential":false,"exportId":"ge39623b3d7e2f5fea5c53b6b5954d51e","items":[{"id":141642,"title":"Multiple Regression and Model Validation - Introduction","type":"WikiPage","indent":0,"locked":false,"requirement":"must_mark_done","completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-regression-introduction\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-regression-introduction/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eIn this section, you'll start to see more advanced regression models which employ multiple predictors. With that, you'll also start to investigate how to validate your models to ensure they are neither overfit nor underfit, and will generalize well to future cases.\u003c/p\u003e\n\u003ch2\u003eMultiple Linear Regression\u003c/h2\u003e\n\u003cp\u003eIn the last section, you learned how to perform a basic linear regression with a single predictor variable. Here, you'll explore how to perform linear regressions using \u003cstrong\u003emultiple\u003c/strong\u003e independent variables to better predict a target variable.\u003c/p\u003e\n\u003ch2\u003eImproving a Baseline Model\u003c/h2\u003e\n\u003cp\u003eOver the next few lessons, you'll see some ways to improve basic regression models using different combinations of features as well as some feature engineering. Often, a simple linear regression can be used as a \"baseline model\" upon which new features can be added to improve the predictions. Any decisions on how to change features should be compared against a simpler model to see if the changes have improved the model or not. This section will give an introduction to many of the techniques which can improve a regression model.\u003c/p\u003e\n\u003ch2\u003eDealing with Categorical Variables\u003c/h2\u003e\n\u003cp\u003eUp to this point you've only seen continuous predictor variables. Here, you'll get a further look at how to identify and then transform categorical variables to utilize them as predictors of our target variable.\u003c/p\u003e\n\u003ch2\u003eMulticollinearity of Features\u003c/h2\u003e\n\u003cp\u003eWhile multiple predictors will ultimately increase model performance and yield better predictions, there are also possible negative effects when using multiple predictors that have a high correlation with each other. This is known as multicollinearity and can muddy model interpretation.\u003c/p\u003e\n\u003ch2\u003eFeature Scaling and Normalization\u003c/h2\u003e\n\u003cp\u003eAnother consideration when using multiple predictors in any model is the scale of those features. For example, a dataset surrounding households might have a \"number of children\" feature and an \"income\" feature. These two variables are of vastly different scales and as such, simply having a feature like income which is on a much larger scale can impact its influence over the model. To avoid this, it is best practice to normalize the scale of all features before feeding the data to a machine learning algorithm.\u003c/p\u003e\n\u003ch2\u003eMultiple Linear Regression in Statsmodels\u003c/h2\u003e\n\u003cp\u003eAfter covering a lot of the key theory, you'll then get some hands-on practice in performing multiple linear regressions using the Statsmodels and Scikit-Learn libraries.\u003c/p\u003e\n\u003ch2\u003eModel Fit and Validation\u003c/h2\u003e\n\u003cp\u003eYou'll continue the section by looking at how we can analyze the results of a regression, and learn the importance of splitting data into training and test sets to determine how well a model predicts \"unknown\" values (the test dataset). Finally, you'll wrap up the section by looking at how k-fold cross-validation can be used to get additional model validations on a limited data set by taking multiple splits of training and testing data.\u003c/p\u003e\n\u003ch2\u003eSummary\u003c/h2\u003e\n\u003cp\u003eIn this section, you'll continue to bolster your understanding of linear regression so that you can solve a wider range of problems more accurately. Many of the principles covered in this section will also apply in later modules as you move onto working with other machine learning models.\u003c/p\u003e\n\u003cfooter class=\"fis-footer\" style=\"visibility: hidden;\"\u003e\n\u003cdiv class=\"fis-feedback\"\u003e\n\u003ch5\u003eHow do you feel about this lesson?\u003c/h5\u003e\n\u003cimg id=\"thumbs-up\" title=\"Thumbs up!\" alt=\"thumbs up\" data-repository=\"dsc-regression-introduction\"\u003e\u003cimg id=\"thumbs-down\" title=\"Thumbs down!\" alt=\"thumbs down\" data-repository=\"dsc-regression-introduction\"\u003e\n\u003c/div\u003e\n\u003ch5\u003eHave specific feedback? \u003ca href=\"https://github.com/learn-co-curriculum/dsc-regression-introduction/issues/new\"\u003eTell us here!\u003c/a\u003e\n\u003c/h5\u003e\n\u003c/footer\u003e","exportId":"multiple-regression-and-model-validation-introduction"},{"id":141645,"title":"Multiple Linear Regression","type":"Assignment","indent":0,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_mark_done","completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-multiple-linear-regression\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-multiple-linear-regression/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","exportId":"g2f97b5f6108903e2f1a7fe06a5926237"},{"id":141650,"title":"Dealing with Categorical Variables","type":"Assignment","indent":0,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_mark_done","completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-dealing-with-categorical-variables\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-dealing-with-categorical-variables/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","exportId":"g874a611b267790693a85b226c8808c22"},{"id":141652,"title":"Dealing with Categorical Variables - Lab","type":"Assignment","indent":1,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_mark_done","completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-dealing-with-categorical-variables-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-dealing-with-categorical-variables-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","exportId":"g665e30e6b9f11ba8729090e01e51d009"},{"id":141656,"title":"Multicollinearity of Features","type":"Assignment","indent":0,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_mark_done","completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-multicollinearity-of-features\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-multicollinearity-of-features/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","exportId":"ge40df0c9a4b0b6948941d3698f093313"},{"id":141659,"title":"Multicollinearity of Features - Lab","type":"Assignment","indent":1,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_mark_done","completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-multicollinearity-of-features-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-multicollinearity-of-features-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","exportId":"g71a9102709a1463e24984220a70d1669"},{"id":141663,"title":"Log Transformations","type":"Assignment","indent":0,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_mark_done","completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-log-transformations\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-log-transformations/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","exportId":"g4c55235a2e51179b89131552c3d353f2"},{"id":141666,"title":"Feature Scaling and Normalization","type":"Assignment","indent":0,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_mark_done","completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-feature-scaling-and-normalization\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-feature-scaling-and-normalization/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","exportId":"ga35eeb5c62e241ff99a9f842c2ea91e3"},{"id":141670,"title":"Feature Scaling and Normalization - Lab","type":"Assignment","indent":1,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_mark_done","completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-feature-scaling-and-normalization-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-feature-scaling-and-normalization-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","exportId":"g32c3a1cee302f611d0acce7ed2939261"},{"id":141674,"title":"Multiple Linear Regression in Statsmodels","type":"Assignment","indent":0,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_mark_done","completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-multiple-linear-regression-in-statsmodels\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-multiple-linear-regression-in-statsmodels/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","exportId":"g8286958c43488707cfd86679298362b1"},{"id":141677,"title":"Multiple Linear Regression in Statsmodels - Lab","type":"Assignment","indent":1,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_mark_done","completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-multiple-linear-regression-statsmodels-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-multiple-linear-regression-statsmodels-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","exportId":"gd4eeb2eb95af559b48aee09488984386"},{"id":141680,"title":"Inference versus Prediction","type":"WikiPage","indent":0,"locked":false,"requirement":"must_mark_done","completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-inference-vs-prediction-v2-1\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-inference-vs-prediction-v2-1/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eIn the last few lessons, you have seen how to deal with categorical variables and why multicollinearity can be an issue with regression analysis. You also learned about log transformations, feature scaling, and normalization in order to accurately determine the coefficients of your features and to improve the model accuracy. Before we proceed further, it is important to discuss two different modeling approaches that you should keep in mind when working with data: modeling for \u003cem\u003einference\u003c/em\u003e and modeling for \u003cem\u003eprediction\u003c/em\u003e. Are you asking yourself \"aren't they the same thing\"? Well, no! In this lesson you will see why and how.\u003c/p\u003e\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eExplain the difference between modeling for inference and prediction\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003eInference\u003c/h2\u003e\n\u003cp\u003eWhen you are modeling for \u003cem\u003einference\u003c/em\u003e, you are asking the question \"How does X (independent variables or features) affect Y (dependent or target or outcome variable)?\". So, in essence, you are trying to figure out which features affect your outcome \u003cstrong\u003eand\u003c/strong\u003e how your outcome changes when these features change.\u003c/p\u003e\n\u003cp\u003eWhen modeling for \u003cem\u003einference\u003c/em\u003e, you are typically focused on only a subset of features because you are trying to understand how the outcome changes when you vary these features. As a result, great emphasis is given to the coefficients of these features as opposed to the overall accuracy of the model.\u003c/p\u003e\n\u003cp\u003eHence, when you are modeling for inference, you typically choose \u003cem\u003esimpler\u003c/em\u003e models, that is, models that are interpretable. Linear regression is a very good example of a model that is interpretable. With some basic training, anyone can understand how the features affect the outcome by observing the coefficients of these features. Some other interpretable models that you will learn later are logistic regression, decision trees, linear SVMs etc.\u003c/p\u003e\n\u003ch2\u003ePrediction\u003c/h2\u003e\n\u003cp\u003eWhen you are modeling for \u003cem\u003eprediction\u003c/em\u003e, you are asking the question \"How well can I use X (independent variables or features) to predict Y (dependent or target or outcome variable)?\" Thus, in this case, you are less concerned about how and which features impact Y as opposed to how you can efficiently use them to predict Y.\u003c/p\u003e\n\u003cp\u003eWhen modeling for \u003cem\u003eprediction\u003c/em\u003e, you typically use all available features (and most likely engineer new features) because you are trying to accurately predict Y, at all costs. As a result, you are less concerned about the coefficients of these features and instead focus on the overall accuracy of the model.\u003c/p\u003e\n\u003cp\u003eHence, when you are modeling for prediction, you typically choose more \u003cem\u003ecomplex\u003c/em\u003e models. In the upcoming modules, as you learn about various Machine Learning models, you will notice that your sole focus is on improving the predictive accuracy of your models. That is, given some data, your job will be to build a model that best predicts the future (your target variable). This can often mean you will be dealing with \u003cem\u003eblack box\u003c/em\u003e models -- models that are difficult to interpret. Given the independent variables, these models can do a great job of predicting the target, but its inner workings will be very very difficult (almost impossible) to understand. These models can include SVMs with radial kernels, random forests, neural networks, and other techniques such regularization, cross-validation, grid search etc.\u003c/p\u003e\n\u003ch2\u003eAdditional Resources\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca class=\"\" href=\"https://www.youtube.com/watch?v=5mcCOdyr5w4\"\u003eInference vs Prediction - Roger Peng\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://www.datascienceblog.net/post/commentary/inference-vs-prediction/\"\u003eInference vs Prediction - R for Data Science Blog\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003eSummary\u003c/h2\u003e\n\u003cp\u003eRemember that how you build your models depends on what question you are asking of your data:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eAre you solely interested in how you can use the data to predict the future? If so, you are most likely \u003cem\u003emodeling for prediction\u003c/em\u003e\n\u003c/li\u003e\n\u003cli\u003eOr, are you interested in understanding how a given set of features affect your outcome? If so, you are most likely \u003cem\u003emodeling for inference\u003c/em\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eDepending on what questions you ask, your modeling approaches will vary significatly, and hence it is very important to first understand the context of your problem and ask yourself what is the end goal of your analysis before you set out building any models.\u003c/p\u003e\n\u003cfooter class=\"fis-footer\" style=\"visibility: hidden;\"\u003e\n\u003cdiv class=\"fis-feedback\"\u003e\n\u003ch5\u003eHow do you feel about this lesson?\u003c/h5\u003e\n\u003cimg id=\"thumbs-up\" title=\"Thumbs up!\" alt=\"thumbs up\" data-repository=\"dsc-inference-vs-prediction-v2-1\"\u003e\u003cimg id=\"thumbs-down\" title=\"Thumbs down!\" alt=\"thumbs down\" data-repository=\"dsc-inference-vs-prediction-v2-1\"\u003e\n\u003c/div\u003e\n\u003ch5\u003eHave specific feedback? \u003ca href=\"https://github.com/learn-co-curriculum/dsc-inference-vs-prediction-v2-1/issues/new\"\u003eTell us here!\u003c/a\u003e\n\u003c/h5\u003e\n\u003c/footer\u003e","exportId":"inference-versus-prediction"},{"id":141683,"title":"Model Fit in Linear Regression","type":"Assignment","indent":0,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_mark_done","completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-model-fit-linear-regression\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-model-fit-linear-regression/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","exportId":"g7e46b1486aeff9c07785f90aa4d92e16"},{"id":141687,"title":"Model Fit in Linear Regression - Lab","type":"Assignment","indent":1,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_mark_done","completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-model-fit-linear-regression-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-model-fit-linear-regression-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","exportId":"g5a1d9ef3651d544423ce6621ba391da1"},{"id":141689,"title":"Regression Model Validation","type":"Assignment","indent":0,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_mark_done","completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-regression-model-validation\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-regression-model-validation/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","exportId":"gcb0d6bd7ca78851d6617cf7e0f9335ed"},{"id":141693,"title":"Regression Model Validation - Lab","type":"Assignment","indent":1,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_mark_done","completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-regression-model-validation-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-regression-model-validation-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","exportId":"gf8f65d322983cab458b839481a43381e"},{"id":141698,"title":"Introduction to Cross Validation","type":"Assignment","indent":0,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_mark_done","completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-cross-validation\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-cross-validation/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","exportId":"g29f11839e9d9e36b6bfa67ca158f1526"},{"id":141701,"title":"Introduction to Cross Validation - Lab","type":"Assignment","indent":1,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_mark_done","completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-cross-validation-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-cross-validation-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","exportId":"g84feb128ef3de817727980976b3b8d85"},{"id":141705,"title":"Pickle","type":"Assignment","indent":0,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_mark_done","completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-pickle\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-pickle/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","exportId":"g0617762981782b8a7e4f723eb234d619"},{"id":141709,"title":"Data Science Processes","type":"WikiPage","indent":0,"locked":false,"requirement":"must_mark_done","completed":false,"content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-data-science-processes\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-data-science-processes\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-data-science-processes/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eAs discussed, this section is all about synthesizing your skills in order to work through a full Data Science workflow. In this lesson, you'll take a look at some general outlines for how Data Scientists organize their workflow and conceptualize their process.\u003c/p\u003e\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\u003cp\u003eYou will be able to:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eList the different data science process frameworks\u003c/li\u003e\n\u003cli\u003eCompare and contrast popular data science process frameworks such as CRISP-DM, KDD, OSEMN\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003eWhat is a Data Science Process?\u003c/h2\u003e\n\u003cp\u003eData Science projects are often complex, with many stakeholders, data sources, and goals. Due to this, the Data Science community has created several methodologies for helping organize and structure Data Science Projects. In this lesson, you'll explore three of the most popular methodologies -- \u003cstrong\u003e\u003cem\u003eCRISP-DM\u003c/em\u003e\u003c/strong\u003e, \u003cstrong\u003e\u003cem\u003eKDD\u003c/em\u003e\u003c/strong\u003e, and \u003cstrong\u003e\u003cem\u003eOSEMN\u003c/em\u003e\u003c/strong\u003e, and explore how you can make use of them to keep your projects well-structured and organized.\u003c/p\u003e\n\u003ch2\u003eCRoss-Industry Standard Process for Data Mining (CRISP-DM)\u003c/h2\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-data-science-processes/master/images/new_crisp-dm.png\" width=\"500\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e\u003cem\u003eCRISP-DM\u003c/em\u003e\u003c/strong\u003e is probably the most popular Data Science process in the Data Science world right now. Take a look at the visualization above to get a feel for CRISP-DM. Notice that CRISP-DM is an iterative process!\u003c/p\u003e\n\u003cp\u003eLet's take a look at the individual steps involved in CRISP-DM.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e\u003cem\u003eBusiness Understanding:\u003c/em\u003e\u003c/strong\u003e This stage is all about gathering facts and requirements. Who will be using the model you build? How will they be using it? How will this help the goals of the business or organization overall? Data Science projects are complex, with many moving parts and stakeholders. They're also time intensive to complete or modify. Because of this, it is very important that the Data Science team working on the project has a deep understanding of what the problem is, and how the solution will be used. Consider the fact that many stakeholders involved in the project may not have technical backgrounds, and may not even be from the same organization. Stakeholders from one part of the organization may have wildly different expectations about the project than stakeholders from a different part of the organization -- for instance, the sales team may be under the impression that a recommendation system project is meant to increase sales by recommending upsells to current customers, while the marketing team may be under the impression that the project is meant to help generate new leads by personalizing product recommendations in a marketing email. These are two very different interpretations of a recommendation system project, and it's understandable that both departments would immediately assume that the primary goal of the project is one that helps their organization. As a Data Scientist, it's up to you clarify the requirements and make sure that everyone involved understands what the project is and isn't.\u003c/p\u003e\n\u003cp\u003eDuring this stage, the goal is to get everyone on the same page and to provide clarity on the scope of the project for everyone involved, not just the Data Science team. Generate and answer as many contextual questions as you can about the project.\u003c/p\u003e\n\u003cp\u003eGood questions for this stage include:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eWho are the stakeholders in this project? Who will be directly affected by the creation of this project?\u003c/li\u003e\n\u003cli\u003eWhat business problem(s) will this Data Science project solve for the organization?\u003c/li\u003e\n\u003cli\u003eWhat problems are inside the scope of this project?\u003c/li\u003e\n\u003cli\u003eWhat problems are outside the scope of this project?\u003c/li\u003e\n\u003cli\u003eWhat data sources are available to us?\u003c/li\u003e\n\u003cli\u003eWhat is the expected timeline for this project? Are there hard deadlines (e.g. \"must be live before holiday season shopping\") or is this an ongoing project?\u003c/li\u003e\n\u003cli\u003eDo stakeholders from different parts of the company or organization all have the exact same understanding about what this project is and isn't?\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003e\u003cem\u003eData Understanding:\u003c/em\u003e\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eOnce we have a solid understanding of the business implications for this project, we move on to understanding our data. During this stage, we'll aim to get a solid understanding of the data needed to complete the project. This step includes both understanding where our data is coming from, as well as the information contained within the data.\u003c/p\u003e\n\u003cp\u003eConsider the following questions when working through this stage:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eWhat data is available to us? Where does it live? Do we have the data, or can we scrape/buy/source the data from somewhere else?\u003c/li\u003e\n\u003cli\u003eWho controls the data sources, and what steps are needed to get access to the data?\u003c/li\u003e\n\u003cli\u003eWhat is our target?\u003c/li\u003e\n\u003cli\u003eWhat predictors are available to us?\u003c/li\u003e\n\u003cli\u003eWhat data types are the predictors we'll be working with?\u003c/li\u003e\n\u003cli\u003eWhat is the distribution of our data?\u003c/li\u003e\n\u003cli\u003eHow many observations does our dataset contain? Do we have a lot of data? Only a little?\u003c/li\u003e\n\u003cli\u003eDo we have enough data to build a model? Will we need to use resampling methods?\u003c/li\u003e\n\u003cli\u003eHow do we know the data is correct? How is the data collected? Is there a chance the data could be wrong?\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003e\u003cem\u003eData Preparation:\u003c/em\u003e\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eOnce we have a strong understanding of our data, we can move onto preparing the data for our modeling steps.\u003c/p\u003e\n\u003cp\u003eDuring this stage, we'll want to handle the following issues:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eDetecting and dealing with missing values\u003c/li\u003e\n\u003cli\u003eData type conversions (e.g. numeric data mistakenly encoded as strings)\u003c/li\u003e\n\u003cli\u003eChecking for and removing multicollinearity (correlated predictors)\u003c/li\u003e\n\u003cli\u003eNormalizing our numeric data\u003c/li\u003e\n\u003cli\u003eConverting categorical data to numeric format through one-hot encoding\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003e\u003cem\u003eModeling:\u003c/em\u003e\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eOnce we have clean data, we can begin modeling! Remember, modeling, as with any of these other steps, is an iterative process. During this stage, we'll try to build and tune models to get the highest performance possible on our task.\u003c/p\u003e\n\u003cp\u003eConsider the following questions during the modeling step:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eIs this a classification task? A regression task? Something else?\u003c/li\u003e\n\u003cli\u003eWhat models will we try?\u003c/li\u003e\n\u003cli\u003eHow do we deal with overfitting?\u003c/li\u003e\n\u003cli\u003eDo we need to use regularization or not?\u003c/li\u003e\n\u003cli\u003eWhat sort of validation strategy will we be using to check that our model works well on unseen data?\u003c/li\u003e\n\u003cli\u003eWhat loss functions will we use?\u003c/li\u003e\n\u003cli\u003eWhat threshold of performance do we consider as successful?\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003e\u003cem\u003eEvaluation:\u003c/em\u003e\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eDuring this step, we'll evaluate the results of our modeling efforts. Does our model solve the problems that we outlined all the way back during step 1? Why or why not? Often times, evaluating the results of our modeling step will raise new questions, or will cause us to consider changing our approach to the problem. Notice from the CRISP-DM diagram above, that the \"Evaluation\" step is unique in that it points to both \u003cem\u003eBusiness Understanding\u003c/em\u003e and \u003cem\u003eDeployment\u003c/em\u003e. As we mentioned before, Data Science is an iterative process -- that means that given the new information our model has provided, we'll often want to start over with another iteration, armed with our newfound knowledge! Perhaps the results of our model showed us something important that we had originally failed to consider the goal of the project or the scope. Perhaps we learned that the model can't be successful without more data, or different data. Perhaps our evaluation shows us that we should reconsider our approach to cleaning and structuring the data, or how we frame the project as a whole (e.g. realizing we should treat the problem as a classification rather than a regression task). In any of these cases, it is totally encouraged to revisit the earlier steps.\u003c/p\u003e\n\u003cp\u003eOf course, if the results are satisfactory, then we instead move onto deployment!\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e\u003cem\u003eDeployment:\u003c/em\u003e\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eDuring this stage, we'll focus on moving our model into production and automating as much as possible. Everything before this serves as a proof-of-concept or an investigation. If the project has proved successful, then you'll work with stakeholders to determine the best way to implement models and insights. For example, you might set up an automated ETL (Extract-Transform-Load) pipelines of raw data in order to feed into a database and reformat it so that it is ready for modeling. During the deployment step, you'll actively work to determine the best course of action for getting the results of your project into the wild, and you'll often be involved with building everything needed to put the software into production.\u003c/p\u003e\n\u003cp\u003eThis is one of the most rewarding steps of the entire Data Science process -- getting to see your work go live!\u003c/p\u003e\n\u003ch2\u003eKnowledge Discovery in Databases\u003c/h2\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-data-science-processes/master/images/new_kdd.png\" width=\"800\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e\u003cem\u003eKnowledge Discovery in Databases\u003c/em\u003e\u003c/strong\u003e, or \u003cstrong\u003e\u003cem\u003eKDD\u003c/em\u003e\u003c/strong\u003e is considered the oldest Data Science process. The creation of this process is credited to Gregory Piatetsky-Shapiro, who also runs the ever-popular Data Science blog, \u003ca href=\"https://www.kdnuggets.com/\"\u003ekdnuggets\u003c/a\u003e. If you're interested, read the original white paper on KDD, which can be found \u003ca href=\"https://www.kdnuggets.com/gpspubs/aimag-kdd-overview-1992.pdf\"\u003ehere\u003c/a\u003e!\u003c/p\u003e\n\u003cp\u003eThe KDD process is quite similar to the CRISP-DM process. The diagram above illustrates every step of the KDD process, as well as the expected output at each stage.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e\u003cem\u003eSelection\u003c/em\u003e\u003c/strong\u003e:\u003c/p\u003e\n\u003cp\u003eDuring this stage, you'll focus on selecting your problem, and the data that will help you answer it. This stage works much like the first stage of CRISP-DM -- you begin by focusing on developing an understanding of the domain the problem resides in (e.g. marketing, finance, increasing customer sales, etc), the previous work done in this domain, and the goals of the stakeholders involved with the process.\u003c/p\u003e\n\u003cp\u003eOnce you've developed a strong understanding of the goals and the domain, you'll work to establish where your data is coming from, and which data will be useful to you. Organizations and companies usually have a ton of data, and only some of it will be relevant to the problem you're trying to solve. During this stage, you'll focus on examining the data sources available to you and gathering the data that you deem useful for the project.\u003c/p\u003e\n\u003cp\u003eThe output of this stage is the dataset you'll be using for the Data Science project.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e\u003cem\u003ePreprocessing\u003c/em\u003e\u003c/strong\u003e:\u003c/p\u003e\n\u003cp\u003eThe preprocessing stage is pretty straightforward -- the goal of this stage is to \"clean\" the data by preprocessing it. For text data, this may include things like tokenization. You'll also identify and deal with issues like outliers and/or missing data in this stage.\u003c/p\u003e\n\u003cp\u003eIn practice, this stage often blurs with the \u003cem\u003eTransformation\u003c/em\u003e stage.\u003c/p\u003e\n\u003cp\u003eThe output of this stage is preprocessed data that is more \"clean\" than it was at the start of this stage -- although the dataset is not quite ready for modeling yet.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e\u003cem\u003eTransformation\u003c/em\u003e\u003c/strong\u003e:\u003c/p\u003e\n\u003cp\u003eDuring this stage, you'll take your preprocessed data and transform it in a way that makes it more ideal for modeling. This may include steps like feature engineering and dimensionality reduction. At this stage, you'll also deal with things like checking for and removing multicollinearity from the dataset. Categorical data should also be converted to numeric format through one-hot encoding during this step.\u003c/p\u003e\n\u003cp\u003eThe output of this stage is a dataset that is now ready for modeling. All null values and outliers are removed, categorical data has been converted to a format that a model can work with, and the dataset is generally ready for experimentation with modeling.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e\u003cem\u003eData Mining\u003c/em\u003e\u003c/strong\u003e:\u003c/p\u003e\n\u003cp\u003eThe Data Mining stage refers to using different modeling techniques to try and build a model that solves the problem we're after -- often, this is a classification or regression task. During this stage, you'll also define your parameters for given models, as well as your overall criteria for measuring the performance of a model.\u003c/p\u003e\n\u003cp\u003eYou may be wondering what Data Mining is, and how it relates to Data Science. In practice, it's just an older term that essentially means the same thing as Data Science. Dr. Piatetsky-Shapiro defines Data Mining as \"the non-trivial extraction of implicit, previously unknown and potentially useful information from data.\" Making of things such as Machine Learning algorithms to find insights in large datasets that aren't immediately obvious without these algorithms is at the heart of the concept of Data Mining, just as it is in Data Science. In a pragmatic sense, this is why the terms Data Mining and Data Science are typically used interchangeably, although the term Data Mining is considered an older term that isn't used as often nowadays.\u003c/p\u003e\n\u003cp\u003eThe output of this stage results from a fit to the data for the problem we're trying to solve.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e\u003cem\u003eInterpretation/Evaluation\u003c/em\u003e\u003c/strong\u003e:\u003c/p\u003e\n\u003cp\u003eDuring this final stage of KDD, we focus on interpreting the \"patterns\" discovered in the previous step to help us make generalizations or predictions that help us answer our original question. During this stage, you'll consolidate everything you've learned to present it to stakeholders for guiding future actions. Your output may be a presentation that you use to communicate to non-technical managers or executives (never discount the importance of knowing PowerPoint as a Data Scientist!). Your conclusions for a project may range from \"this approach didn't work\" or \"we need more data about {X}\" to \"this is ready for production, let's build it!\".\u003c/p\u003e\n\u003ch2\u003eOSEMN\u003c/h2\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-data-science-processes/master/images/new_osemn.png\" width=\"800\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://www.kdnuggets.com/2018/02/data-science-command-line-book-exploring-data.html\" target=\"_blank\"\u003eAdapted from: KDNuggets\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eThis brings us to the Data Science process we'll be using during this section -- OSEMN (sometimes referred as OSEMiN, and pronounced \"OH-sum\", rhymes with \"possum\"). This is the most straightforward of the Data Science processes discussed so far. Note that during this process, just like the others, the stages often blur together. It is completely acceptable (and often a best practice!) to float back and forth between stages as you learn new things about your problem, dataset, requirements, etc. It's quite common to get to the modeling step and realize that you need to scrub your data a bit more or engineer a different feature and jump back to the \"Scrub\" stage, or go all the way back to the \"Obtain\" stage when you realize your current data isn't sufficient to solve this problem. As with any of these frameworks, OSEMN is meant to be treated more like a set of guidelines for structuring your project than set-in-stone steps that cannot be violated.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e\u003cem\u003eObtain\u003c/em\u003e\u003c/strong\u003e:\u003c/p\u003e\n\u003cp\u003eAs with CRISP-DM and KDD, this step involves understanding stakeholder requirements, gathering information on the problem, and finally, sourcing data that we think will be necessary for solving this problem.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e\u003cem\u003eScrub\u003c/em\u003e\u003c/strong\u003e:\u003c/p\u003e\n\u003cp\u003eDuring this stage, we'll focus on preprocessing our data. Important steps such as identifying and removing null values, dealing with outliers, normalizing data, and feature engineering/feature selection are handled around this stage. The line with this stage really blurs with the \u003cem\u003eExplore\u003c/em\u003e stage, as it is common to only realize that certain columns require cleaning or preprocessing as a result of the visualizations and explorations done during Step 3.\u003c/p\u003e\n\u003cp\u003eNote that although technically, categorical data should be one-hot encoded during this step, in practice, it's usually done after data exploration. This is because it is much less time-consuming to visualize and explore a few columns containing categorical data than it is to explore many different dummy columns that have been one-hot encoded.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e\u003cem\u003eExplore\u003c/em\u003e\u003c/strong\u003e:\u003c/p\u003e\n\u003cp\u003eThis step focuses on getting to know the dataset you're working with. As mentioned above, this step tends to blend with the \u003cem\u003eScrub\u003c/em\u003e step mentioned above. During this step, you'll create visualizations to really get a feel for your dataset. You'll focus on things such as understanding the distribution of different columns, checking for multicollinearity, and other tasks like that. If your project is a classification task, you may check the balance of the different classes in your dataset. If your problem is a regression task, you may check that the dataset meets the assumptions necessary for a regression task.\u003c/p\u003e\n\u003cp\u003eAt the end of this step, you should have a dataset ready for modeling that you've thoroughly explored and are extremely familiar with.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e\u003cem\u003eModel\u003c/em\u003e\u003c/strong\u003e:\u003c/p\u003e\n\u003cp\u003eThis step, as with the last two frameworks, is also pretty self-explanatory. It consists of building and tuning models using all the tools you have in your data science toolbox. In practice, this often means defining a threshold for success, selecting machine learning algorithms to test on the project, and tuning the ones that show promise to try and increase your results. As with the other stages, it is both common and accepted to realize something, jump back to a previous stage like \u003cem\u003eScrub\u003c/em\u003e or \u003cem\u003eExplore\u003c/em\u003e, and make some changes to see how it affects the model.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e\u003cem\u003eInterpret\u003c/em\u003e\u003c/strong\u003e:\u003c/p\u003e\n\u003cp\u003eDuring this step, you'll interpret the results of your model(s), and communicate results to stakeholders. As with the other frameworks, communication is incredibly important! During this stage, you may come to realize that further investigation is needed, or more data. That's totally fine -- figure out what's needed, go get it, and start the process over! If your results are satisfactory to all stakeholders involved, you may also go from this stage right into putting your model into production and automating processes necessary to support it.\u003c/p\u003e\n\u003ch2\u003eA Note On Communicating Results\u003c/h2\u003e\n\u003cp\u003eRegardless of the quality of your results, it's very important that you be aware of the business requirements and stakeholder expectations at all times! Generally, no matter which of the above processes you use, you'll communicate your results in a two-pronged manner:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eA short, high-level presentation covering your question, process, and results meant for non-technical audiences\u003c/li\u003e\n\u003cli\u003eA detailed Jupyter Notebook demonstrating your entire process meant for technical audiences\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eIn general, you can see why Data Scientists love Jupyter Notebooks! It is very easy to format results in a reproducible, easy-to-understand way. Although a detailed Jupyter Notebook may seem like the more involved of the two deliverables listed above, the high-level presentation is often the hardest! Just remember -- even if the project took you/your team over a year and utilized the most cutting-edge machine learning techniques available, you still need to be able to communicate your results in about 5 slides (using graphics, not words, whenever possible!), in a 5 minute presentation in a way that someone that can't write code can still understand and be convinced by!\u003c/p\u003e\n\u003ch2\u003eConclusion\u003c/h2\u003e\n\u003cp\u003eIn this lesson, you learned about the different data science process frameworks including CRISP-DM, KDD, and OSEMN. You also learned that the data science process is iterative and that a typical data science project involves many different stakeholders who may not have a technical background. As such, it's important to recognize that data scientists must be able to communicate their findings in a non-technical way.\u003c/p\u003e","exportId":"data-science-processes"},{"id":141711,"title":"Multiple Regression and Model Validation - Recap","type":"WikiPage","indent":0,"locked":false,"requirement":"must_mark_done","completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-regression-model-eval-recap-v2-1\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-regression-model-eval-recap-v2-1/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eIn this section you extended your knowledge of building regression models by adding additional predictive variables and subsequently validating those models using train-test-split and cross validation.\u003c/p\u003e\n\u003ch2\u003eMultiple Regression\u003c/h2\u003e\n\u003cp\u003eYou saw a number of techniques and concepts related to regression. This included the idea of using multiple predictors in order to build a stronger estimator. That said, there were caveats to using multiple predictors. For example, multicollinearity between variables should be avoided. One option for features with particularly high correlation is to only use one of these features. This improves model interpretability. In addition, linear regression is also most effective when features are of a similar scale. Typically, feature scaling and normalization are used to achieve this. There are also other data preparation techniques such as creating dummy variables for categorical variables, and transforming non-normal distributions using functions such as logarithms. Finally, in order to validate models it is essential to always partition your dataset such as with train-test splits or k-fold cross validation.\u003c/p\u003e\n\u003cfooter class=\"fis-footer\" style=\"visibility: hidden;\"\u003e\n\u003cdiv class=\"fis-feedback\"\u003e\n\u003ch5\u003eHow do you feel about this lesson?\u003c/h5\u003e\n\u003cimg id=\"thumbs-up\" title=\"Thumbs up!\" alt=\"thumbs up\" data-repository=\"dsc-regression-model-eval-recap-v2-1\"\u003e\u003cimg id=\"thumbs-down\" title=\"Thumbs down!\" alt=\"thumbs down\" data-repository=\"dsc-regression-model-eval-recap-v2-1\"\u003e\n\u003c/div\u003e\n\u003ch5\u003eHave specific feedback? \u003ca href=\"https://github.com/learn-co-curriculum/dsc-regression-model-eval-recap-v2-1/issues/new\"\u003eTell us here!\u003c/a\u003e\n\u003c/h5\u003e\n\u003c/footer\u003e","exportId":"multiple-regression-and-model-validation-recap"}]},{"id":15116,"name":"Topic 20: Extensions to Linear Models","status":"unlocked","unlockDate":null,"prereqs":[],"requirement":"all","sequential":false,"exportId":"g55793797b0f76c294187bb5448bf487b","items":[{"id":141718,"title":"Extension to Linear Models - Introduction","type":"WikiPage","indent":0,"locked":false,"requirement":"must_mark_done","completed":false,"content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-extensions-to-linear-models-intro-v2-1\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-extensions-to-linear-models-intro-v2-1\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-extensions-to-linear-models-intro-v2-1/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eIn this section, you'll learn about some ways to make linear models more complex to account for complicated relationships in a dataset. Before diving in too deep, here is a roadmap for what you will be covering in this section.\u003c/p\u003e\n\u003ch2\u003eExtensions to linear models\u003c/h2\u003e\n\u003cp\u003eUnfortunately, not every continuous variable can be predicted effectively using a straight line (a linear model). Imagine the relationship between your 5k time as a runner and how many hours a week you train. At the very least the improvements in your time are going to trail off with additional training (going from 0 -\u0026gt; 10 hrs training a week is going to have way more impact than going from 60 -\u0026gt; 70 hrs). And the chances are that at some point in time, you'll overtrain and additional training will actually degrade your performance. You could certainly model the relationship between weekly training hours and 5k time with a straight line, but for some values of weekly training time, the accuracy of the prediction would probably be extremely low.\u003c/p\u003e\n\u003cp\u003eIn this section, you'll be introduced to a number of concepts to help you to make predictions when there is no linear relationship between the predictor and target variables.\u003c/p\u003e\n\u003ch3\u003eInteractions\u003c/h3\u003e\n\u003cp\u003eWe'll start by introducing the concept of interactions - where two or more variables interact in a non-additive manner thus affecting a third variable. Then you'll look at why they are important and how to account for them.\u003c/p\u003e\n\u003ch3\u003ePolynomial regression\u003c/h3\u003e\n\u003cp\u003eYou'll then implement higher-order equations for solving regressions. A linear expression can be described by an equation in the form of \u003cimg src=\"https://render.githubusercontent.com/render/math?math=y%20=%20mx%20%2b%20b\"\u003e . A polynomial expression brings in higher powers of x (squared, cubed, etc). By allowing for equations containing higher-order terms you may be able to better fit a curve to your dataset, thus predicting future values more accurately.\u003c/p\u003e\n\u003ch3\u003eBias-variance tradeoff\u003c/h3\u003e\n\u003cp\u003eWhile polynomial regression can improve the accuracy of your models, they also exacerbate the risk of overfitting the data, making your model extremely accurate for the training set but completely inaccurate for any future data points the model wasn't trained on. You'll also look at the concept of bias-variance tradeoff and how it relates to underfitting and overfitting datasets.\u003c/p\u003e\n\u003ch2\u003eSummary\u003c/h2\u003e\n\u003cp\u003eIt's time to start learning about extensions to linear models, modeling non-linear relationships between the predictor and target variables!\u003c/p\u003e","exportId":"extension-to-linear-models-introduction"},{"id":141722,"title":"Interactions","type":"Assignment","indent":0,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_mark_done","completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-interaction-terms\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-interaction-terms/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","exportId":"ge0e6a2f8332e742c665ecf4969e3e726"},{"id":141724,"title":"Interactions - Lab","type":"Assignment","indent":1,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_mark_done","completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-interaction-terms-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-interaction-terms-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","exportId":"ge1b7089d3bbe8eecb4409c0c95230685"},{"id":141728,"title":"Polynomial Regression","type":"Assignment","indent":0,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_mark_done","completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-polynomial-regression\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-polynomial-regression/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","exportId":"g8db93d4be8ec62a0c5862a645f26d3d3"},{"id":141730,"title":"Polynomial Regression - Lab","type":"Assignment","indent":1,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_mark_done","completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-polynomial-regression-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-polynomial-regression-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","exportId":"g8b845b3e7563955e624cdd0ef4e66247"},{"id":141732,"title":"Bias-Variance Trade-Off","type":"Assignment","indent":0,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_mark_done","completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-bias-variance-trade-off\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-bias-variance-trade-off/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","exportId":"g6dfc40ce560e87b7e40eb2bbb2120f41"},{"id":141735,"title":"Bias-Variance Trade-Off - Lab","type":"Assignment","indent":1,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_mark_done","completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-bias-variance-trade-off-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-bias-variance-trade-off-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","exportId":"ga7e11e9e477ee98e6d60d1d83a3fc387"},{"id":141739,"title":"Extensions To Linear Models - Recap","type":"WikiPage","indent":0,"locked":false,"requirement":"must_mark_done","completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-extensions-to-linear-models-recap-v2-1\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-extensions-to-linear-models-recap-v2-1/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eCongratulations, you've just modeled some complex non-linear relationships with interaction terms and polynomials! Here is a recap of what you learned in this section.\u003c/p\u003e\n\u003ch2\u003eKey Takeaways\u003c/h2\u003e\n\u003cp\u003eThis section gave you the chance to learn about techniques whereby you can model non-linear relationships between the predictor and target variables. It's very rare that real-world problems can be modeled with a simple linear regression, so it's important to get yourself well acquainted with creating new features and selecting the most important ones.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eAn interaction is a particular property of two or more variables where they interact in a non-additive manner when affecting a third variable\u003c/li\u003e\n\u003cli\u003ePolynomial regression allows for bringing in higher orders of predictor variables (such as squared, cubed, etc)\u003c/li\u003e\n\u003cli\u003eThe risk of polynomial regression is that they can easily overfit to data, so it's important to consider the Bias-variance tradeoff when building models with greater complexity\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003eSummary\u003c/h2\u003e\n\u003cp\u003eExcellent work! You learned a substantial amount about different ways to model non-linear relationships. You will continue to use and build upon the concepts learned in this section for the rest of your machine learning career.\u003c/p\u003e\n\u003cfooter class=\"fis-footer\" style=\"visibility: hidden;\"\u003e\n\u003cdiv class=\"fis-feedback\"\u003e\n\u003ch5\u003eHow do you feel about this lesson?\u003c/h5\u003e\n\u003cimg id=\"thumbs-up\" title=\"Thumbs up!\" alt=\"thumbs up\" data-repository=\"dsc-extensions-to-linear-models-recap-v2-1\"\u003e\u003cimg id=\"thumbs-down\" title=\"Thumbs down!\" alt=\"thumbs down\" data-repository=\"dsc-extensions-to-linear-models-recap-v2-1\"\u003e\n\u003c/div\u003e\n\u003ch5\u003eHave specific feedback? \u003ca href=\"https://github.com/learn-co-curriculum/dsc-extensions-to-linear-models-recap-v2-1/issues/new\"\u003eTell us here!\u003c/a\u003e\n\u003c/h5\u003e\n\u003c/footer\u003e","exportId":"extensions-to-linear-models-recap"}]},{"id":15125,"name":"APPENDIX : Multiple Linear Regression Project","status":"completed","unlockDate":null,"prereqs":[],"requirement":null,"sequential":false,"exportId":"g879f0e3624509cdf5a4b21e3facef261","items":[{"id":141788,"title":"A Complete Data Science Project Using Multiple Regression - Introduction","type":"WikiPage","indent":0,"locked":false,"requirement":null,"completed":false,"content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-full-ds-regression-intro\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-full-ds-regression-intro\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-full-ds-regression-intro/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eIn this section, you'll get a chance to synthesize your skills and work through the entire Data Science workflow. To start, you'll extract appropriate data from a SQL database. From there, you'll continue exploring and cleaning your data, modeling the data, and conducting statistical analyses!\u003c/p\u003e\n\u003ch2\u003eData Science Processes\u003c/h2\u003e\n\u003cp\u003eYou'll take a look at three general frameworks for conducting Data Science processes using the skills you've learned thus far:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cstrong\u003eCR\u003c/strong\u003eoss-\u003cstrong\u003eI\u003c/strong\u003endustry \u003cstrong\u003eS\u003c/strong\u003etandard \u003cstrong\u003eP\u003c/strong\u003erocess for \u003cstrong\u003eD\u003c/strong\u003eata \u003cstrong\u003eM\u003c/strong\u003eining - \u003cstrong\u003eCRISP-DM\u003c/strong\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cstrong\u003eK\u003c/strong\u003enowledge \u003cstrong\u003eD\u003c/strong\u003eiscovery in \u003cstrong\u003eD\u003c/strong\u003eatabases - \u003cstrong\u003eKDD\u003c/strong\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cstrong\u003eO\u003c/strong\u003ebtain \u003cstrong\u003eS\u003c/strong\u003ecrub \u003cstrong\u003eE\u003c/strong\u003explore \u003cstrong\u003eM\u003c/strong\u003eodel i\u003cstrong\u003eN\u003c/strong\u003eterpret - \u003cstrong\u003eOSEMN\u003c/strong\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cblockquote\u003e\n\u003cp\u003eNote: OSEMN is pronounced \"OH-sum\" and rhymes with \"possum\"\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eFrom there, the lessons follow a similar structure:\u003c/p\u003e\n\u003ch2\u003eObtaining Data\u003c/h2\u003e\n\u003cp\u003eYou'll review SQL and practice importing data from a relational database using the ETL (Extract, Transform and Load) process.\u003c/p\u003e\n\u003ch2\u003eScrubbing Data\u003c/h2\u003e\n\u003cp\u003eFrom there, you'll practice cleaning data:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eCasting columns to the appropriate data types\u003c/li\u003e\n\u003cli\u003eIdentifying and dealing with null values appropriately\u003c/li\u003e\n\u003cli\u003eRemoving columns that aren't required for modeling\u003c/li\u003e\n\u003cli\u003eChecking for and dealing with multicollinearity\u003c/li\u003e\n\u003cli\u003eNormalizing the data\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003eExploring Data\u003c/h2\u003e\n\u003cp\u003eOnce you've the cleaned data, you'll then do some further EDA (Exploratory Data Analysis) to check out the distributions of the various columns, examine the descriptive statistics for the dataset, and to create some initial visualizations to better understand the dataset.\u003c/p\u003e\n\u003ch2\u003eModeling Data\u003c/h2\u003e\n\u003cp\u003eFinally, you'll create a definitive model. This will include fitting an initial regression model, and then conducting statistical analyses of the results. You'll take a look at the p-values of the various features and perform some feature selection. You'll test for regression assumptions including normality, heteroscedasticity, and independence. From these tests, you'll then refine and improve the model, not just for performance, but for interpretability as well.\u003c/p\u003e\n\u003ch2\u003eSummary\u003c/h2\u003e\n\u003cp\u003eIn this section, you'll conduct end-to-end review of the Data Science process!\u003c/p\u003e","exportId":"a-complete-data-science-project-using-multiple-regression-introduction"},{"id":141792,"title":"Data Science Toolbox Review","type":"WikiPage","indent":0,"locked":false,"requirement":null,"completed":false,"content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-data-science-toolbox-review-v2-1\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-data-science-toolbox-review-v2-1\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-data-science-toolbox-review-v2-1/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eIn this lesson, you'll get a chance to review some of the many skills you've learned to date! You've developed a range of skills from general programming skills to databases, data analysis, and statistics! Hopefully, you're excited to pull all of these pieces together and conduct a full Data Science investigation from data extraction to modeling! With that, here's a quick review of some of the most pertinent skills you'll be using along the way!\u003c/p\u003e\n\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\n\u003cp\u003eYou will be able to:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003ePerform SQL select statements, including joins\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch2\u003eSQL\u003c/h2\u003e\n\n\u003cp\u003eYou've seen how SQL, Structured Query Language, is a wonderful method for organizing data. Having data in a database allows multiple users to access, query, and update shared data simultaneously. As such, many companies store their data in databases and SQL is undoubtedly the most popular implementation. You saw that most statements began with the \u003ccode\u003eSELECT\u003c/code\u003e clause, and from there you specify which columns you wish to select, the table from which you wish to select them, and appropriate filters using the \u003ccode\u003eWHERE\u003c/code\u003e clause. You can also aggregate data with the \u003ccode\u003eGROUP BY\u003c/code\u003e clause, and apply further filters to those roll-ups using the \u003ccode\u003eHAVING\u003c/code\u003e clause. Furthermore, you can select data from multiple tables using a \u003ccode\u003eJOIN\u003c/code\u003e, so long as the two tables have a common key(s) which you specify with the \u003ccode\u003eUSING\u003c/code\u003e clause, if the column names are identical, or more verbosely, you can specify how to join the tables with the \u003ccode\u003eON\u003c/code\u003e clause. \u003c/p\u003e\n\n\u003cp\u003eFor example, here's the schema for the mock customer relationship database that you've seen before: \u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-data-science-toolbox-review-v2-1/master/images/Database-Schema.png\" width=\"550\"\u003e\u003c/p\u003e\n\n\u003cp\u003eIf you wanted to select some aggregate statistics regarding employees and customers on a per office basis for cities with at least 2 offices, you could write a query like this:  \u003c/p\u003e\n\n\u003cpre\u003e\u003ccode class=\"SQL\"\u003eSELECT officeCode,\n       city,\n       COUNT(employeeNumber) AS num_employees,\n       COUNT(customerNumber) AS num_customers,\n       sum(amount) AS total_payments_received\n       FROM offices\n       JOIN employees\n       USING(officeCode)\n       JOIN customers\n       ON employeeNumber = salesRepEmployeeNumber\n       JOIN payments\n       USING(customerNumber)\n       GROUP BY 1, 2\n       WHERE city IN (SELECT city FROM offices GROUP BY 1 HAVING COUNT(*)\u0026gt;1);\n\u003c/code\u003e\u003c/pre\u003e\n\n\u003cp\u003eLook at that; you even got to see the use of a subquery once again!\u003c/p\u003e\n\n\u003ch2\u003eData Exploration and Visualization\u003c/h2\u003e\n\n\u003cp\u003eOnce you've loaded in some data from a SQL database or elsewhere, it's generally time to start investigating and cleaning up that data. Remember that Pandas comes with many useful methods for quickly exploring your dataset. For example, you may want to initially check the datatypes of your dataset and how many null values exist for each of the various features. This is incredibly easy using the \u003ccode\u003e.info()\u003c/code\u003e method. Similarly, if you want to generate aggregate statistics, you can simply call the \u003ccode\u003e.describe()\u003c/code\u003e method on the DataFrame. If you want to quickly explore the distribution and pairwise correlations between your features, then using the \u003ccode\u003epd.plotting.scatter_matrix()\u003c/code\u003e function will quickly display a grid of histograms and scatter plots for the various features; the diagonal entries are the distributions for each of the variables, while any other cell [i,j] is a scatter plot of feature i vs feature j.  \u003c/p\u003e\n\n\u003cp\u003eRecall that when initially exploring your data, you are both looking for insights and anomalies. Typically, you want to get familiar with the data, what it represents, and how it is represented. This will often give you further ideas about how to mine the data for structure and answer questions regarding the application.\u003c/p\u003e\n\n\u003ch2\u003eRegression\u003c/h2\u003e\n\n\u003cp\u003eAfter acquiring and exploring your data (including cleaning it up), you'll then go on to model said data using the regression techniques you learned about earlier. With this, recall that there are four main assumptions underlying a linear regression model.\u003c/p\u003e\n\n\u003ch3\u003e1. Linearity\u003c/h3\u003e\n\n\u003cp\u003eWith linear models, the target variable is being modeled as a linear combination of the independent variables. As such, there should be a linear relationship between the target variable and the various features being used. If the rate of change between the target variable and one of the features is non-linear and displays other characteristics such as an exponential acceleration, then prior transformations of the data are necessary before applying a regression model. \u003c/p\u003e\n\n\u003ch3\u003e2. Normality\u003c/h3\u003e\n\n\u003cp\u003eWith linear models, the errors (residuals) from the model are assumed to be normally distributed. A good heuristic to initially check for this is to use a Q-Q plot. \u003c/p\u003e\n\n\u003ch3\u003e3. Homoscedasticity\u003c/h3\u003e\n\n\u003cp\u003eAlong with the assumption of normal distribution, error terms should also not be correlated with the target variable or other features within the model. If errors indeed appear to be random and there are no discernible trends, then the errors are said to be homoscedastic. Looking at a simple plot of residuals against the target variable or other feature is generally sufficient to gauge this.\u003c/p\u003e\n\n\u003ch3\u003e4. Independence\u003c/h3\u003e\n\n\u003cp\u003eFinally, regression models assume that the various independent features feeding into the model are independent. You'll take a further look at this in this section and investigate how to check for multicollinearity. Multicollinearity is when a variable can be predicted with substantial accuracy by a separate set of features. Previously, you've examined multicollinearity in the context of the \"dummy variable trap\" and the two variable case. It's unwise to include two features in a regression model that are highly correlated. Similarly, in a multivariate case, having a set of features that can effectively predict another independent feature can be problematic. Such phenomenon will not reduce the overall accuracy of the model, but will \u003cstrong\u003eseverely impede interpretation as coefficient weights of the model\u003c/strong\u003e become unstable so it is difficult or impossible to determine which features are most influential.\u003c/p\u003e\n\n\u003ch2\u003eSummary\u003c/h2\u003e\n\n\u003cp\u003eIn this lesson, you took a quick review of SQL, data exploration, and regression models. If you are looking for a more substantial review, feel free to turn back to some of the previous material. With that, it's time to take a look at some general frameworks for Data Science workflows!\u003c/p\u003e","exportId":"data-science-toolbox-review"},{"id":141796,"title":"Obtaining Your Data","type":"Assignment","indent":0,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":null,"completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-obtaining-your-data\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-obtaining-your-data/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","exportId":"gba5530562764eb91a21faad9d3b45296"},{"id":141800,"title":"Obtaining Your Data - Lab","type":"Assignment","indent":1,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":null,"completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-obtaining-your-data-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-obtaining-your-data-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","exportId":"g555884b43af918f3d3eff9f45854edee"},{"id":141803,"title":"Scrubbing and Cleaning Data","type":"WikiPage","indent":0,"locked":false,"requirement":null,"completed":false,"content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-scrubbing-and-cleaning-data\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-scrubbing-and-cleaning-data\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-scrubbing-and-cleaning-data/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eIn this lesson, you'll review common issues to focus on when scrubbing and cleaning data.\u003c/p\u003e\n\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\n\u003cp\u003eYou will be able to:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eCast columns to appropriate data types\u003c/li\u003e\n\u003cli\u003eIdentify and deal with null values appropriately\u003c/li\u003e\n\u003cli\u003eRemove unnecessary columns\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch2\u003eThe \"Scrub\" Step\u003c/h2\u003e\n\n\u003cp\u003eDuring the process of working with data, you'll always reach a point where you've gathered all the data you'll need (our \"Obtain\" step), but the data is not yet in a format where you can use it for modeling.  All the work that you'll be doing in the next lab will be to get the dataset in a format that you can easily explore and build models with. \u003c/p\u003e\n\n\u003ch2\u003eSubsampling to Reduce Size\u003c/h2\u003e\n\n\u003cp\u003eWhen building a model for predictive purposes, more data is always better when training the final model. However, during the development process when working with large datasets, it is common to work with only a subsample of the dataset. Building a model is an iterative process -- often, you fit the model, investigate the results, then train the model again with some small tweaks based on what you noticed.  Since this is an iterative process, you want to avoid long runtimes, and iterate as quickly as possible.  When you're satisfied with the model you've built on the subsample of data, then you would fit the model on the entire dataset. \u003c/p\u003e\n\n\u003cp\u003eIn the next lab, you'll work with a subsample of the dataset to increase the iteration speed in refining your model. \u003c/p\u003e\n\n\u003ch2\u003eDealing With Data Types\u003c/h2\u003e\n\n\u003cp\u003eOne of the most common problems you'll need to deal with during the data scrubbing step is columns that are encoded as the wrong data type. For example, a common formatting issue you may encounter is numeric data that is mistakenly encoded as string data. This makes numerical operations on the data impossible without first reformatting the data. A simple operation such as \u003ccode\u003e2 + 2\u003c/code\u003e will return \u003ccode\u003e22\u003c/code\u003e if the numeric data is accidentally formatted as strings (\u003ccode\u003e'2'+'2'='22'\u003c/code\u003e). Similarly, categorical data is often encoded as integer values. If you don't properly conceptualize how the data is represented, you will fail to formulate meaningful models and insights.\u003c/p\u003e\n\n\u003cp\u003eA first step to uncover and investigate such issues is to use the \u003ccode\u003e.info()\u003c/code\u003e method available for all Pandas DataFrames. This will tell what type of data each column contains, as well as the number of values contained within that column (which can also help us identify columns that contain missing data)!  Here's an example response:\u003c/p\u003e\n\n\u003cpre\u003e\u003ccode\u003e\u0026lt;class 'pandas.core.frame.DataFrame'\u0026gt;\nInt64Index: 97839 entries, 0 to 97838\nData columns (total 16 columns):\nStore           97839 non-null object\nDept            97839 non-null object\nDate            97839 non-null object\nWeekly_Sales    97839 non-null float64\nIsHoliday       97839 non-null bool\nType            97839 non-null object\nSize            97839 non-null int64\nTemperature     97839 non-null float64\nFuel_Price      97839 non-null float64\nMarkDown1       35013 non-null float64\nMarkDown2       27232 non-null float64\nMarkDown3       32513 non-null float64\nMarkDown4       34485 non-null float64\nMarkDown5       35013 non-null float64\nCPI             97839 non-null float64\nUnemployment    97839 non-null float64\ndtypes: bool(1), float64(10), int64(1), object(4)\nmemory usage: 12.0+ MB\n\n\u003c/code\u003e\u003c/pre\u003e\n\n\u003cp\u003eFrom here, a good next step would be to look at examples from each column encoded as strings (remember, Pandas refers to string columns as \u003ccode\u003eobject\u003c/code\u003e) and confirm that this data is supposed to be encoded as strings. One method to do this is to preview a truncated version of the output from \u003ccode\u003e.value_counts()\u003c/code\u003e. For example, you could preview the 5 most frequent entries from each column with a simple loop like this:  \u003c/p\u003e\n\n\u003cpre\u003e\u003ccode class=\"python\"\u003efor col in df.columns:\n    try:\n        print(col, df[col].value_counts()[:5])\n    except:\n        print(col, df[col].value_counts())\n        # If there aren't 5+ unique values for a column the first print statement\n        # will throw an error for an invalid idx slice\n    print('\\n') # Break up the output between columns\n\u003c/code\u003e\u003c/pre\u003e\n\n\u003cp\u003eIt is usually also a good idea to check integer columns to ensure that the data it contains is meant to represent actual numeric data, and is not just categorical data encoded as integers. You may also uncover null values hard coded as strings such as \u003ccode\u003e\"?\"\u003c/code\u003e, \u003ccode\u003e\"999999\"\u003c/code\u003e or other extraneous values depending on the dataset and who created it.\u003c/p\u003e\n\n\u003ch3\u003eNumeric Data Encoded as Strings\u003c/h3\u003e\n\n\u003cp\u003eIf you've identified numeric data encoded as strings, it's typically a pretty easy problem to solve. Often it's as simple as casting the string data to a numeric type:\u003c/p\u003e\n\n\u003cpre\u003e\u003ccode class=\"python\"\u003edf['numeric_string_col'] = df['numeric_string_col'].astype('float')\n\u003c/code\u003e\u003c/pre\u003e\n\n\u003cp\u003eSadly, it's not always that simple. For example, if there is even a single cell that contains a letter or non-numeric character such as a comma or monetary symbol ($) the above statement will fail. In such cases, a more complex cleaning function must be manually created. This could involve stripping extraneous symbols such as ',$/%',  or simply casting non convertible strings as null. Recall that when NumPy sees multiple data types in an array, it defaults to casting everything as a string. If you try to cast a column from string to numeric data types and get an error, consider checking the unique values in that column -- it's likely that you may have a single letter hiding out somewhere that needs to be removed!\u003c/p\u003e\n\n\u003ch3\u003eCategorical Data Encoded as Integers\u003c/h3\u003e\n\n\u003cp\u003eIt's also common to see categorical data encoded as integers.  Given that a big step in the data cleaning process is to convert all categorical columns to numeric equivalents, this may not seem like a problem at first glance.  However, leaving categorical data encoded as integers can have a negative effect by introducing bad information into our model. This is because integer encoding mistakenly adds mathematical relationships between the different categories -- our model may mistakenly think that the category represented by the integer \u003ccode\u003e4\u003c/code\u003e twice as much as category \u003ccode\u003e2\u003c/code\u003e, and so on.  \u003c/p\u003e\n\n\u003cp\u003eThe best way of dealing with this problem is to cast the entire column to a string data type, which will better represent the column's categorical nature. Since it's categorical, we can correctly deal with it when we one-hot encode categorical data later in the process.\u003c/p\u003e\n\n\u003cp\u003eThe following example shows the syntax necessary for converting a column from one data type to another:\u003c/p\u003e\n\n\u003cpre\u003e\u003ccode class=\"python\"\u003edf['some_column'] = df.['some_column'].astype('float32')\n\ndf['some_column'] = df.['some_column'].astype('str')\n\u003c/code\u003e\u003c/pre\u003e\n\n\u003cp\u003eOnce done, it is then common to pass these categorical variables to another function such as \u003ccode\u003epd.get_dummies()\u003c/code\u003e in order to transform these features into representations that are more suitable for machine learning algorithms. It may be necessary to drop the first dummy to avoid the dummy variable trap.\u003c/p\u003e\n\n\u003ch2\u003eDetecting and Dealing With Null Values\u003c/h2\u003e\n\n\u003cp\u003eAnother important data cleaning check is to inspect for missing or null values. Recall from previous labs that Pandas denotes missing values as \u003ccode\u003eNaN\u003c/code\u003e.\u003c/p\u003e\n\n\u003ch3\u003eChecking For \u003ccode\u003eNaN\u003c/code\u003es\u003c/h3\u003e\n\n\u003cp\u003eYou can easily check how many missing values are contained within each column by having Pandas create a truth table where the cells that contain \u003ccode\u003eNaN\u003c/code\u003e are marked as \u003ccode\u003eTrue\u003c/code\u003e and everything else is marked as \u003ccode\u003eFalse\u003c/code\u003e.\u003c/p\u003e\n\n\u003cpre\u003e\u003ccode class=\"python\"\u003edf.isna()\n\u003c/code\u003e\u003c/pre\u003e\n\n\u003cp\u003eSince \u003ccode\u003eFalse=0\u003c/code\u003e and \u003ccode\u003eTrue=1\u003c/code\u003e in programming, you can then \u003ccode\u003esum()\u003c/code\u003e these truth tables to get a column-by-column count of the number of missing values in the dataset. \u003c/p\u003e\n\n\u003cpre\u003e\u003ccode class=\"python\"\u003edf.isna().sum()\n\u003c/code\u003e\u003c/pre\u003e\n\n\u003cp\u003eAs noted above, remember that your dataset may also contain null values that are denoted by placeholder values.  Most datasets that do this will make mention of this in the dataset's data dictionary. However, you may also see these denoted by extreme values that don't make sense (e.g. a person's weight being set to something like 0 or 10000).  Doing a quick manual inspection of the top values for each feature is often the only manner to detect such anomalies.\u003c/p\u003e\n\n\u003ch3\u003eDealing With Null Values\u003c/h3\u003e\n\n\u003cp\u003eThere are several options for dealing with null values. You can always remove observation rows with missing values or similarly remove features with excessive sparsity caused by null values. That said, doing so throws away potentially valuable information. There may be important reasons why said information is missing. Despite this, many machine learning algorithms will not tolerate null values and as such you either have to impute values or drop the data. Some options you have for imputing data include:\u003c/p\u003e\n\n\u003cp\u003e\u003cstrong\u003e\u003cem\u003eNumeric Data\u003c/em\u003e\u003c/strong\u003e\n* Replacing Nulls with the column median\n* Binning the data and converting columns to a categorical format (\u003cem\u003eCoarse Classification)\u003c/em\u003e\u003c/p\u003e\n\n\u003cp\u003e\u003cstrong\u003e\u003cem\u003eCategorical Data\u003c/em\u003e\u003c/strong\u003e\n* Making Null values their own category\n* Replacing null values with the most common category \u003c/p\u003e\n\n\u003cp\u003eAs a data scientist, you will have to decide which method of imputation is appropriate for your particular data set and business objective.\u003c/p\u003e\n\n\u003ch2\u003eChecking For Multicollinearity\u003c/h2\u003e\n\n\u003cp\u003eBefore proceeding to modeling, you also want to check that the data does not have high multicollinearity or correlation/covariance between predictor columns.  \u003c/p\u003e\n\n\u003cp\u003eThe easiest way to do this to build and interpret a correlation heatmap with the \u003ccode\u003eseaborn\u003c/code\u003e package. \u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-scrubbing-and-cleaning-data/master/images/heatmap.png\"\u003e\u003c/p\u003e\n\n\u003cp\u003eColumns with strong correlation should be dealt with by removing one of the offending columns, or by combining the columns through feature engineering (more on this later in the curriculum). After all, highly correlated features makes feature weights unstable and also impede model interpretability. That said, they are not apt to reduce model performance if that is the sole consideration.\u003c/p\u003e\n\n\u003cp\u003eThe \u003ca href=\"https://seaborn.pydata.org/examples/many_pairwise_correlations.html\"\u003eseaborn documentation\u003c/a\u003e provides a great example code on how to build a correlation heatmap with data stored in a Pandas DataFrame. \u003c/p\u003e\n\n\u003ch2\u003eNormalizing Data\u003c/h2\u003e\n\n\u003cp\u003eAn important step during the data cleaning process is to convert all of our data to the same scale by \u003cstrong\u003e\u003cem\u003enormalizing\u003c/em\u003e\u003c/strong\u003e it.  \u003c/p\u003e\n\n\u003cp\u003eThe most common form of data normalization is by converting data to z-scores. This is commonly referred to as standardization.\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=z=%20%5Cdfrac%7Bx-%5Cmu%7D%7B%5Csigma%7D\"\u003e \n\u003cbr\u003e\n \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cmu%20=%20%5Ctext%7BMean%7D\"\u003e \n\u003cbr\u003e\n \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Csigma%20=%20%5Ctext%7BStandard%20Deviation%7D\"\u003e \u003c/p\u003e\n\n\u003cp\u003eThere are also other sorts of scaling methods we can use, such as \u003cstrong\u003e\u003cem\u003eMin-Max normalization\u003c/em\u003e\u003c/strong\u003e:\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=z=%20%5Cdfrac%7Bx-%5Cmin(x)%7D%7B%5Cmax(x)-%5Cmin(x)%7D\"\u003e \u003c/p\u003e\n\n\u003ch2\u003eSummary\u003c/h2\u003e\n\n\u003cp\u003ePhew! That was quite a bit! There's a lot to consider when cleaning your data. Here, you explored casting data to the appropriate data types, identifying and correcting null values, and removing features that exhibit multi-collinearity. While applying this workflow, be sure to stay on your toes and try to wrap your head around the context of the data: Does the current representation seem sensible? Are there any anomalies within it? Cleaning data is always a tricky process and while aspects can be fairly standard, having an inquisitive approach goes a long way.\u003c/p\u003e","exportId":"scrubbing-and-cleaning-data"},{"id":141807,"title":"Scrubbing and Cleaning Data - Lab","type":"Assignment","indent":1,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":null,"completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-scrubbing-and-cleaning-data-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-scrubbing-and-cleaning-data-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","exportId":"g1368b3f1261b26d9ea0ff9bd53be9125"},{"id":141811,"title":"Exploring Your Data","type":"WikiPage","indent":0,"locked":false,"requirement":null,"completed":false,"content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-exploring-your-data\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-exploring-your-data\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-exploring-your-data/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eIn this lesson you'll learn about performing an EDA task, using all the statistical and visual EDA skills you have learned so far. \u003c/p\u003e\n\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\n\u003cp\u003eYou will be able to:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eExamine the descriptive statistics of our data set\u003c/li\u003e\n\u003cli\u003eCreate visualizations to better understand the distributions of variables in a dataset\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch2\u003eExploratory Data Analysis\u003c/h2\u003e\n\n\u003cp\u003eExploratory Data Analysis, or \u003cstrong\u003e\u003cem\u003eEDA\u003c/em\u003e\u003c/strong\u003e, is a crucial part of any Data Science project.  Before you can go off building models on a dataset, you need to be familiar with the actual data it contains -- otherwise, you'll have no intuition about how to interpret the results of these models, or even if you can trust them at all!\u003c/p\u003e\n\n\u003cp\u003eThis lesson will outline the basic steps that should be taken -- and questions that should be answered during EDA. \u003c/p\u003e\n\n\u003ch2\u003eUnderstanding the Distribution of the Dataset\u003c/h2\u003e\n\n\u003cp\u003eOne of the foundational pieces of an EDA investigation is to understand the underlying distribution of the data.  Often, some of the most interesting/important business insights come not from machine learning models, but simply from exploring the distribution of the dataset! If your company or organization has not yet mastered reporting on descriptive analytics, the insights gained here can be invaluable to company strategy -- think questions such as \"who is my most profitable customer segment?\" or \"is there a seasonality to our customer churn rate?\".  These are important questions to any business, and they don't require machine learning models to answer them -- just some basic visualizations, and the ability to ask good questions.\u003c/p\u003e\n\n\u003cp\u003eGetting a feel for the distribution of a dataset is done in a few different ways. Generally, you'll make use of high-level descriptive statistics, followed by visualizations. During the EDA process, it is quite common to uncover interesting things in the data that spur further questions for the investigation.  \u003c/p\u003e\n\n\u003cblockquote\u003e\n\u003cp\u003e\"The most exciting phrase to hear in science, the one that heralds new discoveries, is not 'Eureka!' (I found it!) but 'That's funny...'\"\u003c/p\u003e\n\n\u003cpre\u003e\u003ccode\u003e                                       - Isaac Asimov\n\u003c/code\u003e\u003c/pre\u003e\n\u003c/blockquote\u003e\n\n\u003cp\u003eRecall that Pandas can easily provide descriptive statistics of a DataFrame by using the DataFrame class's built-in \u003ccode\u003e.describe()\u003c/code\u003e method.  The resulting output is a table containing information such as the count, mean, median, min, max, and quartile values for every column in the DataFrame.  This is especially handy for answering questions such as \"how much variance can I expect in column {X}?\"\u003c/p\u003e\n\n\u003ch3\u003eVisualizing Distributions - Histograms\u003c/h3\u003e\n\n\u003cp\u003eThe easiest way to understand the distribution of a dataset is to visualize it! Recall that since \u003ccode\u003epandas\u003c/code\u003e uses the \u003ccode\u003ematplotlib\u003c/code\u003e library, you can easily create histograms showing the distribution of each column by using the DataFrame's built-in \u003ccode\u003e.hist()\u003c/code\u003e method.  \u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-exploring-your-data/master/images/sample_hist.png\"\u003e\u003c/p\u003e\n\n\u003ch3\u003eVisualizing Distributions - Kernel Density Estimation (KDE) Plots\u003c/h3\u003e\n\n\u003cp\u003eAnother great way of quickly visualizing the distribution of a column is to construct a \u003cstrong\u003e\u003cem\u003eKDE Plot\u003c/em\u003e\u003c/strong\u003e. This is often overlaid on a histogram to create a line that visualizes an approximate probability density of the variable. \u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-exploring-your-data/master/images/sample_kde.png\"\u003e\u003c/p\u003e\n\n\u003ch3\u003eUsing Joint Plots\u003c/h3\u003e\n\n\u003cp\u003eA more advanced visualization tool you can make use of is the \u003cstrong\u003e\u003cem\u003eJoint Plot\u003c/em\u003e\u003c/strong\u003e.  This allows you to visualize a scatterplot, the distributions of two different columns, a \u003ca href=\"https://seaborn.pydata.org/generated/seaborn.kdeplot.html\"\u003eKDE plot\u003c/a\u003e, and even a simple regression line all on the same visualization. In practice, this is incredibly handy for doing this like checking the linearity assumption between predictors and a target variable during a regression analysis. \u003c/p\u003e\n\n\u003cp\u003eSince joint plots are more advanced than a basic visualization like a histogram or scatterplot, you'll need to make use of the \u003cstrong\u003e\u003cem\u003eseaborn\u003c/em\u003e\u003c/strong\u003e library to create them. The syntax for creating a joint plot is:\u003c/p\u003e\n\n\u003cpre\u003e\u003ccode class=\"python\"\u003esns.jointplot(x= \u0026lt;column\u0026gt;, y= \u0026lt;column\u0026gt;, data=\u0026lt;dataset\u0026gt;, kind='reg')\n\n\u003c/code\u003e\u003c/pre\u003e\n\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-exploring-your-data/master/images/sample_jointplot.png\"\u003e\u003c/p\u003e\n\n\u003cp\u003eFor full details on how to create joint plots with seaborn, see the \u003ca href=\"https://seaborn.pydata.org/generated/seaborn.jointplot.html\"\u003eseaborn documentation on joint plots\u003c/a\u003e!\u003c/p\u003e\n\n\u003ch2\u003eInterpreting Your EDA Results\u003c/h2\u003e\n\n\u003cp\u003eIt is worth noting that the goal of EDA is not pretty visualizations -- it's \u003cem\u003einsight into your data\u003c/em\u003e!  Don't fall into the trap of thinking that EDA means building a couple of quick visualizations and then moving onto modeling -- you should actively try to generate questions and see if you can answer them by exploring the dataset.  Visualizations are great, but only because they make it easy to quickly interpret our data.  Use them as a tool, not a goal, during the EDA process!\u003c/p\u003e\n\n\u003ch2\u003eSummary\u003c/h2\u003e\n\n\u003cp\u003eIn this lesson, you reviewed descriptive statistics and data visualizations -- two critical components of EDA. Specifically, you reviewed the \u003ccode\u003e.describe()\u003c/code\u003e method for obtaining the descriptive statistics of a DataFrame. You then saw how you can use data visualizations like histograms, KDE plots, and joint plots to gain some insight into your data!\u003c/p\u003e","exportId":"exploring-your-data"},{"id":141814,"title":"Exploring Your Data - Lab","type":"Assignment","indent":1,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":null,"completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-exploring-your-data-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-exploring-your-data-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","exportId":"g80702b58e5a2701f993485d209807a05"},{"id":141818,"title":"Modeling Your Data","type":"Assignment","indent":0,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":null,"completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-modeling-your-data\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-modeling-your-data/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","exportId":"g58055dbb2b7e02e813fa0517692153b1"},{"id":141822,"title":"Modeling Your Data  - Lab","type":"Assignment","indent":1,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":null,"completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-modeling-your-data-lab/\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-modeling-your-data-lab//issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","exportId":"ga55c6aaa21ffa8bde4ea2fa5aeb37635"},{"id":141826,"title":"A Complete Data Science Project Using Multiple Regression - Recap","type":"WikiPage","indent":0,"locked":false,"requirement":null,"completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-full-ds-regression-recap\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-full-ds-regression-recap/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\u003ch2\u003eKey Takeaways\u003c/h2\u003e\n\u003cp\u003eCongratulations! You're really coming along! This section gave you an opportunity to review some of the wide ranging skills you've acquired and conduct a full Data Science project! With that, you should have seen that a good Data Science process requires careful thought and not just about the technical details, but also about the general structure and story behind the data itself. Indeed, substantial business value comes from asking the right questions and persuasively communicating the results. Similarly, even when modeling, much of the predictive value comes from thoughtful selection, and creative feature engineering through exploration of the data.\u003c/p\u003e\n\u003cp\u003eTo further summarize:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eThe most common Data Science frameworks are CRISP-DM, KDD, and OSEMiN.\u003c/li\u003e\n\u003cli\u003eThe process of finding, filtering, and loading the appropriate data to answer a question is non-trivial.\u003c/li\u003e\n\u003cli\u003eDecisions made in the data munging/scrubbing phase can have a huge impact on the accuracy of your predictions.\u003c/li\u003e\n\u003cli\u003eVisualization is a key phase in EDA.\u003c/li\u003e\n\u003cli\u003eAnalyzing regression models:\n\u003cul\u003e\n\u003cli\u003eCheck p-values to determine whether features are significant\u003c/li\u003e\n\u003cli\u003eUse Q-Q plots to check for normality\u003c/li\u003e\n\u003cli\u003ePlot residuals against the target variable to check for homoscedasticity (and rule out heteroscedasticity)\u003c/li\u003e\n\u003cli\u003eUse the Variance Inflation Factor to assess Multicollinearity among independent variables\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003eSummary\u003c/h2\u003e\n\u003cp\u003eAgain, well done! You put a lot of your skills to work and went through a full process of collecting data, cleaning it, analyzing and using it to answer questions.\u003c/p\u003e\n\u003cfooter class=\"fis-footer\" style=\"visibility: hidden;\"\u003e\n\u003cdiv class=\"fis-feedback\"\u003e\n\u003ch5\u003eHow do you feel about this lesson?\u003c/h5\u003e\n\u003cimg id=\"thumbs-up\" title=\"Thumbs up!\" alt=\"thumbs up\" data-repository=\"dsc-full-ds-regression-recap\"\u003e\u003cimg id=\"thumbs-down\" title=\"Thumbs down!\" alt=\"thumbs down\" data-repository=\"dsc-full-ds-regression-recap\"\u003e\n\u003c/div\u003e\n\u003ch5\u003eHave specific feedback? \u003ca href=\"https://github.com/learn-co-curriculum/dsc-full-ds-regression-recap/issues/new\"\u003eTell us here!\u003c/a\u003e\n\u003c/h5\u003e\n\u003c/footer\u003e","exportId":"a-complete-data-science-project-using-multiple-regression-recap"}]},{"id":15124,"name":"APPENDIX: Monte Carlo","status":"completed","unlockDate":null,"prereqs":[],"requirement":null,"sequential":false,"exportId":"g4ee22abf593bce8c6cb238cf49324142","items":[{"id":141776,"title":"(Re)sampling: Monte Carlo Simulations","type":"Assignment","indent":0,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":null,"completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-monte-carlo-simulations\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-monte-carlo-simulations/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","exportId":"g8c8215de715edad3a378b8c8c9b4cbdb"},{"id":141780,"title":"(Re)sampling: Monte Carlo Simulations - Lab","type":"Assignment","indent":1,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":null,"completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-monte-carlo-simulations-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-monte-carlo-simulations-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","exportId":"gb0f67c593ab19062e27ee4db01ca929f"}]},{"id":15131,"name":"🏆 Online Milestones Instructions","status":"completed","unlockDate":null,"prereqs":[],"requirement":null,"sequential":false,"exportId":"g975efdd44bee8f32c6d7c0a2e13bd7b5","items":[{"id":141847,"title":"Blogging Overview","type":"WikiPage","indent":0,"locked":false,"requirement":null,"completed":false,"content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-blogging-overview\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-blogging-overview\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-blogging-overview/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e  \u003ch2\u003eIntroduction\u003c/h2\u003e  \u003cp\u003eIn this lesson, we discuss how to write good blog posts that meet Flatiron School's requirements.\u003c/p\u003e  \u003ch2\u003eObjectives\u003c/h2\u003e  \u003cp\u003eThis lesson covers...\u003c/p\u003e  \u003cul\u003e \u003cli\u003eWhy blogging is valuable\u003c/li\u003e \u003cli\u003eTopics to blog about\u003c/li\u003e \u003cli\u003eWhat makes for a good blog post\u003c/li\u003e \u003cli\u003eHow to start your blog\u003c/li\u003e \u003cli\u003eFlatiron School blog requirements \u003c/li\u003e \u003c/ul\u003e  \u003ch2\u003eWhy Should I Blog?\u003c/h2\u003e  \u003cp\u003eBlogging has many benefits:\u003c/p\u003e  \u003cul\u003e \u003cli\u003e\u003cp\u003e\u003cstrong\u003eDevelop your written communication skills.\u003c/strong\u003e Your writing ability will be critical to your success when completing job applications and presenting your work to colleagues. Blogging is great practice for identifying and clearly communicating the most important points of any subject.\u003c/p\u003e\u003c/li\u003e \u003cli\u003e\u003cp\u003e\u003cstrong\u003eDemonstrate your talent to employers.\u003c/strong\u003e Potential employers will review your blog to determine whether to offer you an interview or a job. Some students have even been invited to interview or exempted from technical interviews based on their blogs.\u003c/p\u003e\u003c/li\u003e \u003cli\u003e\u003cp\u003e\u003cstrong\u003eStrengthen your knowledge.\u003c/strong\u003e Blogging helps you explore new topics, deepen your understanding, and crystallize what you've learned.\u003c/p\u003e\u003c/li\u003e \u003cli\u003e\u003cp\u003e\u003cstrong\u003eHelp your peers and the broader community.\u003c/strong\u003e Have you ever Googled a question you had and found the answer on a blog? Writing blog posts helps others who are following in your footsteps!\u003c/p\u003e\u003c/li\u003e \u003c/ul\u003e  \u003ch2\u003eWhat Should I Blog About?\u003c/h2\u003e  \u003cp\u003eHere are some blog topic ideas:\u003c/p\u003e  \u003cul\u003e \u003cli\u003e\u003cp\u003eWhy did you decide to learn data science?\u003c/p\u003e\u003c/li\u003e \u003cli\u003e\u003cp\u003eDescribe how a DS technique works, when you might use it, and its strengths/weaknesses.\u003c/p\u003e\u003c/li\u003e \u003cli\u003e\u003cp\u003eSummarize an End of Phase Project by explaining your problem, the dataset, your methodology, and your results.\u003c/p\u003e\u003c/li\u003e \u003cli\u003e\u003cp\u003eDive into something that you want to learn more about, maybe because you find it challenging or it wasn't covered in the course.\u003c/p\u003e\u003c/li\u003e \u003cli\u003e\u003cp\u003eWrite a tutorial to help aspiring data scientists to implement a tool or method.\u003c/p\u003e\u003c/li\u003e \u003cli\u003e\u003cp\u003eFind an interesting data science paper and summarize why it is important. This can be a new paper from the past few months, or you can refer to \u003ca href=\"https://docs.google.com/spreadsheets/d/1UYmAT13AAknrOatzLeeAsN4tS7ENjn2fpJNGzOZ67rQ/edit?usp=sharing\"\u003ethis spreadsheet\u003c/a\u003e.\u003c/p\u003e\u003c/li\u003e \u003c/ul\u003e  \u003ch2\u003eWhat Does A Good Blog Post Look Like?\u003c/h2\u003e  \u003cp\u003eWe recommend you take a look at our \u003ca href=\"https://drive.google.com/drive/folders/1UBiRCRLzVP5CHU3PJNwoMZAe3ajUBm2a?usp=sharing\"\u003eblog templates\u003c/a\u003e and \u003ca href=\"https://docs.google.com/document/d/1eqL8Dsj7dH7s_MRnf_4-3kCiSz72POHTfb-sBRN5Zhs/edit?usp=sharing\"\u003eexamples\u003c/a\u003e to get an idea for what makes a blog post good.\u003c/p\u003e  \u003cul\u003e \u003cli\u003e\u003cp\u003eStrike a balance between providing a meaningful investigation of your topic and being concise. Constrain the scope so it will be interesting and digestible in about 1000-3000 words (this is not a firm limit).\u003c/p\u003e\u003c/li\u003e \u003cli\u003e\n\u003cp\u003eUse clear and consistent formatting to make your content accessible and professional-looking.\u003c/p\u003e  \u003cul\u003e \u003cli\u003eWhen presenting code, use code snippets instead of screenshots.\u003c/li\u003e \u003cli\u003eMake URLs into hyperlinks that are easy for readers to click into.\u003c/li\u003e \u003cli\u003eUse headings to provide structure and flow to your post.\u003c/li\u003e \u003c/ul\u003e\n\u003c/li\u003e \u003cli\u003e\u003cp\u003eCite and link to resources you used to write your post.\u003c/p\u003e\u003c/li\u003e \u003c/ul\u003e  \u003ch2\u003eHow Do I Start My Blog?\u003c/h2\u003e  \u003cp\u003eIf you already have a professional blog that you'd like to use for your data science content, you can add your posts to that. Otherwise, you will need to start a new blog. If you have a personal blog, you should avoid using it for this purpose so that you can continue using it for personal content without worrying about how it might be perceived by potential employers.\u003c/p\u003e  \u003cp\u003eThere are multiple blogging platforms to choose from that make it easy to start a blog, here are some of our favorites:\u003c/p\u003e  \u003cul\u003e \u003cli\u003e\u003ca href=\"https://www.blogger.com/\"\u003eBlogger\u003c/a\u003e\u003c/li\u003e \u003cli\u003e\u003ca href=\"https://dev.to/\"\u003edev.to\u003c/a\u003e\u003c/li\u003e \u003cli\u003e\u003ca href=\"https://pages.github.com/\"\u003eGitHub Pages\u003c/a\u003e\u003c/li\u003e \u003cli\u003e\u003ca href=\"https://medium.com/\"\u003eMedium\u003c/a\u003e\u003c/li\u003e \u003cli\u003e\u003ca href=\"https://wordpress.com/\"\u003eWordpress\u003c/a\u003e\u003c/li\u003e \u003c/ul\u003e  \u003cp\u003eDifferent platforms have different pros and cons, so do a little research to decide what is best for you.\u003c/p\u003e  \u003ch2\u003eBlog Requirements\u003c/h2\u003e  \u003cp\u003eTo succeed in your career transition and graduate from Flatiron School, you must complete the following activities. These requirements are designed to give you the best opportunity to deepen your knowledge, practice communication skills, and showcase yourself to potential employers.\u003c/p\u003e  \u003cul\u003e \u003cli\u003e\u003cp\u003eSet up a publicly accessible blog \u003c/p\u003e\u003c/li\u003e \u003cli\u003e\u003cp\u003ePublish at least four blog posts on it, including \u003cstrong\u003eone per Phase for Phases 1-4\u003c/strong\u003e\u003c/p\u003e\u003c/li\u003e \u003cli\u003e\n\u003cp\u003eSubmit URLs to your posts \u003cstrong\u003eby the end of each Phase\u003c/strong\u003e in the Blog Post assignments\u003c/p\u003e  \u003cul\u003e \u003cli\u003eThese assignments are located in the Milestones topics of the Phase 1-4 Canvas courses\u003c/li\u003e \u003c/ul\u003e\n\u003c/li\u003e \u003cli\u003e\n\u003cp\u003eWrite blog posts that...\u003c/p\u003e  \u003cul\u003e \u003cli\u003eDiscuss data science topics\u003c/li\u003e \u003cli\u003eAre composed primarily of original material you wrote\u003c/li\u003e \u003cli\u003eInclude proper attribution\u003c/li\u003e \u003cli\u003eHave high-quality content and formatting\u003c/li\u003e \u003cli\u003eAre something you would proudly show to a potential employer\u003c/li\u003e \u003c/ul\u003e\n\u003c/li\u003e \u003c/ul\u003e  \u003cp\u003eAfter you submit your blog posts, your teacher will grade them as Complete or Incomplete. Your blogs must all be submitted on time and receive Complete grades in order to continue through your program.\u003c/p\u003e  \u003cp\u003e✨Have fun and happy blogging!✨\u003c/p\u003e","exportId":"blogging-overview"},{"id":141850,"title":"Project Submission \u0026 Review (Online)","type":"WikiPage","indent":0,"locked":false,"requirement":null,"completed":false,"content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-project-submissions-online\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-project-submissions-online\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-project-submissions-online/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e  \u003ch2\u003eIntroduction\u003c/h2\u003e  \u003cp\u003eIn this lesson, we review the requirements, submission, and review process for the Phase Projects.\u003c/p\u003e  \u003ch2\u003eObjectives\u003c/h2\u003e  \u003cp\u003eYou will be able to:\u003c/p\u003e  \u003cul\u003e \u003cli\u003eCreate project deliverables that meet Flatiron School requirements\u003c/li\u003e \u003cli\u003eSubmit your project deliverables in Canvas\u003c/li\u003e \u003cli\u003ePrepare for your project review\u003c/li\u003e \u003c/ul\u003e  \u003ch2\u003eSchedule Your Review ASAP\u003c/h2\u003e  \u003cp\u003e\u003cstrong\u003eReach out to an instructor immediately via Slack to let them know you've started your project and schedule your project review.\u003c/strong\u003e If you're not sure who to schedule with, please ask in your cohort channel in Slack.\u003c/p\u003e  \u003ch2\u003eCreate Your Project Deliverables\u003c/h2\u003e  \u003cp\u003eComplete the deliverables for your project, guided by the rubric at the bottom of the main project assignment. Keep in mind that the audience for these deliverables is not only your teacher, but also potential employers. Employers will look at your project deliverables to evaluate multiple skills, including coding, modeling, communication, and domain knowledge. You will want to polish these as much as you can, both during the course and afterwards.\u003c/p\u003e  \u003ch3\u003eGitHub Repository\u003c/h3\u003e  \u003cp\u003eYour GitHub repository is the public-facing version of your project that your instructors and potential employers will see - make it as accessible as you can. At a minimum, it should contain all your project files and a README.md file that summarizes your project and helps visitors navigate the repository.\u003c/p\u003e  \u003ch3\u003eJupyter Notebook\u003c/h3\u003e  \u003cp\u003eYour Jupyter Notebook is the primary source of information about your analysis. At a minimum, it should contain or import all of the code used in your project and walk the reader through your project from start to finish. You may choose to use multiple Jupyter Notebooks in your project, but you should have one that provides a full project overview as a point of entry for visitors.\u003c/p\u003e  \u003ch3\u003eNon-Technical Presentation\u003c/h3\u003e  \u003cp\u003eYour non-technical presentation is your opportunity to communicate clearly and concisely about your project and it's real-world relevance. The target audience should be people with limited technical knowledge who may be interested in leveraging your project. We recommend using Google Slides, PowerPoint or Keynote to create your presentation slides. You will then record yourself delivering the presentation.\u003c/p\u003e  \u003ch2\u003eSubmit Your Project\u003c/h2\u003e  \u003cp\u003eTo submit your project in Canvas, you will create and upload PDF versions of three project deliverables, then upload a recording of your video presentation. You will also submit the URL to your GitHub repository in a separate assignment.\u003c/p\u003e  \u003ch3\u003ePresentation Slides PDF Creation\u003c/h3\u003e  \u003col\u003e \u003cli\u003eExport your presentation as a PDF from the program in which you created it.\u003c/li\u003e \u003cli\u003eGive it a short descriptive file name (e.g. \u003ccode\u003epresentation.pdf\u003c/code\u003e).\u003c/li\u003e \u003cli\u003ePlace a copy of the PDF in your GitHub repository.\u003c/li\u003e \u003c/ol\u003e  \u003ch3\u003eGitHub Repository PDF Creation\u003c/h3\u003e  \u003col\u003e \u003cli\u003eNavigate to the root directory of your project repository on GitHub, using your browser (we recommend Google Chrome).\u003c/li\u003e \u003cli\u003eSave the webpage as a PDF using the browser's Print functionality (\u003ca href=\"https://www.wikihow.com/Save-a-Web-Page-as-a-PDF-in-Google-Chrome\"\u003eGoogle Chrome Save to PDF instructions\u003c/a\u003e)\u003c/li\u003e \u003cli\u003eGive it a short descriptive file name (e.g. \u003ccode\u003egithub.pdf\u003c/code\u003e).\u003c/li\u003e \u003c/ol\u003e  \u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-project-submissions-online/master/repo_pdf.gif\" alt=\"Repository PDF Creation\"\u003e\u003c/p\u003e  \u003ch3\u003eJupyter Notebook PDF Creation\u003c/h3\u003e  \u003col\u003e \u003cli\u003eOpen your Notebook in your browser (we recommend Google Chrome).\u003c/li\u003e \u003cli\u003e\n\u003cstrong\u003eRun the Notebook from start to finish\u003c/strong\u003e so that your output is visible.\u003c/li\u003e \u003cli\u003eSave the page as a PDF using the browser's Print functionality (\u003ca href=\"https://www.wikihow.com/Save-a-Web-Page-as-a-PDF-in-Google-Chrome\"\u003eGoogle Chrome Save to PDF instructions\u003c/a\u003e)\u003c/li\u003e \u003cli\u003eGive it a short descriptive file name (e.g. \u003ccode\u003enotebook.pdf\u003c/code\u003e).\u003c/li\u003e \u003c/ol\u003e  \u003cp\u003eIf you have difficulty creating a PDF version of your notebook, you can use \u003ca href=\"https://htmtopdf.herokuapp.com/ipynbviewer/\"\u003ethis tool\u003c/a\u003e instead. Set the ‘Results Format’ to “HTML + PDF”. Then click ‘View and Convert’. Once it’s done, you should see links to .html and .pdf versions above the ‘View and Convert’ button.\u003c/p\u003e  \u003ch3\u003ePDF Submission in Canvas\u003c/h3\u003e  \u003cp\u003eYou will need to submit all three PDF files as a single submission:\u003c/p\u003e  \u003col\u003e \u003cli\u003eClick \"Submit Assignment\" at the top of the \"Phase X Project\" assignment in the \"Milestones\" topic.\u003c/li\u003e \u003cli\u003eIn the \"File Upload\" box, click \"Choose File\" button to upload a single file.\u003c/li\u003e \u003cli\u003eClick the \"Add Another File\" link to upload an additional file.\u003c/li\u003e \u003cli\u003eRepeat Step 3 to upload one more file. After this is done, all three files should be uploaded.\u003c/li\u003e \u003cli\u003eHit the blue \"Submit Assignment\" button.\u003c/li\u003e \u003c/ol\u003e  \u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-project-submissions-online/master/project_3pdf_submission.gif\" alt=\"Project PDF Submission\"\u003e\u003c/p\u003e  \u003ch3\u003ePresentation Recording and Submission\u003c/h3\u003e  \u003cp\u003eAfter you've submitted the PDF files for the project assignment, you will upload a recording of your presentation as a media comment on your submission:\u003c/p\u003e  \u003col\u003e \u003cli\u003eRecord your live presentation to a video file on your computer. We recommend using Zoom to record your live presentation to a local video file (\u003ca href=\"https://support.zoom.us/hc/en-us/articles/201362473-Local-recording\"\u003einstructions here\u003c/a\u003e). Video files must be under 500 MB and formatted as 3GP, ASF, AVI, FLV, M4V, MOV, MP4, MPEG, QT, or WMV.\u003c/li\u003e \u003cli\u003eClick \"Submission Details\" on the top right of the \"Phase X Project\" assignment in the \"Milestones\" topic.\u003c/li\u003e \u003cli\u003eClick \"Media Comment\" beneath the \"Add a Comment\" box on the right of the page.\u003c/li\u003e \u003cli\u003eClick \"Upload Media\" and \"Select Video File\" to upload your file.\u003c/li\u003e \u003cli\u003eThe thumbnail for your video will appear as a blue rectangle while Zoom processes your file - return to this page later to confirm that your recording uploaded successfully.\u003c/li\u003e \u003c/ol\u003e  \u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-project-submissions-online/master/video_recording_submission.gif\" alt=\"Video Recording Submission\"\u003e\u003c/p\u003e  \u003ch3\u003eURL Submission in Canvas\u003c/h3\u003e  \u003cp\u003eThere is an additional Canvas assignment where you will just enter the URL for your project's GitHub repository. This is located in the \"Milestones\" topic in this course as \"Phase X Project - GitHub Repository URL.\"\u003c/p\u003e  \u003ch2\u003ePrepare For Project Review\u003c/h2\u003e  \u003cp\u003eProject reviews are focused on preparing you for technical interviews. Treat project reviews as if they were technical interviews, in both attitude and technical presentation \u003cem\u003e(sometimes technical interviews will feel arbitrary or unfair - if you want to get the job, commenting on that is seldom a good choice)\u003c/em\u003e.\u003c/p\u003e  \u003cp\u003eThe project review is comprised of a 45 minute 1:1 session with one of the instructors. During your project review, be prepared to:\u003c/p\u003e  \u003ch3\u003e1. Deliver your PDF presentation to a non-technical stakeholder.\u003c/h3\u003e  \u003cp\u003eIn this phase of the review (~10 mins) your instructor will play the part of a non-technical stakeholder that you are presenting your findings to. The presentation  should not exceed 5 minutes, giving the \"stakeholder\" 5 minutes to ask questions.\u003c/p\u003e  \u003cp\u003eIn the first half of the presentation (2-3 mins), you should summarize your methodology in a way that will be comprehensible to someone with no background in data science and that will increase their confidence in you and your findings. In the second half (the remaining 2-3 mins) you should summarize your findings and be ready to answer a couple of non-technical questions from the audience. The questions might relate to technical topics (sampling bias, confidence, etc) but will be asked in a non-technical way and need to be answered in a way that does not assume a background in statistics or machine learning. You can assume a smart, business stakeholder, with a non-quantitative college degree.\u003c/p\u003e  \u003ch3\u003e2. Go through the Jupyter Notebook, answering questions about how you made certain decisions. Be ready to explain things like:\u003c/h3\u003e \u003cpre\u003e\u003ccode\u003e* \"How did you pick the question(s) that you did?\"\u003cbr\u003e* \"Why are these questions important from a business perspective?\"\u003cbr\u003e* \"How did you decide on the data cleaning options you performed?\"\u003cbr\u003e* \"Why did you choose a given method or library?\"\u003cbr\u003e* \"Why did you select those visualizations and what did you learn from each of them?\"\u003cbr\u003e* \"Why did you pick those features as predictors?\"\u003cbr\u003e* \"How would you interpret the results?\"\u003cbr\u003e* \"How confident are you in the predictive quality of the results?\"\u003cbr\u003e* \"What are some of the things that could cause the results to be wrong?\" \u003c/code\u003e\u003c/pre\u003e \u003cp\u003eThink of the first phase of the review (~30 mins) as a technical boss reviewing your work and asking questions about it before green-lighting you to present to the business team. You should practice using the appropriate technical vocabulary to explain yourself. Don't be surprised if the instructor jumps around or sometimes cuts you off - there is a lot of ground to cover, so that may happen.\u003c/p\u003e  \u003cp\u003eIf any requirements are missing or if significant gaps in understanding are uncovered, be prepared to do one or all of the following: * Perform additional data cleanup, visualization, feature selection, modeling and/or model validation * Submit an improved version * Meet again for another Project Review\u003c/p\u003e  \u003cp\u003eWhat won't happen: * You won't be yelled at, belittled, or scolded * You won't be put on the spot without support * There's nothing you can do to instantly fail or blow it\u003c/p\u003e  \u003ch2\u003eGrading\u003c/h2\u003e  \u003cp\u003eYour teacher will use the rubric at the bottom of the main project assignment to grade your project. In order to pass, you must properly submit your project and score \"Accomplished\" or \"Exemplary\" on nearly all rubric elements. You will receive a score of P (Pass) or NP (No Pass) - you must pass in order to move to the next phase with your cohort. Your teacher will grade your submission sometime after your review.\u003c/p\u003e  \u003ch2\u003eConclusion\u003c/h2\u003e  \u003cp\u003eThank you for your hard work on this project - you're going to do great! Remember that future employers will also look at your projects when deciding whether to hire you, so having complete, polished projects will help you tremendously not only to pass this assignment, but also to get the job you want after you graduate.\u003c/p\u003e  \u003cp\u003eIf you have any questions about the project submission or review process, don't hesitate to ask your teacher.\u003c/p\u003e","exportId":"project-submission-and-review-online"},{"id":182501,"title":"Phase 2 Project Checklist and Guidance","type":"ExternalUrl","indent":0,"locked":false,"requirement":null,"completed":false,"content":"https://docs.google.com/document/d/1n8BfoVl0OCmwuIm8wNzahZNGYDEc8JXJl9Oyuw8NLnM/edit"}]},{"id":15134,"name":"🏆 Milestones","status":"completed","unlockDate":null,"prereqs":[],"requirement":null,"sequential":false,"exportId":"gab242c94d3dd60de7eefcc27240a951f","items":[{"id":141862,"title":"Phase 2 Project","type":"Assignment","indent":0,"locked":false,"submissionTypes":"a file upload","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":null,"completed":false,"content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-phase-2-project\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-phase-2-project\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-phase-2-project/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\u003cp\u003eAnother module down--you're almost half way there!\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-phase-2-project-campus/master/halfway-there.gif\" alt=\"awesome\"\u003e\u003c/p\u003e\n\n\u003cp\u003eAll that remains in Phase 2 is to put our newfound data science skills to use with a large project! This project should take 20 to 30 hours to complete.\u003c/p\u003e\n\n\u003ch2\u003eProject Overview\u003c/h2\u003e\n\n\u003cp\u003eFor this project, you will use regression modeling to analyze house sales in a northwestern county.\u003c/p\u003e\n\n\u003ch3\u003eThe Data\u003c/h3\u003e\n\n\u003cp\u003eThis project uses the King County House Sales dataset, which can be found in  \u003ccode\u003ekc_house_data.csv\u003c/code\u003e in the data folder in this repo. The description of the column names can be found in \u003ccode\u003ecolumn_names.md\u003c/code\u003e in the same folder. As with most real world data sets, the column names are not perfectly described, so you'll have to do some research or use your best judgment if you have questions about what the data means.\u003c/p\u003e\n\n\u003cp\u003eIt is up to you to decide what data from this dataset to use and how to use it. If you are feeling overwhelmed or behind, we recommend you ignore some or all of the following features:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003edate\u003c/li\u003e\n\u003cli\u003eview\u003c/li\u003e\n\u003cli\u003esqft_above\u003c/li\u003e\n\u003cli\u003esqft_basement\u003c/li\u003e\n\u003cli\u003eyr_renovated\u003c/li\u003e\n\u003cli\u003ezipcode\u003c/li\u003e\n\u003cli\u003elat\u003c/li\u003e\n\u003cli\u003elong\u003c/li\u003e\n\u003cli\u003esqft_living15\u003c/li\u003e\n\u003cli\u003esqft_lot15\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch3\u003eBusiness Problem\u003c/h3\u003e\n\n\u003cp\u003eIt is up to you to define a stakeholder and business problem appropriate to this dataset.\u003c/p\u003e\n\n\u003cp\u003eIf you are struggling to define a stakeholder, we recommend you complete a project for a real estate agency that helps homeowners buy and/or sell homes. A business problem you could focus on for this stakeholder is the need to provide advice to homeowners about how home renovations might increase the estimated value of their homes, and by what amount.\u003c/p\u003e\n\n\u003ch2\u003eDeliverables\u003c/h2\u003e\n\n\u003cp\u003eThere are three deliverables for this project:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eA \u003cstrong\u003eGitHub repository\u003c/strong\u003e\n\u003c/li\u003e\n\u003cli\u003eA \u003cstrong\u003eJupyter Notebook\u003c/strong\u003e\n\u003c/li\u003e\n\u003cli\u003eA \u003cstrong\u003enon-technical presentation\u003c/strong\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003eReview the \"Project Submission \u0026amp; Review\" page in the \"Milestones Instructions\" topic for instructions on creating and submitting your deliverables. Refer to the rubric associated with this assignment for specifications describing high-quality deliverables.\u003c/p\u003e\n\n\u003ch3\u003eKey Points\u003c/h3\u003e\n\n\u003cul\u003e\n\u003cli\u003e\u003cp\u003e\u003cstrong\u003eYour deliverables should explicitly address each step of the data science process.\u003c/strong\u003e Refer to \u003ca href=\"https://github.com/learn-co-curriculum/dsc-data-science-processes\"\u003ethe Data Science Process lesson\u003c/a\u003e from Topic 19 for more information about process models you can use.\u003c/p\u003e\u003c/li\u003e\n\u003cli\u003e\u003cp\u003e\u003cstrong\u003eYour Jupyter Notebook should demonstrate an iterative approach to modeling.\u003c/strong\u003e This means that you begin with a basic model, evaluate it, and then provide justification for and proceed to a new model. After you finish refining your models, you should provide 1-3 paragraphs discussing your final model - this should include interpreting at least 3 important parameter estimates or statistics.\u003c/p\u003e\u003c/li\u003e\n\u003cli\u003e\u003cp\u003e\u003cstrong\u003eBased on the results of your models, your notebook and presentation should discuss at least two features that have strong relationships with housing prices.\u003c/strong\u003e\u003c/p\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch2\u003eGetting Started\u003c/h2\u003e\n\n\u003cp\u003eStart on this project by forking and cloning \u003ca href=\"https://github.com/learn-co-curriculum/dsc-phase-2-project\"\u003ethis project repository\u003c/a\u003e to get a local copy of the dataset.\u003c/p\u003e\n\n\u003cp\u003eWe recommend structuring your project repository similar to the structure in \u003ca href=\"https://github.com/learn-co-curriculum/dsc-project-template\"\u003ethe Phase 1 Project Template\u003c/a\u003e. You can do this either by creating a new fork of that repository to work in or by building a new repository from scratch that mimics that structure.\u003c/p\u003e\n\n\u003ch2\u003eProject Submission and Review\u003c/h2\u003e\n\n\u003cp\u003eReview the \"Project Submission \u0026amp; Review\" page in the \"Milestones Instructions\" topic to learn how to submit your project and how it will be reviewed. Your project must pass review for you to progress to the next Phase.\u003c/p\u003e\n\n\u003ch2\u003eSummary\u003c/h2\u003e\n\n\u003cp\u003eThis project will give you a valuable opportunity to develop your data science skills using real-world data. The end-of-phase projects are a critical part of the program because they give you a chance to bring together all the skills you've learned, apply them to realistic projects for a business stakeholder, practice communication skills, and get feedback to help you improve. You've got this!\u003c/p\u003e","exportId":"g15af1c0230e37550fe879a92a9a300e1"},{"id":141864,"title":"Phase 2 Project - GitHub Repository URL","type":"Assignment","indent":1,"locked":false,"submissionTypes":"a website url","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":null,"completed":false,"content":"\u003cp\u003e\u003cspan\u003ePlease put the URL to your Phase 2 Project GitHub Repository here.\u0026nbsp;\u003c/span\u003e\u003c/p\u003e","exportId":"ge7423c89c672ad02c9cbc487b4bf235c"},{"id":141871,"title":"Phase 2 Blog Post","type":"Assignment","indent":0,"locked":false,"submissionTypes":"a website url","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":null,"completed":false,"content":"\u003cp\u003e\u003cspan\u003ePlease put the URL to your Phase 2 Blog Post here. \u003c/span\u003e\u003cspan\u003eRefer to the \u003c/span\u003e\u003ca title=\"Blogging Overview\" href=\"pages/blogging-overview\"\u003eBlogging Overview\u003c/a\u003e\u003cspan\u003e to learn about how to write good blog posts that\u003c/span\u003e\u003cspan style=\"font-family: inherit; font-size: 1rem;\"\u003e meet Flatiron School’s requirements.\u003c/span\u003e\u003c/p\u003e","exportId":"g339954613039d123828de5cde75c311b"}]}],"pages":[{"exportId":"inference-versus-prediction","title":"Inference versus Prediction","type":"WikiPage","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-inference-vs-prediction-v2-1\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-inference-vs-prediction-v2-1/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eIn the last few lessons, you have seen how to deal with categorical variables and why multicollinearity can be an issue with regression analysis. You also learned about log transformations, feature scaling, and normalization in order to accurately determine the coefficients of your features and to improve the model accuracy. Before we proceed further, it is important to discuss two different modeling approaches that you should keep in mind when working with data: modeling for \u003cem\u003einference\u003c/em\u003e and modeling for \u003cem\u003eprediction\u003c/em\u003e. Are you asking yourself \"aren't they the same thing\"? Well, no! In this lesson you will see why and how.\u003c/p\u003e\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eExplain the difference between modeling for inference and prediction\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003eInference\u003c/h2\u003e\n\u003cp\u003eWhen you are modeling for \u003cem\u003einference\u003c/em\u003e, you are asking the question \"How does X (independent variables or features) affect Y (dependent or target or outcome variable)?\". So, in essence, you are trying to figure out which features affect your outcome \u003cstrong\u003eand\u003c/strong\u003e how your outcome changes when these features change.\u003c/p\u003e\n\u003cp\u003eWhen modeling for \u003cem\u003einference\u003c/em\u003e, you are typically focused on only a subset of features because you are trying to understand how the outcome changes when you vary these features. As a result, great emphasis is given to the coefficients of these features as opposed to the overall accuracy of the model.\u003c/p\u003e\n\u003cp\u003eHence, when you are modeling for inference, you typically choose \u003cem\u003esimpler\u003c/em\u003e models, that is, models that are interpretable. Linear regression is a very good example of a model that is interpretable. With some basic training, anyone can understand how the features affect the outcome by observing the coefficients of these features. Some other interpretable models that you will learn later are logistic regression, decision trees, linear SVMs etc.\u003c/p\u003e\n\u003ch2\u003ePrediction\u003c/h2\u003e\n\u003cp\u003eWhen you are modeling for \u003cem\u003eprediction\u003c/em\u003e, you are asking the question \"How well can I use X (independent variables or features) to predict Y (dependent or target or outcome variable)?\" Thus, in this case, you are less concerned about how and which features impact Y as opposed to how you can efficiently use them to predict Y.\u003c/p\u003e\n\u003cp\u003eWhen modeling for \u003cem\u003eprediction\u003c/em\u003e, you typically use all available features (and most likely engineer new features) because you are trying to accurately predict Y, at all costs. As a result, you are less concerned about the coefficients of these features and instead focus on the overall accuracy of the model.\u003c/p\u003e\n\u003cp\u003eHence, when you are modeling for prediction, you typically choose more \u003cem\u003ecomplex\u003c/em\u003e models. In the upcoming modules, as you learn about various Machine Learning models, you will notice that your sole focus is on improving the predictive accuracy of your models. That is, given some data, your job will be to build a model that best predicts the future (your target variable). This can often mean you will be dealing with \u003cem\u003eblack box\u003c/em\u003e models -- models that are difficult to interpret. Given the independent variables, these models can do a great job of predicting the target, but its inner workings will be very very difficult (almost impossible) to understand. These models can include SVMs with radial kernels, random forests, neural networks, and other techniques such regularization, cross-validation, grid search etc.\u003c/p\u003e\n\u003ch2\u003eAdditional Resources\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca class=\"\" href=\"https://www.youtube.com/watch?v=5mcCOdyr5w4\"\u003eInference vs Prediction - Roger Peng\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://www.datascienceblog.net/post/commentary/inference-vs-prediction/\"\u003eInference vs Prediction - R for Data Science Blog\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003eSummary\u003c/h2\u003e\n\u003cp\u003eRemember that how you build your models depends on what question you are asking of your data:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eAre you solely interested in how you can use the data to predict the future? If so, you are most likely \u003cem\u003emodeling for prediction\u003c/em\u003e\n\u003c/li\u003e\n\u003cli\u003eOr, are you interested in understanding how a given set of features affect your outcome? If so, you are most likely \u003cem\u003emodeling for inference\u003c/em\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eDepending on what questions you ask, your modeling approaches will vary significatly, and hence it is very important to first understand the context of your problem and ask yourself what is the end goal of your analysis before you set out building any models.\u003c/p\u003e\n\u003cfooter class=\"fis-footer\" style=\"visibility: hidden;\"\u003e\n\u003cdiv class=\"fis-feedback\"\u003e\n\u003ch5\u003eHow do you feel about this lesson?\u003c/h5\u003e\n\u003cimg id=\"thumbs-up\" title=\"Thumbs up!\" alt=\"thumbs up\" data-repository=\"dsc-inference-vs-prediction-v2-1\"\u003e\u003cimg id=\"thumbs-down\" title=\"Thumbs down!\" alt=\"thumbs down\" data-repository=\"dsc-inference-vs-prediction-v2-1\"\u003e\n\u003c/div\u003e\n\u003ch5\u003eHave specific feedback? \u003ca href=\"https://github.com/learn-co-curriculum/dsc-inference-vs-prediction-v2-1/issues/new\"\u003eTell us here!\u003c/a\u003e\n\u003c/h5\u003e\n\u003c/footer\u003e","frontPage":false},{"exportId":"resampling-methods","title":"Resampling Methods","type":"WikiPage","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-resampling-methods\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-resampling-methods/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eResampling techniques are modern statistical techniques that involve taking repeated subsamples from a sample. These procedures tend to be computationally intensive, since they involve computing statistics of a subsample, creating new subsamples and repeating the process thousands or perhaps millions of times. This can allow for additional analysis of the subsamples leading to increased confidence and knowledge of the larger population. The three main techniques we will discuss here are bootstrapping, jackknife, and permutation tests.\u003c/p\u003e\n\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\n\u003cp\u003eYou will be able to:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eIdentify when resampling is used \u003c/li\u003e\n\u003cli\u003eDescribe the process of bootstrapping \u003c/li\u003e\n\u003cli\u003eDescribe permutation testing \u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch2\u003eJackknife and Bootstrapping\u003c/h2\u003e\n\n\u003cp\u003eLet's start by defining the sampling methodology for these techniques. The bootstrap method works by taking random samples with replacement from the original sample of size n. In contrast, the jackknife, the older of the two methods, works by taking samples by removing one, or more, observations at a time. Each one of these (n-1) sized sub-samples is aggregated to create the new jackknife sample. The purpose of these resampling methods is to be able to increase the size of our samples without having to actually go out and obtain more samples. Resampling methods attempt to estimate the variability of point estimators derived from the original samples.\u003c/p\u003e\n\n\u003cp\u003eThe motivating principle behind both is that by analyzing the variance of parameter estimates from these synthetic samples, we can also gauge the variance of our point estimate for the population itself. For example, we might take an original sample from our population and then use the jackknife or bootstrapping method to generate additional synthetic samples. By calculating the point estimate of interest for these synthetic samples, we can better gauge the confidence interval and variability of our original point estimator.\u003c/p\u003e\n\n\u003ch2\u003ePermutation Tests\u003c/h2\u003e\n\n\u003cp\u003eAnother related methodology is permutation tests. Permutation tests can be used in lieu of assumed parameter distributions for any statistical test. For example, we discussed the central limit theorem: that when taking the mean of a repeated sample from a population, the means of these samples will form a normal distribution. From this, we were then able to extrapolate confidence intervals surrounding our estimate for the mean of the entire population by assuming that our sample mean was from a normal distribution. This allowed us to define our confidence bands associated with various levels of type I errors which we set with  \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Calpha\"\u003e . In a hypothesis test, we used the same procedure to calculate the probability of a given sample, and based on alpha, rejected or confirmed the null hypothesis. In a permutation test, rather then assume the distribution itself and calculate p-values, we would calculate all permutations of our relabeling our data and compute the parameter statistic in question for these permutations.\u003c/p\u003e\n\n\u003cp\u003eFor example, let's say we had two samples, one with 37 observations and the other with 45 observations. We calculate the mean of both samples and wish to perform a hypothesis test with a 5% confidence interval for whether the two samples belong to the same overall population. In our previous work, we would use a t-test to perform this comparison. The permutation test alternative would be to compare the difference in these sample means to the difference in sample means of all possible permutations of 37-45 splits between our 82 data points. In other words, we compare the difference between our actual sample means to the difference in sample means between all variations of all those 82 points in order to calculate our p-values and determine whether we accept or reject the null-hypothesis.\u003c/p\u003e\n\n\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003eNote\u003c/strong\u003e: While it's called a permutation test, calculating all of the possible combinations of the observations into two groups is a more pragmatic approach. After all, you are comparing the sample means of the groups and as such the order of group members is irrelevant. When you implement permutation tests in the upcoming lab, you'll use combinations to make the problem computationally feasible. Even so, as you will see, the size of possible variations can quickly explode leading to other estimations of the permutation test, which you'll investigate towards the end of the section. \u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003ch2\u003eAdditional Resources\u003c/h2\u003e\n\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"http://hydrodictyon.eeb.uconn.edu/eebedia/images/9/9d/FelsensteinChap20.pdf\"\u003ehttp://hydrodictyon.eeb.uconn.edu/eebedia/images/9/9d/FelsensteinChap20.pdf\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://www.scss.tcd.ie/Rozenn.Dahyot/453Bootstrap/05_Permutation.pdf\"\u003ehttps://www.scss.tcd.ie/Rozenn.Dahyot/453Bootstrap/05_Permutation.pdf\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch2\u003eSummary\u003c/h2\u003e\n\n\u003cp\u003eIn this lesson, we continued discussing non-parametric statistics and investigated resampling techniques. This included bootstrapping, jackknife, and permutation tests. In the upcoming lab, you'll define functions that implement these techniques and then use them to conduct statistical simulations and tests.\u003c/p\u003e\n\u003cfooter class=\"fis-footer\" style=\"visibility: hidden;\"\u003e\u003cdiv class=\"fis-feedback\"\u003e\n\u003ch5\u003eHow do you feel about this lesson?\u003c/h5\u003e\n\u003cimg id=\"thumbs-up\" data-repository=\"dsc-resampling-methods\" title=\"Thumbs up!\" alt=\"thumbs up\"\u003e\u003cimg id=\"thumbs-down\" data-repository=\"dsc-resampling-methods\" title=\"Thumbs down!\" alt=\"thumbs down\"\u003e\n\u003c/div\u003e\n\u003ch5\u003eHave specific feedback? \u003ca href=\"https://github.com/learn-co-curriculum/dsc-resampling-methods/issues/new\"\u003eTell us here!\u003c/a\u003e\n\u003c/h5\u003e\u003c/footer\u003e","frontPage":false},{"exportId":"bayesian-statistics-introduction","title":"Bayesian Statistics - Introduction","type":"WikiPage","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-bayesian-stats-introduction\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-bayesian-stats-introduction/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eIn this section, you'll investigate the Bayesian statistical framework. Bayesian statistics are an alternative perspective to classical Frequentist approaches which you've seen thus far. Bayesian statistics applies reasoning to unknown probabilities in a manner in which the Frequentist approach does not allow. \u003c/p\u003e\n\n\u003ch2\u003eThomas Bayes\u003c/h2\u003e\n\n\u003cp\u003eBayesian statistics owes its name to the famous mathematician Thomas Bayes. Born (sometime) in the early 1700s, he bucked many academic traditions of his time due to his families religious beliefs. Cambridge and Oxford were known for the most prestigious mathematics of the time, but Bayes was a Presbytarien barring him from these universities which had ties to the Church of England. \u003c/p\u003e\n\n\u003ch2\u003eBayes' theorem\u003c/h2\u003e\n\n\u003cp\u003eBayes' theorem is a method for rewriting conditional probabilities. The formula is:\u003c/p\u003e\n\n\u003ch3\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=P(A%7CB)%20=%5Cdfrac%7BP(B%7CA)P(A)%7D%7BP(B)%7D\"\u003e\u003c/h3\u003e\n\n\u003cp\u003eIn the following lessons, you'll learn more about two traditional interpretations of this formula. The first provides an intuitive understanding, viewing the numerator as the probability of both A and B occuring:  \u003c/p\u003e\n\n\u003ch3\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=P(A%7CB)%20=%5Cdfrac%7BP(A%20%5Ccap%20B)%7D%7BP(B)%7D\"\u003e\u003c/h3\u003e\n\n\u003cp\u003eThis should make perfect sense: the probability that A is true, given B is true, is the probability that A and B are both true, divided by the probability that B was true in the first place. \u003c/p\u003e\n\n\u003cp\u003eThe second interpretation of Bayes theorem leads straight into the Bayesian statistical framework itself, bringing about discussions of priors, likelihoods, and posterior probabilities.\u003c/p\u003e\n\n\u003ch2\u003eSummary\u003c/h2\u003e\n\n\u003cp\u003eGet ready to jump in! This section provides an exciting introduction to Bayes theorem and Bayesian statistics, further rounding out your statistical toolbox!\u003c/p\u003e\n\u003cfooter class=\"fis-footer\" style=\"visibility: hidden;\"\u003e\u003cdiv class=\"fis-feedback\"\u003e\n\u003ch5\u003eHow do you feel about this lesson?\u003c/h5\u003e\n\u003cimg id=\"thumbs-up\" data-repository=\"dsc-bayesian-stats-introduction\" title=\"Thumbs up!\" alt=\"thumbs up\"\u003e\u003cimg id=\"thumbs-down\" data-repository=\"dsc-bayesian-stats-introduction\" title=\"Thumbs down!\" alt=\"thumbs down\"\u003e\n\u003c/div\u003e\n\u003ch5\u003eHave specific feedback? \u003ca href=\"https://github.com/learn-co-curriculum/dsc-bayesian-stats-introduction/issues/new\"\u003eTell us here!\u003c/a\u003e\n\u003c/h5\u003e\u003c/footer\u003e","frontPage":false},{"exportId":"p-values-and-the-null-hypothesis","title":"P-Values and the Null Hypothesis","type":"WikiPage","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-p-values-and-null-hypothesis\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-p-values-and-null-hypothesis/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eIn this lesson, you'll learn about the relationship between p-values and the Null Hypothesis, and their role in designing an experiment. \u003c/p\u003e\n\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\n\u003cp\u003eYou will be able to:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eDescribe what it means to \"reject the null hypothesis\" and how it is related to p-value\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch2\u003eUnderstanding  The Null Hypothesis\u003c/h2\u003e\n\n\u003cp\u003eAs stated previously, scientific experiments actually have 2 hypotheses:\u003c/p\u003e\n\n\u003cp\u003e\u003cstrong\u003e\u003cem\u003eNull Hypothesis\u003c/em\u003e\u003c/strong\u003e: There is no relationship between A and B\u003cbr\u003e\nExample: \"There is no relationship between this flu medication and a reduced recovery time from the flu\".\u003c/p\u003e\n\n\u003cp\u003eThe \u003cem\u003eNull Hypothesis\u003c/em\u003e is usually denoted as  \u003cimg src=\"https://render.githubusercontent.com/render/math?math=H_%7B0%7D\"\u003e \u003c/p\u003e\n\n\u003cp\u003e\u003cstrong\u003e\u003cem\u003eAlternative Hypothesis\u003c/em\u003e\u003c/strong\u003e: The hypothesis traditionally thought of when creating a hypothesis for an experiment\u003cbr\u003e\nExample: \"This flu medication reduces recovery time for the flu.\"\u003c/p\u003e\n\n\u003cp\u003eThe \u003cem\u003eAlternative Hypothesis\u003c/em\u003e is usually denoted as  \u003cimg src=\"https://render.githubusercontent.com/render/math?math=H_%7B1%7D\"\u003e \u003c/p\u003e\n\n\u003cp\u003eAn easy way to differentiate between the Null Hypothesis and the Alternative Hypothesis is that the Null Hypothesis is the more conservative choice. It always assumes that there is no difference between two different population means, and when it is represented mathematically, it should always contain an equals sign. \u003c/p\u003e\n\n\u003cp\u003eThe Alternative Hypothesis is whatever claim you are trying to prove with an experiment.\u003c/p\u003e\n\n\u003ch3\u003eP-Values and Alpha Values\u003c/h3\u003e\n\n\u003cp\u003eNo matter what you're experimenting on, good experiments come down to one question: Is your p-value less than your alpha value? Let's dive into what each of these values represents, and why they're so important to experimental design. \u003c/p\u003e\n\n\u003cp\u003e\u003cstrong\u003e\u003cem\u003ep-value\u003c/em\u003e\u003c/strong\u003e: The probability of observing a test statistic at least as large as the one observed, by random chance, assuming that the null hypothesis is true.\u003c/p\u003e\n\n\u003cp\u003eIf you calculate a p-value and it comes out to 0.03, you can interpret this as saying \"There is a 3% chance of obtaining the results I'm seeing when the null hypothesis is true.\"  \u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Calpha\"\u003e \u003cstrong\u003e\u003cem\u003e(alpha value)\u003c/em\u003e\u003c/strong\u003e: The marginal threshold at which you're okay with rejecting the null hypothesis. \u003c/p\u003e\n\n\u003cp\u003eAn alpha value can be any value set between 0 and 1. However, the most common alpha value in science is 0.05 (although this is somewhat of a controversial topic in the scientific community, currently).  \u003c/p\u003e\n\n\u003cp\u003eIf you set an alpha value of  \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Calpha%20=%200.05\"\u003e , you're essentially saying \"I'm okay with accepting my alternative hypothesis as true if there is less than a 5% chance that the results that I'm seeing are actually due to randomness.\"\u003c/p\u003e\n\n\u003cp\u003eWhen you conduct an experiment, your goal is to calculate a p-value and compare it to the alpha value. If  \u003cimg src=\"https://render.githubusercontent.com/render/math?math=p%20\u003c%20%5Calpha\"\u003e , then you \u003cstrong\u003e\u003cem\u003ereject the null hypothesis\u003c/em\u003e\u003c/strong\u003e and accept that there is not \"no relationship\" between the dependent and independent variables.  Note that any good scientist will admit that this doesn't prove that there is a \u003cem\u003edirect relationship\u003c/em\u003e between the dependent and independent variables--just that they now have enough evidence to the contrary to show that they can no longer believe that there is no relationship between them. \u003c/p\u003e\n\n\u003cp\u003eIn simple terms:\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=p%20\u003c%20%5Calpha\"\u003e : Reject the \u003cem\u003eNull Hypothesis\u003c/em\u003e and accept the \u003cem\u003eAlternative Hypothesis\u003c/em\u003e\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=p%20\u003e=%20%5Calpha\"\u003e : Fail to reject the \u003cem\u003eNull Hypothesis\u003c/em\u003e.  \u003c/p\u003e\n\n\u003cp\u003eThere are many different ways that you can structure a hypothesis statement, but they always come down to this comparison in the end.  In normally distributed data, you calculate p-values from t-statistics or ( \u003cimg src=\"https://render.githubusercontent.com/render/math?math=z\"\u003e -scores if the population parameters are known). This is done a bit differently with discrete data. You may also have \u003cstrong\u003e\u003cem\u003eOne-Tail\u003c/em\u003e\u003c/strong\u003e and \u003cstrong\u003e\u003cem\u003eTwo-Tail\u003c/em\u003e\u003c/strong\u003e tests.  \u003c/p\u003e\n\n\u003cp\u003eA \u003cstrong\u003e\u003cem\u003eOne-Tail Test\u003c/em\u003e\u003c/strong\u003e is when you want to know if a parameter from the treatment group is greater than (or less than) a corresponding parameter from the control group.\u003c/p\u003e\n\n\u003cp\u003e\u003cstrong\u003e\u003cem\u003eExample One-Tail Hypothesis\u003c/em\u003e\u003c/strong\u003e\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=H_%7B1%7D%20:%20%5Cmu_1%20\u003c%20%5Cmu_2\"\u003e The treatment group given this weight loss drug will lose more weight on average than the control group that was given a competitor's weight loss drug \u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=H_%7B0%7D%20:%20%5Cmu1%20\u003e=%20%5Cmu_2\"\u003e  The treatment group given this weight loss drug will not lose more weight on average than the control group that was given a competitor's weight loss drug\". \u003c/p\u003e\n\n\u003cp\u003eA \u003cstrong\u003e\u003cem\u003eTwo-Tail Test\u003c/em\u003e\u003c/strong\u003e is for when you want to test if a parameter falls between (or outside of) a range of two given values. \u003c/p\u003e\n\n\u003cp\u003e\u003cstrong\u003e\u003cem\u003eExample Two-Tail Hypothesis\u003c/em\u003e\u003c/strong\u003e\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=H_%7B1%7D%20:%20%5Cmu_1%20%5Cneq%20%5Cmu_2\"\u003e \"People in the experimental group that are administered this drug will not lose the same amount of weight as the people in the control group.  They will be heavier or lighter\". \u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=H_%7B0%7D%20:%20%5Cmu_1%20=%20%5Cmu_2\"\u003e \"People in the experimental group that are administered this drug will lose the same amount of weight as the people in the control group.\" \u003c/p\u003e\n\n\u003ch4\u003eWhat Does an Experiment Really Prove?\u003c/h4\u003e\n\n\u003cp\u003eYou may be wondering why you need a \u003cstrong\u003e\u003cem\u003eNull Hypothesis\u003c/em\u003e\u003c/strong\u003e at all. This is a good question. It has to do with being honest about what an experiment actually proves.\u003c/p\u003e\n\n\u003cp\u003eScientists use the \u003cstrong\u003e\u003cem\u003eNull Hypothesis\u003c/em\u003e\u003c/strong\u003e so that they can be very specific in their findings. This is because a successful experiment doesn't actually \u003cem\u003eprove a relationship\u003c/em\u003e between a dependent and independent variable.  Instead, it just proves that there is not enough evidence to convincingly believe there is \u003cem\u003eno relationship\u003c/em\u003e between the dependent and the independent variable. There can always be a lurking variable behind the scenes that is actually responsible for the relationship between two variables--it's almost impossible to cover every possible angle. However, a successful experiment where a p-value is less than an alpha value (typically,  \u003cimg src=\"https://render.githubusercontent.com/render/math?math=p%20\u003c%200.05\"\u003e ) does give enough information to confidently allow someone to say that it's statistically unlikely that there is \u003cem\u003eno relationship\u003c/em\u003e between the two, which is what would have to be true in order for the null hypothesis to be correct!\u003c/p\u003e\n\n\u003ch2\u003eThe Null Hypothesis Loves You and Wants You To Be Happy\u003c/h2\u003e\n\n\u003cp\u003eYou've covered a lot about the null hypothesis and how it's used in experiments in this lesson, but there's a lot more to learn about it! \u003c/p\u003e\n\n\u003cp\u003eRead the following article, \u003ca href=\"https://byrslf.co/the-null-hypothesis-loves-you-and-wants-you-to-be-happy-3189413d8cd0\"\u003eThe Null Hypothesis Loves You and Wants You To Be Happy\u003c/a\u003e.  This does an excellent job of explaining why the concept of the \u003cem\u003eNull Hypothesis\u003c/em\u003e is crucial to good science.  \u003c/p\u003e\n\n\u003ch2\u003eSummary\u003c/h2\u003e\n\n\u003cp\u003eIn this lesson, you learned about the relationship between p-values and the Null Hypothesis. Now you'll see how effect sizes affect your tests!\u003c/p\u003e\n\u003cfooter class=\"fis-footer\" style=\"visibility: hidden;\"\u003e\u003cdiv class=\"fis-feedback\"\u003e\n\u003ch5\u003eHow do you feel about this lesson?\u003c/h5\u003e\n\u003cimg id=\"thumbs-up\" data-repository=\"dsc-p-values-and-null-hypothesis\" title=\"Thumbs up!\" alt=\"thumbs up\"\u003e\u003cimg id=\"thumbs-down\" data-repository=\"dsc-p-values-and-null-hypothesis\" title=\"Thumbs down!\" alt=\"thumbs down\"\u003e\n\u003c/div\u003e\n\u003ch5\u003eHave specific feedback? \u003ca href=\"https://github.com/learn-co-curriculum/dsc-p-values-and-null-hypothesis/issues/new\"\u003eTell us here!\u003c/a\u003e\n\u003c/h5\u003e\u003c/footer\u003e","frontPage":false},{"exportId":"permutations-and-factorials","title":"Permutations and Factorials","type":"WikiPage","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-permutations-and-factorials-v2-1\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-permutations-and-factorials-v2-1/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eIn the previous lab, you defined a few sample spaces by counting the total number of possible outcomes. This is not very practical when sample spaces grow. In this lab, you'll be introduced to \u003cem\u003epermutations\u003c/em\u003e, which will provide a structured way to help you define sample space sizes!\u003c/p\u003e\n\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\n\u003cp\u003eYou will be able to: \u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eDescribe how factorials are related to permutations\u003c/li\u003e\n\u003cli\u003eMathematically derive how many permutations there are for large sets\u003c/li\u003e\n\u003cli\u003eCalculate permutations of a subset\u003c/li\u003e\n\u003cli\u003eCalculate permutations with repetition and replacement\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch2\u003eDefining the Sample Space by Counting\u003c/h2\u003e\n\n\u003cp\u003eLet's consider the following example.\u003c/p\u003e\n\n\u003cp\u003eThe Beyoncé tribute band \"The Single Ladies\" is playing a free mini gig in your local park next week. They have selected three all-time classics: \"Drunk in Love\", \"Crazy in Love\" and \"Formation\", but still have to decide the order they will play the songs in. Knowing this, how many playlists are possible?\u003c/p\u003e\n\n\u003cp\u003eIt is easy and fairly quick to write down possible orders here:\u003c/p\u003e\n\n\u003cp\u003e\"Drunk in Love\", \"Crazy in Love\", \"Formation\"\u003c/p\u003e\n\n\u003cp\u003e\"Drunk in Love\", \"Formation\", \"Crazy in Love\"\u003c/p\u003e\n\n\u003cp\u003e\"Crazy in Love\", \"Drunk in Love\", \"Formation\"\u003c/p\u003e\n\n\u003cp\u003e\"Crazy in Love\", \"Formation\", \"Drunk in Love\" \u003c/p\u003e\n\n\u003cp\u003e\"Formation\", \"Drunk in Love\", \"Crazy in Love\"\u003c/p\u003e\n\n\u003cp\u003e\"Formation\", \"Crazy in Love\", \"Drunk in Love\"\u003c/p\u003e\n\n\u003cp\u003eThat's it! When we count the possible outcomes, we get to 6 elements in the sample set. Now what if \"The Single Ladies\" plays a setlist of 4 songs? or 5? That's where the notion of \u003cem\u003epermutations\u003c/em\u003e comes in handy.\u003c/p\u003e\n\n\u003ch2\u003ePermutations\u003c/h2\u003e\n\n\u003cp\u003eThe problem setting, in general, is that there are  \u003cimg src=\"https://render.githubusercontent.com/render/math?math=n\"\u003e objects and we want to know how many \u003cem\u003epermutations\u003c/em\u003e are possible.\u003c/p\u003e\n\n\u003cp\u003eThis is a way how you can tackle this. You're the lead singer and have to decide which song to play first. You have 3 songs to choose from, so 3 ways of choosing a first song. Then, you move on to the second song. You've chosen the first one, so you have 2 songs to choose from now, etc. Mathematically, this boils down to:\u003c/p\u003e\n\n\u003cp\u003e# Beyoncé permutations\u003cimg src=\"https://render.githubusercontent.com/render/math?math==%203*2*1%20=%203!%20=%206\"\u003e \u003c/p\u003e\n\n\u003cp\u003eGeneralizing this to  \u003cimg src=\"https://render.githubusercontent.com/render/math?math=n\"\u003e , this means that the number of permutations with  \u003cimg src=\"https://render.githubusercontent.com/render/math?math=n\"\u003e distinct objects is  \u003cimg src=\"https://render.githubusercontent.com/render/math?math=n!\"\u003e , or the factorial of  \u003cimg src=\"https://render.githubusercontent.com/render/math?math=n\"\u003e .\u003c/p\u003e\n\n\u003ch2\u003ePermutations of a Subset\u003c/h2\u003e\n\n\u003cp\u003eNow, let's consider another example. \"The Single Ladies\" are still playing a concert at central park, but they disagree on the final three songs that they will play. They only get a 12 min gig slot, so they really can't play more than 3, yet they have a shortlist of 8 they need to pick from. How many final song selections are possible given this info? As for the first example, the order of the songs played is still important.\u003c/p\u003e\n\n\u003cp\u003eWhen the band members decide on the first song, they have 8 possible songs to choose from. When choosing the second song, they have 7 to choose from. Then for the third song, they have 6 left.\u003c/p\u003e\n\n\u003cp\u003e# Beyoncé k-permutations\u003cimg src=\"https://render.githubusercontent.com/render/math?math==%208*7*6%20=%20336\"\u003e \u003c/p\u003e\n\n\u003cp\u003eformalizing this, the question is how many ways we can select  \u003cimg src=\"https://render.githubusercontent.com/render/math?math=k\"\u003e elements out of a pool of  \u003cimg src=\"https://render.githubusercontent.com/render/math?math=n\"\u003e objects. The answer is \u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=n*(n-1)*...*(n-k%2b1)\"\u003e or in other words,  \u003cimg src=\"https://render.githubusercontent.com/render/math?math=P_%7Bk%7D%5E%7Bn%7D=%20%5Cdfrac%7Bn!%7D%7B(n-k)!%7D\"\u003e \u003c/p\u003e\n\n\u003cp\u003eThis is known as a  \u003cimg src=\"https://render.githubusercontent.com/render/math?math=k\"\u003e -permutation of  \u003cimg src=\"https://render.githubusercontent.com/render/math?math=n\"\u003e .\u003c/p\u003e\n\n\u003cp\u003eThe idea is here that we only \"care\" about the order of the first  \u003cimg src=\"https://render.githubusercontent.com/render/math?math=k\"\u003e objects. The order of the other  \u003cimg src=\"https://render.githubusercontent.com/render/math?math=(n-k)\"\u003e objects doesn't matter, hence they're left out of the equation.\u003c/p\u003e\n\n\u003ch2\u003ePermutations with Replacement\u003c/h2\u003e\n\n\u003cp\u003eWhen talking about setlists, it makes total sense to assume that songs will not be played twice. This is not always how it works though. Imagine a bag with three marbles in it: a green one, a red one, and a blue one. Now we'll draw marbles three times in a row, but each time, we'll write down the marble color and \u003cem\u003eput it back in the bag\u003c/em\u003e before drawing again.\u003c/p\u003e\n\n\u003cp\u003eNow the number of possible outcomes is  \u003cimg src=\"https://render.githubusercontent.com/render/math?math=3%20*%203%20*%203\"\u003e .\u003c/p\u003e\n\n\u003cp\u003eGeneralizing this to  \u003cimg src=\"https://render.githubusercontent.com/render/math?math=n\"\u003e , this means that the number of permutations with replacement when having  \u003cimg src=\"https://render.githubusercontent.com/render/math?math=n\"\u003e distinct objects is equal to  \u003cimg src=\"https://render.githubusercontent.com/render/math?math=n%5Ej\"\u003e where  \u003cimg src=\"https://render.githubusercontent.com/render/math?math=j\"\u003e is the number of \"draws\".\u003c/p\u003e\n\n\u003ch2\u003ePermutations with Repetition\u003c/h2\u003e\n\n\u003cp\u003eWhen using permutations, some elements may be \u003cem\u003erepeated\u003c/em\u003e.\u003c/p\u003e\n\n\u003cp\u003eA classic example is using permutations on words. Let's say you have the letters of the word \"TENNESSEE\". How many different words can you create using these letters?\u003c/p\u003e\n\n\u003cp\u003eSimply saying that there are 9 letters so the answer is  \u003cimg src=\"https://render.githubusercontent.com/render/math?math=9!\"\u003e does not give you the correct answer. Looking at the word TENNESSEE by itself, you can swap the 3rd and the 4th letter and have the same word. So the total number is less than  \u003cimg src=\"https://render.githubusercontent.com/render/math?math=9!\"\u003e .\u003c/p\u003e\n\n\u003cp\u003eThe solution is to divide  \u003cimg src=\"https://render.githubusercontent.com/render/math?math=9!\"\u003e by the factorials for each letter that is repeated!\u003c/p\u003e\n\n\u003cp\u003eThe answer here is then (9 letters, 4 x E, 2 x N, 2 x S)\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cdfrac%7B9!%7D%7B4!2!2!%7D%20=%203780\"\u003e \u003c/p\u003e\n\n\u003cp\u003eThe general formula can be written as:\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cdfrac%7Bn!%7D%7Bn_1!n_2!%5Cldots%20n_k!%7D\"\u003e \u003c/p\u003e\n\n\u003cp\u003ewhere  \u003cimg src=\"https://render.githubusercontent.com/render/math?math=n_j\"\u003e stands for identical objects of type  \u003cimg src=\"https://render.githubusercontent.com/render/math?math=j\"\u003e (the distinct letters in our TENNESSEE example). \u003c/p\u003e\n\n\u003ch2\u003eLevel-Up: Factorials and Recursion\u003c/h2\u003e\n\n\u003cp\u003eAt the start of this lesson, when discussing the number of possible permutations we can obtain for n distinct objects, we mentioned the concept of the factorial of n, denoted by  \u003cimg src=\"https://render.githubusercontent.com/render/math?math=n!\"\u003e . \u003c/p\u003e\n\n\u003cp\u003eIn the example presented to you, we wanted to count all possible ways in which three different Beyoncé songs could be played by the Beyoncé tribute band \"The Single Ladies\". There were 3 possible ways of choosing a first song, 2 possible ways of choosing a second song, and only 1 way of choosing a third and final song, for  \u003cimg src=\"https://render.githubusercontent.com/render/math?math=3%20*%202%20*1%20=%206\"\u003e different ways in which the three different songs could be played. This number, 6, is equal to the factorial of 3,  \u003cimg src=\"https://render.githubusercontent.com/render/math?math=3!\"\u003e , the number of permutations of 3 distinct objects. \u003c/p\u003e\n\n\u003cp\u003eHere,  \u003cimg src=\"https://render.githubusercontent.com/render/math?math=3!%20=%20(3%20*%202%20*%201)%20=%206\"\u003e . Notice that this is the same as writing  \u003cimg src=\"https://render.githubusercontent.com/render/math?math=3%20*%202!%20=%203%20*%20(2%20*%201)\"\u003e and  \u003cimg src=\"https://render.githubusercontent.com/render/math?math=3%20*%202%20*%201!%20=%203%20*%202%20*%20(1)\"\u003e . (By definition, the factorial of 1,  \u003cimg src=\"https://render.githubusercontent.com/render/math?math=1!\"\u003e , is equal to 1. The factorial of 0,  \u003cimg src=\"https://render.githubusercontent.com/render/math?math=0!\"\u003e is also defined to be equal to 1.)\u003c/p\u003e\n\n\u003cp\u003eWe can generalize this to the case of computing the factorial of an integer n,  \u003cimg src=\"https://render.githubusercontent.com/render/math?math=n!\"\u003e . The factorial of n,  \u003cimg src=\"https://render.githubusercontent.com/render/math?math=n!\"\u003e , can be written as  \u003cimg src=\"https://render.githubusercontent.com/render/math?math=n%20*%20(n-1)!\"\u003e , which itself can be written as  \u003cimg src=\"https://render.githubusercontent.com/render/math?math=n%20*%20(n-1)%20*%20(n-2)!\"\u003e . That is, we can define the factorial of n in terms of the product of  \u003cimg src=\"https://render.githubusercontent.com/render/math?math=n\"\u003e and the factorial of  \u003cimg src=\"https://render.githubusercontent.com/render/math?math=(n-1)\"\u003e , and the factorial  \u003cimg src=\"https://render.githubusercontent.com/render/math?math=(n-1)\"\u003e can be defined in terms of the product of  \u003cimg src=\"https://render.githubusercontent.com/render/math?math=(n-1)\"\u003e and the factorial of  \u003cimg src=\"https://render.githubusercontent.com/render/math?math=(n-2)\"\u003e , and so on and so forth, as seen in the equation below, until we get to  \u003cimg src=\"https://render.githubusercontent.com/render/math?math=1!\"\u003e , which is defined to be equal to 1: \u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=n!%20=%20n%20*%20(n-1)!%20=%20n%20*%20(n-1)%20*%20(n-2)!%20=%20...%20=%20n%20*%20(n-1)%20*%20(n-2)%20*%20%5Cldots%20*%202!%20=%20n*%20(n-1)%20*%20(n-2)%20*%20%5Cldots%20*%202%20*%201!\"\u003e \u003c/p\u003e\n\n\u003ch3\u003eRecursion\u003c/h3\u003e\n\n\u003cp\u003eWhen we define a function in terms of itself, in this case, the factorial of n in terms of the factorial of (n-1), we are using \u003cstrong\u003erecursion\u003c/strong\u003e.  Recursive functions are functions that can call themselves in order to loop until a condition is met. In the next lab, you'll get a glimpse on how to write a recursive function in Python, but in the Appendix to this Module, we go over recursive functions in Python in much more detail.\u003c/p\u003e\n\n\u003ch2\u003eSummary\u003c/h2\u003e\n\n\u003cp\u003eNow you're well on your way to calculate all sorts of permutations using factorials - both for understanding the sample space, subsets, etc! Let's move on for some practice!\u003c/p\u003e\n\u003cfooter class=\"fis-footer\" style=\"visibility: hidden;\"\u003e\u003cdiv class=\"fis-feedback\"\u003e\n\u003ch5\u003eHow do you feel about this lesson?\u003c/h5\u003e\n\u003cimg id=\"thumbs-up\" data-repository=\"dsc-permutations-and-factorials-v2-1\" title=\"Thumbs up!\" alt=\"thumbs up\"\u003e\u003cimg id=\"thumbs-down\" data-repository=\"dsc-permutations-and-factorials-v2-1\" title=\"Thumbs down!\" alt=\"thumbs down\"\u003e\n\u003c/div\u003e\n\u003ch5\u003eHave specific feedback? \u003ca href=\"https://github.com/learn-co-curriculum/dsc-permutations-and-factorials-v2-1/issues/new\"\u003eTell us here!\u003c/a\u003e\n\u003c/h5\u003e\u003c/footer\u003e","frontPage":false},{"exportId":"extension-to-linear-models-introduction","title":"Extension to Linear Models - Introduction","type":"WikiPage","content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-extensions-to-linear-models-intro-v2-1\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-extensions-to-linear-models-intro-v2-1\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-extensions-to-linear-models-intro-v2-1/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eIn this section, you'll learn about some ways to make linear models more complex to account for complicated relationships in a dataset. Before diving in too deep, here is a roadmap for what you will be covering in this section.\u003c/p\u003e\n\u003ch2\u003eExtensions to linear models\u003c/h2\u003e\n\u003cp\u003eUnfortunately, not every continuous variable can be predicted effectively using a straight line (a linear model). Imagine the relationship between your 5k time as a runner and how many hours a week you train. At the very least the improvements in your time are going to trail off with additional training (going from 0 -\u0026gt; 10 hrs training a week is going to have way more impact than going from 60 -\u0026gt; 70 hrs). And the chances are that at some point in time, you'll overtrain and additional training will actually degrade your performance. You could certainly model the relationship between weekly training hours and 5k time with a straight line, but for some values of weekly training time, the accuracy of the prediction would probably be extremely low.\u003c/p\u003e\n\u003cp\u003eIn this section, you'll be introduced to a number of concepts to help you to make predictions when there is no linear relationship between the predictor and target variables.\u003c/p\u003e\n\u003ch3\u003eInteractions\u003c/h3\u003e\n\u003cp\u003eWe'll start by introducing the concept of interactions - where two or more variables interact in a non-additive manner thus affecting a third variable. Then you'll look at why they are important and how to account for them.\u003c/p\u003e\n\u003ch3\u003ePolynomial regression\u003c/h3\u003e\n\u003cp\u003eYou'll then implement higher-order equations for solving regressions. A linear expression can be described by an equation in the form of \u003cimg src=\"https://render.githubusercontent.com/render/math?math=y%20=%20mx%20%2b%20b\"\u003e . A polynomial expression brings in higher powers of x (squared, cubed, etc). By allowing for equations containing higher-order terms you may be able to better fit a curve to your dataset, thus predicting future values more accurately.\u003c/p\u003e\n\u003ch3\u003eBias-variance tradeoff\u003c/h3\u003e\n\u003cp\u003eWhile polynomial regression can improve the accuracy of your models, they also exacerbate the risk of overfitting the data, making your model extremely accurate for the training set but completely inaccurate for any future data points the model wasn't trained on. You'll also look at the concept of bias-variance tradeoff and how it relates to underfitting and overfitting datasets.\u003c/p\u003e\n\u003ch2\u003eSummary\u003c/h2\u003e\n\u003cp\u003eIt's time to start learning about extensions to linear models, modeling non-linear relationships between the predictor and target variables!\u003c/p\u003e","frontPage":false},{"exportId":"multiple-regression-and-model-validation-recap","title":"Multiple Regression and Model Validation - Recap","type":"WikiPage","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-regression-model-eval-recap-v2-1\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-regression-model-eval-recap-v2-1/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eIn this section you extended your knowledge of building regression models by adding additional predictive variables and subsequently validating those models using train-test-split and cross validation.\u003c/p\u003e\n\u003ch2\u003eMultiple Regression\u003c/h2\u003e\n\u003cp\u003eYou saw a number of techniques and concepts related to regression. This included the idea of using multiple predictors in order to build a stronger estimator. That said, there were caveats to using multiple predictors. For example, multicollinearity between variables should be avoided. One option for features with particularly high correlation is to only use one of these features. This improves model interpretability. In addition, linear regression is also most effective when features are of a similar scale. Typically, feature scaling and normalization are used to achieve this. There are also other data preparation techniques such as creating dummy variables for categorical variables, and transforming non-normal distributions using functions such as logarithms. Finally, in order to validate models it is essential to always partition your dataset such as with train-test splits or k-fold cross validation.\u003c/p\u003e\n\u003cfooter class=\"fis-footer\" style=\"visibility: hidden;\"\u003e\n\u003cdiv class=\"fis-feedback\"\u003e\n\u003ch5\u003eHow do you feel about this lesson?\u003c/h5\u003e\n\u003cimg id=\"thumbs-up\" title=\"Thumbs up!\" alt=\"thumbs up\" data-repository=\"dsc-regression-model-eval-recap-v2-1\"\u003e\u003cimg id=\"thumbs-down\" title=\"Thumbs down!\" alt=\"thumbs down\" data-repository=\"dsc-regression-model-eval-recap-v2-1\"\u003e\n\u003c/div\u003e\n\u003ch5\u003eHave specific feedback? \u003ca href=\"https://github.com/learn-co-curriculum/dsc-regression-model-eval-recap-v2-1/issues/new\"\u003eTell us here!\u003c/a\u003e\n\u003c/h5\u003e\n\u003c/footer\u003e","frontPage":false},{"exportId":"project-submission-and-review-online","title":"Project Submission \u0026 Review (Online)","type":"WikiPage","content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-project-submissions-online\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-project-submissions-online\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-project-submissions-online/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e  \u003ch2\u003eIntroduction\u003c/h2\u003e  \u003cp\u003eIn this lesson, we review the requirements, submission, and review process for the Phase Projects.\u003c/p\u003e  \u003ch2\u003eObjectives\u003c/h2\u003e  \u003cp\u003eYou will be able to:\u003c/p\u003e  \u003cul\u003e \u003cli\u003eCreate project deliverables that meet Flatiron School requirements\u003c/li\u003e \u003cli\u003eSubmit your project deliverables in Canvas\u003c/li\u003e \u003cli\u003ePrepare for your project review\u003c/li\u003e \u003c/ul\u003e  \u003ch2\u003eSchedule Your Review ASAP\u003c/h2\u003e  \u003cp\u003e\u003cstrong\u003eReach out to an instructor immediately via Slack to let them know you've started your project and schedule your project review.\u003c/strong\u003e If you're not sure who to schedule with, please ask in your cohort channel in Slack.\u003c/p\u003e  \u003ch2\u003eCreate Your Project Deliverables\u003c/h2\u003e  \u003cp\u003eComplete the deliverables for your project, guided by the rubric at the bottom of the main project assignment. Keep in mind that the audience for these deliverables is not only your teacher, but also potential employers. Employers will look at your project deliverables to evaluate multiple skills, including coding, modeling, communication, and domain knowledge. You will want to polish these as much as you can, both during the course and afterwards.\u003c/p\u003e  \u003ch3\u003eGitHub Repository\u003c/h3\u003e  \u003cp\u003eYour GitHub repository is the public-facing version of your project that your instructors and potential employers will see - make it as accessible as you can. At a minimum, it should contain all your project files and a README.md file that summarizes your project and helps visitors navigate the repository.\u003c/p\u003e  \u003ch3\u003eJupyter Notebook\u003c/h3\u003e  \u003cp\u003eYour Jupyter Notebook is the primary source of information about your analysis. At a minimum, it should contain or import all of the code used in your project and walk the reader through your project from start to finish. You may choose to use multiple Jupyter Notebooks in your project, but you should have one that provides a full project overview as a point of entry for visitors.\u003c/p\u003e  \u003ch3\u003eNon-Technical Presentation\u003c/h3\u003e  \u003cp\u003eYour non-technical presentation is your opportunity to communicate clearly and concisely about your project and it's real-world relevance. The target audience should be people with limited technical knowledge who may be interested in leveraging your project. We recommend using Google Slides, PowerPoint or Keynote to create your presentation slides. You will then record yourself delivering the presentation.\u003c/p\u003e  \u003ch2\u003eSubmit Your Project\u003c/h2\u003e  \u003cp\u003eTo submit your project in Canvas, you will create and upload PDF versions of three project deliverables, then upload a recording of your video presentation. You will also submit the URL to your GitHub repository in a separate assignment.\u003c/p\u003e  \u003ch3\u003ePresentation Slides PDF Creation\u003c/h3\u003e  \u003col\u003e \u003cli\u003eExport your presentation as a PDF from the program in which you created it.\u003c/li\u003e \u003cli\u003eGive it a short descriptive file name (e.g. \u003ccode\u003epresentation.pdf\u003c/code\u003e).\u003c/li\u003e \u003cli\u003ePlace a copy of the PDF in your GitHub repository.\u003c/li\u003e \u003c/ol\u003e  \u003ch3\u003eGitHub Repository PDF Creation\u003c/h3\u003e  \u003col\u003e \u003cli\u003eNavigate to the root directory of your project repository on GitHub, using your browser (we recommend Google Chrome).\u003c/li\u003e \u003cli\u003eSave the webpage as a PDF using the browser's Print functionality (\u003ca href=\"https://www.wikihow.com/Save-a-Web-Page-as-a-PDF-in-Google-Chrome\"\u003eGoogle Chrome Save to PDF instructions\u003c/a\u003e)\u003c/li\u003e \u003cli\u003eGive it a short descriptive file name (e.g. \u003ccode\u003egithub.pdf\u003c/code\u003e).\u003c/li\u003e \u003c/ol\u003e  \u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-project-submissions-online/master/repo_pdf.gif\" alt=\"Repository PDF Creation\"\u003e\u003c/p\u003e  \u003ch3\u003eJupyter Notebook PDF Creation\u003c/h3\u003e  \u003col\u003e \u003cli\u003eOpen your Notebook in your browser (we recommend Google Chrome).\u003c/li\u003e \u003cli\u003e\n\u003cstrong\u003eRun the Notebook from start to finish\u003c/strong\u003e so that your output is visible.\u003c/li\u003e \u003cli\u003eSave the page as a PDF using the browser's Print functionality (\u003ca href=\"https://www.wikihow.com/Save-a-Web-Page-as-a-PDF-in-Google-Chrome\"\u003eGoogle Chrome Save to PDF instructions\u003c/a\u003e)\u003c/li\u003e \u003cli\u003eGive it a short descriptive file name (e.g. \u003ccode\u003enotebook.pdf\u003c/code\u003e).\u003c/li\u003e \u003c/ol\u003e  \u003cp\u003eIf you have difficulty creating a PDF version of your notebook, you can use \u003ca href=\"https://htmtopdf.herokuapp.com/ipynbviewer/\"\u003ethis tool\u003c/a\u003e instead. Set the ‘Results Format’ to “HTML + PDF”. Then click ‘View and Convert’. Once it’s done, you should see links to .html and .pdf versions above the ‘View and Convert’ button.\u003c/p\u003e  \u003ch3\u003ePDF Submission in Canvas\u003c/h3\u003e  \u003cp\u003eYou will need to submit all three PDF files as a single submission:\u003c/p\u003e  \u003col\u003e \u003cli\u003eClick \"Submit Assignment\" at the top of the \"Phase X Project\" assignment in the \"Milestones\" topic.\u003c/li\u003e \u003cli\u003eIn the \"File Upload\" box, click \"Choose File\" button to upload a single file.\u003c/li\u003e \u003cli\u003eClick the \"Add Another File\" link to upload an additional file.\u003c/li\u003e \u003cli\u003eRepeat Step 3 to upload one more file. After this is done, all three files should be uploaded.\u003c/li\u003e \u003cli\u003eHit the blue \"Submit Assignment\" button.\u003c/li\u003e \u003c/ol\u003e  \u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-project-submissions-online/master/project_3pdf_submission.gif\" alt=\"Project PDF Submission\"\u003e\u003c/p\u003e  \u003ch3\u003ePresentation Recording and Submission\u003c/h3\u003e  \u003cp\u003eAfter you've submitted the PDF files for the project assignment, you will upload a recording of your presentation as a media comment on your submission:\u003c/p\u003e  \u003col\u003e \u003cli\u003eRecord your live presentation to a video file on your computer. We recommend using Zoom to record your live presentation to a local video file (\u003ca href=\"https://support.zoom.us/hc/en-us/articles/201362473-Local-recording\"\u003einstructions here\u003c/a\u003e). Video files must be under 500 MB and formatted as 3GP, ASF, AVI, FLV, M4V, MOV, MP4, MPEG, QT, or WMV.\u003c/li\u003e \u003cli\u003eClick \"Submission Details\" on the top right of the \"Phase X Project\" assignment in the \"Milestones\" topic.\u003c/li\u003e \u003cli\u003eClick \"Media Comment\" beneath the \"Add a Comment\" box on the right of the page.\u003c/li\u003e \u003cli\u003eClick \"Upload Media\" and \"Select Video File\" to upload your file.\u003c/li\u003e \u003cli\u003eThe thumbnail for your video will appear as a blue rectangle while Zoom processes your file - return to this page later to confirm that your recording uploaded successfully.\u003c/li\u003e \u003c/ol\u003e  \u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-project-submissions-online/master/video_recording_submission.gif\" alt=\"Video Recording Submission\"\u003e\u003c/p\u003e  \u003ch3\u003eURL Submission in Canvas\u003c/h3\u003e  \u003cp\u003eThere is an additional Canvas assignment where you will just enter the URL for your project's GitHub repository. This is located in the \"Milestones\" topic in this course as \"Phase X Project - GitHub Repository URL.\"\u003c/p\u003e  \u003ch2\u003ePrepare For Project Review\u003c/h2\u003e  \u003cp\u003eProject reviews are focused on preparing you for technical interviews. Treat project reviews as if they were technical interviews, in both attitude and technical presentation \u003cem\u003e(sometimes technical interviews will feel arbitrary or unfair - if you want to get the job, commenting on that is seldom a good choice)\u003c/em\u003e.\u003c/p\u003e  \u003cp\u003eThe project review is comprised of a 45 minute 1:1 session with one of the instructors. During your project review, be prepared to:\u003c/p\u003e  \u003ch3\u003e1. Deliver your PDF presentation to a non-technical stakeholder.\u003c/h3\u003e  \u003cp\u003eIn this phase of the review (~10 mins) your instructor will play the part of a non-technical stakeholder that you are presenting your findings to. The presentation  should not exceed 5 minutes, giving the \"stakeholder\" 5 minutes to ask questions.\u003c/p\u003e  \u003cp\u003eIn the first half of the presentation (2-3 mins), you should summarize your methodology in a way that will be comprehensible to someone with no background in data science and that will increase their confidence in you and your findings. In the second half (the remaining 2-3 mins) you should summarize your findings and be ready to answer a couple of non-technical questions from the audience. The questions might relate to technical topics (sampling bias, confidence, etc) but will be asked in a non-technical way and need to be answered in a way that does not assume a background in statistics or machine learning. You can assume a smart, business stakeholder, with a non-quantitative college degree.\u003c/p\u003e  \u003ch3\u003e2. Go through the Jupyter Notebook, answering questions about how you made certain decisions. Be ready to explain things like:\u003c/h3\u003e \u003cpre\u003e\u003ccode\u003e* \"How did you pick the question(s) that you did?\"\u003cbr\u003e* \"Why are these questions important from a business perspective?\"\u003cbr\u003e* \"How did you decide on the data cleaning options you performed?\"\u003cbr\u003e* \"Why did you choose a given method or library?\"\u003cbr\u003e* \"Why did you select those visualizations and what did you learn from each of them?\"\u003cbr\u003e* \"Why did you pick those features as predictors?\"\u003cbr\u003e* \"How would you interpret the results?\"\u003cbr\u003e* \"How confident are you in the predictive quality of the results?\"\u003cbr\u003e* \"What are some of the things that could cause the results to be wrong?\" \u003c/code\u003e\u003c/pre\u003e \u003cp\u003eThink of the first phase of the review (~30 mins) as a technical boss reviewing your work and asking questions about it before green-lighting you to present to the business team. You should practice using the appropriate technical vocabulary to explain yourself. Don't be surprised if the instructor jumps around or sometimes cuts you off - there is a lot of ground to cover, so that may happen.\u003c/p\u003e  \u003cp\u003eIf any requirements are missing or if significant gaps in understanding are uncovered, be prepared to do one or all of the following: * Perform additional data cleanup, visualization, feature selection, modeling and/or model validation * Submit an improved version * Meet again for another Project Review\u003c/p\u003e  \u003cp\u003eWhat won't happen: * You won't be yelled at, belittled, or scolded * You won't be put on the spot without support * There's nothing you can do to instantly fail or blow it\u003c/p\u003e  \u003ch2\u003eGrading\u003c/h2\u003e  \u003cp\u003eYour teacher will use the rubric at the bottom of the main project assignment to grade your project. In order to pass, you must properly submit your project and score \"Accomplished\" or \"Exemplary\" on nearly all rubric elements. You will receive a score of P (Pass) or NP (No Pass) - you must pass in order to move to the next phase with your cohort. Your teacher will grade your submission sometime after your review.\u003c/p\u003e  \u003ch2\u003eConclusion\u003c/h2\u003e  \u003cp\u003eThank you for your hard work on this project - you're going to do great! Remember that future employers will also look at your projects when deciding whether to hire you, so having complete, polished projects will help you tremendously not only to pass this assignment, but also to get the job you want after you graduate.\u003c/p\u003e  \u003cp\u003eIf you have any questions about the project submission or review process, don't hesitate to ask your teacher.\u003c/p\u003e","frontPage":false},{"exportId":"a-slash-b-testing","title":"A/B Testing","type":"WikiPage","content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-ab-testing\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-ab-testing\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-ab-testing/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eYou've now seen all of the statistical techniques and background to design and conduct your own A/B tests in practice! To do this, you'll go through the process of stating the null hypothesis and the alternative hypothesis which will include some test statistic for comparison. For example, you might compare the average purchase price between customers on two different versions of your online store, or a pharmaceutical researcher might compare the blood pressure of patients before and after taking a prescription. You've also seen that good test design requires multiple decisions. Recall both the multiple comparisons problem and Goodheart's law: you can't effectively measure everything (without increasing the risk of mistakes), and incentivizing measurements can lead to unforeseen consequences. With that, this section will give you a chance to put your new statistical techniques into practical applications.\u003c/p\u003e\n\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\n\u003cp\u003eYou will be able to:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eList the steps required to design, structure, and run an A/B test\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch2\u003eChoosing a Metric\u003c/h2\u003e\n\n\u003cp\u003eAny hypothesis testing will start with a given scenario. This will determine everything from what metrics are deemed important to realistically obtainable sample sizes. Goodheart's law can also be an important consideration when performing ongoing tests in attempts to optimize performance metrics.  \u003c/p\u003e\n\n\u003ch2\u003eDefining the Null Hypothesis:  \u003cimg src=\"https://render.githubusercontent.com/render/math?math=H_0\"\u003e\n\u003c/h2\u003e\n\n\u003cp\u003eOnce an appropriate metric has been selected, it's time to formally define the experiment with a null hypothesis. Typically, the null hypothesis is the claim that a researcher is hoping to refute. For example, a medical researcher might hope to show that a new drug is more effective than a previous treatment option. A common practice is then to define the null hypothesis as the contrary: there is no difference between the two drugs. The researcher hopes to refute the null hypothesis thereby proving their claim by contradiction. \u003c/p\u003e\n\n\u003cp\u003eYou might start with something like \" \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Ctext%7Bdrug%7D_a\"\u003e is more effective than  \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Ctext%7Bdrug%7D_b\"\u003e \".\u003c/p\u003e\n\n\u003cp\u003eWhile this is a good start, proper formulation of the null hypothesis should ensure that it is written with a quantitative measurement. Perhaps the drugs are for high blood pressure and so the statement becomes, \" \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Ctext%7Bdrug%7D_a\"\u003e at  \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Ctext%7Bdose%7D_a\"\u003e lowers blood pressure more than  \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Ctext%7Bdrug%7D_b\"\u003e at  \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Ctext%7Bdose%7D_b\"\u003e \".\u003c/p\u003e\n\n\u003cp\u003eFormulating the null-hypothesis like this is apt to lead you into conducting a paired t-test for the mean blood pressure of two groups: one representing  \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Ctext%7Bdrug%7D_a\"\u003e , and another representing  \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Ctext%7Bdrug%7D_b\"\u003e . \u003c/p\u003e\n\n\u003cp\u003eAlternatively, if one of these two medications were more heavily researched, one might wish to compare the effectiveness of the new medication with a predetermined metric such as the average drop in blood pressure from medication. This would lead to a 1-sample t-test, as opposed to a two-sample t-test.\u003c/p\u003e\n\n\u003ch2\u003eInvestigating  \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Calpha\"\u003e , power, effect size, and sample size\u003c/h2\u003e\n\n\u003cp\u003eFinally, one must formulate the various surrounding parameters required to conduct the test. You've seen that there is an intimate relationship between  \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Calpha\"\u003e , power, sample size, and effect size. With that, questions such as \"How costly is sample size?\" are instrumental in experiment design. For example, in an online scenario, it might be quite easy to conduct experiments at scale. On the other hand, in medical research, larger sample sizes are apt to be extremely costly. Investigating the relationships between  \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Calpha\"\u003e , power, effect size, and sample size is important for all experiments, and a suitable combination will depend on these contextual factors regarding implementation.\u003c/p\u003e\n\n\u003ch2\u003eSummary\u003c/h2\u003e\n\n\u003cp\u003eWhen researching, you are often presented with two choices for stating a question. One is to estimate a parameter in question, such as the procedures previously examined for estimating the mean of a population. Alternatively, you may wish to test the validity of a claim—whether you can refute that claim, or whether you should withhold judgment. In practice, it is up to the practitioner to determine the appropriate alpha, beta, and sample size that is determined to be both satisfactory confidence and a viable sample size to attain. In the upcoming labs, you'll get to practice this setup and design work-flow for a few scenarios.\u003c/p\u003e","frontPage":false},{"exportId":"statistical-power-and-anova-recap","title":"Statistical Power and ANOVA - Recap","type":"WikiPage","content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-statistical-power-anova-recap\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-statistical-power-anova-recap\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-statistical-power-anova-recap/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eYou've covered quite a bit in this section and should be gearing up to start conducting your own hypothesis testing! Before moving on to that exciting realm, take a minute to review some of the key takeaways.\u003c/p\u003e\n\u003ch2\u003eKey Takeaways\u003c/h2\u003e\n\u003cp\u003eRemember that the section began where the last left off, examining the relationship between \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Calpha\"\u003e , power, effect size, and sample size. As you saw, these 4 quantities form a deterministic relationship; know any 3, and you can caulculate the fourth. While a lower alpha value will lead to fewer type I errors, and a higher power will lead to fewer type II errors, in practice these are often set to common default standards due to exploding sample sizes required to detect various effect sizes. Some common thresholds used are:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eSetting alpha equal to 0.05 (or 0.01)\u003c/li\u003e\n\u003cli\u003eRequiring power values of 0.8 or greater\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eAfter a thorough investigation of this relationship, you then also saw an alternative t-test, Welch's t-test which can be used for comparing samples of different sizes or different variances. While the formula was a bit complicated, the most important piece to remember is that when the assumptions that sample size and sample variance are equal for the two samples is violated, use Welch's t-test rather than the Student's t-test.\u003c/p\u003e\n\u003cp\u003eAside from ensuring that the assumptions of a t-test are met, it's also important to know how type I errors are compounded if you perform multiple tests. This is known as the multiple comparison problem and you saw that type I errors compound under multiple tests. So while the probability of a type I error is equal to \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Calpha\"\u003e for any one test, the collective probability that there is at least 1 type I error continues to increase as you perform more tests, further detracting from the confidence that you have uncovered a meaningful relationship. In order to account for this, you can use stricter criteria when defining \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Calpha\"\u003e such as the Bonferroni correction. Alternatively, ANOVA is equivalent to a 2-sided t-test when comparing two groups, but also generalizes appropriately to multiple group comparisons.\u003c/p\u003e\n\u003ch2\u003eSummary\u003c/h2\u003e\n\u003cp\u003eRemember that simply observing a low p-value is not meaningful in and of itself. There are a number of factors to take into consideration when interpreting the results of a statistical test, from alpha, power, sample size, effect size, and the formulation of the problem itself. Good hypothesis testing requires careful thought and design.\u003c/p\u003e","frontPage":false},{"exportId":"introduction-to-experimental-design","title":"Introduction to Experimental Design","type":"WikiPage","content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-experimental-design\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-experimental-design\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-experimental-design/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eIn this lesson, you'll learn about the importance of sound experimental design, and how it underpins every decision you will make as a Data Scientist!\u003c/p\u003e\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\u003cp\u003eYou will be able to:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eList the steps of the scientific method\u003c/li\u003e\n\u003cli\u003eExplain the purpose of control/experimental groups\u003c/li\u003e\n\u003cli\u003eList four assumptions for appropriate sampling techniques and sample size\u003c/li\u003e\n\u003cli\u003eCompare and explain the importance of different kinds of randomized control trials\u003c/li\u003e\n\u003cli\u003eSet up null and alternative hypotheses\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003eThe Scientific Method\u003c/h2\u003e\n\u003cp\u003eYou probably remember at least a little bit about the \u003cstrong\u003e\u003cem\u003eScientific Method\u003c/em\u003e\u003c/strong\u003e from your time in school. This lesson will focus on the thing that makes it work--sound experimental design! The scientific method has been responsible for all the great progress humanity has seen in everything from medicine to physics to electronics, all because scientists working on problems knew how to design experiments in a way that helped them answer important questions with as little ambiguity as possible. If the scientific method was a car, then experimental design would be the engine that allows that car to move. This is especially important to Data Scientists because it allows them to examine any problem through the lens of the \u003cstrong\u003e\u003cem\u003eNull Hypothesis\u003c/em\u003e\u003c/strong\u003e!\u003c/p\u003e\n\u003cp\u003eThe general structure of an experiment is as follows:\u003c/p\u003e\n\u003ch3\u003e1. Make an Observation\u003c/h3\u003e\n\u003cp\u003eThe first step of the scientific method is to observe something that you want to test. During this step, you must observe phenomena to help refine the question that you want to answer. This might be anything from \"does this drug have an effect on headaches?\" to \"does the color of this button affect the number of sales a website makes in a day?\". Before testing these ideas, you need to observe that there might be some phenomena occurring and then come up with a specific question to answer.\u003c/p\u003e\n\u003ch3\u003e2. Examine the Research\u003c/h3\u003e\n\u003cp\u003eGood data scientists work smart before they work hard. In the case of the scientific method, this means seeing what research already exists that may help you answer your question, directly or indirectly. It could be that someone else has already done an experiment that answers your question--if that's the case, you should be aware of that experiment before starting your own, as it could inform your approach to structuring your experiment, or maybe even answer your question outright!\u003c/p\u003e\n\u003ch3\u003e3. Form a Hypothesis\u003c/h3\u003e\n\u003cp\u003eThis is the stage that most people remember from learning the scientific method in grade school. In school, you learned that a hypothesis is just an educated guess that you will try to prove by conducting an experiment. In reality, it's a bit more complicated than that. During this stage, you'll formulate 2 hypotheses to test--your educated guess about the outcome is called the \u003cstrong\u003e\u003cem\u003eAlternative Hypothesis\u003c/em\u003e\u003c/strong\u003e, while the opposite of it is called the \u003cstrong\u003e\u003cem\u003eNull Hypothesis\u003c/em\u003e\u003c/strong\u003e. This is where the language behind experimental design (and the idea of what you can actually \u003cstrong\u003e\u003cem\u003eprove\u003c/em\u003e\u003c/strong\u003e using an experiment) gets a bit complicated--more on this below.\u003c/p\u003e\n\u003ch3\u003e4. Conduct an Experiment\u003c/h3\u003e\n\u003cp\u003eThis step is the part of the scientific method that will be the focus of this section. You can only test a hypothesis by gathering data from a well-structured experiment. A well-structured experiment is one that accounts for all of the mistakes and randomness that could give you false signals relating to the effect of an intervention. Just because you're running an experiment doesn't prove that A causes B, or that there's even a relationship between A and B! A poorly designed experiment will lead to false conclusions that you haven't considered or controlled for. A well-designed experiment leaves you no choice but to acknowledge that the effects seen in a dependent variable are related to an independent variable. The world is messy and random. You have to account for this messiness and randomness in experiments so that you can filter it out and be left only with the things you're actively trying to measure.\u003c/p\u003e\n\u003ch3\u003e5. Analyze Experimental Results\u003c/h3\u003e\n\u003cp\u003eWhether you realize it or not, you've already gotten pretty good at this step! All the work you've done with statistics is usually in service of this goal--looking at the data and understanding what happened. During this step, you will tease out relationships, filter out noise, and try to determine if something that happened is \u003cstrong\u003e\u003cem\u003estatistically significant\u003c/em\u003e\u003c/strong\u003e or not.\u003c/p\u003e\n\u003ch3\u003e6. Draw Conclusions\u003c/h3\u003e\n\u003cp\u003eThis step is the logical endpoint for an experiment. You've asked a question, looked at experimental results from others that could be related to your question, made an educated guess, designed an experiment, collected data, and analyzed the results. All that is left is to use the results of the analysis step to evaluate whether you believe the hypothesis was correct or not! While the public generally oversimplifies this step for determining causal relationships (e.g. \"my experiment showed that {x} causes {y}\"), true scientists rarely make claims so bold. The reality of this step is that you use your analysis of the data to do one of two things: either \u003cstrong\u003e\u003cem\u003ereject the null hypothesis or fail to reject the null hypothesis\u003c/em\u003e\u003c/strong\u003e. This is a tricky concept, so you'll explore it in much more detail in a future lesson.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-experimental-design/master/images/new_the-scientific-method.png\" width=\"650\"\u003e\u003c/p\u003e\n\u003ch2\u003eThe Foundations of a Sound Experiment\u003c/h2\u003e\n\u003cp\u003eAll experiments are not created equal--simply following the steps outlined above does not guarantee that the results of any experiment will be meaningful. For instance, there's nothing stopping a person from testing the hypothesis that \"wearing a green shirt will make it rain tomorrow!\", seeing rain the next day, and rejecting the null hypothesis, thereby incorrectly \"proving\" that their choice of wardrobe affected the weather. Good experiments demonstrate that independent variables {X} have an effect on the dependent variables {Y} because you control for all the other things that could be affecting {Y}, until you are forced to conclude that the only thing that explains what happened to {Y} is {X}!\u003c/p\u003e\n\u003cp\u003eAlthough there are many different kinds of experiments, there are some fundamental aspects of experimental design that all experiments have:\u003c/p\u003e\n\u003ch3\u003e1. A Control Group/Random Controlled Trials\u003c/h3\u003e\n\u003cp\u003eOne of the most important aspects of a sound experiment is the use of a \u003cstrong\u003e\u003cem\u003eControl Group\u003c/em\u003e\u003c/strong\u003e. A Control Group is a cohort that receives no treatment or intervention--for them, it's just business as usual. In a medical test, this might be a \u003cstrong\u003e\u003cem\u003eplacebo\u003c/em\u003e\u003c/strong\u003e, such as a sugar pill. In the example of testing the color of a button on a website, this would be customers that are shown a version of the website with the button color unchanged. Using a control group allows researchers to compare the results of doing nothing (the control) with the effects of doing something (the \u003cstrong\u003e\u003cem\u003eintervention\u003c/em\u003e\u003c/strong\u003e). Without a control group, you have no way of knowing how much of the results you see can be attributed to the intervention, and how much would have happened anyways.\u003c/p\u003e\n\u003cp\u003eTo make this more obvious, consider what you can actually know with confidence after an experiment that doesn't use a control. Assume that a pharmaceutical company decides to test a new drug that is supposed to reduce the amount of time someone has the flu. The company gives the drug to all participants in the study. After analyzing the data, you find that the average length of time a person had the flu was 12 days. Was the drug effective, or not? Without a control, you don't know how long this flu would have lasted if these people were never given a drug. It could be that your drug reduced the time of infection down to 12 days. Then again, it could be that these people would have gotten better on their own after 12 days, and the drug didn't really do anything--or maybe they would have gotten better in 10 days, and the drug made it worse! By using a control group that gets no drugs and recovers naturally, you can compare the results of the treatment (people that received the experimental flu drug) to your control group (people that recovered naturally).\u003c/p\u003e\n\u003cp\u003eNote that a control group is only a control group if they are sampled from the same population as the treatment groups! If they aren't the same, then there's no way of knowing how much the difference in recovery time should be attributed to the flu drug, and how much should be attributed to the way(s) in which the control group is different. For instance, the experiment would not be very effective if the average age of one group was much higher or lower than another. If that was the case, how would you know the age difference isn't actually causing the difference in results (or lack thereof) between the control and treatment groups, instead of the drug intervention?\u003c/p\u003e\n\u003cp\u003eThe main way scientists deal with this is through \u003cstrong\u003e\u003cem\u003eRandom Controlled Trials\u003c/em\u003e\u003c/strong\u003e. In a Random Controlled Trial, there is a control group and an intervention (also called treatment) group, where subjects are \u003cstrong\u003e\u003cem\u003erandomly assigned to each\u003c/em\u003e\u003c/strong\u003e. You may have heard the term \u003cstrong\u003e\u003cem\u003eSingle-Blind\u003c/em\u003e\u003c/strong\u003e and \u003cstrong\u003e\u003cem\u003eDouble-Blind\u003c/em\u003e\u003c/strong\u003e studies--these refer to people knowing which groups they are in. In a sound experiment, people should not know if they are in the treatment group or the control group, as that could potentially affect the outcome of the trial!\u003c/p\u003e\n\u003cp\u003eA \u003cstrong\u003e\u003cem\u003eSingle-Blind\u003c/em\u003e\u003c/strong\u003e or \u003cstrong\u003e\u003cem\u003eBlind Trial\u003c/em\u003e\u003c/strong\u003e is one where the participant does not know if they are receiving the treatment or a placebo.\u003c/p\u003e\n\u003cp\u003eA \u003cstrong\u003e\u003cem\u003eDouble-Blind Trial\u003c/em\u003e\u003c/strong\u003e is one where the participant does not know if they are receiving the treatment or a placebo, and neither does the person administering the experiment (because their bias could affect the outcomes, too!). Instead, knowing whether someone received the treatment or a placebo is kept hidden from everyone until after the experiment is over (obviously, \u003cem\u003esomeone\u003c/em\u003e has to know for recordkeeping purposes, but that person stays away from the actual experiment to avoid contaminating it with bias from that knowledge).\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-experimental-design/master/images/new_double_blind.png\" width=\"650\"\u003e\u003c/p\u003e\n\u003ch3\u003e2. Appropriate Sampling Techniques and Sample Size\u003c/h3\u003e\n\u003cp\u003eWhen data scientists are performing experiments, they rarely have the opportunity to work with an entire population of data. Rather, they must obtain a sample that is representative of the population. In order to get a high quality sample, you should follow these four assumptions related to sampling techniques and sample size.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eSample is independent\u003c/strong\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eIndependence means the value of one observation does not influence or affect the value of other observations. Independent data items are not connected with one another in any way (unless you account for it in your model). This includes the observations in both the “between” and “within” groups of your sample. Non-independent observations introduce bias and can make your statistical test give too many false positives.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eSample is collected randomly\u003c/strong\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eA sample is random when each data point in your population has an equal chance of being included in the sample; therefore, the selection of any individual observation happens by chance, rather than by choice. This reduces the chance that differences in materials or conditions strongly bias results. Random samples are more likely to be representative of the population; therefore, you can be more confident with your statistical inferences with a random sample.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eThe sample is approximately normally distributed\u003c/strong\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThe normal distribution assumption is that the sampling distribution of the mean is normal. That is, if you took a sample, calculated its mean, and then you took another (independent) sample (from the same population) and got its mean (and repeated this an infinite number of times), then the distribution of the values that you wrote down would always be a perfect bell curve. This is the principle behind the Central Limit Theorem, and it is this idea that allows us to perform hypothesis tests. While maybe surprising, this assumption turns out to be relatively uncontroversial, at least when each of the samples is large, such as N ≥ 30.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eAppropriate Sample Size\u003c/strong\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eRandomness is a big problem in experiments. It can lead you to false conclusions by making you think that something doesn't matter when it does, or vice versa. Small sample sizes make experiments susceptible to the problem of randomness; whereas, large sample sizes protect experiments from it. The following scenario illustrates this point:\u003c/p\u003e\n\u003cp\u003eA person tells you that they can predict the outcome of a fair coin flip. You flip a coin, they call \"tails\", and they are correct. Is this enough evidence to accept or reject this person's statement? What if they got it right 2 times in a row? 5 times in a row? 55 times out of 100?\u003c/p\u003e\n\u003cp\u003eThis situation illustrates two things that are important for us to understand and acknowledge:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003eNo matter how large your sample size, there's always a chance that your results can be attributed to randomness or luck.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eAt some point, you would cross a threshold where random chance is small enough that you'd say \"this probably isn't random\", and are okay with accepting the results as the result of something other than randomness or luck.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eWith the situation above, you probably wouldn't assume that this person can predict coin flips after only seeing them get 1 correct. However, if this person got 970 out of 1000 correct, you would probably believe very strongly that this person \u003cem\u003ecan\u003c/em\u003e predict coin flips because the odds of guessing randomly and getting 970/1000 correct are very, very small--but not 0!\u003c/p\u003e\n\u003cp\u003eLarge sample sizes protect us from randomness and variance. A more realistic example would be testing a treatment for HIV. Less than 1% of the global population carries a protective mutation that makes them resistant to HIV infection. If you took a randomly selected sample of 1 person from the population, there is a ~1% chance that you may mistakenly attribute successful prevention to the drug you're testing, when the results really happened because you randomly selected a person with this mutation. However, if your sample size was 100 people per sample, your odds of randomly selecting 100 people with that mutation are \u003cimg src=\"https://render.githubusercontent.com/render/math?math=0.01%5E%7B100%7D\"\u003e . The larger your sample size, the more unlikely it is that you randomly draw people that happen to affect your study in a way that is not reflected by the general population.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-experimental-design/master/images/new_sample_size.png\" width=\"650\"\u003e\u003c/p\u003e\n\u003ch3\u003e3. Reproducibility\u003c/h3\u003e\n\u003cp\u003eThis one is a big one, and it represents a bit of a crisis in some parts of the scientific community right now. Good scientific experiments have \u003cstrong\u003e\u003cem\u003eReproducible Results\u003c/em\u003e\u003c/strong\u003e! This means that if someone else follows the steps you outline for your experiment and performs it themselves, they should get pretty much the same results as you did (allowing for natural variance and randomness). If many different people try reproducing your experiment and don't get the same results, this might suggest that your results are due to randomness, or to a \u003cstrong\u003e\u003cem\u003elurking variable\u003c/em\u003e\u003c/strong\u003e that was present in your samples that wasn't present in others. Either way, a lack of reproducibility often casts serious doubts on the results of a study or experiment.\u003c/p\u003e\n\u003cp\u003eThis is less of a problem for data scientists, since reproducibility usually just means providing the dataset you worked with and the corresponding Jupyter notebook. However, this isn't always the case! Luckily, you can use code to easily run your experiments multiple times and show reproducibility. When planning experiments, consider running them multiple times to ensure to really help show that your results are sound, and not due to randomness!\u003c/p\u003e\n\u003ch2\u003eSummary\u003c/h2\u003e\n\u003cp\u003eGreat, you now know about experimental design and the fundamental aspects of experiments!\u003c/p\u003e","frontPage":false},{"exportId":"extensions-to-linear-models-recap","title":"Extensions To Linear Models - Recap","type":"WikiPage","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-extensions-to-linear-models-recap-v2-1\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-extensions-to-linear-models-recap-v2-1/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eCongratulations, you've just modeled some complex non-linear relationships with interaction terms and polynomials! Here is a recap of what you learned in this section.\u003c/p\u003e\n\u003ch2\u003eKey Takeaways\u003c/h2\u003e\n\u003cp\u003eThis section gave you the chance to learn about techniques whereby you can model non-linear relationships between the predictor and target variables. It's very rare that real-world problems can be modeled with a simple linear regression, so it's important to get yourself well acquainted with creating new features and selecting the most important ones.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eAn interaction is a particular property of two or more variables where they interact in a non-additive manner when affecting a third variable\u003c/li\u003e\n\u003cli\u003ePolynomial regression allows for bringing in higher orders of predictor variables (such as squared, cubed, etc)\u003c/li\u003e\n\u003cli\u003eThe risk of polynomial regression is that they can easily overfit to data, so it's important to consider the Bias-variance tradeoff when building models with greater complexity\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003eSummary\u003c/h2\u003e\n\u003cp\u003eExcellent work! You learned a substantial amount about different ways to model non-linear relationships. You will continue to use and build upon the concepts learned in this section for the rest of your machine learning career.\u003c/p\u003e\n\u003cfooter class=\"fis-footer\" style=\"visibility: hidden;\"\u003e\n\u003cdiv class=\"fis-feedback\"\u003e\n\u003ch5\u003eHow do you feel about this lesson?\u003c/h5\u003e\n\u003cimg id=\"thumbs-up\" title=\"Thumbs up!\" alt=\"thumbs up\" data-repository=\"dsc-extensions-to-linear-models-recap-v2-1\"\u003e\u003cimg id=\"thumbs-down\" title=\"Thumbs down!\" alt=\"thumbs down\" data-repository=\"dsc-extensions-to-linear-models-recap-v2-1\"\u003e\n\u003c/div\u003e\n\u003ch5\u003eHave specific feedback? \u003ca href=\"https://github.com/learn-co-curriculum/dsc-extensions-to-linear-models-recap-v2-1/issues/new\"\u003eTell us here!\u003c/a\u003e\n\u003c/h5\u003e\n\u003c/footer\u003e","frontPage":false},{"exportId":"the-cumulative-distribution-function","title":"The Cumulative Distribution Function","type":"WikiPage","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-cumulative-distribution-function\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-cumulative-distribution-function/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eThe PMF function that we saw before works great for inspecting discrete random variables and calculating their expected values. However, we did see that when moving towards continuous random variables, obtaining probabilities for observing a specific outcome is not possible (or, simply put, the probabilities were 0). We also noted that when working with PDFs, you can't really read the y-axis and have to be careful with interpretation. In this lesson, you'll learn about the cumulative distribution function (CDF) and how it is useful to overcome these issues.\u003c/p\u003e\n\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\n\u003cp\u003eYou will be able to:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eDifferentiate between a PMF, PDF, and a CDF in terms of cumulative probabilities \u003c/li\u003e\n\u003cli\u003eCalculate CDF in Python for a given discrete variable with limited set of possible values\u003c/li\u003e\n\u003cli\u003eVisualize and inspect a given CDF in order to make assumptions about the underlying data \u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch2\u003eLimitations of PMFs and PDFs\u003c/h2\u003e\n\n\u003cp\u003eTo illustrate the use of Cumulative Distribution Functions, let's have another look at the PMF and PDF of our dice and temperature example:\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-cumulative-distribution-function/master/images/dice_and_temp.png\" width=\"900\"\u003e\u003c/p\u003e\n\n\u003cp\u003eRecall how we could easily read probabilities from the dice PMF plot (\"The probability of throwing a 4 is 16.66%\"), but it is much harder to interpret the temperature PDF. What is the probability that the temperature is exactly 80 degrees? We learned in the previous lesson that all these so-called \"point probabilities\" are 0, so the bottom line is that it is very hard to \"read\" any interesting information from a PDF. The PDF is mainly there to get a sense of the data density, but you cannot readily read the y-axis to get to probabilities.\u003c/p\u003e\n\n\u003cp\u003eWe did see last that when you want to calculate probabilities, you need to take integrals and look at ranges of values of your continuous random variables. For example, you can ask yourself the question: \"What is the probability the temperature in NYC is between 82 and 85 degrees on June 1?\" The answer is the surface of the red shaded area!\u003c/p\u003e\n\n\u003cp\u003eFrom the last lesson, you learned that you can use the integral to get this \"area under the curve\" value by taking the integral as follows:\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-cumulative-distribution-function/master/images/section_temp.png\" width=\"650\"\u003e\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Clarge%20P(82%20%5Cleq%20X%20%5Cleq%2085)%20=%20%5Cint_%7B82%7D%5E%7B85%7D%20f_x(x)%20dx%20%5Cgeq%200\"\u003e \u003c/p\u003e\n\n\u003cp\u003eThis is the rationale that is being used when working with Cumulative Density Functions, which will be introduced next.\u003c/p\u003e\n\n\u003ch2\u003eHow does a Cumulative Density Function (CDF) work?\u003c/h2\u003e\n\n\u003cp\u003eThe CDF is a function of \u003cimg src=\"https://render.githubusercontent.com/render/math?math=x\"\u003ejust like a PMF or a PDF, where \u003cimg src=\"https://render.githubusercontent.com/render/math?math=x\"\u003eis any value that can possibly appear in a given distribution. To calculate the \u003cimg src=\"https://render.githubusercontent.com/render/math?math=CDF(x)\"\u003efor any value of \u003cimg src=\"https://render.githubusercontent.com/render/math?math=x\"\u003e, we compute the proportion of values in the distribution less than or equal to \u003cimg src=\"https://render.githubusercontent.com/render/math?math=x\"\u003eas follows:\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Clarge%20F(x)%20=%20P(X%20%5Cleq%20x)\"\u003e\u003c/p\u003e\n\n\u003cblockquote\u003e\n\u003cp\u003eThe Cumulative Distribution Function, CDF, gives the probability that the variable \u003cimg src=\"https://render.githubusercontent.com/render/math?math=X\"\u003eis less than or equal to a certain possible value \u003cimg src=\"https://render.githubusercontent.com/render/math?math=x\"\u003e. \u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003cp\u003eThe cumulative distribution functions for a dice roll and the weather in NYC are plotted below.\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-cumulative-distribution-function/master/images/cdfs_dice_nyc_2.png\" width=\"950\"\u003e\u003c/p\u003e\n\n\u003cp\u003eThis is also what \"cumulative\" means - you're simply adding up probabilities.\u003c/p\u003e\n\n\u003cp\u003eYou'll notice that in general, CDFs are smooth curves for continuous random variables, where they are \"step functions\" when looking at discrete random variables. Looking at these curves, we can answer questions by looking at the y-axis.\u003c/p\u003e\n\n\u003cp\u003eWhat is the probability that you throw a value \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cleq4\"\u003e when throwing a dice? 0.6667 or 66.67. For this discrete example it is pretty straightforward, as this is the probability of throwing a 1 OR 2 OR 3 OR 4, so \u003cimg src=\"https://render.githubusercontent.com/render/math?math=0.1666%20*%204\"\u003e.\u003c/p\u003e\n\n\u003cp\u003eWhat is the probability that the temperature in NYC is \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cleq79\"\u003e? Looking at the associated y-value when looking at an \u003cimg src=\"https://render.githubusercontent.com/render/math?math=x\"\u003e-value of 79, this probability is around 40% or 0.4.\u003c/p\u003e\n\n\u003ch2\u003eCalculating more probabilities using the CDF\u003c/h2\u003e\n\n\u003cp\u003eLet's go back to our weather example introduced before. An additional advantage of CDFs is that you can use them to easily calculate things like:\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-cumulative-distribution-function/master/images/section_temp.png\" width=\"650\"\u003e\u003c/p\u003e\n\n\u003cp\u003eThe idea is that\n\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Clarge%20P(82%20%5Cleq%20X%20%5Cleq%2085)%20=%20P(X%20%5Cleq%2085)%20-%20P(X%20%5Cleq%2082)=%20F_X(85)-%20F_X(82)\"\u003e\u003c/p\u003e\n\n\u003cp\u003eThis means that you can look at the y-value of your cumulative density function to get the answer to this question.\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Clarge%20F_X(85)-%20F_X(82)%20%5Capprox%200.95-0.6%20=%200.35\"\u003e\u003c/p\u003e\n\n\u003ch2\u003eSummary\u003c/h2\u003e\n\n\u003cp\u003eIn this lesson, we looked at a CDF as a so-called \"percentile probability function\" of discrete or continuous random variables. You learned how to calculate and visualize a CDF and how to use them to calculate certain probabilities.\u003c/p\u003e\n\u003cfooter class=\"fis-footer\" style=\"visibility: hidden;\"\u003e\u003cdiv class=\"fis-feedback\"\u003e\n\u003ch5\u003eHow do you feel about this lesson?\u003c/h5\u003e\n\u003cimg id=\"thumbs-up\" data-repository=\"dsc-cumulative-distribution-function\" title=\"Thumbs up!\" alt=\"thumbs up\"\u003e\u003cimg id=\"thumbs-down\" data-repository=\"dsc-cumulative-distribution-function\" title=\"Thumbs down!\" alt=\"thumbs down\"\u003e\n\u003c/div\u003e\n\u003ch5\u003eHave specific feedback? \u003ca href=\"https://github.com/learn-co-curriculum/dsc-cumulative-distribution-function/issues/new\"\u003eTell us here!\u003c/a\u003e\n\u003c/h5\u003e\u003c/footer\u003e","frontPage":false},{"exportId":"introduction-to-probability","title":"Introduction to Probability","type":"WikiPage","content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-intro-to-probability\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-intro-to-probability\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-intro-to-probability/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eNow that you understand the basics of sets, you'll learn how this knowledge can be used to calculate your first probabilities! In this section, you'll learn how to use sets to create probabilities and you'll learn about the very foundations of probability through the three probability axioms.\u003c/p\u003e\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\u003cp\u003eYou will be able to:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eCompare experiments, outcomes, and the event space\u003c/li\u003e\n\u003cli\u003eCalculate probabilities by using relative frequency of outcomes to event space\u003c/li\u003e\n\u003cli\u003eDescribe the three axioms of probability\u003c/li\u003e\n\u003cli\u003eDescribe the addition law of probability\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003eExperiment and outcomes\u003c/h2\u003e\n\u003cp\u003ePreviously, we defined sets and related concepts. Now let's look at the set\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=S=%20%20%20%5C%7B1,2,3,4,5,6%20%5C%7D\"\u003e , which contains all possible outcomes when throwing a dice.\u003c/p\u003e\n\u003cp\u003eWhen you throw a dice once, you can consider this a \u003cem\u003erandom experiment\u003c/em\u003e. The result of this \"experiment\" is the \u003cem\u003eoutcome\u003c/em\u003e.\u003c/p\u003e\n\u003cp\u003eYou can then say that:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cimg src=\"https://render.githubusercontent.com/render/math?math=S\"\u003e defines all the \u003cstrong\u003epossible outcomes\u003c/strong\u003e when throwing the dice once\u003c/li\u003e\n\u003cli\u003e\n\u003cimg src=\"https://render.githubusercontent.com/render/math?math=S\"\u003e is our Universal set \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5COmega\"\u003e , as seen before\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eWhen conducting experiments, you say that your universal set is your \u003cstrong\u003esample space\u003c/strong\u003e: it is the universe in which your possible outcomes are listed as elements.\u003c/p\u003e\n\u003cp\u003eOther examples of sample spaces: - The number of text messages you send each day: in this case, S is equal to some number x, with x being a \u003cstrong\u003epositive integer\u003c/strong\u003e, or mathematically: \u003cimg src=\"https://render.githubusercontent.com/render/math?math=S%20=%20%20%20%5C%7Bx%20%20%20%5Cmid%20x%20%20%20%5Cin%20%20%20%5Cmathbb%7BZ%7D,%20x%20%20%20%5Cgeq%200%20%5C%7D\"\u003e - The number of hours someone watches TV each day: \u003cimg src=\"https://render.githubusercontent.com/render/math?math=S%20=%20%20%20%5C%7Bx%20%20%20%5Cmid%20x%20%20%20%5Cin%20%20%20%5Cmathbb%7BR%7D,%200%20%20%20%5Cleq%20x%20%20%20%5Cleq%2024%20%20%20%5C%7D\"\u003e\u003c/p\u003e\n\u003ch2\u003eEvent space\u003c/h2\u003e\n\u003cp\u003eNext, let's define event space. The \u003cstrong\u003eevent space\u003c/strong\u003e is a subset of the sample space, \u003cimg src=\"https://render.githubusercontent.com/render/math?math=E%20%20%20%5Csubseteq%20S\"\u003e\u003c/p\u003e\n\u003cp\u003eFor example, the event \"throwing a number higher than 4\" when throwing a dice would result in an event space \u003cimg src=\"https://render.githubusercontent.com/render/math?math=E=%20%20%20%5C%7B5,6%20%5C%7D\"\u003e . Throwing an odd number would lead to an event space \u003cimg src=\"https://render.githubusercontent.com/render/math?math=E=%20%20%20%5C%7B1,3,5%20%5C%7D\"\u003e .\u003c/p\u003e\n\u003cp\u003eSummarized, the event space is a collection of events that we \u003cem\u003ecare\u003c/em\u003e about. We say that event \u003cimg src=\"https://render.githubusercontent.com/render/math?math=E\"\u003e happened if the actual outcome after rolling the dice belongs to the predefined event space \u003cimg src=\"https://render.githubusercontent.com/render/math?math=E\"\u003e .\u003c/p\u003e\n\u003cp\u003eWith \u003cstrong\u003esample space\u003c/strong\u003e and \u003cstrong\u003eevent space\u003c/strong\u003e, you now understand the two foundational concepts of \u003cstrong\u003eprobability\u003c/strong\u003e.\u003c/p\u003e\n\u003cp\u003eOther examples of event spaces based on previously defined sample spaces:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eIf you define that the event \"low daily number of text messages sent\" means 20 or fewer text messages, the event space is defined as: \u003cimg src=\"https://render.githubusercontent.com/render/math?math=E%20=%20%20%20%5C%7Bx%20%20%20%5Cmid%20x%20%20%20%5Cin%20%20%20%5Cmathbb%7BZ%7D,%200%20%20%20%5Cleq%20x%20%20%20%5Cleq%2020%20%20%20%5C%7D\"\u003e\n\u003c/li\u003e\n\u003cli\u003eBinge-watch day: \u003cimg src=\"https://render.githubusercontent.com/render/math?math=E%20=%20%20%20%5C%7Bx%20%20%20%5Cmid%20x%20%20%20%5Cin%20%20%20%5Cmathbb%7BR%7D,%20x%20%20%20%5Cgeq%206%20%20%20%5C%7D\"\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003eIntroduction to probability\u003c/h2\u003e\n\u003ch3\u003eThe law of relative frequency\u003c/h3\u003e\n\u003cp\u003eWhile conducting an endless stream of experiments, the relative frequency by which an event will happen becomes a fixed number.\u003c/p\u003e\n\u003cp\u003eLet's denote an event by \u003cimg src=\"https://render.githubusercontent.com/render/math?math=E\"\u003e , and the \u003cem\u003eprobability\u003c/em\u003e of the event \u003cimg src=\"https://render.githubusercontent.com/render/math?math=E\"\u003e occurring by \u003cimg src=\"https://render.githubusercontent.com/render/math?math=P(E)\"\u003e . Next, let \u003cimg src=\"https://render.githubusercontent.com/render/math?math=n\"\u003e be the number of conducted experiments, and \u003cimg src=\"https://render.githubusercontent.com/render/math?math=S(n)\"\u003e the count of \"successful\" experiments (i.e. the times that event \u003cimg src=\"https://render.githubusercontent.com/render/math?math=E\"\u003e happened). The formal definition of probability as a relative frequency is given by:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=P(E)%20=%20%20%20%5Clim_%7Bn%20%5Crightarrow%20%5Cinfty%7D%20%20%20%5Cdfrac%7BS%7B(n)%7D%7D%7Bn%7D\"\u003e\u003c/p\u003e\n\u003cp\u003eThis is the basis of a frequentist statistical interpretation: an event's probability is the ratio of the positive trials to the total number of trials as we repeat the process infinitely.\u003c/p\u003e\n\u003cp\u003eFor example, the probability of rolling a 5 on a 6 sided dice is the limit of the successes to trials as the number of trials goes to infinity.\u003c/p\u003e\n\u003ch3\u003eProbability axioms\u003c/h3\u003e\n\u003cp\u003eIn the early 20th century, Kolmogorov and Von Mises came up with three axioms that further expand on the idea of probability. The three axioms are:\u003c/p\u003e\n\u003ch4\u003e1. Positivity\u003c/h4\u003e\n\u003cp\u003eA probability is always bigger than or equal to 0, or \u003cimg src=\"https://render.githubusercontent.com/render/math?math=0%20%20%20%5Cleq%20P(E)%20%20%20%5Cleq%201\"\u003e\u003c/p\u003e\n\u003ch4\u003e2. Probability of a certain event\u003c/h4\u003e\n\u003cp\u003eIf the event of interest is the sample space, we say that the outcome is a certain event, or \u003cimg src=\"https://render.githubusercontent.com/render/math?math=P(S)%20=%201\"\u003e\u003c/p\u003e\n\u003ch4\u003e3. Additivity\u003c/h4\u003e\n\u003cp\u003eThe probability of the union of two exclusive events is equal to the sum of the probabilities of the individual events happening.\u003c/p\u003e\n\u003cp\u003eIf \u003cimg src=\"https://render.githubusercontent.com/render/math?math=A%20%20%20%20%5Ccap%20B%20=%20%20%20%5Cemptyset\"\u003e , then \u003cimg src=\"https://render.githubusercontent.com/render/math?math=P(A%20%20%5Ccup%20B)%20=%20P(A)%20%2b%20P(B)\"\u003e\u003c/p\u003e\n\u003ch3\u003eAddition law of probability\u003c/h3\u003e\n\u003cp\u003eThe additivity axiom is great, but most of the time events are not exclusive. A very important property is the \u003cstrong\u003eaddition law or probability\u003c/strong\u003e or the \u003cstrong\u003esum rule\u003c/strong\u003e.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=P(A%20%20%5Ccup%20B)%20=%20P(A)%20%2b%20P(B)%20-%20P(A%20%20%20%20%5Ccap%20B)\"\u003e\u003c/p\u003e\n\u003cp\u003ePut in words, the probability that \u003cimg src=\"https://render.githubusercontent.com/render/math?math=A\"\u003e or \u003cimg src=\"https://render.githubusercontent.com/render/math?math=B\"\u003e will happen is the sum of the probabilities that \u003cimg src=\"https://render.githubusercontent.com/render/math?math=A\"\u003e will happen and that \u003cimg src=\"https://render.githubusercontent.com/render/math?math=B\"\u003e will happen, minus the probability that \u003cem\u003eboth\u003c/em\u003e \u003cimg src=\"https://render.githubusercontent.com/render/math?math=A\"\u003e and \u003cimg src=\"https://render.githubusercontent.com/render/math?math=B\"\u003e will happen.\u003c/p\u003e\n\u003ch2\u003eExamples\u003c/h2\u003e\n\u003cp\u003eLet's reconsider the dice example to explain what was explained before:\u003c/p\u003e\n\u003ch3\u003eAdditivity of exclusive events\u003c/h3\u003e\n\u003cp\u003eLet's consider two events: event \u003cimg src=\"https://render.githubusercontent.com/render/math?math=M\"\u003e means throwing a 6, event \u003cimg src=\"https://render.githubusercontent.com/render/math?math=N\"\u003e means that you throw an odd number \u003cimg src=\"https://render.githubusercontent.com/render/math?math=N=%7B1,3,5%7D\"\u003e . These events are exclusive, and you can use the additivity rule if you want to know the answer to the question:\u003c/p\u003e\n\u003cp\u003e\u003cem\u003e\"what is the probability that your outcome will be a 6, or an odd number?\"\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=P(M%20%20%5Ccup%20N)%20=%20P(M)%20%2b%20P(N)%20=%20%20%20%5Cdfrac%7B1%7D%7B6%7D%2b%20%5Cdfrac%7B3%7D%7B6%7D=%20%5Cdfrac%7B4%7D%7B6%7D\"\u003e\u003c/p\u003e\n\u003ch3\u003eAddition law of probability\u003c/h3\u003e\n\u003cp\u003eNow, let's consider the same event \u003cimg src=\"https://render.githubusercontent.com/render/math?math=N=%7B1,3,5%7D\"\u003e and another event \u003cimg src=\"https://render.githubusercontent.com/render/math?math=Q=%7B4,5%7D\"\u003e . These events are \u003cem\u003enot\u003c/em\u003e mutually exclusive, so if you want to know the probability that \u003cimg src=\"https://render.githubusercontent.com/render/math?math=N\"\u003e or \u003cimg src=\"https://render.githubusercontent.com/render/math?math=Q\"\u003e will happen, you need to use the addition law of probability.\u003c/p\u003e\n\u003cp\u003eNote that \u003cimg src=\"https://render.githubusercontent.com/render/math?math=(N%20%20%20%20%5Ccap%20Q)\"\u003e is equal to getting an outcome of 5, as that is the \"common\" element in the respective event spaces of \u003cimg src=\"https://render.githubusercontent.com/render/math?math=N\"\u003e and \u003cimg src=\"https://render.githubusercontent.com/render/math?math=Q\"\u003e . This means that \u003cimg src=\"https://render.githubusercontent.com/render/math?math=P(N%20%20%20%20%5Ccap%20Q)%20=%20%20%20%5Cdfrac%7B1%7D%7B6%7D\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=P(N%20%20%5Ccup%20Q)%20=%20P(N)%20%2b%20P(Q)%20-%20P(N%20%20%20%20%5Ccap%20Q)%20=%20%20%20%5Cdfrac%7B3%7D%7B6%7D%20%2b%20%20%20%5Cdfrac%7B2%7D%7B6%7D%20-%20%20%20%5Cdfrac%7B1%7D%7B6%7D%20=%20%20%20%5Cdfrac%7B4%7D%7B6%7D\"\u003e\u003c/p\u003e\n\u003ch2\u003eFinal Note\u003c/h2\u003e\n\u003cp\u003eIn the previous examples, you noticed that for our dice example, it is easy to use these fairly straightforward probability formulas to calculate probabilities of certain outcomes.\u003c/p\u003e\n\u003cp\u003eHowever, if you think about our text message example, things are less straightforward, e.g.:\u003c/p\u003e\n\u003cp\u003e\u003cem\u003e\"What is the probability of sending less than 20 text messages in a day?\"\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003eThis is where the probability concepts introduced here fall short. The probability of throwing any number between 1 and 6 with a die is always exactly \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cdfrac%7B1%7D%7B6%7D\"\u003e , but we can't simply count our messages event space. In words, the probability of sending 20 messages is likely different than the probability of sending, say, 5 messages, and will be different for any number of messages sent. You'll learn about tools to solve problems like these later on.\u003c/p\u003e\n\u003ch2\u003eSummary\u003c/h2\u003e\n\u003cp\u003eWell done! In this section, you learned how to use sets to get to probabilities. You learned about experiments, event spaces, and outcomes. Next, you learned about the law of relative frequency and how it can be used to calculate probabilities, along with the three probability axioms.\u003c/p\u003e","frontPage":false},{"exportId":"conditional-probability","title":"Conditional Probability","type":"WikiPage","content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-conditional-probability\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-conditional-probability\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-conditional-probability/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eIn the previous lessons and labs, you learned about some fundamentals of probability theory, along with basic combinatorics such as permutations and combinations. You'll now extend your knowledge of probability by learning about \u003cstrong\u003eConditional Probability\u003c/strong\u003e. You'll see how Conditional Probability is extremely important in Statistics, and the foundation of many applications. Understanding conditional probability is essential when exploring fields in Machine Learning and Artificial Intelligence.\u003c/p\u003e\n\u003cp\u003eIn this lesson, you'll learn about conditional probability, what it is, and how and when to use it. Later on, you'll see how this simple idea becomes a key component in most statistical machine learning algorithms.\u003c/p\u003e\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\u003cp\u003eYou will be able to:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eDifferentiate between independent and dependent events\u003c/li\u003e\n\u003cli\u003eUse the multiplication rule to find the probability of the intersection of two events\u003c/li\u003e\n\u003cli\u003eUse conditional probability to explain the Product Rule, Chain Rule, and Bayes Theorem\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003eEvents and Sample Space\u003c/h2\u003e\n\u003cp\u003eBefore introducing you to specific event types, let's do a quick recap of the notion of event and sample space.\u003c/p\u003e\n\u003cp\u003eAn \u003cstrong\u003eevent\u003c/strong\u003e is the outcome of an experiment, for example, obtaining heads when tossing of a coin or getting 3 after a dice roll. Note: an event can also be a collection of different events grouped together (or a so-called \u003cstrong\u003ecompound\u003c/strong\u003e event), e.g. getting a 3 twice when rolling a dice twice.\u003c/p\u003e\n\u003cp\u003eA \u003cstrong\u003esample space\u003c/strong\u003e is a collection of every single possible outcome in a trial, generally represented by \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5COmega\"\u003e . The sample space for 1 random dice throw is {1,2,3,4,5,6}.\u003c/p\u003e\n\u003cp\u003eAs you remember for the previous lessons, we can combine events and sample space to compute event probability.\u003c/p\u003e\n\u003cp\u003eYou'll learn about 3 important event types: \u003cstrong\u003eindependent\u003c/strong\u003e, \u003cstrong\u003edisjoint\u003c/strong\u003e, and \u003cstrong\u003edependent\u003c/strong\u003e events.\u003c/p\u003e\n\u003ch3\u003eIndependent Events\u003c/h3\u003e\n\u003cp\u003e\u003cstrong\u003eEvents \u003cimg src=\"https://render.githubusercontent.com/render/math?math=A\"\u003e and \u003cimg src=\"https://render.githubusercontent.com/render/math?math=B\"\u003e are independent when the occurrence of \u003cimg src=\"https://render.githubusercontent.com/render/math?math=A\"\u003e has no effect on whether \u003cimg src=\"https://render.githubusercontent.com/render/math?math=B\"\u003e will occur (or not).\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eConsider the following independent events\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eGetting heads after flipping a coin \u003cstrong\u003eand\u003c/strong\u003e getting a 5 after throwing a fair dice\u003c/li\u003e\n\u003cli\u003eChoosing a marble from a container \u003cstrong\u003eand\u003c/strong\u003e getting heads after flipping a coin\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch4\u003eTwo independent events\u003c/h4\u003e\n\u003cp\u003eFormally, events A and B are independent if:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=P(A%20%20%5Ccap%20B)%20=%20P(A)P(B)\"\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThe probability of A or B occurring, \u003cimg src=\"https://render.githubusercontent.com/render/math?math=P%20(A%20%20%5Ccup%20B)\"\u003e , is given by the addition rule of probability:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=P%20(A%20%20%5Ccup%20B)%20=%20P(A)%20%2b%20P(B)%20-%20P(A%20%20%5Ccap%20B)\"\u003e\u003c/p\u003e\n\u003cp\u003eWe subtract the intersection of the two events to avoid over-counting. See the diagram below for some intuition:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-conditional-probability/master/images/Image_67_independent.png\" width=\"500\"\u003e\u003c/p\u003e\n\u003cp\u003eThus, in the case of two independent events, by substitution,\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=P%20(A%20%20%5Ccup%20B)%20=%20P(A)%20%2b%20P(B)%20-%20P(A)P(B).\"\u003e\u003c/p\u003e\n\u003ch4\u003eThree independent events\u003c/h4\u003e\n\u003cp\u003eThree events, A, B and C, are independent if:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=P(A%20%20%5Ccap%20B)%20=%20P(A)P(B)\"\u003e\u003c/li\u003e\n\u003cli\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=P(A%20%20%5Ccap%20C)%20=%20P(A)P(C)\"\u003e\u003c/li\u003e\n\u003cli\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=P(B%20%20%5Ccap%20C)%20=%20P(B)P(C)\"\u003e\u003c/li\u003e\n\u003cli\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=P(A%20%20%5Ccap%20B%20%20%5Ccap%20C)%20=%20P(A)P(B)P(C)\"\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eSo you need both \u003cem\u003epairwise independence\u003c/em\u003e and \u003cem\u003ethree-way independence\u003c/em\u003e\u003c/p\u003e\n\u003ch3\u003eDisjoint Events\u003c/h3\u003e\n\u003cp\u003e\u003cstrong\u003eEvents \u003cimg src=\"https://render.githubusercontent.com/render/math?math=A\"\u003e and \u003cimg src=\"https://render.githubusercontent.com/render/math?math=B\"\u003e are disjoint if \u003cimg src=\"https://render.githubusercontent.com/render/math?math=A\"\u003e occurring means that \u003cimg src=\"https://render.githubusercontent.com/render/math?math=B\"\u003e cannot occur.\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eDisjoint events are \u003cstrong\u003emutually exclusive\u003c/strong\u003e. \u003cimg src=\"https://render.githubusercontent.com/render/math?math=P%20(A%20%20%5Ccap%20B)\"\u003e is \u003cstrong\u003eempty\u003c/strong\u003e.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-conditional-probability/master/images/Image_68Disjoint.png\" width=\"500\"\u003e\u003c/p\u003e\n\u003ch3\u003eDependent Events\u003c/h3\u003e\n\u003cp\u003e\u003cstrong\u003eEvents \u003cimg src=\"https://render.githubusercontent.com/render/math?math=A\"\u003e and \u003cimg src=\"https://render.githubusercontent.com/render/math?math=B\"\u003e are dependent when the occurrence of \u003cimg src=\"https://render.githubusercontent.com/render/math?math=A\"\u003e somehow has an effect on whether \u003cimg src=\"https://render.githubusercontent.com/render/math?math=B\"\u003e will occur (or not).\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eNow things start getting a bit more interesting.\u003c/p\u003e\n\u003cp\u003eLet's look at an example. Let's say event \u003cimg src=\"https://render.githubusercontent.com/render/math?math=A\"\u003e is taking an orange or purple marble out of a jar. The jar contains 3 orange and 2 purple marbles.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-conditional-probability/master/images/Image_69_Marb.png\" width=\"300\"\u003e\u003c/p\u003e\n\u003cp\u003eThe probability of getting a purple marble is \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cdfrac%7B2%7D%7B5%7D\"\u003e and getting an orange marble is \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cdfrac%7B3%7D%7B5%7D\"\u003e .\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-conditional-probability/master/images/Image_70_Cond3.png\" width=\"300\"\u003e\u003c/p\u003e\n\u003cp\u003eAt that point, one marble is taken out and we now take another marble from the jar (event \u003cimg src=\"https://render.githubusercontent.com/render/math?math=B\"\u003e ).\u003c/p\u003e\n\u003cp\u003eHere you can see that our second event is dependent on the outcome of the first draw.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eIf we drew an orange marble first, the probability of getting a purple marble for event B is \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cdfrac%7B2%7D%7B4%7D\"\u003e .\u003c/li\u003e\n\u003cli\u003eIf we saw a purple marble first, however, the probability of seeing a purple in the second trial is \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cdfrac%7B1%7D%7B4%7D\"\u003e .\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eIn simple terms, the probability of seeing an event \u003cimg src=\"https://render.githubusercontent.com/render/math?math=B\"\u003e in the second trial depends on the outcome \u003cimg src=\"https://render.githubusercontent.com/render/math?math=A\"\u003e of the first trial. We say that \u003cimg src=\"https://render.githubusercontent.com/render/math?math=P(B)\"\u003e is \u003cstrong\u003econditional\u003c/strong\u003e on \u003cimg src=\"https://render.githubusercontent.com/render/math?math=P(A)\"\u003e .\u003c/p\u003e\n\u003cp\u003eA \u003cstrong\u003etree diagram\u003c/strong\u003e can be used to explore all possible events.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-conditional-probability/master/images/Image_71_TreeDiag.png\" width=\"500\"\u003e\u003c/p\u003e\n\u003ch2\u003eConditional Probability\u003c/h2\u003e\n\u003cp\u003e\u003cstrong\u003eConditional probability emerges when the outcome a trial may influence the results of the upcoming trials.\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eWhile calculating the probability of the second event (event \u003cimg src=\"https://render.githubusercontent.com/render/math?math=B\"\u003e ) given that the primary event (event \u003cimg src=\"https://render.githubusercontent.com/render/math?math=A\"\u003e ) has just happened, we say that the probability of event \u003cimg src=\"https://render.githubusercontent.com/render/math?math=B\"\u003e relies on the occurrence of event \u003cimg src=\"https://render.githubusercontent.com/render/math?math=A\"\u003e .\u003c/p\u003e\n\u003cp\u003eHere are some more examples:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eDrawing a 2nd Ace from a deck of cards given that the first card you drew was an Ace.\u003c/li\u003e\n\u003cli\u003eFinding the probability of liking \"The Matrix\" given that you know this person likes science fiction.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eLet's say that \u003cimg src=\"https://render.githubusercontent.com/render/math?math=P(A)\"\u003e is the event we are interested in, and this event depends on a certain event \u003cimg src=\"https://render.githubusercontent.com/render/math?math=B\"\u003e that has happened.\u003c/p\u003e\n\u003cp\u003eThe conditional probability (Probability of \u003cimg src=\"https://render.githubusercontent.com/render/math?math=A\"\u003e \u003cstrong\u003egiven\u003c/strong\u003e \u003cimg src=\"https://render.githubusercontent.com/render/math?math=B\"\u003e ) can be written as:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=P%20(A%20%5Cmid%20B)%20=%20%5Cdfrac%7BP(A%20%20%5Ccap%20B)%7D%7BP(B)%7D\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=P(A%7CB)\"\u003e , is the probability A \u003cstrong\u003egiven\u003c/strong\u003e that \u003cimg src=\"https://render.githubusercontent.com/render/math?math=B\"\u003e has just happened.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-conditional-probability/master/images/Image_72_Cond4.png\" width=\"300\"\u003e\u003c/p\u003e\n\u003cp\u003eUnderstanding this formula may be easier if you look at two simple Venn Diagrams and use the multiplication rule. Here's how to derive this formula:\u003c/p\u003e\n\u003cp\u003eStep 1: Write out the multiplication rule:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=P(A%20%20%5Ccap%20B)=%20P(B)*P(A%5Cmid%20B)\"\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eStep 2: Divide both sides of the equation by P(B):\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cdfrac%7BP(A%20%20%5Ccap%20B)%7D%7B%20P(B)%7D%20=%20%5Cdfrac%7BP(B)*P(A%5Cmid%20B)%7D%7BP(B)%7D\"\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eStep 3: Cancel P(B) on the right side of the equation:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cdfrac%7BP(A%20%20%5Ccap%20B)%7D%7BP(B)%7D%20=%20P(A%20%5Cmid%20B)\"\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eStep 4: This is of course equal to:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=P(A%20%5Cmid%20B)=%5Cdfrac%7BP(A%20%20%5Ccap%20B)%7D%7BP(B)%7D\"\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eAnd this is our conditional probability formula.\u003c/p\u003e\n\u003cp\u003eThere are a few variations and theorems that are related to and/or results of this conditional probability formula. The most important ones are: the \u003cstrong\u003eproduct rule\u003c/strong\u003e, the \u003cstrong\u003echain rule\u003c/strong\u003e and \u003cstrong\u003eBayes Theorem\u003c/strong\u003e\u003c/p\u003e\n\u003ch3\u003eTheorem 1 - Product Rule\u003c/h3\u003e\n\u003cp\u003eThe \u003cstrong\u003eproduct rule\u003c/strong\u003e was used to derive the conditional probability formula above, but is often used in situations where the conditional probability is easy to compute, but the probability of intersections of events isn't.\u003c/p\u003e\n\u003cp\u003eThe intersection of events \u003cimg src=\"https://render.githubusercontent.com/render/math?math=A\"\u003e and \u003cimg src=\"https://render.githubusercontent.com/render/math?math=B\"\u003e can be given by:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=P(A%20%5Ccap%20B)%20=%20P(B)%20P(A%20%5Cmid%20B)%20=%20P(A)%20P(B%20%5Cmid%20A)\"\u003e\u003c/p\u003e\n\u003cp\u003eRemember that if \u003cimg src=\"https://render.githubusercontent.com/render/math?math=A\"\u003e and \u003cimg src=\"https://render.githubusercontent.com/render/math?math=B\"\u003e are independent, then conditioning on \u003cimg src=\"https://render.githubusercontent.com/render/math?math=B\"\u003e means nothing (and vice-versa) so \u003cimg src=\"https://render.githubusercontent.com/render/math?math=P(A%7CB)%20=%20P(A)\"\u003e , and \u003cimg src=\"https://render.githubusercontent.com/render/math?math=P(A%20%20%5Ccap%20B)%20=%20P(A)%20P(B)\"\u003e .\u003c/p\u003e\n\u003ch3\u003eTheorem 2 - Chain Rule\u003c/h3\u003e\n\u003cp\u003eThe \u003cstrong\u003echain rule\u003c/strong\u003e (also called the \u003cstrong\u003egeneral product rule\u003c/strong\u003e) permits the calculation of any member of the joint distribution of a set of random variables using only conditional probabilities.\u003c/p\u003e\n\u003cp\u003eRecall the product rule:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=P(A%20%20%5Ccap%20B)%20=%20P(A%20%5Cmid%20B)%20P(B)\"\u003e\u003c/p\u003e\n\u003cp\u003eWhen you extend this for three variables:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=P(A%20%5Ccap%20B%20%20%5Ccap%20C)%20=%20P(A%20%5Ccap(%20B%20%20%5Ccap%20C))%20=%20P(A%5Cmid%20B%20%20%5Ccap%20C)%20P(B%20%20%5Ccap%20C)%20=%20P(A%20%5Cmid%20B%20%20%5Ccap%20C)%20P(B%20%5Cmid%20C)%20P(C)\"\u003e\u003c/p\u003e\n\u003cp\u003eAnd you can keep extending this to \u003cimg src=\"https://render.githubusercontent.com/render/math?math=n\"\u003e variables:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=P(A_1%20%20%5Ccap%20A_2%20%20%5Ccap%20%5Cldots%20%20%5Ccap%20A_n)%20=%20P(A_1%20%5Cmid%20A_2%20%20%5Ccap%20%5Cldots%20%5Ccap%20A_n)%20P(A_2%20%5Cmid%20A_3%20%20%20%5Ccap%20%5Cldots%20%20%5Ccap%20%5C%20A_n)%20P(A_%7Bn-1%7D%7CA_n)%20P(A_n)\"\u003e\u003c/p\u003e\n\u003cp\u003eThis idea is known as the \u003cstrong\u003echain rule\u003c/strong\u003e.\u003c/p\u003e\n\u003cp\u003eIf on the other hand you have disjoint events \u003cimg src=\"https://render.githubusercontent.com/render/math?math=C_1,%20C_2,...,C_m\"\u003e such that \u003cimg src=\"https://render.githubusercontent.com/render/math?math=C_1%20%5Ccup%20C_2%20%5Ccup%20%C2%B7%C2%B7%C2%B7%20%20%5Ccup%20%20C_m%20=%20%5COmega\"\u003e , the probability of any event can be decomposed as:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=P(A)%20=%20P(A%20%5Cmid%20C_1)P(C_1)%20+%20P(A%20%5Cmid%20C_2)P(C_2)%20+%20%5Cldots%20+%20P(A%20%5Cmid%20C_m)P(C_m)\"\u003e\u003c/p\u003e\n\u003ch3\u003eTheorem 3 - Bayes Theorem\u003c/h3\u003e\n\u003cp\u003eThe \u003cstrong\u003eBayes theorem\u003c/strong\u003e, which is the outcome of this section. Below is the formula that we will dig deeper into in upcoming lessons.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=P(A%7CB)%20=%20%5Cfrac%20%7BP(B%7CA)P(A)%7D%7BP(B)%7D%20%5Ctext%7B%20%C2%A0%C2%A0%C2%A0%20-this%20follows%20from%20Theorem%201%7D\"\u003e\u003c/p\u003e\n\u003ch3\u003eAdditional note: the complement of an event\u003c/h3\u003e\n\u003cp\u003eYou learned about (absolute and relative) complements before, but the complement of an event is also applicable to conditional probabilities.\u003c/p\u003e\n\u003cp\u003eThe basic rule is:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=P(A)%20%2b%20P(A')%20=%201\"\u003e\u003c/p\u003e\n\u003cp\u003ewith A' being the complement of A.\u003c/p\u003e\n\u003cp\u003eSimilarly, extending this to conditional probabilities:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=P(A%7CB)%20%2b%20P(A'%7CB)%20=%201\"\u003e\u003c/p\u003e\n\u003ch2\u003eExample: An Aspiring Data Scientist's Dilemma\u003c/h2\u003e\n\u003cp\u003eLet's see a very simple use of the conditional probability formula. A data scientist comes across the following infographic:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-conditional-probability/master/images/Image_73_Mood.png\" width=\"600\"\u003e\u003c/p\u003e\n\u003cp\u003eCurious as data scientists are, he starts collecting data about weather conditions and his own mood.\u003c/p\u003e\n\u003cp\u003eConsider the data in the following table, recorded over a month with 50 days by our data scientist. On each day he recorded whether it was sunny or Cloudy, and whether his mood was good or not.\u003c/p\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003e\u0026nbsp;\u003c/th\u003e\n\u003cth\u003eSunny weather\u003c/th\u003e\n\u003cth\u003eCloudy weather\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd\u003eGood mood\u003c/td\u003e\n\u003ctd\u003e14\u003c/td\u003e\n\u003ctd\u003e11\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eBad mood\u003c/td\u003e\n\u003ctd\u003e2\u003c/td\u003e\n\u003ctd\u003e23\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003cp\u003eHe wants to now know if his mood had anything to do with the weather on a particular day and how he can calculate the probability of having a good mood given the weather conditions.\u003c/p\u003e\n\u003ch3\u003eIf he picked a day at random from the 50 days on record, what is the probability that he was in a good mood on that day, \u003cimg src=\"https://render.githubusercontent.com/render/math?math=P(G)\"\u003e ?\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eThe sample space is 50 days here\u003c/li\u003e\n\u003cli\u003eThe event space is \"good mood\", so \u003cimg src=\"https://render.githubusercontent.com/render/math?math=14%20%2b%2011%20=%2025\"\u003e .\u003c/li\u003e\n\u003cli\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=P(G)%20=%20%5Cdfrac%7B25%7D%7B50%7D%20=%200.5\"\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003eWhat is the probability that the day chosen was a Sunny day, \u003cimg src=\"https://render.githubusercontent.com/render/math?math=P(S)\"\u003e ?\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eThe sample space is still 50 days\u003c/li\u003e\n\u003cli\u003eIt was sunny on \u003cimg src=\"https://render.githubusercontent.com/render/math?math=14%20%2b%202%20=%2016\"\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=P(S)%20=%20%5Cdfrac%7B16%7D%7B50%7D%20=%200.32\"\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003eWhat is the probability of having a good mood given it's a sunny day \u003cimg src=\"https://render.githubusercontent.com/render/math?math=P(G%20%5Cmid%20S)\"\u003e ?\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cimg src=\"https://render.githubusercontent.com/render/math?math=P(G%20%5Cmid%20S)%20=%20%5Cdfrac%7BP(G%20%20%5Ccap%20S)%7D%7B%20P(S)%7D\"\u003e , so we need to calculate \u003cimg src=\"https://render.githubusercontent.com/render/math?math=P(G%20%20%5Ccap%20S)\"\u003e first.\u003c/li\u003e\n\u003cli\u003e\n\u003cimg src=\"https://render.githubusercontent.com/render/math?math=P(G%20%20%5Ccap%20S)\"\u003e consists of sunny days in which he is in a good mood. There were 14 of them, so \u003cimg src=\"https://render.githubusercontent.com/render/math?math=P(G%20%20%5Ccap%20S)%20=%20%5Cdfrac%7B14%7D%7B50%7D\"\u003e\n\u003c/li\u003e\n\u003cli\u003eTherefore \u003cimg src=\"https://render.githubusercontent.com/render/math?math=P(G%20%5Cmid%20S)%20=%5Cdfrac%7B%5Cfrac%7B14%7D%7B50%7D%7D%7B%5Cfrac%7B16%7D%7B50%7D%7D%20=%200.875\"\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThe infographic had some truth in it indeed. There's a 87.5% chance that our curious data scientist would be in good mood on a sunny day.\u003c/p\u003e\n\u003cp\u003eThe data scientist is satisfied and thinks the outcome is comforting.\u003c/p\u003e\n\u003cp\u003eSurfing the Internet, however, he comes across a Garth Stein quote. Although not very scientific, this raises his curiosity further. The quote goes as follows:\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\"That which is around me does not affect my mood; my mood affects that which is around me\"\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eWhat if...?\u003c/p\u003e\n\u003ch3\u003eNow the data scientist wants to know if his mood had any impact on the weather. What is \u003cimg src=\"https://render.githubusercontent.com/render/math?math=P(S%20%5Cmid%20G)\"\u003e ?\u003c/h3\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=P(S%20%5Cmid%20G)%20=%5Cdfrac%7BP(G%20%20%5Ccap%20S)%7D%7B%20P(G)%7D%20=%20%5Cdfrac%7B%5Cfrac%7B14%7D%7B50%7D%7D%7B%5Cfrac%7B25%7D%7B50%7D%7D%20=%200.56\"\u003e\u003c/p\u003e\n\u003cp\u003eHe finds that the probability is slightly higher than random chance (50%). In other words, there's a 56% chance that it will be all nice and sunny given that he is in a good mood.\u003c/p\u003e\n\u003cp\u003eHe also realizes that \u003cimg src=\"https://render.githubusercontent.com/render/math?math=P(G%20%5Cmid%20S)\"\u003e is not equal to \u003cimg src=\"https://render.githubusercontent.com/render/math?math=P(S%20%5Cmid%20G)\"\u003e . So does this mean that weather has a higher impact on his mood than his mood has on the weather...?\u003c/p\u003e\n\u003cp\u003eThis doesn't really make sense. Our mood doesn't \u003cem\u003ecause\u003c/em\u003e the weather, so there is no cause-effect relationship. In the example above, the weather and other such external conditions can have a positive effect on human mood and behavior, and this can be said with reference to literature. However, it is unlikely that mood has any effect on weather. There is no scientific evidence to support this notion (and it's very unlikely that there will ever be). What is clear, however, is that there is a relationship between weather and mood.\u003c/p\u003e\n\u003ch3\u003eSay Hello to Reverend Thomas Bayes\u003c/h3\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-conditional-probability/master/images/Thomas_Bayes.gif\" width=\"300\"\u003e\u003c/p\u003e\n\u003cp\u003eBayes theorem is a very foundational theorem that uses the fact that \u003cimg src=\"https://render.githubusercontent.com/render/math?math=P(A%20%20%5Ccap%20B)%20=%20P(B)%20P(A%20%5Cmid%20B)%20=%20P(A)%20P(B%20%5Cmid%20A)\"\u003e . Note that, using Bayes theorem, you can compute conditional probabilities without explicitly needing to know \u003cimg src=\"https://render.githubusercontent.com/render/math?math=P(A%20%20%5Ccap%20B)\"\u003e !\u003c/p\u003e\n\u003cp\u003eThis theorem is extremely important in many machine learning algorithms.\u003c/p\u003e\n\u003cp\u003eOur data scientist realizes that he needs to learn a bit of Bayesian reasoning in order to get more meaningful results. And that is exactly what we will discuss further. First, we need to cover a few topics to fully understand how this simple equation lets you do some serious predictive analysis.\u003c/p\u003e\n\u003cp\u003eYou'll do a few exercises next to get a good grip on conditional probability calculations.\u003c/p\u003e\n\u003ch2\u003eAdditional Resources\u003c/h2\u003e\n\u003cp\u003eYou are strongly advised to visit the following links to get an in-depth understanding with examples and proofs for explaining the formulas highlighted in this lesson.\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"http://faculty.arts.ubc.ca/vmarmer/econ327/327_02_cond_probability.pdf\"\u003eConditional probability, Independence and Bayes rule\u003c/a\u003e - A deeper mathematical explanation around Independence and theorems we have seen above (and some we shall cover in upcoming lessons)\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://www.mathsisfun.com/data/probability-tree-diagrams.html\"\u003eTree Diagrams\u003c/a\u003e - Drawing tree diagrams to calculate conditional probability\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://www.mathgoodies.com/lessons/vol6/conditional\"\u003eConditional Probability, Examples and simple exercises\u003c/a\u003e - Practice with probability calculations\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"http://setosa.io/conditional/\"\u003eConditional probability: A visual explanation\u003c/a\u003e - A great little interactive animation to explain how conditional probability works\u003c/p\u003e\n\u003ch2\u003eSummary\u003c/h2\u003e\n\u003cp\u003eIn this lesson, you learned about disjoint, independent, and dependent events, and how to use the addition and multiplication rule to find the probability of the union and intersection of two events, respectively. You also learned how to compute conditional probabilities in case you have dependent events in your sample space, with a step-by-step derivation of the formula used to compute conditional probabilities. You also worked through an example to see this concept in action. Finally, Bayes' theorem was discussed. Later in the course, you'll build further on these ideas towards having a clear understanding of Bayesian Logic and its role in machine learning. Next up, you'll practice solving problems with conditional probability!\u003c/p\u003e","frontPage":false},{"exportId":"central-limit-theorem","title":"Central Limit Theorem","type":"WikiPage","content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-central-limit-theorem\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-central-limit-theorem\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-central-limit-theorem/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eIn this lesson, we'll start to investigate a \u003cem\u003ecentral\u003c/em\u003e statistical concept; the central limit theorem! (And how to write a good dry math pun.)\u003c/p\u003e\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\u003cp\u003eYou will be able to:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eDescribe how the central limit theorem is related to sampling\u003c/li\u003e\n\u003cli\u003eDescribe how the central limit theorem can be used for parameter estimation\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003eWhat does the Central Limit Theorem stand for?\u003c/h2\u003e\n\u003cp\u003eThe central limit theorem states that, under many conditions, independent random variables summed together will converge to a normal distribution as the number of variables increases. This becomes very useful for applying statistical logic to sample statistics in order to estimate population parameters. For example, as we saw in the previous lecture, the averages of samples will form a normal distribution. We can then use this information to put further bounds on our estimates of the population. We can also use this information to estimate the probability of samples taking on extreme values that deviate from the population mean.\u003c/p\u003e\n\u003cp\u003eFor example, let's say that we know the mean and standard deviation of asthma rates in the United States. If we then take a sample from a specific city and find that the mean of this sample is substantially lower than that of the overall population, we may be interested in questions such as \"what is the probability that this was just caused by random chance in sampling?\" If the probability is exceedingly low, we have further reason to believe that this city has higher rates of asthma and that its population is statistically different then that of the general population.\u003c/p\u003e\n\u003cp\u003eThe computation would be something like this: we know the mean population, and by the central limit theorem, the average of various samples takes on a normal distribution. From that normal distribution of sample means, we can then compare the mean of our actual sample and compare it to the distribution of means. It should be quite rare that our sample mean falls outside 2 or 3 standard deviations from the mean of sample means, (roughly 2.35% and .15% respectively for each tail). As such, having a sample mean that falls outside of these scopes is worthy of further investigation.\u003c/p\u003e\n\u003cp\u003eFor reference, here's is a rough empirical rule for percentiles within a normal distribution. (And again, by the central limit theorem, we expect our sample means to take on a normal distribution!)\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-central-limit-theorem/master/images/new_CentralLimitTheorem.png\" width=\"600\"\u003e\u003c/p\u003e\n\u003ch2\u003eAdditional Resources\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"http://sphweb.bumc.bu.edu/otlt/MPH-Modules/BS/BS704_Probability/BS704_Probability13.html\"\u003eApplication of the Central Limit Theorem page, by the Boston University School of Public Health\u003c/a\u003e\u003c/p\u003e\n\u003ch2\u003eSummary\u003c/h2\u003e\n\u003cp\u003eIn this brief lesson, we continued to discuss the central limit theorem and its application for sampling statistics and confidence intervals.\u003c/p\u003e","frontPage":false},{"exportId":"partitioning-and-the-law-of-total-probability","title":"Partitioning and the Law of Total Probability","type":"WikiPage","content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-law-of-total-probability\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-law-of-total-probability\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-law-of-total-probability/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eIn this lesson, we'll look at the law of total probability. In probability theory, the law (or formula) of total probability is a fundamental rule relating \u003cstrong\u003emarginal probabilities\u003c/strong\u003e to conditional probabilities. It expresses the total probability of an outcome that can be realized via several distinct events.\u003c/p\u003e\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\u003cp\u003eYou will be able to:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eState the law of total probabilities based on a partitioned event space\u003c/li\u003e\n\u003cli\u003eExplain the concept of event space and partitioning\u003c/li\u003e\n\u003cli\u003eDescribe conditional independence\u003c/li\u003e\n\u003cli\u003ePerform partitioning based on known and unknown probabilities to solve a problem\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003ePartitioning a Sample Space\u003c/h2\u003e\n\u003cp\u003ethe Law of Total Probability can be used to calculate \u003cimg src=\"https://render.githubusercontent.com/render/math?math=P(B)\"\u003e . The law requires that you have a set of disjoint events \u003cimg src=\"https://render.githubusercontent.com/render/math?math=A_i\"\u003e that collectively \"cover\" the event \u003cimg src=\"https://render.githubusercontent.com/render/math?math=B\"\u003e . Then, instead of calculating \u003cimg src=\"https://render.githubusercontent.com/render/math?math=P(B)\"\u003e directly, you add up the intersection of \u003cimg src=\"https://render.githubusercontent.com/render/math?math=B\"\u003e with each of the events \u003cimg src=\"https://render.githubusercontent.com/render/math?math=A_i\"\u003e . Let's see this graphically below:\u003c/p\u003e\n\u003cp\u003eLet \u003cimg src=\"https://render.githubusercontent.com/render/math?math=A_1,%20A_2,%20%5Cdots,%20A_n\"\u003e partition sample space \u003cimg src=\"https://render.githubusercontent.com/render/math?math=S\"\u003e into disjoint regions that sum up to \u003cimg src=\"https://render.githubusercontent.com/render/math?math=S\"\u003e . In the example, the four regions \u003cimg src=\"https://render.githubusercontent.com/render/math?math=A_1,%20A_2,%20A_3\"\u003e and \u003cimg src=\"https://render.githubusercontent.com/render/math?math=A_4\"\u003e sum up to sample space \u003cimg src=\"https://render.githubusercontent.com/render/math?math=S\"\u003e .\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-law-of-total-probability/master/images/Image_55_TotProb.png\" width=\"500\"\u003e\u003c/p\u003e\n\u003cp\u003eThe probability of a random event \u003cimg src=\"https://render.githubusercontent.com/render/math?math=B\"\u003e (orange area) can be written down as:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=P(B)\"\u003e = \u003cimg src=\"https://render.githubusercontent.com/render/math?math=P(B%20%5Ccap%20A1)%20%2b%20P(B%20%5Ccap%20A2)%20%2b%20P(B%20%5Ccap%20A3)%20%2b%20P(B%20%5Ccap%20A4)\"\u003e = \u003cimg src=\"https://render.githubusercontent.com/render/math?math=P(B%20%5Cmid%20A1)P(A1)%20%2b%20P(B%20%5Cmid%20A2)P(A2)%20%2b%20P(B%20%5Cmid%20A3)P(A3)%20%2b%20P(B%20%5Cmid%20A4)P(A4)\"\u003e\u003c/p\u003e\n\u003cp\u003eHere we use the first theorem mentioned in the previous lesson to find the combined probabilities.\u003c/p\u003e\n\u003ch3\u003eExample\u003c/h3\u003e\n\u003cp\u003eLet's use a simple example to clarify the image above! The example is created to match the image.\u003c/p\u003e\n\u003cp\u003eIn a certain country, there are four provinces (eg. disjoint regions) \u003cimg src=\"https://render.githubusercontent.com/render/math?math=A_1,%20A_2\"\u003e , \u003cimg src=\"https://render.githubusercontent.com/render/math?math=A_3\"\u003e and \u003cimg src=\"https://render.githubusercontent.com/render/math?math=A_4\"\u003e .\u003c/p\u003e\n\u003cp\u003eYou are interested in the total forest area, \u003cimg src=\"https://render.githubusercontent.com/render/math?math=B\"\u003e , in the country.\u003c/p\u003e\n\u003cp\u003eSuppose that you know that the forest area in \u003cimg src=\"https://render.githubusercontent.com/render/math?math=A_1\"\u003e , \u003cimg src=\"https://render.githubusercontent.com/render/math?math=A_2\"\u003e , and \u003cimg src=\"https://render.githubusercontent.com/render/math?math=A_3\"\u003e are 100\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Ctext%7Bkm%7D%5E2\"\u003e , 50\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Ctext%7Bkm%7D%5E2\"\u003e , and 150\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Ctext%7Bkm%7D%5E2\"\u003e , and 0\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Ctext%7Bkm%7D%5E2\"\u003e respectively. What is the total forest area in the country?\u003c/p\u003e\n\u003cp\u003e100\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Ctext%7Bkm%7D%5E2\"\u003e + 50\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Ctext%7Bkm%7D%5E2\"\u003e + 150\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Ctext%7Bkm%7D%5E2\"\u003e + 0\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Ctext%7Bkm%7D%5E2\"\u003e = 300\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Ctext%7Bkm%7D%5E2\"\u003e\u003c/p\u003e\n\u003cp\u003eWe can simply add forest areas in each province to obtain the forest area in the whole country.\u003c/p\u003e\n\u003cp\u003eThis is the idea behind the law of total probability, in which the area of forest is replaced by probability of an event \u003cimg src=\"https://render.githubusercontent.com/render/math?math=B\"\u003e . In particular, if you want to find \u003cimg src=\"https://render.githubusercontent.com/render/math?math=P(B)\"\u003e , you can look at a partition of \u003cimg src=\"https://render.githubusercontent.com/render/math?math=S\"\u003e (our sample space composed of \u003cimg src=\"https://render.githubusercontent.com/render/math?math=A_1,%5Cldots,%20A_4\"\u003e ), and add the amount of probability of \u003cimg src=\"https://render.githubusercontent.com/render/math?math=A\"\u003e that falls in each partition.\u003c/p\u003e\n\u003ch3\u003eTwo Events\u003c/h3\u003e\n\u003cp\u003eIn general, we can say that for any two events \u003cimg src=\"https://render.githubusercontent.com/render/math?math=A\"\u003e and \u003cimg src=\"https://render.githubusercontent.com/render/math?math=B\"\u003e :\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=P(A)=P(A%20%20%5Ccap%20B)%2bP(A%20%20%5Ccap%20B')\"\u003e\u003c/p\u003e\n\u003cp\u003eand using the definition of conditional probability, \u003cimg src=\"https://render.githubusercontent.com/render/math?math=P(A%20%20%5Ccap%20B)=P(A%20%5Cmid%20B)P(B)\"\u003e , we can write\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=P(A)=P(A%20%5Cmid%20B)P(B)%2bP(A%20%5Cmid%20B')P(B')\"\u003e\u003c/p\u003e\n\u003cp\u003eThe law of total probability is basically a general version of this.\u003c/p\u003e\n\u003ch2\u003eLaw of Total Probability\u003c/h2\u003e\n\u003cp\u003eIf \u003cimg src=\"https://render.githubusercontent.com/render/math?math=B_1\"\u003e , \u003cimg src=\"https://render.githubusercontent.com/render/math?math=B_2\"\u003e , \u003cimg src=\"https://render.githubusercontent.com/render/math?math=B_3\"\u003e , \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cdots\"\u003e is a partition of the sample space S, then for any event A we have\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=P(A)=%20%5Csum_i%20P(A%20%20%5Ccap%20B_i)=%20%5Csum_i%20P(A%20%5Cmid%20B_i)P(B_i)\"\u003e\u003c/p\u003e\n\u003cp\u003eUsing a Venn diagram, we can pictorially see the idea behind the law of total probability. In the figure below, we have\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=A_1%20=%20A%20%20%5Ccap%20B_1\"\u003e\u003c/li\u003e\n\u003cli\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=A_2%20=%20A%20%20%5Ccap%20B_2\"\u003e\u003c/li\u003e\n\u003cli\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=A_3%20=%20A%20%20%5Ccap%20B_3\"\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-law-of-total-probability/master/images/Image_56_vent.png\" width=\"400\"\u003e\u003c/p\u003e\n\u003cp\u003eAs it can be seen from the figure, \u003cimg src=\"https://render.githubusercontent.com/render/math?math=A_1\"\u003e , \u003cimg src=\"https://render.githubusercontent.com/render/math?math=A_2\"\u003e , and \u003cimg src=\"https://render.githubusercontent.com/render/math?math=A_3\"\u003e form a partition of the set A, and thus\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=P(A)=P(A_1)%2bP(A_2)%2bP(A_3)\"\u003e\u003c/p\u003e\n\u003cp\u003eHere is a typical scenario in which we use the law of total probability. We are interested in finding the probability of an event \u003cimg src=\"https://render.githubusercontent.com/render/math?math=A\"\u003e , but we don't know how to find P(A) directly. Instead, we know the conditional probability of \u003cimg src=\"https://render.githubusercontent.com/render/math?math=A\"\u003e given some events \u003cimg src=\"https://render.githubusercontent.com/render/math?math=B_i\"\u003e , where the \u003cimg src=\"https://render.githubusercontent.com/render/math?math=B_i\"\u003e 's form a partition of the sample space. This way, you can use \u003cimg src=\"https://render.githubusercontent.com/render/math?math=P(A)\"\u003e using the law of total probability\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=P(A)=%5Csum_i%20P(A%20%5Cmid%20B_i)P(B_i)\"\u003e\u003c/p\u003e\n\u003ch2\u003eMore on Partitions\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eThe natural numbers \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cmathbb%7BN%7D\"\u003e can be partitioned into even and odd numbers.\u003c/li\u003e\n\u003cli\u003eThe set of animal species in the world can be partitioned into subsets where a subset reflects a continent and each species is positioned in a subset depending on which continent they originated from.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eIn statistics, choosing the right partitioning is key as bad choices of partitions may results in many sub-problems that are even more difficult to solve.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-law-of-total-probability/master/images/Image_57_TotProb_2.png\" width=\"500\"\u003e\u003c/p\u003e\n\u003cp\u003eThe probability of \u003cimg src=\"https://render.githubusercontent.com/render/math?math=A\"\u003e can be written as sums of event \u003cimg src=\"https://render.githubusercontent.com/render/math?math=B\"\u003e (note that \u003cimg src=\"https://render.githubusercontent.com/render/math?math=B%5Ec\"\u003e is another way of writing \u003cimg src=\"https://render.githubusercontent.com/render/math?math=B'\"\u003e) The total probability rule is:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=P(A)%20=%20P(A%20%20%5Ccap%20B)%20%2b%20P(A%20%20%5Ccap%20B%5Ec)\"\u003e\u003c/p\u003e\n\u003cp\u003eAn alternate version of the total probability rule (found with the multiplication rule) can be used when the necessary probabilities are known:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=P(A)%20=%20P(A%20%5Cmid%20B)%20%20P(B)%20%2b%20P(A%20%5Cmid%20B%5Ec)P(B%5Ec)\"\u003e\u003c/p\u003e\n\u003cp\u003eYou need to be careful when dealing with conditional probabilities and conditioning. Let's look at a few examples to see this idea in action.\u003c/p\u003e\n\u003ch3\u003eExample 1\u003c/h3\u003e\n\u003cp\u003eIn a certain county in the United States, 60% of registered voters are Republicans, 30% are Democrats and 10% are Independents.\u003c/p\u003e\n\u003cp\u003eWhen those voters were asked about increasing military spending.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e40% of Republicans opposed it.\u003c/li\u003e\n\u003cli\u003e65% of the Democrats opposed it.\u003c/li\u003e\n\u003cli\u003e55% of the Independents opposed it.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eWhat is the probability that a randomly selected voter in this county opposes increased military spending?\u003c/p\u003e\n\u003cp\u003eYou know that:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5COmega\"\u003e = {registered voters in the county}\u003c/li\u003e\n\u003cli\u003e\n\u003cimg src=\"https://render.githubusercontent.com/render/math?math=R\"\u003e = {registered republicans}, \u003cimg src=\"https://render.githubusercontent.com/render/math?math=P(R)\"\u003e = 0.6\u003c/li\u003e\n\u003cli\u003e\n\u003cimg src=\"https://render.githubusercontent.com/render/math?math=D\"\u003e = {registered democrats}, \u003cimg src=\"https://render.githubusercontent.com/render/math?math=P(D)\"\u003e = 0.3\u003c/li\u003e\n\u003cli\u003e\n\u003cimg src=\"https://render.githubusercontent.com/render/math?math=I\"\u003e = {registered independents}, \u003cimg src=\"https://render.githubusercontent.com/render/math?math=P(I)\"\u003e = 0.1\u003c/li\u003e\n\u003cli\u003e\n\u003cimg src=\"https://render.githubusercontent.com/render/math?math=B\"\u003e = {registered voters opposing increased military spending}\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eYou also know that:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=P(B%20%5Cmid%20R)%20=%200.4\"\u003e\u003c/li\u003e\n\u003cli\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=P(B%20%5Cmid%20D)%20=%200.65\"\u003e\u003c/li\u003e\n\u003cli\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=P(B%20%5Cmid%20I)%20=%200.55\"\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eBy the total probability theorem:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=Pr(B)%20=%20Pr(B%20%5Cmid%20R)%20Pr(R)%20%2b%20Pr(B%20%5Cmid%20D)%20Pr(D)%20%2b%20Pr(B%20%5Cmid%20I)%20Pr(I)\"\u003e \u003cimg src=\"https://render.githubusercontent.com/render/math?math==%20(0.4%20*%200.6)%20%2b%20(0.65%20*%200.3)%20%2b%20(0.55%20*%200.1)%20=%200.49\"\u003e\u003c/p\u003e\n\u003ch3\u003eExample 2\u003c/h3\u003e\n\u003cp\u003eLet's consider a 2-card hand drawn from a standard playing deck. What is the probability of drawing 2 aces, given that we know one of the cards is an ace?\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=P(%5Ctext%7Bboth%20are%20aces%20%7C%20one%20is%20ace%7D)%20=%20%5Cdfrac%7BP(%5Ctext%7Bboth%20are%20aces%7D)%7D%7BP(%5Ctext%7Bone%20is%20ace%7D)%7D%20=%20%5Cdfrac%7BP(%5Ctext%7Bboth%20are%20aces%7D)%7D%7B1%20-%20P(%5Ctext%7Bneither%20is%20ace%7D)%7D%20=%5Cdfrac%7B%5Cbinom%7B4%7D%7B2%7D/%5Cbinom%7B52%7D%7B2%7D%7D%7B1%20-%20%5Cbinom%7B48%7D%7B2%7D/%5Cbinom%7B52%7D%7B2%7D%7D=%5Cdfrac%7B1%7D%7B33%7D\"\u003e\u003c/p\u003e\n\u003cp\u003eBut now think about this: What is the probability of drawing 2 aces, knowing that one of the cards \u003cstrong\u003eis the ace of spades\u003c/strong\u003e?\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=P(%5Ctext%7Bboth%20are%20aces%20%7C%20ace%20of%20spades%7D)%20=%20P(%5Ctext%7Bother%20card%20is%20also%20an%20ace%7D)%20=%20%5Cdfrac%7B3%7D%7B51%7D=%20%5Cdfrac%7B1%7D%7B17%7D\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cem\u003eNotice how the fact that we know we have the ace of spades nearly doubles the probability of having 2 aces\u003c/em\u003e\u003c/p\u003e\n\u003ch3\u003eExample 3\u003c/h3\u003e\n\u003cp\u003eSuppose there is a test for a disease, and this test is said to be \"95% accurate\". The disease in question afflicts 1% of the population. Now say that there is a patient who tests positive for this disease under this test.\u003c/p\u003e\n\u003cp\u003eFirst, we define the events in question:\u003c/p\u003e\n\u003cp\u003eLet \u003cimg src=\"https://render.githubusercontent.com/render/math?math=D\"\u003e be the event that the patient actually has the disease.\u003c/p\u003e\n\u003cp\u003eLet \u003cimg src=\"https://render.githubusercontent.com/render/math?math=T\"\u003e be the event that the patient tests positive.\u003c/p\u003e\n\u003cp\u003eSince that phrase \"95% accurate\" is ambiguous, we need to clarify that.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=P(T%7CD)%20=%20P(T%5Ec%7CD%5Ec)%20=%200.95\"\u003e\u003c/p\u003e\n\u003cp\u003eIn other words, \u003cstrong\u003econditioning on whether or not the patient has the disease\u003c/strong\u003e, we will assume that the test is 95% accurate.\u003c/p\u003e\n\u003cp\u003e\u003cem\u003eWhat exactly are we trying to find?\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003eWhat the patient really wants to know is not \u003cimg src=\"https://render.githubusercontent.com/render/math?math=P(T%7CD)\"\u003e , which is the accuracy of the test; but rather \u003cimg src=\"https://render.githubusercontent.com/render/math?math=P(D%7CT)\"\u003e , or the probability she has the disease given that the test returns positive. Fortunately, we know how \u003cimg src=\"https://render.githubusercontent.com/render/math?math=P(T%7CD)\"\u003e relates to \u003cimg src=\"https://render.githubusercontent.com/render/math?math=P(D%7CT)\"\u003e .\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=P(D%7CT)\"\u003e = \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cfrac%7BP(T%7CD)P(D)%7D%7BP(T)%7D\"\u003e (Bayes Rule)\u003c/p\u003e\n\u003cp\u003e= \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cfrac%7BP(T%7CD)P(D)%7D%7BP(T%7CD)P(D)%20%2b%20P(T%7CD%5Ec)P(D%5Ec)%7D\"\u003e (by the Law of Total Probability)\u003c/p\u003e\n\u003cp\u003e= \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cfrac%7B(0.95)(0.01)%7D%7B(0.95)(0.01)%20%2b%20(0.05)(0.99)%7D\"\u003e (the rarity of the disease competes with the rarity of true negatives)\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Capprox%200.16\"\u003e\u003c/p\u003e\n\u003ch2\u003eCommon Pitfalls\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eMistaking \u003cimg src=\"https://render.githubusercontent.com/render/math?math=P(A%7CB)\"\u003e for \u003cimg src=\"https://render.githubusercontent.com/render/math?math=P(B%7CA)\"\u003e\u003c/p\u003e\n\u003cp\u003eThis is also known as the \u003ca href=\"https://en.wikipedia.org/wiki/Prosecutor%27s_fallacy\"\u003eProsecutor's Fallacy\u003c/a\u003e, where instead of asking about the \u003cem\u003eprobability of guilt (or innocence) given all the evidence\u003c/em\u003e, we make the mistake of concerning ourselves with the \u003cem\u003eprobability of the evidence given guilt\u003c/em\u003e.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eConfusing \u003cem\u003eprior\u003c/em\u003e \u003cimg src=\"https://render.githubusercontent.com/render/math?math=P(A)\"\u003e with \u003cem\u003eposterior\u003c/em\u003e \u003cimg src=\"https://render.githubusercontent.com/render/math?math=P(A%20%5Cmid%20B)\"\u003e\u003c/p\u003e\n\u003cp\u003eObserving that event \u003cimg src=\"https://render.githubusercontent.com/render/math?math=A\"\u003e occurred does \u003cstrong\u003enot\u003c/strong\u003e mean that \u003cimg src=\"https://render.githubusercontent.com/render/math?math=P(A)%20=%201\"\u003e . But \u003cimg src=\"https://render.githubusercontent.com/render/math?math=P(A%20%5Cmid%20A)%20=%201\"\u003e and \u003cimg src=\"https://render.githubusercontent.com/render/math?math=P(A)%20%5Cneq%201\"\u003e .\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eConfusing \u003cem\u003eindependence\u003c/em\u003e with \u003cstrong\u003econditional independence\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eThis is more subtle than the other two. Let's look at this in a bit more detail\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003eConditional Independence\u003c/h2\u003e\n\u003cp\u003eEvents \u003cimg src=\"https://render.githubusercontent.com/render/math?math=A\"\u003e and \u003cimg src=\"https://render.githubusercontent.com/render/math?math=B\"\u003e are \u003cstrong\u003econditionally independent\u003c/strong\u003e given event \u003cimg src=\"https://render.githubusercontent.com/render/math?math=C\"\u003e , if:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=P(A%20%20%5Ccap%20B%20%5Cmid%20C)%20=%20P(A%20%5Cmid%20C)P(B%20%5Cmid%20C)\"\u003e\u003c/p\u003e\n\u003cp\u003ei.e. conditioning on event \u003cimg src=\"https://render.githubusercontent.com/render/math?math=C\"\u003e does not give us any additional information on \u003cimg src=\"https://render.githubusercontent.com/render/math?math=A\"\u003e or \u003cimg src=\"https://render.githubusercontent.com/render/math?math=B\"\u003e .\u003c/p\u003e\n\u003ch3\u003eConditional independence given \u003cimg src=\"https://render.githubusercontent.com/render/math?math=C\"\u003e DOES NOT imply unconditional independence\u003c/h3\u003e\n\u003cp\u003eConsider playing a series of 5 games against a chess opponent of unknown strength. Winning all five games would give you a good idea that you are a better player. So winning each successive game is actually providing us with information about the strength of our opponent. If you have prior knowledge about the strength of your opponent, you condition on the strength of our opponent i.e. winning one game would not provide us with any additional information on the probability of winning the next. Having no prior knowledge of your opponent and winning a string a games will give you information about the probability of winning the next game.\u003c/p\u003e\n\u003cp\u003eThe games are conditionally independent given the strength of our opponent, but \u003cstrong\u003enot\u003c/strong\u003e independent unconditionally.\u003c/p\u003e\n\u003ch3\u003eUnconditional independence DOES NOT imply conditional independence given \u003cimg src=\"https://render.githubusercontent.com/render/math?math=C\"\u003e\n\u003c/h3\u003e\n\u003cp\u003eFor example, let \u003cimg src=\"https://render.githubusercontent.com/render/math?math=A\"\u003e be the event of the fire alarm going off, \u003cimg src=\"https://render.githubusercontent.com/render/math?math=F\"\u003e be the event of a fire, and \u003cimg src=\"https://render.githubusercontent.com/render/math?math=C\"\u003e be the event of someone making popcorn. Suppose that either \u003cimg src=\"https://render.githubusercontent.com/render/math?math=F\"\u003e or \u003cimg src=\"https://render.githubusercontent.com/render/math?math=C\"\u003e will result in \u003cimg src=\"https://render.githubusercontent.com/render/math?math=A\"\u003e and the fire alarm going off. Now if \u003cimg src=\"https://render.githubusercontent.com/render/math?math=F\"\u003e and \u003cimg src=\"https://render.githubusercontent.com/render/math?math=C\"\u003e are independent: knowing that there's a fire \u003cimg src=\"https://render.githubusercontent.com/render/math?math=F\"\u003e doesn't tell you anything about anyone making popcorn \u003cimg src=\"https://render.githubusercontent.com/render/math?math=C\"\u003e , and vice versa. But the probability of a fire given that the alarm goes off \u003cstrong\u003eand\u003c/strong\u003e no one is making any popcorn is given by \u003cimg src=\"https://render.githubusercontent.com/render/math?math=P(F%20%5Cmid%20A,C%5Ec)%20=%201\"\u003e . After all, if the fire alarm goes off and no one is making popcorn, there can only be one explanation: \u003cem\u003ethere must be a fire\u003c/em\u003e.\u003c/p\u003e\n\u003cp\u003eSo \u003cimg src=\"https://render.githubusercontent.com/render/math?math=F\"\u003e and \u003cimg src=\"https://render.githubusercontent.com/render/math?math=C\"\u003e may be independent, but they are not \u003cem\u003econditionally independent\u003c/em\u003e when we condition on event \u003cimg src=\"https://render.githubusercontent.com/render/math?math=A\"\u003e . Knowing that nobody is making any popcorn when the alarm goes off can only mean that there is a fire.\u003c/p\u003e\n\u003ch2\u003eAdditional Resources\u003c/h2\u003e\n\u003cp\u003eYou are strongly advised to visit following links to get an indepth understanding with examples and proofs for formulas highlighted in this lesson.\u003c/p\u003e\n\u003cp\u003e\u003ca class=\"\" href=\"https://www.youtube.com/watch?v=J7Evcn4lfhc\"\u003eThe law of total probability - concept and proof\u003c/a\u003e - Excellent YouTube video by Phil Chan.\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://jeremykun.com/2013/03/28/conditional-partitioned-probability-a-primer/\"\u003eConditional (Partitioned) Probability — A Primer\u003c/a\u003e - Deep dive into partitions (A Must Read)\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://www.sangakoo.com/en/unit/law-of-total-probability\"\u003eLaw of Total Probability\u003c/a\u003e - More examples for a deeper understanding around partitioning\u003c/p\u003e\n\u003ch2\u003eSummary\u003c/h2\u003e\n\u003cp\u003eIn this lesson, you further learned about the ideas of conditional probability covered in the previous lessons to explain the law of total probability using partitioning of the sample space. You learned how you can partition probabilities with respect to some other event, when the direct probabilities are not known. Let's move on to some practice!\u003c/p\u003e","frontPage":false},{"exportId":"hypothesis-testing-introduction","title":"Hypothesis Testing - Introduction","type":"WikiPage","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-hypothesis-testing-intro-v2-1\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-hypothesis-testing-intro-v2-1/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eIn this section, you'll learn about experimental design and hypothesis testing. All scientific research that comes out of universities uses hypothesis testing to determine if the results of an experiment are significant or not. As a data scientist, you might be tasked with designing, performing, and analyzing the results of an experiment. Finally, you'll also learn about resampling methods, which are modern statistical techniques that involve taking repeated subsamples from a sample and help better estimate the precision of your sample statistics.\u003c/p\u003e\n\u003ch2\u003eHypothesis Testing\u003c/h2\u003e\n\u003cp\u003eIn this section, you'll be looking at experimental design, effect size, T-tests, Type 1 and Type 2 errors, and resampling techniques like the jackknife, bootstrap, and permutation tests.\u003c/p\u003e\n\u003ch3\u003eExperimental Design\u003c/h3\u003e\n\u003cp\u003eWithout good experimental design, it's very easy to draw the wrong conclusions from your experiments. Because of that, you'll kick this section off by looking at the scientific method and the key elements of good experimental design - forming \u003cstrong\u003ealternative\u003c/strong\u003e and \u003cstrong\u003enull hypotheses\u003c/strong\u003e, conducting an experiment, analyzing the results for statistical significance and drawing conclusions.\u003c/p\u003e\n\u003ch3\u003eEffect Size\u003c/h3\u003e\n\u003cp\u003eWe then look at how to calculate and interpret the size of the difference between control and test groups. We'll see how the \"Effect Size\" can be used to communicate the practical significance of experimental results, to perform meta-analyses of multiple studies, and to perform power analysis to determine the number of participants that a study would require to achieve a certain probability of finding a true effect.\u003c/p\u003e\n\u003ch3\u003eOne and Two Sample T-tests\u003c/h3\u003e\n\u003cp\u003eNext, you'll also look at t-tests and how they can be used to compare two averages to see how significant the differences are between one or two samples once we have defined the experimental design.\u003c/p\u003e\n\u003ch3\u003eType 1 and Type 2 Errors\u003c/h3\u003e\n\u003cp\u003eFrom there, you'll learn about \u003cstrong\u003etype 1 (false positive)\u003c/strong\u003e and \u003cstrong\u003etype 2 (false negative) errors\u003c/strong\u003e and the inherent tradeoff between them.\u003c/p\u003e\n\u003ch3\u003eJackknife, Bootstrap, and Permutation Tests\u003c/h3\u003e\n\u003cp\u003eWe'll look at techniques for taking repeated subsamples from a sample using bootstrapping, jackknife and permutation tests to better estimate the precision of your sample statistics or validate models by using random subsets.\u003c/p\u003e\n\u003ch2\u003eSummary\u003c/h2\u003e\n\u003cp\u003eWithout a good understanding of experimental design, it's easy to end up confusing spurious correlations for meaningful results or placing too much (or too little) weight on the results of any given test. In this section, we cover a range of tools and techniques to ensure that you design your experiments rigorously and interpret them thoughtfully.\u003c/p\u003e\n\u003cfooter class=\"fis-footer\" style=\"visibility: hidden;\"\u003e\n\u003cdiv class=\"fis-feedback\"\u003e\n\u003ch5\u003eHow do you feel about this lesson?\u003c/h5\u003e\n\u003cimg id=\"thumbs-up\" title=\"Thumbs up!\" alt=\"thumbs up\" data-repository=\"dsc-hypothesis-testing-intro-v2-1\"\u003e\u003cimg id=\"thumbs-down\" title=\"Thumbs down!\" alt=\"thumbs down\" data-repository=\"dsc-hypothesis-testing-intro-v2-1\"\u003e\n\u003c/div\u003e\n\u003ch5\u003eHave specific feedback? \u003ca href=\"https://github.com/learn-co-curriculum/dsc-hypothesis-testing-intro-v2-1/issues/new\"\u003eTell us here!\u003c/a\u003e\n\u003c/h5\u003e\n\u003c/footer\u003e","frontPage":false},{"exportId":"simple-linear-regression","title":"Simple Linear Regression","type":"WikiPage","content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-simple-linear-regression\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-simple-linear-regression\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-simple-linear-regression/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eRegression analysis is often the first real learning application that aspiring data scientists will come across. It is one of the simplest techniques to master, but it still requires some mathematical and statistical understanding of the underlying process. This lesson will introduce you to the regression process based on the statistical ideas we have discovered so far.\u003c/p\u003e\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\u003cp\u003eYou will be able to:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003ePerform a linear regression using self-constructed functions\u003c/li\u003e\n\u003cli\u003eInterpret the parameters of a simple linear regression model in relation to what they signify for specific data\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003eLinear Regression\u003c/h2\u003e\n\u003cp\u003eRegression analysis is one of the most important statistical techniques for business applications. It’s a statistical methodology that helps estimate the strength and direction of the relationship between two (or more) variables. Regression results show whether the relationship is valid or not. It also helps to \u003cem\u003epredict\u003c/em\u003e an unknown value based on the derived relationship.\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eRegression Analysis is a \u003cstrong\u003eparametric\u003c/strong\u003e technique meaning a set of parameters are used to \u003cstrong\u003epredict\u003c/strong\u003e the value of an unknown target variable (or dependent variable) \u003cimg src=\"https://render.githubusercontent.com/render/math?math=y\"\u003e based on one or more of known input features (or independent variables, predictors), often denoted by \u003cimg src=\"https://render.githubusercontent.com/render/math?math=x\"\u003e .\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eLet's consider another example. Someone's height and foot size are generally considered to be related. Generally speaking, taller people tend to have bigger feet (and, obviously, shoe size).\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-simple-linear-regression/master/images/heightfoot1.png\" width=\"450\"\u003e\u003c/p\u003e\n\u003cp\u003eWe can use a linear regression analysis here to predict foot size (dependent variable), given height (independent variable) of an individual. Regression is proven to give credible results if the data follows some assumptions which will be covered in upcoming lessons in detail. In general, regression analysis helps us in the following ways:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eFinding an \u003cstrong\u003eassociation\u003c/strong\u003e or relationship between certain phenomena or variables\u003c/li\u003e\n\u003cli\u003eIdentifying \u003cstrong\u003ewhich variables contribute\u003c/strong\u003e more towards the outcomes\u003c/li\u003e\n\u003cli\u003e\n\u003cstrong\u003ePrediction\u003c/strong\u003e of future observations\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003eWhy \"linear\" regression?\u003c/h3\u003e\n\u003cp\u003eThe term \u003cstrong\u003elinear\u003c/strong\u003e implies that the model functions along with a straight (or nearly straight) line. \u003cstrong\u003eLinearity\u003c/strong\u003e, one of the assumptions of this approach, suggests that the relationship between dependent and independent variables can be expressed as a straight line.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eSimple Linear Regression\u003c/strong\u003e uses a single feature (one independent variable) to model a linear relationship with a target (the dependent variable) by fitting an optimal model (i.e. the best straight line) to describe this relationship.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eMultiple Linear Regression\u003c/strong\u003e uses more than one feature to predict a target variable by fitting the best linear relationship.\u003c/p\u003e\n\u003cp\u003eIn this section, we will mainly focus on simple regression to build a sound understanding. For the example shown above i.e. height vs foot size, a simple linear regression model would fit a line to the data points as follows:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-simple-linear-regression/master/images/heightfoot2.png\" width=\"450\"\u003e\u003c/p\u003e\n\u003cp\u003eThis line can then be used to describe the data and conduct further experiments using this fitted model. So let's move on and see how to calculate this \"best-fit line\" in a simple linear regression context.\u003c/p\u003e\n\u003ch2\u003eCalculating Regression Coefficients: Slope and Intercepts\u003c/h2\u003e\n\u003cp\u003eA straight line can be written as :\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=y=mx%2bc\"\u003e\u003c/p\u003e\n\u003cp\u003eor, alternatively\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=y%20=%20%20%5Cbeta_0%2b%20%5Cbeta_1%20x\"\u003e\u003c/p\u003e\n\u003cp\u003eYou may come across other ways of expressing this straight line equation for simple linear regression. Yet there are \u003cstrong\u003efour key components\u003c/strong\u003e you'll want to keep in mind:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-simple-linear-regression/master/images/linreg.png\" width=\"650\"\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eA \u003cstrong\u003edependent variable\u003c/strong\u003e that needs to estimated and predicted (here: \u003cimg src=\"https://render.githubusercontent.com/render/math?math=y\"\u003e )\u003c/li\u003e\n\u003cli\u003eAn \u003cstrong\u003eindependent variable\u003c/strong\u003e, the input variable (here: \u003cimg src=\"https://render.githubusercontent.com/render/math?math=x\"\u003e )\u003c/li\u003e\n\u003cli\u003eThe \u003cstrong\u003eslope\u003c/strong\u003e which determines the angle of the line. Here, the slope is denoted as \u003cimg src=\"https://render.githubusercontent.com/render/math?math=m\"\u003e , or \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cbeta_1\"\u003e .\u003c/li\u003e\n\u003cli\u003eThe \u003cstrong\u003eintercept\u003c/strong\u003e which is the constant determining the value of \u003cimg src=\"https://render.githubusercontent.com/render/math?math=y\"\u003e when \u003cimg src=\"https://render.githubusercontent.com/render/math?math=x\"\u003e is 0. We denoted the intercept here as \u003cimg src=\"https://render.githubusercontent.com/render/math?math=c\"\u003e or \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cbeta_0\"\u003e .\u003c/li\u003e\n\u003c/ul\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cem\u003eSlope\u003c/em\u003e and \u003cem\u003eIntercept\u003c/em\u003e are the \u003cstrong\u003ecoefficients\u003c/strong\u003e or the \u003cstrong\u003eparameters\u003c/strong\u003e of a linear regression model. Calculating the regression model simply involves the calculation of these two values.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003e\u003cstrong\u003eLinear regression is simply a manifestation of this simple equation!\u003c/strong\u003e So this is as complicated as our linear regression model gets. The equation here is the same one used to find a line in algebra, but in statistics, the actual data points don't necessarily lie on a line!\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eThe real challenge for regression analysis is to fit a line, out of an infinite number of lines that best describes the data.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eConsider the line below to see how we calculate slope and intercept.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-simple-linear-regression/master/images/linregall.png\" width=\"650\"\u003e\u003c/p\u003e\n\u003cp\u003eIn our example:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=c\"\u003e is equal to 15, which is where our line intersects with the y-axis.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=m\"\u003e is equal to 3, which is our slope.\u003c/p\u003e\n\u003cp\u003eYou can find a slope by taking an arbitrary part of the line, looking at the differences for the x-value and the y-value for that part of the line, and dividing \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5CDelta%20y\"\u003e by \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5CDelta%20x\"\u003e . In other words, you can look at the \u003cstrong\u003echange in y over the change in x\u003c/strong\u003e to find the slope!\u003c/p\u003e\n\u003ch3\u003eImportant note on notation\u003c/h3\u003e\n\u003cp\u003eNow that you know how the slope and intercept define the line, it's time for some more notation.\u003c/p\u003e\n\u003cp\u003eLooking at the above plots, you know that you have the green dots that are our observations associated with x- and y-values.\u003c/p\u003e\n\u003cp\u003eNow, when we draw our regression line based on these few green dots, we use the following notations:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Chat%7By%7D=%5Chat%20m%20x%2b%20%5Chat%7Bc%7D\"\u003e or \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Chat%20y%20=%20%20%5Chat%20%5Cbeta_0%2b%20%5Chat%20%5Cbeta_1%20x\"\u003e\u003c/p\u003e\n\u003cp\u003eAs you can see, you're using a \"hat\" notation which stands for the fact that we are working with \u003cstrong\u003eestimations\u003c/strong\u003e. - When trying to draw a \"best fit line\", you're \u003cstrong\u003eestimating\u003c/strong\u003e the most appropriate value possible for your intercept and your slope, hence \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Chat%7Bc%7D\"\u003e / \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Chat%20%5Cbeta_0\"\u003e and \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Chat%7Bm%7D\"\u003e / \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Chat%20%5Cbeta_1\"\u003e . - Next, when we use our line to predict new values \u003cimg src=\"https://render.githubusercontent.com/render/math?math=y\"\u003e given \u003cimg src=\"https://render.githubusercontent.com/render/math?math=x\"\u003e , your estimate is an \u003cstrong\u003eapproximation\u003c/strong\u003e based on our estimated parameter values. Hence we use \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Chat%20y\"\u003e instead of \u003cimg src=\"https://render.githubusercontent.com/render/math?math=y\"\u003e . \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Chat%20y\"\u003e lies \u003cem\u003eON\u003c/em\u003e your regression line, \u003cimg src=\"https://render.githubusercontent.com/render/math?math=y\"\u003e is the associated y-value for each of the green dots in the plot above. The \u003cstrong\u003eerror\u003c/strong\u003e or the \u003cstrong\u003evertical offset\u003c/strong\u003e between the line and the actual observation values is denoted by the red vertical lines in the plot above. Mathematically, the vertical offset can be written as \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cmid%20%5Chat%20y%20-%20y%5Cmid\"\u003e .\u003c/p\u003e\n\u003cp\u003eSo how do you find the line with the best fit? You may think that you have to try lots and lots of different lines to see which one fits best. Fortunately, this task is not as complicated as in may seem. Given some data points, the best-fit line always has a distinct slope and y-intercept that can be calculated using simple linear algebraic approaches. Let's quickly visit the required formulas.\u003c/p\u003e\n\u003ch3\u003eBest-Fit Line Ingredients\u003c/h3\u003e\n\u003cp\u003eBefore we calculate the best-fit line, we have to make sure that we have calculated the following measures for variables X and Y:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eThe mean of the X \u003cimg src=\"https://render.githubusercontent.com/render/math?math=(%5Cbar%7BX%7D)\"\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eThe mean of the Y \u003cimg src=\"https://render.githubusercontent.com/render/math?math=(%5Cbar%7BY%7D)\"\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eThe standard deviation of the X values \u003cimg src=\"https://render.githubusercontent.com/render/math?math=(S_X)\"\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eThe standard deviation of the y values \u003cimg src=\"https://render.githubusercontent.com/render/math?math=(S_Y)\"\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eThe correlation between X and Y ( often denoted by the Greek letter \"Rho\" or \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Crho\"\u003e - Pearson Correlation)\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003eCalculating Slope\u003c/h2\u003e\n\u003cp\u003eWith the above ingredients in hand, we can calculate the slope (shown as \u003cimg src=\"https://render.githubusercontent.com/render/math?math=b\"\u003e below) of the best-fit line, using the formula:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Chat%20m%20=%20%5Crho%20%5Cfrac%7BS_Y%7D%7BS_X%7D\"\u003e\u003c/p\u003e\n\u003cp\u003eThis formula is also known as the \u003cstrong\u003eleast-squares method\u003c/strong\u003e.\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://en.wikipedia.org/wiki/Simple_linear_regression#Fitting_the_regression_line\"\u003eYou can visit this Wikipedia link\u003c/a\u003e to get take a look into the math behind the derivation of this formula.\u003c/p\u003e\n\u003cp\u003eThe slope of the best-fit line can be a negative number following a negative correlation. For example, if an increase in police officers is related to a decrease in the number of crimes in a linear fashion, the correlation and hence the slope of the best-fitting line in this particular setting is negative.\u003c/p\u003e\n\u003ch2\u003eCalculating Intercept\u003c/h2\u003e\n\u003cp\u003eSo now that we have the slope value (\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Chat%20m\"\u003e), we can put it back into our formula \u003cimg src=\"https://render.githubusercontent.com/render/math?math=(%5Chat%20y%20=%20%5Chat%20m%20x%2b%20%5Chat%20c)\"\u003e to calculate intercept. The idea is that\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cbar%7BY%7D%20=%20%5Chat%20c%20%2b%20%5Chat%20m%20%5Cbar%7BX%7D\"\u003e \u003cbr\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Chat%20c%20=%20%5Cbar%7BY%7D%20-%20%5Chat%20m%5Cbar%7BX%7D\"\u003e\u003c/p\u003e\n\u003cp\u003eRecall that \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cbar%7BX%7D\"\u003e and \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cbar%7BY%7D\"\u003e are the mean values for variables X and Y. So, in order to calculate the \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Chat%20y\"\u003e -intercept of the best-fit line, we start by finding the slope of the best-fit line using the above formula. Then to find the \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Chat%20y\"\u003e -intercept, we multiply the slope value by the mean of x and subtract the result from the mean of y.\u003c/p\u003e\n\u003ch2\u003ePredicting from the model\u003c/h2\u003e\n\u003cp\u003eAs mentioned before, when you have a regression line with defined parameters for slope and intercept as calculated above, you can easily predict the \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Chat%7By%7D\"\u003e (target) value for a new \u003cimg src=\"https://render.githubusercontent.com/render/math?math=x\"\u003e (feature) value using the estimated parameter values:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Chat%7By%7D%20=%20%5Chat%20mx%20%2b%20%5Chat%20c\"\u003e\u003c/p\u003e\n\u003cp\u003eRemember that the difference between y and \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Chat%7By%7D\"\u003e is that \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Chat%7By%7D\"\u003e is the value predicted by the fitted model, whereas \u003cimg src=\"https://render.githubusercontent.com/render/math?math=y\"\u003e carries actual values of the variable (called the truth values) that were used to calculate the best fit.\u003c/p\u003e\n\u003cp\u003eNext, let's move on and try to code these equations to fit a regression line to a simple dataset to see all of this in action.\u003c/p\u003e\n\u003ch2\u003eAdditional Reading\u003c/h2\u003e\n\u003cp\u003eVisit the following series of blogs by Bernadette Low for details on topics covered in this lesson.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://towardsdatascience.com/super-simple-machine-learning-by-me-simple-linear-regression-part-1-concept-and-r-4b5b39bbdb5d\"\u003eSuper Simple Machine Learning — Simple Linear Regression Part 1\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://towardsdatascience.com/super-simple-machine-learning-simple-linear-regression-part-2-math-and-python-1137acb4c352\"\u003eSuper Simple Machine Learning — Simple Linear Regression Part 2\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003eSummary\u003c/h2\u003e\n\u003cp\u003eIn this lesson, you learned the basics of a simple linear regression. Specifically, you learned some details about performing the actual technique and got some practice interpreting regression parameters. Finally, you saw how the parameters can be used to make predictions!\u003c/p\u003e","frontPage":false},{"exportId":"statistical-learning-theory","title":"Statistical Learning Theory","type":"WikiPage","content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-stat-learning-theory\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-stat-learning-theory\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-stat-learning-theory/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eIn this lesson, you'll be introduced to Statistical Learning Theory and some key components in the framework of this theory. This is a particularly important theory as it encompasses the majority of statistical inference and functional analyses approaches. Statistical Learning Theory has applications in a wide variety of fields such as image and speech recognition, bioinformatics, sports, etc.\u003c/p\u003e\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\u003cp\u003eYou will be able to:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eIdentify independent and dependent variables in a statistical model\u003c/li\u003e\n\u003cli\u003eDescribe loss and its importance in relation to model creation\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003eStatistical Learning Theory\u003c/h2\u003e\n\u003cblockquote\u003e\n\u003cp\u003eStatistical Learning Theory is based on the idea of using data along with statistics to provide a framework for learning.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eIn Statistical Learning Theory, the main idea is to \u003cstrong\u003econstruct a model\u003c/strong\u003e to draw certain conclusions from data, and next, to \u003cstrong\u003euse this model\u003c/strong\u003e to make predictions.\u003c/p\u003e\n\u003ch2\u003eTypes of Data in Statistical Learning\u003c/h2\u003e\n\u003cp\u003eIn the context of Statistical learning, there are two main types of data:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cstrong\u003eDependent variables\u003c/strong\u003e: data that can be controlled directly (other names: outcome variables, target variables, response variables)\u003c/li\u003e\n\u003cli\u003e\n\u003cstrong\u003eIndependent variables\u003c/strong\u003e: data that cannot be controlled directly (other names: predictor variables, input variables, explanatory variables, features)\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eIn models, the independent variable(s) are the variables that will affect (or will lead to a change in) the dependent variable(s).\u003c/p\u003e\n\u003cp\u003eTwo examples of common \u003cstrong\u003eindependent variables\u003c/strong\u003e are age and time. There is nothing you can do to speed up or slow down time or increase or decrease age. They are independent of everything else.\u003c/p\u003e\n\u003cp\u003eAn example of a \u003cstrong\u003edependent variable\u003c/strong\u003e is how much you weigh at different ages. Here, the dependent variable (weight) depends on the independent variable (age). As someone's weight fluctuates over time, you can observe and record your weight as a dependent variable on your age.\u003c/p\u003e\n\u003cp\u003eIndependent and dependent variables are normally shown on a graph under a standardized approach. This makes it easy for you to quickly see which variable is independent and which is dependent when looking at a graph or chart.\u003c/p\u003e\n\u003cp\u003eConventionally, the independent variable goes on the x-axis, or the horizontal axis. Let's consider another example, one where we look at someone's income depending on their age. Below, you see a scatter plot where age is the independent variable, and income is the dependent variable. In this setting, \u003cstrong\u003ewe want to study if age has some effect on annual income\u003c/strong\u003e.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-stat-learning-theory/master/images/scatter_age_income.png\" width=\"600\"\u003e\u003c/p\u003e\n\u003ch2\u003eStatistical Model\u003c/h2\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003eA statistical model can be thought of as some kind of a transformation that helps us express dependent variables as a function of one or more independent variables\u003c/strong\u003e.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eA statistical model defines a \u003cstrong\u003erelationship\u003c/strong\u003e between a dependent and an independent variable.\u003c/p\u003e\n\u003cp\u003eFor the plot we see above, the relationship between age and income can be shown using a \u003cstrong\u003estraight line\u003c/strong\u003e connecting all the individual observations in the data. So this line here would be our \u003cstrong\u003emodel\u003c/strong\u003e as shown in the image below.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-stat-learning-theory/master/images/scatter_line_age_income.png\" width=\"600\"\u003e\u003c/p\u003e\n\u003cp\u003eWe can define and \u003cstrong\u003efit\u003c/strong\u003e such a straight line to our data following a straight line equation:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=y%20=%20m%20%20x%20%2b%20c\"\u003e\u003c/p\u003e\n\u003cp\u003eYou'll often come across greek letters talking about models like this. Another common way of writing a linear equation is ( \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cbeta\"\u003e is the Greek letter \"beta\"):\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=y%20=%20%5Cbeta_0%20%2b%20%5Cbeta_1%20%20x\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cbeta_0\"\u003e has the same role as \u003cimg src=\"https://render.githubusercontent.com/render/math?math=c\"\u003e in the first expression and denotes the \u003cem\u003eintercept with the y-axis\u003c/em\u003e. \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cbeta_1\"\u003e has the same role as \u003cimg src=\"https://render.githubusercontent.com/render/math?math=m\"\u003e in the first expression and denotes the \u003cem\u003eslope of the line\u003c/em\u003e. More on this below.\u003c/p\u003e\n\u003cp\u003eSuch a simple model would describe a person's height has \u003cstrong\u003ealmost\u003c/strong\u003e a linear relationship with weight i.e. weight increases with height.\u003c/p\u003e\n\u003cp\u003eSo this is our simple model for the relationship. Of course, we can use more sophisticated models like quadratic equations or polynomial equations for a \u003cstrong\u003ebetter fit\u003c/strong\u003e, and you may see this later on if you dig into more advanced modeling.\u003c/p\u003e\n\u003cp\u003eLooking at this line above, we can define it as \u003cstrong\u003eIncome = 1500 + 1000 * Age\u003c/strong\u003e, based on slope ( \u003cimg src=\"https://render.githubusercontent.com/render/math?math=m\"\u003e or \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cbeta_1\"\u003e ) and intercept (c or \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cbeta_0\"\u003e ) values.\u003c/p\u003e\n\u003cp\u003eThis would be our \u003cstrong\u003elinear model\u003c/strong\u003e (Linear refers to a model consisting of a straight line, or \"linear regression\"), which can help us work out a weight value for a given height. In summary,\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eA model is expressed as a mathematical equation showing the relationship between dependent and independent variables.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch2\u003eStatistical Model Parameters\u003c/h2\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003eModel Parameters are the coefficients of the model equation for estimating the output\u003c/strong\u003e.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eStatistical Learning is all about learning these parameters. A statistical learning approach would help us \u003cstrong\u003elearn\u003c/strong\u003e these parameters so we have a clear description of their relationship which we can replicate and analyze under different circumstances.\u003c/p\u003e\n\u003cp\u003eFor the straight line above, we need to learn the \u003cstrong\u003eslope\u003c/strong\u003e and \u003cstrong\u003eintercept\u003c/strong\u003e for the line that best describes the relationship between the data elements in the dataset. We gave you the two values here, but in general, you'll have to \u003cstrong\u003elearn these values\u003c/strong\u003e. These values are denoted by \u003cstrong\u003eparameters\u003c/strong\u003e.\u003c/p\u003e\n\u003cp\u003eOnce we have learned the \u003cimg src=\"https://render.githubusercontent.com/render/math?math=m\"\u003e (or \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cbeta_1\"\u003e ) and \u003cimg src=\"https://render.githubusercontent.com/render/math?math=c\"\u003e (or \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cbeta_0\"\u003e ) values, we can predict a value of \u003cimg src=\"https://render.githubusercontent.com/render/math?math=y\"\u003e (income in our example) for a given value of \u003cimg src=\"https://render.githubusercontent.com/render/math?math=x\"\u003e (age). In our next lab, you'll learn how to calculate these for a given dataset. Let's have a look at another example:\u003c/p\u003e\n\u003ch3\u003eWhat Else Determines an Individual's Income?\u003c/h3\u003e\n\u003cp\u003eIf we suppose that income is a function of not only age, but also education level. A model that estimates the income could look like:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=income%20=%20%5Cbeta_0%20%2b%20%5Cbeta_1%20*%20%20%5Ctext%7Bage%7D%20%2b%20%5Cbeta_2%20*%20%5Ctext%7Beducation%20level%7D\"\u003e\u003c/p\u003e\n\u003cp\u003eHere we have two independent variables i.e. age and education level, with the same dependent variable, income. \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cbeta_0\"\u003e , \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cbeta_1\"\u003e and \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cbeta_2\"\u003e are model parameters.\u003c/p\u003e\n\u003ch2\u003eModel Generalization\u003c/h2\u003e\n\u003cp\u003eAs the data which is available to us for modeling is finite, the available data needs to be used very effectively to build and \u003cstrong\u003evalidate\u003c/strong\u003e a model. Validation of the model usually makes the model more \u003cstrong\u003egeneralizable\u003c/strong\u003e for unseen situations.\u003c/p\u003e\n\u003cp\u003eTraining the model is like the infancy stage for humans. Examples are presented to the model and the model tweaks its parameters to better understand the data. Once the training is over, the model is unleashed upon new data and then uses what it has learned to explain that data. This is where problems can emerge. If we \u003cstrong\u003eover-train\u003c/strong\u003e the model on the training data i.e. make the model every detail of shown data, it will be able to identify all the relevant information in the training data, but will fail miserably when presented with the new data.\u003c/p\u003e\n\u003cp\u003eWe then say that the \u003cstrong\u003emodel is not capable of generalizing\u003c/strong\u003e, or that \u003cstrong\u003emodel is over-fitting the training data\u003c/strong\u003e.\u003c/p\u003e\n\u003cp\u003eHere's a great example of the phenomenon: modeling happiness as a function of wealth.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-stat-learning-theory/master/images/new_happy.png\" width=\"600\"\u003e\u003c/p\u003e\n\u003cp\u003eIn the top three diagrams, we have data and models (dashed curves). From left to right the models have been trained longer and longer on the training data. The training error curve in the bottom box shows that the training error gets better and better as we train longer (increasing model complexity). You may think that if we train longer we'll get better! Well, yes, but \u003cstrong\u003eonly better at describing the training data\u003c/strong\u003e. The top right box shows a very complex model that hits all the data points. This model does great on the training data, but when presented with new data (examine the Prediction error curve in the bottom box) then it does worse!\u003c/p\u003e\n\u003cp\u003eIn order to create good predictive models in machine learning that are capable of generalizing, one needs to know when to stop training the model so that it doesn't over-fit.\u003c/p\u003e\n\u003ch3\u003eModel Validation\u003c/h3\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003eModel validation is a process of controlling overfitting and allows a higher degree of generalizability.\u003c/strong\u003e\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eHere is how we perform validation, in its simplest form:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eSplit the data into two parts with a 70/30, 80/20 or a similar split\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eUse the larger part for training so the model learns from it. This set of data is normally called the \u003cstrong\u003eTraining Data\u003c/strong\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eUse the smaller part for testing the model. This is data is not being used during the model learning process and used only for testing the performance of a learned model. This dataset is called as the \u003cstrong\u003eTesting Data\u003c/strong\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThis setup looks like as shown below:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-stat-learning-theory/master/images/new_train_test_sets.png\" width=\"600\"\u003e\u003c/p\u003e\n\u003cp\u003eIn statistical learning, if the model has learned well from the training data, it will perform well on both training data \u003cstrong\u003eand\u003c/strong\u003e test data. You can then use the test data to calculate the \u003cstrong\u003eaccuracy\u003c/strong\u003e, which is assessed based on how close it has estimated the output to the actual value.\u003c/p\u003e\n\u003ch2\u003eModel Loss\u003c/h2\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003eA loss function evaluates how well your model represents the relationship between data variables\u003c/strong\u003e.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eIf the model is unable to identify the underlying relationship between the independent and dependent variable(s), the loss function will output a very high number. Consider the age vs. income example above. You can see that the linear model is not exactly touching each data point because these points do not exist in a line. the individual distance of each point from the line is the \u003cstrong\u003eloss\u003c/strong\u003e that the model exhibits.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-stat-learning-theory/master/images/new_loss.png\" width=\"400\"\u003e\u003c/p\u003e\n\u003cp\u003eThese individual losses, which is essentially the \u003cstrong\u003evertical distance between the individual data points and the line\u003c/strong\u003e are taken into account to calculate the overall model loss.\u003c/p\u003e\n\u003cp\u003eIf the relationship is well modeled, the loss will be low. As we change the parameters of our model to try and improve results, our loss function is our best friend, telling us if we are on the right track.\u003c/p\u003e\n\u003cp\u003eYou'll learn about loss in further detail in upcoming lessons.\u003c/p\u003e\n\u003ch2\u003eAdditional Resources\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e\u003ca class=\"\" href=\"https://www.youtube.com/watch?v=rqJ8SrnmWu0\" target=\"_blank\"\u003eYoutube: Introduction to Statistical Learning Theory\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003ca href=\"https://www.princeton.edu/~harman/Papers/SLT-tutorial.pdf\" target=\"_blank\"\u003eAn Overview of Statistical Learning Theory with examples\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003eSummary\u003c/h2\u003e\n\u003cp\u003eIn this lesson, you briefly looked at statistical learning theory and its main components. You looked at what a statistical model is and what the model parameters represent. You also got a feel for the differences between independent and dependent variables plus learned about loss and its role in model creation. You looked at all of this in the context of a simple model, a straight line. Next, you‘ll see the \"learning\" part of statistical learning theory by learning slope and intercept parameters of a straight line.\u003c/p\u003e","frontPage":false},{"exportId":"goodharts-law-and-metric-tracking","title":"Goodhart's Law and Metric Tracking","type":"WikiPage","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-goodharts-law-and-metric-tracking\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-goodharts-law-and-metric-tracking/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eIn this lesson, you'll learn about \u003cstrong\u003e\u003cem\u003eGoodhart's law\u003c/em\u003e\u003c/strong\u003e and why you should be cautious and thoughtful when making policy recommendations based on data.\u003c/p\u003e\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\u003cp\u003eYou will be able to:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eDefine Goodhart's law and its relationship to hypothesis testing\u003c/li\u003e\n\u003cli\u003eIdentify real-world examples of Goodhart's law in action\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003eWhat is Goodhart's law?\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"https://en.wikipedia.org/wiki/Goodhart%27s_law\"\u003eGoodhart's law\u003c/a\u003e is an observation made by the British economist Charles Goodhart in 1975. Charles Goodhart famously said:\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\"Any observed statistical regularity will tend to collapse once pressure is placed upon it for control purposes.\" -- Charles Goodhart\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eIn plain English, this translates to:\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\"Any measure which becomes a target ceases to be an effective measure!\"\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch3\u003eSo what does that mean?\u003c/h3\u003e\n\u003cp\u003eGoodhart's law succinctly explains a cardinal sin that many data scientists, project managers, and CEOs make all the time without realizing it -- they make policy or set goals based on statistical metrics without considering the unintended consequences and effects these policies might have!\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-goodharts-law-and-metric-tracking/master/images/goodhart.jpg\" width=\"500\"\u003e\u003c/p\u003e\n\u003cp\u003eImage from: \u003ca href=\"https://www.sketchplanations.com/post/167369765942/goodharts-law-when-a-measure-becomes-a-target\"\u003eSketchplantations\u003c/a\u003e\u003c/p\u003e\n\u003ch2\u003eExample 1: Cobra skins\u003c/h2\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-goodharts-law-and-metric-tracking/master/images/new_cobra.png\" width=\"300\"\u003e\u003c/p\u003e\n\u003cp\u003eThe \u003ca href=\"https://en.wikipedia.org/wiki/Cobra_effect\"\u003eCobra effect\u003c/a\u003e refers to an anecdote that demonstrates an example of Goodhart's law in effect during the time of British rule of colonial India. As the story goes, a high-ranking officer in the British military was concerned about the number of highly venomous cobras that could be found in Delhi. He had the bright idea of offering a bounty for every cobra skin brought to him! Initially, this seemed to work -- people hunted cobras, sold the skins to the British government for their bounty, and the cobra population dipped slightly in the city. However, this soon backfired spectacularly, when citizens started breeding cobras! As a result, the cobra population stopped declining and even repopulated a bit. After a while, the officer caught onto the breeding, as he realized they were paying out many bounties but the cobra problem in the city was still prevalent as ever. After realizing this, he canceled the bounty. Ironically, this meant that all the cobra breeders now had no reason to keep the cobras they were breeding, so they dumped them in the street -- causing the city to have even more cobras than before the bounty program had been implemented in the first place!\u003c/p\u003e\n\u003cp\u003e(Fun fact: The French military made the same mistake when Hanoi, Vietnam was under their colonial rule with a rat bounty, and there is solid evidence to prove that this actually happened!)\u003c/p\u003e\n\u003ch3\u003eThe problem with proxy metrics\u003c/h3\u003e\n\u003cp\u003eThe first mistake by this British commander was using a \u003cstrong\u003e\u003cem\u003eproxy metric\u003c/em\u003e\u003c/strong\u003e in the form of \"cobra skins collected\". He mistakenly assumed that there was an inverse relationship between the number of skins turned in for a bounty and the number of wild cobras in the city of Delhi! Although this may have been the case at first, as hunting cobras was pretty much the only way to obtain skins to turn in for the bounty, he failed to realize that there were other possible sources for cobra skins that he hadn't accounted for. He wanted to reduce one metric, \u003cem\u003eCobra population\u003c/em\u003e, but he wasn't actually tracking that metric -- he was tracking a proxy for that metric which he assumed he could use to gauge what was happening to his target metric. The system he implemented had no way of determining if the cobra skins turned in for bounties were skins from cobras on the streets of Delhi -- with no way to tell, he had no way of knowing as his proxy metric became less and less relevant.\u003c/p\u003e\n\u003ch3\u003ePolicies can change things you didn't plan for\u003c/h3\u003e\n\u003cp\u003eThis leads to his other mistake -- he failed to account for how his policies might change things. Policies do not happen in a vacuum. They have a tendency to change things in unexpected ways, if not crafted thoughtfully and carefully! At first glance, introducing a monetary incentive for cobra skins seems like a good way to reduce the cobra population. However, he failed to account for the way this new incentive might change people's behaviors. By making cobra skins highly valuable, he inadvertently caused people to realize that breeding cobras was much safer, easier, and more lucrative than hunting them. Although his policy may have caused the change he wanted, in the beginning, he had no way of knowing what other sorts of behaviors this new policy might create or encourage.\u003c/p\u003e\n\u003ch2\u003eExample 2: Standardized testing in US schools\u003c/h2\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-goodharts-law-and-metric-tracking/master/images/new_test.png\" width=\"400\"\u003e\u003c/p\u003e\n\u003cp\u003eA more depressing real-world example of Goodhart's law in action is the prevalence of standardized testing in the American public school system. These tests were originally designed as a way to measure both individual student performance and overall teacher and school effectiveness. However, school funding is tied directly to test scores. This incentivizes schools to \"teach to the test\", spending a disproportionate amount of class time each year focusing on test preparation. By having incentives for schools to focus heavily on preparing students for these tests, the system has created ripple effects including reorientating student's focus on preparing for tests rather than other learning goals that might be more characteristic of real-world applications such as project orientated tasks. In this case, policymakers started out with a harmless, positive intention -- measure student and school performance -- but failing to account for Goodhart's law and offering strong incentives in relation to these metrics has degraded the usefulness of these test scores by altering behaviors.\u003c/p\u003e\n\u003ch2\u003eWhy does this matter for Data Scientists?\u003c/h2\u003e\n\u003cp\u003eGoodhart's law is something that matters much to Data Scientists because it is our findings and experiments that often drive the policies and decisions made by a company. Data Science is complex, and often, project managers, CEOs, and other decision makers don't want to know about experimental methodologies or confidence intervals -- they just want to know what the best decision they can make is, based on what the data says! It's quite common for decision makers to not realize that setting a target for one metric can negatively affect other metrics in ways that aren't immediately obvious. For instance, pushing employees at a call center to reduce call times might reduce customer satisfaction; it seems reasonable to imagine employees hustling to get off the phone based on this shorter call time \"target\" handed down from management.\u003c/p\u003e\n\u003cp\u003eAs a data scientist, it is important to communicate your results clearly to stakeholders -- but it is also important to be the voice of reason at times. This is why communication with stakeholders is important throughout the process of any data science project. The sooner you know how they plan on using your results, the more you can help them avoid ugly unforeseen problems that come from Goodhart's law -- always remember that massive amounts of data are no substitute for \u003cem\u003ecritical thinking\u003c/em\u003e! At the very least, you should get a bit nervous when you see targets being set for certain metrics. Note that this doesn't necessarily mean \"don't set targets\" -- instead, seek to encourage decision makers to think critically about any unintended consequences these targets could have, and track changes in metrics early and often when new policies or targets are put in place to ensure that unintended consequences are caught early!\u003c/p\u003e\n\u003ch2\u003eSummary\u003c/h2\u003e\n\u003cp\u003eIn this lesson, you learned about Goodhart's law and why you should be cautious and thoughtful when making policy recommendations based on data.\u003c/p\u003e","frontPage":false},{"exportId":"skewness-and-kurtosis","title":"Skewness and Kurtosis","type":"WikiPage","content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-skewness-and-kurtosis\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-skewness-and-kurtosis\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-skewness-and-kurtosis/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eWe have previously identified a normal distribution to be symmetrical in shape. But when you're dealing with real-world data you'll often come across asymmetric distributions as well. In this lesson, you'll learn how to measure asymmetry (or skewness) in a distribution. Additionally, you'll learn about kurtosis. Kurtosis defines whether a distribution is truly \"normal\" or whether it may have so-called \"fatter\" or \"thinner\" tails than you would observe when data are normally distributed.\u003c/p\u003e\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\u003cp\u003eYou will be able to:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eDefine skewness and kurtosis and their relationship to symmetric distributions\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003eSymmetric Distributions\u003c/h2\u003e\n\u003cp\u003eA distribution is symmetric if the relative frequency or probability of certain values are equal at equal distances from the point of symmetry. The point of symmetry for normal distributions is the mean (and at the same time median and mode!)\u003c/p\u003e\n\u003cp\u003eHave a look at following histogram:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-skewness-and-kurtosis/master/images/retirement.png\" width=\"450\"\u003e\u003c/p\u003e\n\u003cp\u003eThis distribution meets all of the conditions of being symmetrical.\u003c/p\u003e\n\u003cp\u003eThe most common symmetric distribution is the normal distribution, however, there are a number of other distributions that are symmetric. \u003ca href=\"https://www.statisticshowto.datasciencecentral.com/symmetric-distribution-2/\"\u003eHere is a good article\u003c/a\u003e that looks into all sorts of symmetrical distributions. We'll focus on normal distributions (by far the most common group) here, and see how these can lose symmetry!\u003c/p\u003e\n\u003ch2\u003eSkewness\u003c/h2\u003e\n\u003cp\u003eSkewness is the degree of distortion or deviation from the symmetrical normal distribution. Skewness can be seen as a measure to calculate the lack of symmetry in the data distribution.\u003c/p\u003e\n\u003cp\u003eSkewness helps you identify extreme values in one of the tails. Symmetrical distributions have a skewness of 0.\u003c/p\u003e\n\u003cp\u003eDistributions can be \u003cstrong\u003epositively\u003c/strong\u003e or \u003cstrong\u003enegatively\u003c/strong\u003e skewed.\u003c/p\u003e\n\u003ch3\u003ePositive Skewness\u003c/h3\u003e\n\u003cp\u003eA distribution is \u003cstrong\u003epositively skewed\u003c/strong\u003e when the tail on the right side of the distribution is longer (also often called \"fatter\"). When there is positive skewness, the mean and median are bigger than the mode.\u003c/p\u003e\n\u003ch3\u003eNegative Skewness\u003c/h3\u003e\n\u003cp\u003eDistributions are \u003cstrong\u003enegatively skewed\u003c/strong\u003e when the tail on the left side of the distribution is longer or fatter than the tail on the right side. When there is negative skewness, the mean and median are smaller than the mode.\u003c/p\u003e\n\u003cp\u003eThis behavior is shown in the images below:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-skewness-and-kurtosis/master/images/skewness.png\" width=\"700\"\u003e\u003c/p\u003e\n\u003cp\u003eSkewness can have implications for data analysis and the usage of certain models. The \"normality assumption\" seen before does not hold when data is skewed. When data is skewed, you'll need to transform the data first.\u003c/p\u003e\n\u003ch3\u003eMeasuring Skewness\u003c/h3\u003e\n\u003cp\u003eFor univariate data \u003cimg src=\"https://render.githubusercontent.com/render/math?math=Y_1,%20Y_2,%20...,%20Y_n\"\u003e the formula for skewness is:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cdfrac%7B%5Cdfrac%7B%5Cdisplaystyle%5Csum%5En_%7Bi=1%7D(Y_i-Y)%5E3%7D%7Bn%7D%7D%7Bs%5E3%7D\"\u003e\u003c/p\u003e\n\u003cp\u003ewhere \u003cimg src=\"https://render.githubusercontent.com/render/math?math=Y\"\u003e is the mean, \u003cimg src=\"https://render.githubusercontent.com/render/math?math=s\"\u003e is the standard deviation, and \u003cimg src=\"https://render.githubusercontent.com/render/math?math=n\"\u003e is the number of data points. This formula for skewness is referred to as the \u003cstrong\u003eFisher-Pearson coefficient of skewness\u003c/strong\u003e. There are also other ways to calculate skewness, yet this one is the one that is used most commonly.\u003c/p\u003e\n\u003ch3\u003eUsing this formula, when is data skewed?\u003c/h3\u003e\n\u003cp\u003eThe rule of thumb seems to be:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eA skewness between -0.5 and 0.5 means that the data are pretty symmetrical\u003c/li\u003e\n\u003cli\u003eA skewness between -1 and -0.5 (negatively skewed) or between 0.5 and 1 (positively skewed) means that the data are moderately skewed.\u003c/li\u003e\n\u003cli\u003eA skewness smaller than -1 (negatively skewed) or bigger than 1 (positively skewed) means that the data are highly skewed.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003eExample\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eImagine you have house values ranging from 200,000 USD to 1,500,000 USD with an average of 800,000 USD.\u003c/p\u003e\n\u003cp\u003eIf the peak of the distribution is left of the average value, the house prices are positively skewed. This means that more than half of the houses were sold for less than the average value 800,000 USD, and that there are a limited number of houses that were sold for a \u003cem\u003emuch\u003c/em\u003e higher value than 800,000 USD, leading to a long tail in the higher price ranges.\u003c/p\u003e\n\u003cp\u003eIf the peak of the distributed data is on the right-hand side of the average value, this means there is negative skewness, meaning that more than half of the houses were sold for more than the average value of 800,000 USD. Additionally, this means that there is a long tail in the lower price ranges.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-skewness-and-kurtosis/master/images/homeskewed.png\" width=\"500\"\u003e\u003c/p\u003e\n\u003ch2\u003eKurtosis\u003c/h2\u003e\n\u003cp\u003eKurtosis deals with the lengths of tails in the distribution.\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003eWhere skewness talks about extreme values in one tail versus the other, kurtosis aims at identifying extreme values in both tails at the same time!\u003c/strong\u003e\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eYou can think of Kurtosis as a \u003cstrong\u003emeasure of outliers\u003c/strong\u003e present in the distribution.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-skewness-and-kurtosis/master/images/kurtosis.png\" width=\"550\"\u003e\u003c/p\u003e\n\u003cp\u003eThe distribution denoted in the image above has relatively more observations around the mean, then a steep decline and longer tails compared to the normal distribution.\u003c/p\u003e\n\u003ch3\u003eMeasuring Kurtosis\u003c/h3\u003e\n\u003cp\u003eFor univariate data \u003cimg src=\"https://render.githubusercontent.com/render/math?math=Y_1,%20Y_2,%20%5Cdots,%20Y_n\"\u003e the formula for kurtosis is:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cdfrac%7B%5Cdfrac%7B%5Cdisplaystyle%5Csum%5En_%7Bi=1%7D(Y_i-Y)%5E4%7D%7Bn%7D%7D%7Bs%5E4%7D\"\u003e\u003c/p\u003e\n\u003cp\u003eIf there is a high kurtosis, then you may want to investigate why there are so many outliers. The presence of outliers could be indications of errors on the one hand, but they could also be some interesting observations that may need to be explored further. For banking transactions, for example, an outlier may signify fraudulent activity. How we deal with outliers mainly depends on the domain.\u003c/p\u003e\n\u003cp\u003eLow kurtosis in a data set is an indication that data has light tails or lacks outliers. If we get low kurtosis, then also we need to investigate and trim the dataset of unwanted results.\u003c/p\u003e\n\u003ch3\u003eHow much kurtosis is bad kurtosis?\u003c/h3\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-skewness-and-kurtosis/master/images/mesokurtosis.png\" width=\"550\"\u003e\u003c/p\u003e\n\u003ch4\u003eMesokurtic ( \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Ctext%7Bkurtosis%7D%20%5Capprox%203\"\u003e ):\u003c/h4\u003e\n\u003cp\u003eA mesokurtic distribution has kurtosis statistics that lie close to the ones of a normal distribution. Mesokurtic distributions have a kurtosis of around 3. According to this definition, the standard normal distribution has a kurtosis of 3.\u003c/p\u003e\n\u003ch4\u003ePlatykurtic ( \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Ctext%7Bkurtosis%7D%20\u003c%203\"\u003e ):\u003c/h4\u003e\n\u003cp\u003eWhen a distribution is platykurtic, the distribution is shorter and tails are thinner than the normal distribution. The peak is lower and broader than Mesokurtic, which means that the tails are light and that there are fewer outliers than in a normal distribution.\u003c/p\u003e\n\u003ch4\u003eLeptokurtic ( \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Ctext%7Bkurtosis%7D%20\u003e%203\"\u003e ):\u003c/h4\u003e\n\u003cp\u003eWhen you have a leptokurtic distribution, you have a distribution with longer and fatter tails. The peak is higher and sharper than the peak of a normal distribution, which means that data have heavy tails and that there are more outliers.\u003c/p\u003e\n\u003cp\u003eOutliers stretch your horizontal axis of the distribution, which means that the majority of the data appear in a narrower vertical range. This is why the leptokurtic distribution looks \"skinny\".\u003c/p\u003e\n\u003ch2\u003eSummary\u003c/h2\u003e\n\u003cp\u003eIn this lesson, you learned about skewness and kurtosis. In the next lab, you'll learn how to measure skewness and kurtosis in Python.\u003c/p\u003e","frontPage":false},{"exportId":"exploring-your-data","title":"Exploring Your Data","type":"WikiPage","content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-exploring-your-data\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-exploring-your-data\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-exploring-your-data/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eIn this lesson you'll learn about performing an EDA task, using all the statistical and visual EDA skills you have learned so far. \u003c/p\u003e\n\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\n\u003cp\u003eYou will be able to:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eExamine the descriptive statistics of our data set\u003c/li\u003e\n\u003cli\u003eCreate visualizations to better understand the distributions of variables in a dataset\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch2\u003eExploratory Data Analysis\u003c/h2\u003e\n\n\u003cp\u003eExploratory Data Analysis, or \u003cstrong\u003e\u003cem\u003eEDA\u003c/em\u003e\u003c/strong\u003e, is a crucial part of any Data Science project.  Before you can go off building models on a dataset, you need to be familiar with the actual data it contains -- otherwise, you'll have no intuition about how to interpret the results of these models, or even if you can trust them at all!\u003c/p\u003e\n\n\u003cp\u003eThis lesson will outline the basic steps that should be taken -- and questions that should be answered during EDA. \u003c/p\u003e\n\n\u003ch2\u003eUnderstanding the Distribution of the Dataset\u003c/h2\u003e\n\n\u003cp\u003eOne of the foundational pieces of an EDA investigation is to understand the underlying distribution of the data.  Often, some of the most interesting/important business insights come not from machine learning models, but simply from exploring the distribution of the dataset! If your company or organization has not yet mastered reporting on descriptive analytics, the insights gained here can be invaluable to company strategy -- think questions such as \"who is my most profitable customer segment?\" or \"is there a seasonality to our customer churn rate?\".  These are important questions to any business, and they don't require machine learning models to answer them -- just some basic visualizations, and the ability to ask good questions.\u003c/p\u003e\n\n\u003cp\u003eGetting a feel for the distribution of a dataset is done in a few different ways. Generally, you'll make use of high-level descriptive statistics, followed by visualizations. During the EDA process, it is quite common to uncover interesting things in the data that spur further questions for the investigation.  \u003c/p\u003e\n\n\u003cblockquote\u003e\n\u003cp\u003e\"The most exciting phrase to hear in science, the one that heralds new discoveries, is not 'Eureka!' (I found it!) but 'That's funny...'\"\u003c/p\u003e\n\n\u003cpre\u003e\u003ccode\u003e                                       - Isaac Asimov\n\u003c/code\u003e\u003c/pre\u003e\n\u003c/blockquote\u003e\n\n\u003cp\u003eRecall that Pandas can easily provide descriptive statistics of a DataFrame by using the DataFrame class's built-in \u003ccode\u003e.describe()\u003c/code\u003e method.  The resulting output is a table containing information such as the count, mean, median, min, max, and quartile values for every column in the DataFrame.  This is especially handy for answering questions such as \"how much variance can I expect in column {X}?\"\u003c/p\u003e\n\n\u003ch3\u003eVisualizing Distributions - Histograms\u003c/h3\u003e\n\n\u003cp\u003eThe easiest way to understand the distribution of a dataset is to visualize it! Recall that since \u003ccode\u003epandas\u003c/code\u003e uses the \u003ccode\u003ematplotlib\u003c/code\u003e library, you can easily create histograms showing the distribution of each column by using the DataFrame's built-in \u003ccode\u003e.hist()\u003c/code\u003e method.  \u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-exploring-your-data/master/images/sample_hist.png\"\u003e\u003c/p\u003e\n\n\u003ch3\u003eVisualizing Distributions - Kernel Density Estimation (KDE) Plots\u003c/h3\u003e\n\n\u003cp\u003eAnother great way of quickly visualizing the distribution of a column is to construct a \u003cstrong\u003e\u003cem\u003eKDE Plot\u003c/em\u003e\u003c/strong\u003e. This is often overlaid on a histogram to create a line that visualizes an approximate probability density of the variable. \u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-exploring-your-data/master/images/sample_kde.png\"\u003e\u003c/p\u003e\n\n\u003ch3\u003eUsing Joint Plots\u003c/h3\u003e\n\n\u003cp\u003eA more advanced visualization tool you can make use of is the \u003cstrong\u003e\u003cem\u003eJoint Plot\u003c/em\u003e\u003c/strong\u003e.  This allows you to visualize a scatterplot, the distributions of two different columns, a \u003ca href=\"https://seaborn.pydata.org/generated/seaborn.kdeplot.html\"\u003eKDE plot\u003c/a\u003e, and even a simple regression line all on the same visualization. In practice, this is incredibly handy for doing this like checking the linearity assumption between predictors and a target variable during a regression analysis. \u003c/p\u003e\n\n\u003cp\u003eSince joint plots are more advanced than a basic visualization like a histogram or scatterplot, you'll need to make use of the \u003cstrong\u003e\u003cem\u003eseaborn\u003c/em\u003e\u003c/strong\u003e library to create them. The syntax for creating a joint plot is:\u003c/p\u003e\n\n\u003cpre\u003e\u003ccode class=\"python\"\u003esns.jointplot(x= \u0026lt;column\u0026gt;, y= \u0026lt;column\u0026gt;, data=\u0026lt;dataset\u0026gt;, kind='reg')\n\n\u003c/code\u003e\u003c/pre\u003e\n\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-exploring-your-data/master/images/sample_jointplot.png\"\u003e\u003c/p\u003e\n\n\u003cp\u003eFor full details on how to create joint plots with seaborn, see the \u003ca href=\"https://seaborn.pydata.org/generated/seaborn.jointplot.html\"\u003eseaborn documentation on joint plots\u003c/a\u003e!\u003c/p\u003e\n\n\u003ch2\u003eInterpreting Your EDA Results\u003c/h2\u003e\n\n\u003cp\u003eIt is worth noting that the goal of EDA is not pretty visualizations -- it's \u003cem\u003einsight into your data\u003c/em\u003e!  Don't fall into the trap of thinking that EDA means building a couple of quick visualizations and then moving onto modeling -- you should actively try to generate questions and see if you can answer them by exploring the dataset.  Visualizations are great, but only because they make it easy to quickly interpret our data.  Use them as a tool, not a goal, during the EDA process!\u003c/p\u003e\n\n\u003ch2\u003eSummary\u003c/h2\u003e\n\n\u003cp\u003eIn this lesson, you reviewed descriptive statistics and data visualizations -- two critical components of EDA. Specifically, you reviewed the \u003ccode\u003e.describe()\u003c/code\u003e method for obtaining the descriptive statistics of a DataFrame. You then saw how you can use data visualizations like histograms, KDE plots, and joint plots to gain some insight into your data!\u003c/p\u003e","frontPage":false},{"exportId":"a-complete-data-science-project-using-multiple-regression-recap","title":"A Complete Data Science Project Using Multiple Regression - Recap","type":"WikiPage","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-full-ds-regression-recap\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-full-ds-regression-recap/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\u003ch2\u003eKey Takeaways\u003c/h2\u003e\n\u003cp\u003eCongratulations! You're really coming along! This section gave you an opportunity to review some of the wide ranging skills you've acquired and conduct a full Data Science project! With that, you should have seen that a good Data Science process requires careful thought and not just about the technical details, but also about the general structure and story behind the data itself. Indeed, substantial business value comes from asking the right questions and persuasively communicating the results. Similarly, even when modeling, much of the predictive value comes from thoughtful selection, and creative feature engineering through exploration of the data.\u003c/p\u003e\n\u003cp\u003eTo further summarize:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eThe most common Data Science frameworks are CRISP-DM, KDD, and OSEMiN.\u003c/li\u003e\n\u003cli\u003eThe process of finding, filtering, and loading the appropriate data to answer a question is non-trivial.\u003c/li\u003e\n\u003cli\u003eDecisions made in the data munging/scrubbing phase can have a huge impact on the accuracy of your predictions.\u003c/li\u003e\n\u003cli\u003eVisualization is a key phase in EDA.\u003c/li\u003e\n\u003cli\u003eAnalyzing regression models:\n\u003cul\u003e\n\u003cli\u003eCheck p-values to determine whether features are significant\u003c/li\u003e\n\u003cli\u003eUse Q-Q plots to check for normality\u003c/li\u003e\n\u003cli\u003ePlot residuals against the target variable to check for homoscedasticity (and rule out heteroscedasticity)\u003c/li\u003e\n\u003cli\u003eUse the Variance Inflation Factor to assess Multicollinearity among independent variables\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003eSummary\u003c/h2\u003e\n\u003cp\u003eAgain, well done! You put a lot of your skills to work and went through a full process of collecting data, cleaning it, analyzing and using it to answer questions.\u003c/p\u003e\n\u003cfooter class=\"fis-footer\" style=\"visibility: hidden;\"\u003e\n\u003cdiv class=\"fis-feedback\"\u003e\n\u003ch5\u003eHow do you feel about this lesson?\u003c/h5\u003e\n\u003cimg id=\"thumbs-up\" title=\"Thumbs up!\" alt=\"thumbs up\" data-repository=\"dsc-full-ds-regression-recap\"\u003e\u003cimg id=\"thumbs-down\" title=\"Thumbs down!\" alt=\"thumbs down\" data-repository=\"dsc-full-ds-regression-recap\"\u003e\n\u003c/div\u003e\n\u003ch5\u003eHave specific feedback? \u003ca href=\"https://github.com/learn-co-curriculum/dsc-full-ds-regression-recap/issues/new\"\u003eTell us here!\u003c/a\u003e\n\u003c/h5\u003e\n\u003c/footer\u003e","frontPage":false},{"exportId":"introduction-to-linear-regression-introduction","title":"Introduction to Linear Regression - Introduction","type":"WikiPage","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-linear-regression-section-intro-v2-1\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-linear-regression-section-intro-v2-1/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eIn this section, you're going to learn about one of the most basic machine learning models, linear regression! Many of the ideas you learn in this section will be foundational knowledge for more complex machine learning models.\u003c/p\u003e\n\u003ch2\u003eStatistical Learning Theory\u003c/h2\u003e\n\u003cp\u003eWe'll start this section by exploring Statistical Learning Theory and how dependent and independent variables relate to it. Statistical Learning Theory provides an important framework for understanding machine learning.\u003c/p\u003e\n\u003ch2\u003eLinear Regression\u003c/h2\u003e\n\u003cp\u003eIn this section, we'll introduce our first machine learning model - linear regression. It's really just a fancy way of saying \"(straight) line of best fit\", but it will introduce a number of concepts that will be important as you continue to learn about more sophisticated models.\u003c/p\u003e\n\u003ch2\u003eCoefficient of Determination\u003c/h2\u003e\n\u003cp\u003eWe're then going to introduce the idea of \"R squared\" as the coefficient of determination to quantify how well a particular line fits a particular data set.\u003c/p\u003e\n\u003ch2\u003eA Complete Regression\u003c/h2\u003e\n\u003cp\u003eFrom there we look at calculating a complete linear regression, just using code. We'll cover some of the assumptions that must be held for a \"least squares regression\", introduce Ordinary Least Squares in Statsmodels and introduce some tools for diagnosing your linear regression such as Q-Q plots, the Jarque-Bera test for normal distribution of residuals and the Goldfield-Quandt test for heteroscedasticity. We then look at the interpretation of significance and p-value and finish up by doing a regression model of the Boston Housing data set.\u003c/p\u003e\n\u003ch2\u003eSummary\u003c/h2\u003e\n\u003cp\u003eCongratulations! You've made it through much of the introductory data and we've finally got enough context to take a look at our first machine learning model, while broadening our experience of both coding and math so we'll be able to introduce more sophisticated machine learning models as the course progresses.\u003c/p\u003e\n\u003cfooter class=\"fis-footer\" style=\"visibility: hidden;\"\u003e\n\u003cdiv class=\"fis-feedback\"\u003e\n\u003ch5\u003eHow do you feel about this lesson?\u003c/h5\u003e\n\u003cimg id=\"thumbs-up\" title=\"Thumbs up!\" alt=\"thumbs up\" data-repository=\"dsc-linear-regression-section-intro-v2-1\"\u003e\u003cimg id=\"thumbs-down\" title=\"Thumbs down!\" alt=\"thumbs down\" data-repository=\"dsc-linear-regression-section-intro-v2-1\"\u003e\n\u003c/div\u003e\n\u003ch5\u003eHave specific feedback? \u003ca href=\"https://github.com/learn-co-curriculum/dsc-linear-regression-section-intro-v2-1/issues/new\"\u003eTell us here!\u003c/a\u003e\n\u003c/h5\u003e\n\u003c/footer\u003e","frontPage":false},{"exportId":"bayes-theorem","title":"Bayes' Theorem","type":"WikiPage","content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-bayes-theorem\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-bayes-theorem\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-bayes-theorem/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eBayes theorem is an indispensable law of probability, allowing you to deductively quantify unknown probabilities. The theory rests upon conditional probability. Let's take a look at it in practice.\u003c/p\u003e\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\u003cp\u003eYou will be able to:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eDefine Bayes' theorem in relation to conditional probabilities\u003c/li\u003e\n\u003cli\u003eIdentify examples of applications of Bayes' theorem\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003eBayes' formula\u003c/h2\u003e\n\u003ch3\u003eBreaking the formula apart\u003c/h3\u003e\n\u003cp\u003eBayes' theorem is quite intuitive, decomposing the conditional probability of 'A given B' in terms of the probability that both events are true divided by the probability that B is true. Bayes theorem takes this natural idea a step further, expressing the probability that both events are true as a conditional probability multiplied by the condition itself.\u003c/p\u003e\n\u003cp\u003eTo recap:\u003c/p\u003e\n\u003cp\u003eBayes' Theorem takes the definition of the conditional likelihood:\u003c/p\u003e\n\u003ch3\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=P(A%7CB)%20=%20%5Cdfrac%7BP(A%20%20%5Ccap%20B)%7D%7BP(B)%7D\"\u003e\u003c/h3\u003e\n\u003cp\u003eand rewrites the \u003cimg src=\"https://render.githubusercontent.com/render/math?math=P(A%20%20%5Ccap%20B)\"\u003e as \u003cimg src=\"https://render.githubusercontent.com/render/math?math=P(B%7CA)P(A)\"\u003e , which makes perfect sense; the probability of B given A is true, multiplied by the probability that A is true, gives us the probability that both are true.\u003c/p\u003e\n\u003cp\u003eMaking this substitution, you have Bayes' Theorem:\u003c/p\u003e\n\u003ch3\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=P(A%7CB)%20=%20%5Cdfrac%7BP(B%7CA)P(A)%7D%7BP(B)%7D\"\u003e\u003c/h3\u003e\n\u003ch2\u003eA simple example\u003c/h2\u003e\n\u003cp\u003eLet's take a simple theoretical example to demonstrate. Imagine there are two fish tanks at the local pet store. The small tank holds 10 Betta fish. The large tank has 200 goldfish and 35 Betta fish. Given that a fish is a Betta fish, what's the probability it comes from the small tank?\u003c/p\u003e\n\u003cp\u003eOn the one hand, it seems that if you were to select a fish from the large tank, you'd probably end up with a goldfish. However, because these tanks are of such vastly different sizes, the probability that the fish came from the larger tank is actually more probable.\u003c/p\u003e\n\u003cp\u003eUsing Bayes' theorem, you are looking to find the probability that the fish came from the small tank, given that it is a Betta fish:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=P(%5Ctext%7Bsmall%20tank%20%7C%20Betta%20fish%7D)%20=%20%5Cdfrac%7BP(%5Ctext%7BBetta%20fish%20%7C%20small%20tank%7D)P(%5Ctext%7Bsmall%20tank%7D)%7D%7BP(%5Ctext%7BBetta%20fish%7D)%7D\"\u003e\u003c/p\u003e\n\u003cp\u003eFurthermore, you know:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=P(%5Ctext%7BBetta%20fish%20%7C%20small%20tank%7D)%20=%201\"\u003e\u003cbr\u003e\u003cbr\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=P(%5Ctext%7Bsmall%20tank%7D)%20=%20%5Cdfrac%7B%5Ctext%7Bnumber%20of%20fish%20in%20small%20tank%7D%7D%7B%5Ctext%7Bnumber%20of%20all%20fish%7D%7D%20=%20%5Cdfrac%7B10%7D%7B245%7D\"\u003e\u003cbr\u003e\u003cbr\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=P(%5Ctext%7BBetta%20fish%7D)%20=%20%5Cdfrac%7B45%7D%7B245%7D\"\u003e\u003c/p\u003e\n\u003cp\u003eGiving you:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=P(%5Ctext%7Bsmall%20tank%20%7C%20Betta%20fish%7D)%20=%20%5Cdfrac%7B1%20%5Ccdot%20%5Cdfrac%7B10%7D%7B245%7D%7D%7B%5Cdfrac%7B45%7D%7B245%7D%7D\"\u003e \u003cbr\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=P(%5Ctext%7Bsmall%20tank%20%7C%20Betta%20fish%7D)%20=%20%5Cdfrac%7B10%7D%7B45%7D\"\u003e\u003c/p\u003e\n\u003cp\u003eWhile concrete, this example fails to demonstrate the full power of Bayes' theorem since you had all of the underlying information, so you don't even need to use Bayes' theorem. You could have simply looked at the number of Betta fish in the small tank versus the number of Betta fish overall:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cdfrac%7B10%7D%7B45%7D\"\u003e\u003c/p\u003e\n\u003cp\u003egiving you exactly the same result.\u003c/p\u003e\n\u003ch2\u003eAn NLP example\u003c/h2\u003e\n\u003cp\u003eWith this simple example out of the way, let's examine a more practical example from the field of Natural Language Processing.\u003c/p\u003e\n\u003cp\u003eA common introductory example to Natural Language Processing or classification is detecting spam. While you may enjoy spam in a can, you probably don't enjoy getting spam in your inbox. Bayes' theorem can serve as a natural classification method in these scenarios. Assume that the word \"offer\" (as in Special Offer, We Have an Offer for You, or Don't Miss This Offer!) occurs in 73% of the spam messages you receive. In comparison, only 10% of your desired mail contains the word \"offer\". If 20% of the messages you receive are spam, and you receive another message with the word \"offer\", what is the probability that it is spam?\u003c/p\u003e\n\u003cp\u003eAs you might have guessed, you can solve this using the Bayes' theorem!\u003c/p\u003e\n\u003cp\u003eFirst, set up the problem:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=P(%5Ctext%7BSpam%20%7C%20Offer%7D)%20=%20%5Cdfrac%7BP(%5Ctext%7BOffer%20%7C%20Spam%7D)P(%5Ctext%7BSpam%7D)%7D%7BP(%5Ctext%7BOffer%7D)%7D\"\u003e\u003c/p\u003e\n\u003cp\u003eThen substituting some of the immediate knowledge we have from the scenario:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=P(%5Ctext%7BSpam%20%7C%20Offer%7D)%20=%20%5Cdfrac%7B.73%20%5Ccdot%20.20%7D%7BP(%5Ctext%7BOffer%7D)%7D\"\u003e\u003c/p\u003e\n\u003cp\u003eFinally, the probability of receiving an email with the word \"offer\", \u003cimg src=\"https://render.githubusercontent.com/render/math?math=P(%5Ctext%7BOffer%7D)\"\u003e , can be evaluated by decomposing it into the two subsets spam and not spam:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=P(%5Ctext%7BOffer%7D)%20=%20P(%5Ctext%7BSpam%7D)%5Ccdot%20P(%5Ctext%7BOffer%20%7C%20Spam%7D)%20%2b%20P(%5Ctext%7B~Spam)%7D%20%5Ccdot%20P(%5Ctext%7BOffer%20%7C%20~Spam%7D)\"\u003e\u003cbr\u003e\u003cbr\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=P(%5Ctext%7BOffer%7D)%20=%20.20%20%5Ccdot%20.73%20%2b%20.8%20%5Ccdot%20.10\"\u003e\u003cbr\u003e\u003cbr\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=P(%5Ctext%7BOffer%7D)%20=%20.146%20%2b%20.08\"\u003e\u003cbr\u003e\u003cbr\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=P(%5Ctext%7BOffer%7D)%20=%20.226\"\u003e\u003c/p\u003e\n\u003cp\u003eFinally, substituting this into the original Bayes formula you have:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=P(%5Ctext%7BSpam%20%7C%20Offer%7D)%20=%20%5Cdfrac%7B.73%20%5Ccdot%20.20%7D%7BP(%5Ctext%7BOffer%7D)%7D\"\u003e\u003cbr\u003e\u003cbr\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=P(%5Ctext%7BSpam%20%7C%20Offer%7D)%20=%20%5Cdfrac%7B.73%20%5Ccdot%20.20%7D%7B.226%7D\"\u003e\u003cbr\u003e\u003cbr\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=P(%5Ctext%7BSpam%20%7C%20Offer%7D)%20=%20.6460\"\u003e\u003c/p\u003e\n\u003cp\u003eAs you can see, while spam has a much higher occurrence of the word \"offer\", the presence of the word alone does not provide strong confidence that the message is spam. To provide more statistical power, you will eventually extend Bayes' theorem to multiple observations simultaneously using the relative probabilities of multiple words.\u003c/p\u003e\n\u003ch2\u003eSummary\u003c/h2\u003e\n\u003cp\u003eIn this lesson, you were introduced to the Bayes' theorem, and saw how it can be used to quantify conditional probabilities. With that, let's turn to some more simple examples for you to practice and deepen your understanding.\u003c/p\u003e","frontPage":false},{"exportId":"blogging-overview","title":"Blogging Overview","type":"WikiPage","content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-blogging-overview\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-blogging-overview\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-blogging-overview/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e  \u003ch2\u003eIntroduction\u003c/h2\u003e  \u003cp\u003eIn this lesson, we discuss how to write good blog posts that meet Flatiron School's requirements.\u003c/p\u003e  \u003ch2\u003eObjectives\u003c/h2\u003e  \u003cp\u003eThis lesson covers...\u003c/p\u003e  \u003cul\u003e \u003cli\u003eWhy blogging is valuable\u003c/li\u003e \u003cli\u003eTopics to blog about\u003c/li\u003e \u003cli\u003eWhat makes for a good blog post\u003c/li\u003e \u003cli\u003eHow to start your blog\u003c/li\u003e \u003cli\u003eFlatiron School blog requirements \u003c/li\u003e \u003c/ul\u003e  \u003ch2\u003eWhy Should I Blog?\u003c/h2\u003e  \u003cp\u003eBlogging has many benefits:\u003c/p\u003e  \u003cul\u003e \u003cli\u003e\u003cp\u003e\u003cstrong\u003eDevelop your written communication skills.\u003c/strong\u003e Your writing ability will be critical to your success when completing job applications and presenting your work to colleagues. Blogging is great practice for identifying and clearly communicating the most important points of any subject.\u003c/p\u003e\u003c/li\u003e \u003cli\u003e\u003cp\u003e\u003cstrong\u003eDemonstrate your talent to employers.\u003c/strong\u003e Potential employers will review your blog to determine whether to offer you an interview or a job. Some students have even been invited to interview or exempted from technical interviews based on their blogs.\u003c/p\u003e\u003c/li\u003e \u003cli\u003e\u003cp\u003e\u003cstrong\u003eStrengthen your knowledge.\u003c/strong\u003e Blogging helps you explore new topics, deepen your understanding, and crystallize what you've learned.\u003c/p\u003e\u003c/li\u003e \u003cli\u003e\u003cp\u003e\u003cstrong\u003eHelp your peers and the broader community.\u003c/strong\u003e Have you ever Googled a question you had and found the answer on a blog? Writing blog posts helps others who are following in your footsteps!\u003c/p\u003e\u003c/li\u003e \u003c/ul\u003e  \u003ch2\u003eWhat Should I Blog About?\u003c/h2\u003e  \u003cp\u003eHere are some blog topic ideas:\u003c/p\u003e  \u003cul\u003e \u003cli\u003e\u003cp\u003eWhy did you decide to learn data science?\u003c/p\u003e\u003c/li\u003e \u003cli\u003e\u003cp\u003eDescribe how a DS technique works, when you might use it, and its strengths/weaknesses.\u003c/p\u003e\u003c/li\u003e \u003cli\u003e\u003cp\u003eSummarize an End of Phase Project by explaining your problem, the dataset, your methodology, and your results.\u003c/p\u003e\u003c/li\u003e \u003cli\u003e\u003cp\u003eDive into something that you want to learn more about, maybe because you find it challenging or it wasn't covered in the course.\u003c/p\u003e\u003c/li\u003e \u003cli\u003e\u003cp\u003eWrite a tutorial to help aspiring data scientists to implement a tool or method.\u003c/p\u003e\u003c/li\u003e \u003cli\u003e\u003cp\u003eFind an interesting data science paper and summarize why it is important. This can be a new paper from the past few months, or you can refer to \u003ca href=\"https://docs.google.com/spreadsheets/d/1UYmAT13AAknrOatzLeeAsN4tS7ENjn2fpJNGzOZ67rQ/edit?usp=sharing\"\u003ethis spreadsheet\u003c/a\u003e.\u003c/p\u003e\u003c/li\u003e \u003c/ul\u003e  \u003ch2\u003eWhat Does A Good Blog Post Look Like?\u003c/h2\u003e  \u003cp\u003eWe recommend you take a look at our \u003ca href=\"https://drive.google.com/drive/folders/1UBiRCRLzVP5CHU3PJNwoMZAe3ajUBm2a?usp=sharing\"\u003eblog templates\u003c/a\u003e and \u003ca href=\"https://docs.google.com/document/d/1eqL8Dsj7dH7s_MRnf_4-3kCiSz72POHTfb-sBRN5Zhs/edit?usp=sharing\"\u003eexamples\u003c/a\u003e to get an idea for what makes a blog post good.\u003c/p\u003e  \u003cul\u003e \u003cli\u003e\u003cp\u003eStrike a balance between providing a meaningful investigation of your topic and being concise. Constrain the scope so it will be interesting and digestible in about 1000-3000 words (this is not a firm limit).\u003c/p\u003e\u003c/li\u003e \u003cli\u003e\n\u003cp\u003eUse clear and consistent formatting to make your content accessible and professional-looking.\u003c/p\u003e  \u003cul\u003e \u003cli\u003eWhen presenting code, use code snippets instead of screenshots.\u003c/li\u003e \u003cli\u003eMake URLs into hyperlinks that are easy for readers to click into.\u003c/li\u003e \u003cli\u003eUse headings to provide structure and flow to your post.\u003c/li\u003e \u003c/ul\u003e\n\u003c/li\u003e \u003cli\u003e\u003cp\u003eCite and link to resources you used to write your post.\u003c/p\u003e\u003c/li\u003e \u003c/ul\u003e  \u003ch2\u003eHow Do I Start My Blog?\u003c/h2\u003e  \u003cp\u003eIf you already have a professional blog that you'd like to use for your data science content, you can add your posts to that. Otherwise, you will need to start a new blog. If you have a personal blog, you should avoid using it for this purpose so that you can continue using it for personal content without worrying about how it might be perceived by potential employers.\u003c/p\u003e  \u003cp\u003eThere are multiple blogging platforms to choose from that make it easy to start a blog, here are some of our favorites:\u003c/p\u003e  \u003cul\u003e \u003cli\u003e\u003ca href=\"https://www.blogger.com/\"\u003eBlogger\u003c/a\u003e\u003c/li\u003e \u003cli\u003e\u003ca href=\"https://dev.to/\"\u003edev.to\u003c/a\u003e\u003c/li\u003e \u003cli\u003e\u003ca href=\"https://pages.github.com/\"\u003eGitHub Pages\u003c/a\u003e\u003c/li\u003e \u003cli\u003e\u003ca href=\"https://medium.com/\"\u003eMedium\u003c/a\u003e\u003c/li\u003e \u003cli\u003e\u003ca href=\"https://wordpress.com/\"\u003eWordpress\u003c/a\u003e\u003c/li\u003e \u003c/ul\u003e  \u003cp\u003eDifferent platforms have different pros and cons, so do a little research to decide what is best for you.\u003c/p\u003e  \u003ch2\u003eBlog Requirements\u003c/h2\u003e  \u003cp\u003eTo succeed in your career transition and graduate from Flatiron School, you must complete the following activities. These requirements are designed to give you the best opportunity to deepen your knowledge, practice communication skills, and showcase yourself to potential employers.\u003c/p\u003e  \u003cul\u003e \u003cli\u003e\u003cp\u003eSet up a publicly accessible blog \u003c/p\u003e\u003c/li\u003e \u003cli\u003e\u003cp\u003ePublish at least four blog posts on it, including \u003cstrong\u003eone per Phase for Phases 1-4\u003c/strong\u003e\u003c/p\u003e\u003c/li\u003e \u003cli\u003e\n\u003cp\u003eSubmit URLs to your posts \u003cstrong\u003eby the end of each Phase\u003c/strong\u003e in the Blog Post assignments\u003c/p\u003e  \u003cul\u003e \u003cli\u003eThese assignments are located in the Milestones topics of the Phase 1-4 Canvas courses\u003c/li\u003e \u003c/ul\u003e\n\u003c/li\u003e \u003cli\u003e\n\u003cp\u003eWrite blog posts that...\u003c/p\u003e  \u003cul\u003e \u003cli\u003eDiscuss data science topics\u003c/li\u003e \u003cli\u003eAre composed primarily of original material you wrote\u003c/li\u003e \u003cli\u003eInclude proper attribution\u003c/li\u003e \u003cli\u003eHave high-quality content and formatting\u003c/li\u003e \u003cli\u003eAre something you would proudly show to a potential employer\u003c/li\u003e \u003c/ul\u003e\n\u003c/li\u003e \u003c/ul\u003e  \u003cp\u003eAfter you submit your blog posts, your teacher will grade them as Complete or Incomplete. Your blogs must all be submitted on time and receive Complete grades in order to continue through your program.\u003c/p\u003e  \u003cp\u003e✨Have fun and happy blogging!✨\u003c/p\u003e","frontPage":false},{"exportId":"combinatorics-and-probability-recap","title":"Combinatorics and Probability - Recap","type":"WikiPage","content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-probability-section-recap\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-probability-section-recap\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-probability-section-recap/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eIn this section, you learned about the concepts of combinatorics and probability.\u003c/p\u003e\n\u003ch2\u003eKey Takeaways\u003c/h2\u003e\n\u003cp\u003eIn this section, we dug into a number of foundational concepts:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eProbability is \"how likely\" it is that an event will happen\u003c/li\u003e\n\u003cli\u003eSets in Python are unordered collections of unique elements\u003c/li\u003e\n\u003cli\u003eA sample space is a collection of every single possible outcome in a trial\u003c/li\u003e\n\u003cli\u003eThe inclusion exclusion principle is a counting technique used to calculate the number of elements in a collection of sets with overlapping elements.\u003c/li\u003e\n\u003cli\u003eFactorials provide the basis for calculating permutations\u003c/li\u003e\n\u003cli\u003eThe difference between permutations and combinations is that with combinations, order is not important\u003c/li\u003e\n\u003cli\u003eThe \"sum rule\" of probability states that \u003cimg src=\"https://render.githubusercontent.com/render/math?math=P(A%20%5Ccup%20B)%20=%20P(A)%20%2b%20P(B)%20-%20P(A%20%20%5Ccap%20B)\"\u003e\n\u003c/li\u003e\n\u003cli\u003eIndependent events don't affect each other - e.g. consecutive coin tosses\u003c/li\u003e\n\u003cli\u003eDependent events do affect each other - e.g. picking consecutive colored marbles from a bag\u003c/li\u003e\n\u003cli\u003eThe product rule is useful when the conditional probability is easy to compute, but the probability of intersections of events is not\u003c/li\u003e\n\u003cli\u003eThe chain rule (also called the general product rule) permits the calculation of any member of the joint distribution of a set of random variables using only conditional probabilities.\u003c/li\u003e\n\u003cli\u003eBayes theorem describes the probability of an event based on prior knowledge of conditions that might be related to the event\u003c/li\u003e\n\u003cli\u003eThe law of total probability states that the probability for a sample space is the sum of the probabilities for partitions of that sample space\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eIn this section, we introduced the ideas of combinatorics and probability. In the next section, you'll use this knowledge and take it a step further by learning about statistical distributions and their applications!\u003c/p\u003e","frontPage":false},{"exportId":"ab-testing-recap","title":"AB Testing - Recap","type":"WikiPage","content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-ab-testing-recap\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-ab-testing-recap\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-ab-testing-recap/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eIn this section, you got a chance to further practice hypothesis testing with some real-world scenarios. Here's a brief recap of some of the key takeaways from the section.\u003c/p\u003e\n\n\u003ch2\u003eA Brief Review\u003c/h2\u003e\n\n\u003cp\u003eWhen conducting statistical tests, there's simply no substitute for critical thinking. As you've seen, there are numerous considerations from formulating appropriate hypotheses to thinking about the context of the problem itself. While the techniques presented are extremely powerful if properly employed, you've also seen how each technique comes with its own assumptions, and ignoring these can invalidate results. Similarly, how the test is structured can lead to widely different results. \u003c/p\u003e\n\n\u003cp\u003eThere are also additional statistical tests not touched upon here. To date, your primary method has been conducting t-tests. This the most common statistical test used in practice and is very effective at comparing averages (of any metric) between groups. You've also seen how ANOVA can generalize this process to multiple groups. If you wish to continue to strengthen your knowledge of other statistical tests, take a look at some of the resources below.\u003c/p\u003e\n\n\u003ch2\u003eAdditional Resources\u003c/h2\u003e\n\n\u003cul\u003e\n\u003cli\u003e\u003cp\u003e\u003ca href=\"https://stats.idre.ucla.edu/other/mult-pkg/whatstat/\"\u003eChoosing Statistical Tests\u003c/a\u003e\u003c/p\u003e\u003c/li\u003e\n\u003cli\u003e\u003cp\u003e\u003ca href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3116565/\"\u003eHow to choose the right statistical test?\u003c/a\u003e\u003c/p\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch2\u003eSummary\u003c/h2\u003e\n\n\u003cp\u003eWell done! You've not only learned about a variety of statistical tests and the theory behind them, but you've now also put these techniques into practice in order to carry out hypothesis testing!\u003c/p\u003e","frontPage":false},{"exportId":"statistical-distributions-recap","title":"Statistical Distributions - Recap","type":"WikiPage","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-distributions-section-recap-v2-1\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-distributions-section-recap-v2-1/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eThis short lesson summarizes the topics we covered in this section and why they'll be important to you as a data scientist.\u003c/p\u003e\n\n\u003ch2\u003eKey Takeaways\u003c/h2\u003e\n\n\u003cp\u003eIn this section, we really dug into statistical distributions. \u003c/p\u003e\n\n\u003cp\u003eKey takeaways include:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eThere are two types of distributions - continuous, where (subject to measurement and/or storage precision) there are effectively an infinite number of possible values, and discrete, where there are a distinct, non-infinite number of options. For example, a person's height is continuous - assuming a suitably precise tape measure - whereas the number of bedrooms in a house is discrete\u003c/li\u003e\n\u003cli\u003eHow to describe the distribution of data sets using Probability Mass Functions, Cumulative Distribution Functions, and Probability Density Functions\u003c/li\u003e\n\u003cli\u003eOne type of discrete distribution deals with a series of boolean events or trials - often called Bernoulli Trials\u003c/li\u003e\n\u003cli\u003eA Normal distribution is the classic \"bell curve\" with 68% of the probability mass within 1 SD of the mean, 95% within 2 SDs and 99.7% within 3 SDs\u003c/li\u003e\n\u003cli\u003eDifferences between the normal and the standard normal distribution\u003c/li\u003e\n\u003cli\u003eThe uses of \u003cimg src=\"https://render.githubusercontent.com/render/math?math=z\"\u003e-scores and p-values for describing a distribution\u003c/li\u003e\n\u003cli\u003eHow a one sample \u003cimg src=\"https://render.githubusercontent.com/render/math?math=z\"\u003e-test is a very simple form of hypothesis testing.\u003c/li\u003e\n\u003cli\u003eHow skewness and kurtosis can be used to measure how different a given distribution is from a normal distribution\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003eIn the Appendix to this Module, you'll have the opportunity to learn about:  \u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eThe uniform distribution, which represents processes where each outcome is equally likely, like rolling a dice.\u003cbr\u003e\n\u003c/li\u003e\n\u003cli\u003eThe Poisson distribution, which can be used to display the likelihood of a given number of successes over a given time period.\u003cbr\u003e\n\u003c/li\u003e\n\u003cli\u003eThe exponential distribution, which can be used to describe the probability distribution of the amount of time it may take before a given event occurs.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cfooter class=\"fis-footer\" style=\"visibility: hidden;\"\u003e\u003cdiv class=\"fis-feedback\"\u003e\n\u003ch5\u003eHow do you feel about this lesson?\u003c/h5\u003e\n\u003cimg id=\"thumbs-up\" data-repository=\"dsc-distributions-section-recap-v2-1\" title=\"Thumbs up!\" alt=\"thumbs up\"\u003e\u003cimg id=\"thumbs-down\" data-repository=\"dsc-distributions-section-recap-v2-1\" title=\"Thumbs down!\" alt=\"thumbs down\"\u003e\n\u003c/div\u003e\n\u003ch5\u003eHave specific feedback? \u003ca href=\"https://github.com/learn-co-curriculum/dsc-distributions-section-recap-v2-1/issues/new\"\u003eTell us here!\u003c/a\u003e\n\u003c/h5\u003e\u003c/footer\u003e","frontPage":false},{"exportId":"scrubbing-and-cleaning-data","title":"Scrubbing and Cleaning Data","type":"WikiPage","content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-scrubbing-and-cleaning-data\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-scrubbing-and-cleaning-data\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-scrubbing-and-cleaning-data/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eIn this lesson, you'll review common issues to focus on when scrubbing and cleaning data.\u003c/p\u003e\n\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\n\u003cp\u003eYou will be able to:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eCast columns to appropriate data types\u003c/li\u003e\n\u003cli\u003eIdentify and deal with null values appropriately\u003c/li\u003e\n\u003cli\u003eRemove unnecessary columns\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch2\u003eThe \"Scrub\" Step\u003c/h2\u003e\n\n\u003cp\u003eDuring the process of working with data, you'll always reach a point where you've gathered all the data you'll need (our \"Obtain\" step), but the data is not yet in a format where you can use it for modeling.  All the work that you'll be doing in the next lab will be to get the dataset in a format that you can easily explore and build models with. \u003c/p\u003e\n\n\u003ch2\u003eSubsampling to Reduce Size\u003c/h2\u003e\n\n\u003cp\u003eWhen building a model for predictive purposes, more data is always better when training the final model. However, during the development process when working with large datasets, it is common to work with only a subsample of the dataset. Building a model is an iterative process -- often, you fit the model, investigate the results, then train the model again with some small tweaks based on what you noticed.  Since this is an iterative process, you want to avoid long runtimes, and iterate as quickly as possible.  When you're satisfied with the model you've built on the subsample of data, then you would fit the model on the entire dataset. \u003c/p\u003e\n\n\u003cp\u003eIn the next lab, you'll work with a subsample of the dataset to increase the iteration speed in refining your model. \u003c/p\u003e\n\n\u003ch2\u003eDealing With Data Types\u003c/h2\u003e\n\n\u003cp\u003eOne of the most common problems you'll need to deal with during the data scrubbing step is columns that are encoded as the wrong data type. For example, a common formatting issue you may encounter is numeric data that is mistakenly encoded as string data. This makes numerical operations on the data impossible without first reformatting the data. A simple operation such as \u003ccode\u003e2 + 2\u003c/code\u003e will return \u003ccode\u003e22\u003c/code\u003e if the numeric data is accidentally formatted as strings (\u003ccode\u003e'2'+'2'='22'\u003c/code\u003e). Similarly, categorical data is often encoded as integer values. If you don't properly conceptualize how the data is represented, you will fail to formulate meaningful models and insights.\u003c/p\u003e\n\n\u003cp\u003eA first step to uncover and investigate such issues is to use the \u003ccode\u003e.info()\u003c/code\u003e method available for all Pandas DataFrames. This will tell what type of data each column contains, as well as the number of values contained within that column (which can also help us identify columns that contain missing data)!  Here's an example response:\u003c/p\u003e\n\n\u003cpre\u003e\u003ccode\u003e\u0026lt;class 'pandas.core.frame.DataFrame'\u0026gt;\nInt64Index: 97839 entries, 0 to 97838\nData columns (total 16 columns):\nStore           97839 non-null object\nDept            97839 non-null object\nDate            97839 non-null object\nWeekly_Sales    97839 non-null float64\nIsHoliday       97839 non-null bool\nType            97839 non-null object\nSize            97839 non-null int64\nTemperature     97839 non-null float64\nFuel_Price      97839 non-null float64\nMarkDown1       35013 non-null float64\nMarkDown2       27232 non-null float64\nMarkDown3       32513 non-null float64\nMarkDown4       34485 non-null float64\nMarkDown5       35013 non-null float64\nCPI             97839 non-null float64\nUnemployment    97839 non-null float64\ndtypes: bool(1), float64(10), int64(1), object(4)\nmemory usage: 12.0+ MB\n\n\u003c/code\u003e\u003c/pre\u003e\n\n\u003cp\u003eFrom here, a good next step would be to look at examples from each column encoded as strings (remember, Pandas refers to string columns as \u003ccode\u003eobject\u003c/code\u003e) and confirm that this data is supposed to be encoded as strings. One method to do this is to preview a truncated version of the output from \u003ccode\u003e.value_counts()\u003c/code\u003e. For example, you could preview the 5 most frequent entries from each column with a simple loop like this:  \u003c/p\u003e\n\n\u003cpre\u003e\u003ccode class=\"python\"\u003efor col in df.columns:\n    try:\n        print(col, df[col].value_counts()[:5])\n    except:\n        print(col, df[col].value_counts())\n        # If there aren't 5+ unique values for a column the first print statement\n        # will throw an error for an invalid idx slice\n    print('\\n') # Break up the output between columns\n\u003c/code\u003e\u003c/pre\u003e\n\n\u003cp\u003eIt is usually also a good idea to check integer columns to ensure that the data it contains is meant to represent actual numeric data, and is not just categorical data encoded as integers. You may also uncover null values hard coded as strings such as \u003ccode\u003e\"?\"\u003c/code\u003e, \u003ccode\u003e\"999999\"\u003c/code\u003e or other extraneous values depending on the dataset and who created it.\u003c/p\u003e\n\n\u003ch3\u003eNumeric Data Encoded as Strings\u003c/h3\u003e\n\n\u003cp\u003eIf you've identified numeric data encoded as strings, it's typically a pretty easy problem to solve. Often it's as simple as casting the string data to a numeric type:\u003c/p\u003e\n\n\u003cpre\u003e\u003ccode class=\"python\"\u003edf['numeric_string_col'] = df['numeric_string_col'].astype('float')\n\u003c/code\u003e\u003c/pre\u003e\n\n\u003cp\u003eSadly, it's not always that simple. For example, if there is even a single cell that contains a letter or non-numeric character such as a comma or monetary symbol ($) the above statement will fail. In such cases, a more complex cleaning function must be manually created. This could involve stripping extraneous symbols such as ',$/%',  or simply casting non convertible strings as null. Recall that when NumPy sees multiple data types in an array, it defaults to casting everything as a string. If you try to cast a column from string to numeric data types and get an error, consider checking the unique values in that column -- it's likely that you may have a single letter hiding out somewhere that needs to be removed!\u003c/p\u003e\n\n\u003ch3\u003eCategorical Data Encoded as Integers\u003c/h3\u003e\n\n\u003cp\u003eIt's also common to see categorical data encoded as integers.  Given that a big step in the data cleaning process is to convert all categorical columns to numeric equivalents, this may not seem like a problem at first glance.  However, leaving categorical data encoded as integers can have a negative effect by introducing bad information into our model. This is because integer encoding mistakenly adds mathematical relationships between the different categories -- our model may mistakenly think that the category represented by the integer \u003ccode\u003e4\u003c/code\u003e twice as much as category \u003ccode\u003e2\u003c/code\u003e, and so on.  \u003c/p\u003e\n\n\u003cp\u003eThe best way of dealing with this problem is to cast the entire column to a string data type, which will better represent the column's categorical nature. Since it's categorical, we can correctly deal with it when we one-hot encode categorical data later in the process.\u003c/p\u003e\n\n\u003cp\u003eThe following example shows the syntax necessary for converting a column from one data type to another:\u003c/p\u003e\n\n\u003cpre\u003e\u003ccode class=\"python\"\u003edf['some_column'] = df.['some_column'].astype('float32')\n\ndf['some_column'] = df.['some_column'].astype('str')\n\u003c/code\u003e\u003c/pre\u003e\n\n\u003cp\u003eOnce done, it is then common to pass these categorical variables to another function such as \u003ccode\u003epd.get_dummies()\u003c/code\u003e in order to transform these features into representations that are more suitable for machine learning algorithms. It may be necessary to drop the first dummy to avoid the dummy variable trap.\u003c/p\u003e\n\n\u003ch2\u003eDetecting and Dealing With Null Values\u003c/h2\u003e\n\n\u003cp\u003eAnother important data cleaning check is to inspect for missing or null values. Recall from previous labs that Pandas denotes missing values as \u003ccode\u003eNaN\u003c/code\u003e.\u003c/p\u003e\n\n\u003ch3\u003eChecking For \u003ccode\u003eNaN\u003c/code\u003es\u003c/h3\u003e\n\n\u003cp\u003eYou can easily check how many missing values are contained within each column by having Pandas create a truth table where the cells that contain \u003ccode\u003eNaN\u003c/code\u003e are marked as \u003ccode\u003eTrue\u003c/code\u003e and everything else is marked as \u003ccode\u003eFalse\u003c/code\u003e.\u003c/p\u003e\n\n\u003cpre\u003e\u003ccode class=\"python\"\u003edf.isna()\n\u003c/code\u003e\u003c/pre\u003e\n\n\u003cp\u003eSince \u003ccode\u003eFalse=0\u003c/code\u003e and \u003ccode\u003eTrue=1\u003c/code\u003e in programming, you can then \u003ccode\u003esum()\u003c/code\u003e these truth tables to get a column-by-column count of the number of missing values in the dataset. \u003c/p\u003e\n\n\u003cpre\u003e\u003ccode class=\"python\"\u003edf.isna().sum()\n\u003c/code\u003e\u003c/pre\u003e\n\n\u003cp\u003eAs noted above, remember that your dataset may also contain null values that are denoted by placeholder values.  Most datasets that do this will make mention of this in the dataset's data dictionary. However, you may also see these denoted by extreme values that don't make sense (e.g. a person's weight being set to something like 0 or 10000).  Doing a quick manual inspection of the top values for each feature is often the only manner to detect such anomalies.\u003c/p\u003e\n\n\u003ch3\u003eDealing With Null Values\u003c/h3\u003e\n\n\u003cp\u003eThere are several options for dealing with null values. You can always remove observation rows with missing values or similarly remove features with excessive sparsity caused by null values. That said, doing so throws away potentially valuable information. There may be important reasons why said information is missing. Despite this, many machine learning algorithms will not tolerate null values and as such you either have to impute values or drop the data. Some options you have for imputing data include:\u003c/p\u003e\n\n\u003cp\u003e\u003cstrong\u003e\u003cem\u003eNumeric Data\u003c/em\u003e\u003c/strong\u003e\n* Replacing Nulls with the column median\n* Binning the data and converting columns to a categorical format (\u003cem\u003eCoarse Classification)\u003c/em\u003e\u003c/p\u003e\n\n\u003cp\u003e\u003cstrong\u003e\u003cem\u003eCategorical Data\u003c/em\u003e\u003c/strong\u003e\n* Making Null values their own category\n* Replacing null values with the most common category \u003c/p\u003e\n\n\u003cp\u003eAs a data scientist, you will have to decide which method of imputation is appropriate for your particular data set and business objective.\u003c/p\u003e\n\n\u003ch2\u003eChecking For Multicollinearity\u003c/h2\u003e\n\n\u003cp\u003eBefore proceeding to modeling, you also want to check that the data does not have high multicollinearity or correlation/covariance between predictor columns.  \u003c/p\u003e\n\n\u003cp\u003eThe easiest way to do this to build and interpret a correlation heatmap with the \u003ccode\u003eseaborn\u003c/code\u003e package. \u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-scrubbing-and-cleaning-data/master/images/heatmap.png\"\u003e\u003c/p\u003e\n\n\u003cp\u003eColumns with strong correlation should be dealt with by removing one of the offending columns, or by combining the columns through feature engineering (more on this later in the curriculum). After all, highly correlated features makes feature weights unstable and also impede model interpretability. That said, they are not apt to reduce model performance if that is the sole consideration.\u003c/p\u003e\n\n\u003cp\u003eThe \u003ca href=\"https://seaborn.pydata.org/examples/many_pairwise_correlations.html\"\u003eseaborn documentation\u003c/a\u003e provides a great example code on how to build a correlation heatmap with data stored in a Pandas DataFrame. \u003c/p\u003e\n\n\u003ch2\u003eNormalizing Data\u003c/h2\u003e\n\n\u003cp\u003eAn important step during the data cleaning process is to convert all of our data to the same scale by \u003cstrong\u003e\u003cem\u003enormalizing\u003c/em\u003e\u003c/strong\u003e it.  \u003c/p\u003e\n\n\u003cp\u003eThe most common form of data normalization is by converting data to z-scores. This is commonly referred to as standardization.\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=z=%20%5Cdfrac%7Bx-%5Cmu%7D%7B%5Csigma%7D\"\u003e \n\u003cbr\u003e\n \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cmu%20=%20%5Ctext%7BMean%7D\"\u003e \n\u003cbr\u003e\n \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Csigma%20=%20%5Ctext%7BStandard%20Deviation%7D\"\u003e \u003c/p\u003e\n\n\u003cp\u003eThere are also other sorts of scaling methods we can use, such as \u003cstrong\u003e\u003cem\u003eMin-Max normalization\u003c/em\u003e\u003c/strong\u003e:\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=z=%20%5Cdfrac%7Bx-%5Cmin(x)%7D%7B%5Cmax(x)-%5Cmin(x)%7D\"\u003e \u003c/p\u003e\n\n\u003ch2\u003eSummary\u003c/h2\u003e\n\n\u003cp\u003ePhew! That was quite a bit! There's a lot to consider when cleaning your data. Here, you explored casting data to the appropriate data types, identifying and correcting null values, and removing features that exhibit multi-collinearity. While applying this workflow, be sure to stay on your toes and try to wrap your head around the context of the data: Does the current representation seem sensible? Are there any anomalies within it? Cleaning data is always a tricky process and while aspects can be fairly standard, having an inquisitive approach goes a long way.\u003c/p\u003e","frontPage":false},{"exportId":"multiple-regression-and-model-validation-introduction","title":"Multiple Regression and Model Validation - Introduction","type":"WikiPage","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-regression-introduction\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-regression-introduction/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eIn this section, you'll start to see more advanced regression models which employ multiple predictors. With that, you'll also start to investigate how to validate your models to ensure they are neither overfit nor underfit, and will generalize well to future cases.\u003c/p\u003e\n\u003ch2\u003eMultiple Linear Regression\u003c/h2\u003e\n\u003cp\u003eIn the last section, you learned how to perform a basic linear regression with a single predictor variable. Here, you'll explore how to perform linear regressions using \u003cstrong\u003emultiple\u003c/strong\u003e independent variables to better predict a target variable.\u003c/p\u003e\n\u003ch2\u003eImproving a Baseline Model\u003c/h2\u003e\n\u003cp\u003eOver the next few lessons, you'll see some ways to improve basic regression models using different combinations of features as well as some feature engineering. Often, a simple linear regression can be used as a \"baseline model\" upon which new features can be added to improve the predictions. Any decisions on how to change features should be compared against a simpler model to see if the changes have improved the model or not. This section will give an introduction to many of the techniques which can improve a regression model.\u003c/p\u003e\n\u003ch2\u003eDealing with Categorical Variables\u003c/h2\u003e\n\u003cp\u003eUp to this point you've only seen continuous predictor variables. Here, you'll get a further look at how to identify and then transform categorical variables to utilize them as predictors of our target variable.\u003c/p\u003e\n\u003ch2\u003eMulticollinearity of Features\u003c/h2\u003e\n\u003cp\u003eWhile multiple predictors will ultimately increase model performance and yield better predictions, there are also possible negative effects when using multiple predictors that have a high correlation with each other. This is known as multicollinearity and can muddy model interpretation.\u003c/p\u003e\n\u003ch2\u003eFeature Scaling and Normalization\u003c/h2\u003e\n\u003cp\u003eAnother consideration when using multiple predictors in any model is the scale of those features. For example, a dataset surrounding households might have a \"number of children\" feature and an \"income\" feature. These two variables are of vastly different scales and as such, simply having a feature like income which is on a much larger scale can impact its influence over the model. To avoid this, it is best practice to normalize the scale of all features before feeding the data to a machine learning algorithm.\u003c/p\u003e\n\u003ch2\u003eMultiple Linear Regression in Statsmodels\u003c/h2\u003e\n\u003cp\u003eAfter covering a lot of the key theory, you'll then get some hands-on practice in performing multiple linear regressions using the Statsmodels and Scikit-Learn libraries.\u003c/p\u003e\n\u003ch2\u003eModel Fit and Validation\u003c/h2\u003e\n\u003cp\u003eYou'll continue the section by looking at how we can analyze the results of a regression, and learn the importance of splitting data into training and test sets to determine how well a model predicts \"unknown\" values (the test dataset). Finally, you'll wrap up the section by looking at how k-fold cross-validation can be used to get additional model validations on a limited data set by taking multiple splits of training and testing data.\u003c/p\u003e\n\u003ch2\u003eSummary\u003c/h2\u003e\n\u003cp\u003eIn this section, you'll continue to bolster your understanding of linear regression so that you can solve a wider range of problems more accurately. Many of the principles covered in this section will also apply in later modules as you move onto working with other machine learning models.\u003c/p\u003e\n\u003cfooter class=\"fis-footer\" style=\"visibility: hidden;\"\u003e\n\u003cdiv class=\"fis-feedback\"\u003e\n\u003ch5\u003eHow do you feel about this lesson?\u003c/h5\u003e\n\u003cimg id=\"thumbs-up\" title=\"Thumbs up!\" alt=\"thumbs up\" data-repository=\"dsc-regression-introduction\"\u003e\u003cimg id=\"thumbs-down\" title=\"Thumbs down!\" alt=\"thumbs down\" data-repository=\"dsc-regression-introduction\"\u003e\n\u003c/div\u003e\n\u003ch5\u003eHave specific feedback? \u003ca href=\"https://github.com/learn-co-curriculum/dsc-regression-introduction/issues/new\"\u003eTell us here!\u003c/a\u003e\n\u003c/h5\u003e\n\u003c/footer\u003e","frontPage":false},{"exportId":"statistical-power-and-anova-introduction","title":"Statistical Power and ANOVA -  Introduction","type":"WikiPage","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-statistical-power-anova-introduction\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-statistical-power-anova-introduction/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eIn this section you'll continue to deepen your knowledge of hypothesis testing and t-tests by examining the concept of power; an idea closely related to type II errors. With that, you'll see how the rate of type I errors, power, sample size, and effect size are intrinsically related to one another. You will then move on to ANOVA - Analysis of Variance, which allows you to test for the influence of multiple factors all at once.\u003c/p\u003e\n\n\u003ch2\u003eStatistical power\u003c/h2\u003e\n\n\u003cp\u003eStatistical power is equal to  \u003cimg src=\"https://render.githubusercontent.com/render/math?math=1%20-%20%5Cbeta\"\u003e where  \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cbeta\"\u003e is the rate of type II errors. As you will see, power is related to  \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Calpha\"\u003e , sample size, and effect size. Typically a researcher will select an acceptable alpha value and then examine required sample sizes to achieve the desired power such as 0.8 (or higher). \u003c/p\u003e\n\n\u003ch2\u003eWelch's t-test\u003c/h2\u003e\n\n\u003cp\u003eAfter an initial exploration of statistical power, you'll take a look at Welch's t-test. This is an adaptation of the unpaired student's t-test you've seen previously which allows for different sample sizes or different variances between the two groups.\u003c/p\u003e\n\n\u003ch2\u003eMultiple comparisons\u003c/h2\u003e\n\n\u003cp\u003eFrom there, you'll look at some of the issues that arise when trying to perform multiple comparisons - from the risks of spurious correlations to the importance of corrections such as the Bonferroni correction to deal with the cumulative risks of type I errors inherent in multiple comparisons.\u003c/p\u003e\n\n\u003ch2\u003eANOVA\u003c/h2\u003e\n\n\u003cp\u003eFinally, you'll take a look at the more generalized procedure for conducting multiple comparisons: Analysis of Variance or ANOVA. You'll see that ANOVA of only two groups is statistically equivalent to a two sided t-test. That said, ANOVA fully supports comparing multiple factors simultaneously.\u003c/p\u003e\n\n\u003ch2\u003eSummary\u003c/h2\u003e\n\n\u003cp\u003eWithout a good understanding of experimental design, it's easy to end up drawing false conclusions. In this section, you'll cover a range of tools and techniques to deepen your understanding of hypothesis testing and ensure that you design experiments rigorously and interpret them thoughtfully.\u003c/p\u003e\n\u003cfooter class=\"fis-footer\" style=\"visibility: hidden;\"\u003e\u003cdiv class=\"fis-feedback\"\u003e\n\u003ch5\u003eHow do you feel about this lesson?\u003c/h5\u003e\n\u003cimg id=\"thumbs-up\" data-repository=\"dsc-statistical-power-anova-introduction\" title=\"Thumbs up!\" alt=\"thumbs up\"\u003e\u003cimg id=\"thumbs-down\" data-repository=\"dsc-statistical-power-anova-introduction\" title=\"Thumbs down!\" alt=\"thumbs down\"\u003e\n\u003c/div\u003e\n\u003ch5\u003eHave specific feedback? \u003ca href=\"https://github.com/learn-co-curriculum/dsc-statistical-power-anova-introduction/issues/new\"\u003eTell us here!\u003c/a\u003e\n\u003c/h5\u003e\u003c/footer\u003e","frontPage":false},{"exportId":"central-limit-theorem-and-confidence-intervals-recap","title":"Central Limit Theorem and Confidence Intervals - Recap","type":"WikiPage","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-inferential-statistics-section-recap\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-inferential-statistics-section-recap/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eThis short lesson summarizes the topics we covered in this section and why they'll be important to you as a data scientist.\u003c/p\u003e\n\u003ch2\u003eKey Takeaways\u003c/h2\u003e\n\u003cp\u003eThis section was all about building further on your statistics foundations by introducing the Central Limit Theorem and confidence intervals. Some of the key takeaways include:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eThe Central Limit Theorem states that often, independent random variables summed together will converge to a normal distribution as the number of variables increases\u003c/li\u003e\n\u003cli\u003eUsing the Central Limit Theorem, we can work with non-normally distributed data sets as if they were normally distributed\u003c/li\u003e\n\u003cli\u003eThe Standard Error is a measure of spread - it is the standard deviation of samples from the sample mean\u003c/li\u003e\n\u003cli\u003eIf you take repeated samples and compute the 95% confidence interval for a given parameter for each sample, 95% of the intervals would contain the population parameter.\u003c/li\u003e\n\u003cli\u003eThe z-critical value is the number of standard deviations you'd have to go from the mean of the normal distribution to capture the proportion of the data associated with the desired confidence level.\u003c/li\u003e\n\u003cli\u003eIf you don't know the standard deviation for a population, you need to use t-distributions to compute the margin of error for calculating a confidence interval.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cfooter class=\"fis-footer\" style=\"visibility: hidden;\"\u003e\n\u003cdiv class=\"fis-feedback\"\u003e\n\u003ch5\u003eHow do you feel about this lesson?\u003c/h5\u003e\n\u003cimg id=\"thumbs-up\" title=\"Thumbs up!\" alt=\"thumbs up\" data-repository=\"dsc-inferential-statistics-section-recap\"\u003e\u003cimg id=\"thumbs-down\" title=\"Thumbs down!\" alt=\"thumbs down\" data-repository=\"dsc-inferential-statistics-section-recap\"\u003e\n\u003c/div\u003e\n\u003ch5\u003eHave specific feedback? \u003ca href=\"https://github.com/learn-co-curriculum/dsc-inferential-statistics-section-recap/issues/new\"\u003eTell us here!\u003c/a\u003e\n\u003c/h5\u003e\n\u003c/footer\u003e","frontPage":false},{"exportId":"the-multiple-comparisons-problem","title":"The Multiple Comparisons Problem","type":"WikiPage","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-multiple-comparisons-problem\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-multiple-comparisons-problem/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eIn this lesson, you'll learn about the problems that can arise from doing multiple comparisons in a single experiment.\u003c/p\u003e\n\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\n\u003cp\u003eYou will be able to:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eExplain why multiple comparisons increases the likelihood of misleading results \u003c/li\u003e\n\u003cli\u003eExplain the concept of spurious correlation \u003c/li\u003e\n\u003cli\u003eUse corrections to deal with multiple comparison problems \u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch2\u003eWhat is the multiple comparisons problem?\u003c/h2\u003e\n\n\u003cp\u003eObtaining an incredibly low p-value does not guarantee that the null-hypothesis is incorrect. For example, a p-value of 0.001 states that there is still a 1 in 1000 chance that the null hypothesis is true. Yet, as you've seen, p-values alone can be misleading. For example, if you perform repeated experiments, at some point you're apt to stumble upon a small p-value, whether or not the null hypothesis is valid.\u003c/p\u003e\n\n\u003cp\u003eTo restate this, imagine we take 100 scientific studies with a p-value of 0.03. Are all of these conclusions valid? Sadly, probably not. Remember, for any experiment with a p-value of 0.03, there is still a 3% chance that the null-hypothesis is actually true. So collectively, the probability that \u003cstrong\u003eall\u003c/strong\u003e of these null hypotheses are false is actually quite small. You can be fairly confident in each study, but there is also apt to be a false-conclusion drawn somewhere. (In fact, the p-value itself implies that, on average, 3 of these 100 conclusions will be false.)\u003c/p\u003e\n\n\u003cpre\u003e\u003ccode class=\"python\"\u003e0.97**100 # Probability all 100 experiments with p=0.03 are all true \n\u003c/code\u003e\u003c/pre\u003e\n\n\u003cpre\u003e\u003ccode\u003e0.04755250792540563\n\u003c/code\u003e\u003c/pre\u003e\n\n\u003cp\u003eSimilarly, if you are testing multiple metrics simultaneously in an experiment, the chances that one of these will satisfy your alpha threshold increases. A fun similar phenomenon is spurious correlation. If we start comparing a multitude of quantities, we are bound to find some quantities that are highly correlated, whether or not an actual relationship exists. Tyler Vigen set out to find such relationships; here are a few entertaining ones (of many):  \u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-multiple-comparisons-problem/master/images/nicolas_cage_vs_drowning.svg\"\u003e\n\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-multiple-comparisons-problem/master/images/chicken_vs_oil.svg\"\u003e\n\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-multiple-comparisons-problem/master/images/math_phds_vs_uranium.svg\"\u003e\n\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-multiple-comparisons-problem/master/images/spelling_vs_spiders.svg\"\u003e\u003c/p\u003e\n\n\u003cp\u003eAs we can see, although these graphs show that each pair of quantities is strongly correlated, it seems unreasonable to expect that any of them have any causal relationships. Regardless of what the statistics tell us, there is no relationship through which the length of spelling bee word affects the number of people killed by venomous spiders.\u003c/p\u003e\n\n\u003ch2\u003eHow do multiple comparisons increase the chances of finding spurious correlations?\u003c/h2\u003e\n\n\u003cp\u003eSpurious correlation is a \u003cstrong\u003e\u003cem\u003eType 1 Error\u003c/em\u003e\u003c/strong\u003e, meaning that it's a type of \u003cstrong\u003e\u003cem\u003eFalse Positive\u003c/em\u003e\u003c/strong\u003e. We think we've found something important when really there isn't any.  With each comparison we make in an experiment, we try to set a really low p-value to limit our exposure to type 1 errors.  When we only reject the null hypothesis when p \u0026lt; 0.05, for example, we are effectively saying \"I'm only going to accept these results as true if there is less than a 5% chance that I didn't actually find anything important, and my data only looks like this due to randomness\".  However, when we make \u003cstrong\u003e\u003cem\u003emultiple comparisons\u003c/em\u003e\u003c/strong\u003e by checking for many things at once, each of the small risks of a Type 1 error becomes cumulative! \u003c/p\u003e\n\n\u003cp\u003eHere's another easy to way to phrase this -- a p-value threshold of less than 0.05 means that we will only make a Type 1 error 1 in every 20 times. This means that statistically, if we have 20 findings where the p-value is less than 0.05 at the same time, 1 of them is almost guaranteed to be a Type 1 error (False Positive) -- but we have no idea of which one!\u003c/p\u003e\n\n\u003ch2\u003eThe Bonferroni correction\u003c/h2\u003e\n\n\u003cp\u003eBack to the problem of multiple comparisons. Due to the cumulative risk of drawing false conclusions when statistically testing multiple quantities simultaneously, statisticians have devised methods to minimize the chance of type 1 errors. One of these is the \u003cstrong\u003e\u003cem\u003eBonferroni correction\u003c/em\u003e\u003c/strong\u003e.  With the Bonferroni correction, you divide  \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Calpha\"\u003e by the number of comparisons you are making to set a new, adjusted threshold rejecting the null hypothesis.\u003c/p\u003e\n\n\u003cp\u003eFor example, if you desire  \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Calpha%20=%200.05\"\u003e , but are making 10 comparisons simultaneously, the Bonferroni Correction would advise you set our adjusted p-value threshold to  \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cfrac%7B0.05%7D%7B10%7D%20=%200.005\"\u003e !  The stricter p-value threshold helps control for Type 1 errors.  This doesn't mean that you are immune to them -- it just helps reduce the cumulative chance that one occurs. That said, the effective power of these tests is therefore reduced (and in turn type 2 errors are more likely).\u003c/p\u003e\n\n\u003ch2\u003eAdditional Resources\u003c/h2\u003e\n\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"http://tylervigen.com/spurious-correlations\"\u003eTyle Vigen - Spurious correlations\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://www.nationalgeographic.com/science/phenomena/2015/09/11/nick-cage-movies-vs-drownings-and-more-strange-but-spurious-correlations/\"\u003eNick Cage movies vs. drownings, and more strange (but spurious) correlations\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch2\u003eSummary\u003c/h2\u003e\n\n\u003cp\u003eIn this lesson, you learned about the problems that can arise from doing multiple comparisons in a single experiment, as well as some entertaining spurious correlations that exist with real-world data.\u003c/p\u003e\n\u003cfooter class=\"fis-footer\" style=\"visibility: hidden;\"\u003e\u003cdiv class=\"fis-feedback\"\u003e\n\u003ch5\u003eHow do you feel about this lesson?\u003c/h5\u003e\n\u003cimg id=\"thumbs-up\" data-repository=\"dsc-multiple-comparisons-problem\" title=\"Thumbs up!\" alt=\"thumbs up\"\u003e\u003cimg id=\"thumbs-down\" data-repository=\"dsc-multiple-comparisons-problem\" title=\"Thumbs down!\" alt=\"thumbs down\"\u003e\n\u003c/div\u003e\n\u003ch5\u003eHave specific feedback? \u003ca href=\"https://github.com/learn-co-curriculum/dsc-multiple-comparisons-problem/issues/new\"\u003eTell us here!\u003c/a\u003e\n\u003c/h5\u003e\u003c/footer\u003e","frontPage":false},{"exportId":"data-science-processes","title":"Data Science Processes","type":"WikiPage","content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-data-science-processes\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-data-science-processes\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-data-science-processes/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eAs discussed, this section is all about synthesizing your skills in order to work through a full Data Science workflow. In this lesson, you'll take a look at some general outlines for how Data Scientists organize their workflow and conceptualize their process.\u003c/p\u003e\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\u003cp\u003eYou will be able to:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eList the different data science process frameworks\u003c/li\u003e\n\u003cli\u003eCompare and contrast popular data science process frameworks such as CRISP-DM, KDD, OSEMN\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003eWhat is a Data Science Process?\u003c/h2\u003e\n\u003cp\u003eData Science projects are often complex, with many stakeholders, data sources, and goals. Due to this, the Data Science community has created several methodologies for helping organize and structure Data Science Projects. In this lesson, you'll explore three of the most popular methodologies -- \u003cstrong\u003e\u003cem\u003eCRISP-DM\u003c/em\u003e\u003c/strong\u003e, \u003cstrong\u003e\u003cem\u003eKDD\u003c/em\u003e\u003c/strong\u003e, and \u003cstrong\u003e\u003cem\u003eOSEMN\u003c/em\u003e\u003c/strong\u003e, and explore how you can make use of them to keep your projects well-structured and organized.\u003c/p\u003e\n\u003ch2\u003eCRoss-Industry Standard Process for Data Mining (CRISP-DM)\u003c/h2\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-data-science-processes/master/images/new_crisp-dm.png\" width=\"500\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e\u003cem\u003eCRISP-DM\u003c/em\u003e\u003c/strong\u003e is probably the most popular Data Science process in the Data Science world right now. Take a look at the visualization above to get a feel for CRISP-DM. Notice that CRISP-DM is an iterative process!\u003c/p\u003e\n\u003cp\u003eLet's take a look at the individual steps involved in CRISP-DM.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e\u003cem\u003eBusiness Understanding:\u003c/em\u003e\u003c/strong\u003e This stage is all about gathering facts and requirements. Who will be using the model you build? How will they be using it? How will this help the goals of the business or organization overall? Data Science projects are complex, with many moving parts and stakeholders. They're also time intensive to complete or modify. Because of this, it is very important that the Data Science team working on the project has a deep understanding of what the problem is, and how the solution will be used. Consider the fact that many stakeholders involved in the project may not have technical backgrounds, and may not even be from the same organization. Stakeholders from one part of the organization may have wildly different expectations about the project than stakeholders from a different part of the organization -- for instance, the sales team may be under the impression that a recommendation system project is meant to increase sales by recommending upsells to current customers, while the marketing team may be under the impression that the project is meant to help generate new leads by personalizing product recommendations in a marketing email. These are two very different interpretations of a recommendation system project, and it's understandable that both departments would immediately assume that the primary goal of the project is one that helps their organization. As a Data Scientist, it's up to you clarify the requirements and make sure that everyone involved understands what the project is and isn't.\u003c/p\u003e\n\u003cp\u003eDuring this stage, the goal is to get everyone on the same page and to provide clarity on the scope of the project for everyone involved, not just the Data Science team. Generate and answer as many contextual questions as you can about the project.\u003c/p\u003e\n\u003cp\u003eGood questions for this stage include:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eWho are the stakeholders in this project? Who will be directly affected by the creation of this project?\u003c/li\u003e\n\u003cli\u003eWhat business problem(s) will this Data Science project solve for the organization?\u003c/li\u003e\n\u003cli\u003eWhat problems are inside the scope of this project?\u003c/li\u003e\n\u003cli\u003eWhat problems are outside the scope of this project?\u003c/li\u003e\n\u003cli\u003eWhat data sources are available to us?\u003c/li\u003e\n\u003cli\u003eWhat is the expected timeline for this project? Are there hard deadlines (e.g. \"must be live before holiday season shopping\") or is this an ongoing project?\u003c/li\u003e\n\u003cli\u003eDo stakeholders from different parts of the company or organization all have the exact same understanding about what this project is and isn't?\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003e\u003cem\u003eData Understanding:\u003c/em\u003e\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eOnce we have a solid understanding of the business implications for this project, we move on to understanding our data. During this stage, we'll aim to get a solid understanding of the data needed to complete the project. This step includes both understanding where our data is coming from, as well as the information contained within the data.\u003c/p\u003e\n\u003cp\u003eConsider the following questions when working through this stage:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eWhat data is available to us? Where does it live? Do we have the data, or can we scrape/buy/source the data from somewhere else?\u003c/li\u003e\n\u003cli\u003eWho controls the data sources, and what steps are needed to get access to the data?\u003c/li\u003e\n\u003cli\u003eWhat is our target?\u003c/li\u003e\n\u003cli\u003eWhat predictors are available to us?\u003c/li\u003e\n\u003cli\u003eWhat data types are the predictors we'll be working with?\u003c/li\u003e\n\u003cli\u003eWhat is the distribution of our data?\u003c/li\u003e\n\u003cli\u003eHow many observations does our dataset contain? Do we have a lot of data? Only a little?\u003c/li\u003e\n\u003cli\u003eDo we have enough data to build a model? Will we need to use resampling methods?\u003c/li\u003e\n\u003cli\u003eHow do we know the data is correct? How is the data collected? Is there a chance the data could be wrong?\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003e\u003cem\u003eData Preparation:\u003c/em\u003e\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eOnce we have a strong understanding of our data, we can move onto preparing the data for our modeling steps.\u003c/p\u003e\n\u003cp\u003eDuring this stage, we'll want to handle the following issues:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eDetecting and dealing with missing values\u003c/li\u003e\n\u003cli\u003eData type conversions (e.g. numeric data mistakenly encoded as strings)\u003c/li\u003e\n\u003cli\u003eChecking for and removing multicollinearity (correlated predictors)\u003c/li\u003e\n\u003cli\u003eNormalizing our numeric data\u003c/li\u003e\n\u003cli\u003eConverting categorical data to numeric format through one-hot encoding\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003e\u003cem\u003eModeling:\u003c/em\u003e\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eOnce we have clean data, we can begin modeling! Remember, modeling, as with any of these other steps, is an iterative process. During this stage, we'll try to build and tune models to get the highest performance possible on our task.\u003c/p\u003e\n\u003cp\u003eConsider the following questions during the modeling step:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eIs this a classification task? A regression task? Something else?\u003c/li\u003e\n\u003cli\u003eWhat models will we try?\u003c/li\u003e\n\u003cli\u003eHow do we deal with overfitting?\u003c/li\u003e\n\u003cli\u003eDo we need to use regularization or not?\u003c/li\u003e\n\u003cli\u003eWhat sort of validation strategy will we be using to check that our model works well on unseen data?\u003c/li\u003e\n\u003cli\u003eWhat loss functions will we use?\u003c/li\u003e\n\u003cli\u003eWhat threshold of performance do we consider as successful?\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003e\u003cem\u003eEvaluation:\u003c/em\u003e\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eDuring this step, we'll evaluate the results of our modeling efforts. Does our model solve the problems that we outlined all the way back during step 1? Why or why not? Often times, evaluating the results of our modeling step will raise new questions, or will cause us to consider changing our approach to the problem. Notice from the CRISP-DM diagram above, that the \"Evaluation\" step is unique in that it points to both \u003cem\u003eBusiness Understanding\u003c/em\u003e and \u003cem\u003eDeployment\u003c/em\u003e. As we mentioned before, Data Science is an iterative process -- that means that given the new information our model has provided, we'll often want to start over with another iteration, armed with our newfound knowledge! Perhaps the results of our model showed us something important that we had originally failed to consider the goal of the project or the scope. Perhaps we learned that the model can't be successful without more data, or different data. Perhaps our evaluation shows us that we should reconsider our approach to cleaning and structuring the data, or how we frame the project as a whole (e.g. realizing we should treat the problem as a classification rather than a regression task). In any of these cases, it is totally encouraged to revisit the earlier steps.\u003c/p\u003e\n\u003cp\u003eOf course, if the results are satisfactory, then we instead move onto deployment!\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e\u003cem\u003eDeployment:\u003c/em\u003e\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eDuring this stage, we'll focus on moving our model into production and automating as much as possible. Everything before this serves as a proof-of-concept or an investigation. If the project has proved successful, then you'll work with stakeholders to determine the best way to implement models and insights. For example, you might set up an automated ETL (Extract-Transform-Load) pipelines of raw data in order to feed into a database and reformat it so that it is ready for modeling. During the deployment step, you'll actively work to determine the best course of action for getting the results of your project into the wild, and you'll often be involved with building everything needed to put the software into production.\u003c/p\u003e\n\u003cp\u003eThis is one of the most rewarding steps of the entire Data Science process -- getting to see your work go live!\u003c/p\u003e\n\u003ch2\u003eKnowledge Discovery in Databases\u003c/h2\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-data-science-processes/master/images/new_kdd.png\" width=\"800\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e\u003cem\u003eKnowledge Discovery in Databases\u003c/em\u003e\u003c/strong\u003e, or \u003cstrong\u003e\u003cem\u003eKDD\u003c/em\u003e\u003c/strong\u003e is considered the oldest Data Science process. The creation of this process is credited to Gregory Piatetsky-Shapiro, who also runs the ever-popular Data Science blog, \u003ca href=\"https://www.kdnuggets.com/\"\u003ekdnuggets\u003c/a\u003e. If you're interested, read the original white paper on KDD, which can be found \u003ca href=\"https://www.kdnuggets.com/gpspubs/aimag-kdd-overview-1992.pdf\"\u003ehere\u003c/a\u003e!\u003c/p\u003e\n\u003cp\u003eThe KDD process is quite similar to the CRISP-DM process. The diagram above illustrates every step of the KDD process, as well as the expected output at each stage.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e\u003cem\u003eSelection\u003c/em\u003e\u003c/strong\u003e:\u003c/p\u003e\n\u003cp\u003eDuring this stage, you'll focus on selecting your problem, and the data that will help you answer it. This stage works much like the first stage of CRISP-DM -- you begin by focusing on developing an understanding of the domain the problem resides in (e.g. marketing, finance, increasing customer sales, etc), the previous work done in this domain, and the goals of the stakeholders involved with the process.\u003c/p\u003e\n\u003cp\u003eOnce you've developed a strong understanding of the goals and the domain, you'll work to establish where your data is coming from, and which data will be useful to you. Organizations and companies usually have a ton of data, and only some of it will be relevant to the problem you're trying to solve. During this stage, you'll focus on examining the data sources available to you and gathering the data that you deem useful for the project.\u003c/p\u003e\n\u003cp\u003eThe output of this stage is the dataset you'll be using for the Data Science project.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e\u003cem\u003ePreprocessing\u003c/em\u003e\u003c/strong\u003e:\u003c/p\u003e\n\u003cp\u003eThe preprocessing stage is pretty straightforward -- the goal of this stage is to \"clean\" the data by preprocessing it. For text data, this may include things like tokenization. You'll also identify and deal with issues like outliers and/or missing data in this stage.\u003c/p\u003e\n\u003cp\u003eIn practice, this stage often blurs with the \u003cem\u003eTransformation\u003c/em\u003e stage.\u003c/p\u003e\n\u003cp\u003eThe output of this stage is preprocessed data that is more \"clean\" than it was at the start of this stage -- although the dataset is not quite ready for modeling yet.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e\u003cem\u003eTransformation\u003c/em\u003e\u003c/strong\u003e:\u003c/p\u003e\n\u003cp\u003eDuring this stage, you'll take your preprocessed data and transform it in a way that makes it more ideal for modeling. This may include steps like feature engineering and dimensionality reduction. At this stage, you'll also deal with things like checking for and removing multicollinearity from the dataset. Categorical data should also be converted to numeric format through one-hot encoding during this step.\u003c/p\u003e\n\u003cp\u003eThe output of this stage is a dataset that is now ready for modeling. All null values and outliers are removed, categorical data has been converted to a format that a model can work with, and the dataset is generally ready for experimentation with modeling.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e\u003cem\u003eData Mining\u003c/em\u003e\u003c/strong\u003e:\u003c/p\u003e\n\u003cp\u003eThe Data Mining stage refers to using different modeling techniques to try and build a model that solves the problem we're after -- often, this is a classification or regression task. During this stage, you'll also define your parameters for given models, as well as your overall criteria for measuring the performance of a model.\u003c/p\u003e\n\u003cp\u003eYou may be wondering what Data Mining is, and how it relates to Data Science. In practice, it's just an older term that essentially means the same thing as Data Science. Dr. Piatetsky-Shapiro defines Data Mining as \"the non-trivial extraction of implicit, previously unknown and potentially useful information from data.\" Making of things such as Machine Learning algorithms to find insights in large datasets that aren't immediately obvious without these algorithms is at the heart of the concept of Data Mining, just as it is in Data Science. In a pragmatic sense, this is why the terms Data Mining and Data Science are typically used interchangeably, although the term Data Mining is considered an older term that isn't used as often nowadays.\u003c/p\u003e\n\u003cp\u003eThe output of this stage results from a fit to the data for the problem we're trying to solve.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e\u003cem\u003eInterpretation/Evaluation\u003c/em\u003e\u003c/strong\u003e:\u003c/p\u003e\n\u003cp\u003eDuring this final stage of KDD, we focus on interpreting the \"patterns\" discovered in the previous step to help us make generalizations or predictions that help us answer our original question. During this stage, you'll consolidate everything you've learned to present it to stakeholders for guiding future actions. Your output may be a presentation that you use to communicate to non-technical managers or executives (never discount the importance of knowing PowerPoint as a Data Scientist!). Your conclusions for a project may range from \"this approach didn't work\" or \"we need more data about {X}\" to \"this is ready for production, let's build it!\".\u003c/p\u003e\n\u003ch2\u003eOSEMN\u003c/h2\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-data-science-processes/master/images/new_osemn.png\" width=\"800\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://www.kdnuggets.com/2018/02/data-science-command-line-book-exploring-data.html\" target=\"_blank\"\u003eAdapted from: KDNuggets\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eThis brings us to the Data Science process we'll be using during this section -- OSEMN (sometimes referred as OSEMiN, and pronounced \"OH-sum\", rhymes with \"possum\"). This is the most straightforward of the Data Science processes discussed so far. Note that during this process, just like the others, the stages often blur together. It is completely acceptable (and often a best practice!) to float back and forth between stages as you learn new things about your problem, dataset, requirements, etc. It's quite common to get to the modeling step and realize that you need to scrub your data a bit more or engineer a different feature and jump back to the \"Scrub\" stage, or go all the way back to the \"Obtain\" stage when you realize your current data isn't sufficient to solve this problem. As with any of these frameworks, OSEMN is meant to be treated more like a set of guidelines for structuring your project than set-in-stone steps that cannot be violated.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e\u003cem\u003eObtain\u003c/em\u003e\u003c/strong\u003e:\u003c/p\u003e\n\u003cp\u003eAs with CRISP-DM and KDD, this step involves understanding stakeholder requirements, gathering information on the problem, and finally, sourcing data that we think will be necessary for solving this problem.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e\u003cem\u003eScrub\u003c/em\u003e\u003c/strong\u003e:\u003c/p\u003e\n\u003cp\u003eDuring this stage, we'll focus on preprocessing our data. Important steps such as identifying and removing null values, dealing with outliers, normalizing data, and feature engineering/feature selection are handled around this stage. The line with this stage really blurs with the \u003cem\u003eExplore\u003c/em\u003e stage, as it is common to only realize that certain columns require cleaning or preprocessing as a result of the visualizations and explorations done during Step 3.\u003c/p\u003e\n\u003cp\u003eNote that although technically, categorical data should be one-hot encoded during this step, in practice, it's usually done after data exploration. This is because it is much less time-consuming to visualize and explore a few columns containing categorical data than it is to explore many different dummy columns that have been one-hot encoded.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e\u003cem\u003eExplore\u003c/em\u003e\u003c/strong\u003e:\u003c/p\u003e\n\u003cp\u003eThis step focuses on getting to know the dataset you're working with. As mentioned above, this step tends to blend with the \u003cem\u003eScrub\u003c/em\u003e step mentioned above. During this step, you'll create visualizations to really get a feel for your dataset. You'll focus on things such as understanding the distribution of different columns, checking for multicollinearity, and other tasks like that. If your project is a classification task, you may check the balance of the different classes in your dataset. If your problem is a regression task, you may check that the dataset meets the assumptions necessary for a regression task.\u003c/p\u003e\n\u003cp\u003eAt the end of this step, you should have a dataset ready for modeling that you've thoroughly explored and are extremely familiar with.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e\u003cem\u003eModel\u003c/em\u003e\u003c/strong\u003e:\u003c/p\u003e\n\u003cp\u003eThis step, as with the last two frameworks, is also pretty self-explanatory. It consists of building and tuning models using all the tools you have in your data science toolbox. In practice, this often means defining a threshold for success, selecting machine learning algorithms to test on the project, and tuning the ones that show promise to try and increase your results. As with the other stages, it is both common and accepted to realize something, jump back to a previous stage like \u003cem\u003eScrub\u003c/em\u003e or \u003cem\u003eExplore\u003c/em\u003e, and make some changes to see how it affects the model.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e\u003cem\u003eInterpret\u003c/em\u003e\u003c/strong\u003e:\u003c/p\u003e\n\u003cp\u003eDuring this step, you'll interpret the results of your model(s), and communicate results to stakeholders. As with the other frameworks, communication is incredibly important! During this stage, you may come to realize that further investigation is needed, or more data. That's totally fine -- figure out what's needed, go get it, and start the process over! If your results are satisfactory to all stakeholders involved, you may also go from this stage right into putting your model into production and automating processes necessary to support it.\u003c/p\u003e\n\u003ch2\u003eA Note On Communicating Results\u003c/h2\u003e\n\u003cp\u003eRegardless of the quality of your results, it's very important that you be aware of the business requirements and stakeholder expectations at all times! Generally, no matter which of the above processes you use, you'll communicate your results in a two-pronged manner:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eA short, high-level presentation covering your question, process, and results meant for non-technical audiences\u003c/li\u003e\n\u003cli\u003eA detailed Jupyter Notebook demonstrating your entire process meant for technical audiences\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eIn general, you can see why Data Scientists love Jupyter Notebooks! It is very easy to format results in a reproducible, easy-to-understand way. Although a detailed Jupyter Notebook may seem like the more involved of the two deliverables listed above, the high-level presentation is often the hardest! Just remember -- even if the project took you/your team over a year and utilized the most cutting-edge machine learning techniques available, you still need to be able to communicate your results in about 5 slides (using graphics, not words, whenever possible!), in a 5 minute presentation in a way that someone that can't write code can still understand and be convinced by!\u003c/p\u003e\n\u003ch2\u003eConclusion\u003c/h2\u003e\n\u003cp\u003eIn this lesson, you learned about the different data science process frameworks including CRISP-DM, KDD, and OSEMN. You also learned that the data science process is iterative and that a typical data science project involves many different stakeholders who may not have a technical background. As such, it's important to recognize that data scientists must be able to communicate their findings in a non-technical way.\u003c/p\u003e","frontPage":false},{"exportId":"introduction-to-sets","title":"Introduction to Sets","type":"WikiPage","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-intro-to-sets\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-intro-to-sets/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eYou have definitely heard of sets before. In this section, however, you will learn about the formal definition of sets, which will serve as a foundation for everything related to probability and combinatorics!\u003c/p\u003e\n\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\n\u003cp\u003eYou will be able to: \u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eDefine a set in the context of probability theory\u003c/li\u003e\n\u003cli\u003eDefine a universal set and subsets\u003c/li\u003e\n\u003cli\u003eDescribe the process of making unions, intersections, and complements\u003c/li\u003e\n\u003cli\u003eUse Venn Diagrams to visually demonstrate set operations\u003c/li\u003e\n\u003cli\u003eDescribe the inclusion-exclusion principle\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch2\u003eWhat is a Set?\u003c/h2\u003e\n\n\u003cp\u003eIn probability theory, a set is defined as a \u003cem\u003ewell-defined collection of objects\u003c/em\u003e. \u003c/p\u003e\n\n\u003cp\u003eMathematically, you can denote a set by \u003cimg src=\"https://render.githubusercontent.com/render/math?math=S\"\u003e. If an element \u003cimg src=\"https://render.githubusercontent.com/render/math?math=x\"\u003ebelongs to a set \u003cimg src=\"https://render.githubusercontent.com/render/math?math=S\"\u003e, then you'd write \u003cimg src=\"https://render.githubusercontent.com/render/math?math=x%5Cin%20S\"\u003e. On the other hand, if \u003cimg src=\"https://render.githubusercontent.com/render/math?math=x\"\u003edoes not belong to a set \u003cimg src=\"https://render.githubusercontent.com/render/math?math=S\"\u003e, then you'd write \u003cimg src=\"https://render.githubusercontent.com/render/math?math=x%5Cnotin%20S\"\u003e.\u003c/p\u003e\n\n\u003cp\u003eExample: If \u003cimg src=\"https://render.githubusercontent.com/render/math?math=S\"\u003eis defined as the set of even numbers, then:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003e\u003cp\u003eIf \u003cimg src=\"https://render.githubusercontent.com/render/math?math=x%20=%202\"\u003e, \u003cimg src=\"https://render.githubusercontent.com/render/math?math=x%5Cin%20S\"\u003ebecause \u003cimg src=\"https://render.githubusercontent.com/render/math?math=x\"\u003eis an even number.\u003c/p\u003e\u003c/li\u003e\n\u003cli\u003e\u003cp\u003eIf \u003cimg src=\"https://render.githubusercontent.com/render/math?math=x%20=%209\"\u003e, \u003cimg src=\"https://render.githubusercontent.com/render/math?math=x%5Cnotin%20S\"\u003ebecause \u003cimg src=\"https://render.githubusercontent.com/render/math?math=x\"\u003eis not an even number.\u003c/p\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch2\u003eSubsets\u003c/h2\u003e\n\n\u003cp\u003eSet \u003cimg src=\"https://render.githubusercontent.com/render/math?math=T\"\u003eis a subset of set \u003cimg src=\"https://render.githubusercontent.com/render/math?math=S\"\u003eif \u003cem\u003eevery element\u003c/em\u003e in set \u003cimg src=\"https://render.githubusercontent.com/render/math?math=T\"\u003eis also in set \u003cimg src=\"https://render.githubusercontent.com/render/math?math=S\"\u003e. The mathematical notation for a subset is \u003cimg src=\"https://render.githubusercontent.com/render/math?math=T%20%5Csubseteq%20S\"\u003e. \u003c/p\u003e\n\n\u003cp\u003eTypically, you'll be more interested in \u003cem\u003eproper subsets\u003c/em\u003e. All proper subsets are subsets. The only difference between subsets and proper subsets is that a subset can technically be the entire set. In other words, if A = {1,2,3} and B = {1,2,3} A is subset of B. If C = {1,2} then C is both a subset and proper subset of A. C is also a subset and proper subset of B. The mathematical notation for proper subsets is : \u003cimg src=\"https://render.githubusercontent.com/render/math?math=C%20%5Csubset%20A\"\u003e\u003c/p\u003e\n\n\u003cp\u003e\u003cstrong\u003eExample\u003c/strong\u003e: If S is the set of even numbers, set \u003cimg src=\"https://render.githubusercontent.com/render/math?math=T%20=%5C%7B2,%206,%2022%5C%7D\"\u003eis a proper subset of \u003cimg src=\"https://render.githubusercontent.com/render/math?math=S\"\u003e. Formally, you can write this as \u003cimg src=\"https://render.githubusercontent.com/render/math?math=T%20%5Csubset%20S\"\u003e. \u003cimg src=\"https://render.githubusercontent.com/render/math?math=T%20%5Csubseteq%20S\"\u003eis also correct in this case!\u003c/p\u003e\n\n\u003ch2\u003eUniversal Sets\u003c/h2\u003e\n\n\u003cp\u003eThe collection of all possible outcomes in a certain context or universe is called the \u003cstrong\u003euniversal set\u003c/strong\u003e.\nA universal set is often denoted by \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5COmega\"\u003e.\u003c/p\u003e\n\n\u003cp\u003eExample of a universal set: All the possible outcomes when rolling a dice.\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5COmega%20=%5C%7B1,2,3,4,5,6%5C%7D\"\u003e\u003c/p\u003e\n\n\u003cp\u003eRemember that a universal set is not necessarily all the possible things that have ever existed. Typically, a universal set is just all the possible elements within certain bounds, e.g., the set of all countries in the world, the set of all the animal species in the Bronx Zoo, etc.\u003c/p\u003e\n\n\u003cp\u003eA universal set can have an infinite number of elements, for example, the set of all real numbers!\u003c/p\u003e\n\n\u003ch2\u003eElementary Set Operations\u003c/h2\u003e\n\n\u003cp\u003eNext, let's talk about set operations. Imagine you have two sets of numbers, say the first 4 multiples of 3 in set \u003cimg src=\"https://render.githubusercontent.com/render/math?math=S\"\u003e:\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=S%20=%5C%7B3,6,9,12%5C%7D\"\u003e\u003c/p\u003e\n\n\u003cp\u003eand the first 4 multiples of 2 in set \u003cimg src=\"https://render.githubusercontent.com/render/math?math=T\"\u003e:\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=T%20=%5C%7B2,4,6,8%5C%7D\"\u003e.\u003c/p\u003e\n\n\u003ch3\u003ea) Union of Two Sets\u003c/h3\u003e\n\n\u003cp\u003eThe union of two sets \u003cimg src=\"https://render.githubusercontent.com/render/math?math=S\"\u003eand \u003cimg src=\"https://render.githubusercontent.com/render/math?math=T\"\u003eis the set of elements of either S or T, or in both.  \u003c/p\u003e\n\n\u003cp\u003eApplied to our example, the union of \u003cimg src=\"https://render.githubusercontent.com/render/math?math=S\"\u003eand \u003cimg src=\"https://render.githubusercontent.com/render/math?math=T\"\u003eis given by the elements \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5C%7B2,3,4,6,8,9,12%5C%7D\"\u003e. \u003c/p\u003e\n\n\u003cp\u003eIn mathematical terms, the union of \u003cimg src=\"https://render.githubusercontent.com/render/math?math=S\"\u003eand \u003cimg src=\"https://render.githubusercontent.com/render/math?math=T\"\u003eis denoted as \u003cimg src=\"https://render.githubusercontent.com/render/math?math=S%20%5Ccup%20T\"\u003e.\u003c/p\u003e\n\n\u003cp\u003eA popular way to represent sets and their relationships is through Venn Diagrams, (\u003ca href=\"https://en.wikipedia.org/wiki/Venn_diagram\"\u003ehttps://en.wikipedia.org/wiki/Venn_diagram\u003c/a\u003e), see picture below!\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-intro-to-sets/master/images/new_union.png\" width=\"250\"\u003e\u003c/p\u003e\n\n\u003ch3\u003eb) Intersection of Two Sets\u003c/h3\u003e\n\n\u003cp\u003eThe intersection of two sets \u003cimg src=\"https://render.githubusercontent.com/render/math?math=S\"\u003eand \u003cimg src=\"https://render.githubusercontent.com/render/math?math=T\"\u003eis the set that contains all elements of \u003cimg src=\"https://render.githubusercontent.com/render/math?math=S\"\u003ethat also belong to \u003cimg src=\"https://render.githubusercontent.com/render/math?math=T\"\u003e. \u003c/p\u003e\n\n\u003cp\u003eApplied to our example, the intersection of \u003cimg src=\"https://render.githubusercontent.com/render/math?math=S\"\u003eand \u003cimg src=\"https://render.githubusercontent.com/render/math?math=T\"\u003eis given by {6}, so it contains the elements that are multiples of both 2 AND 3.\u003c/p\u003e\n\n\u003cp\u003eIn mathematical terms, the intersection of \u003cimg src=\"https://render.githubusercontent.com/render/math?math=S\"\u003eand \u003cimg src=\"https://render.githubusercontent.com/render/math?math=T\"\u003eis denoted as \u003cimg src=\"https://render.githubusercontent.com/render/math?math=S%20%5Ccap%20T\"\u003e.\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-intro-to-sets/master/images/new_intersection.png\" width=\"250\"\u003e\u003c/p\u003e\n\n\u003ch3\u003ec) Relative Complement or the Difference\u003c/h3\u003e\n\n\u003cp\u003eIf you have S and T, the relative complement of S contains all the elements of T that are NOT in S. This is also sometimes referred to as the \u003cem\u003edifference\u003c/em\u003e. The difference is denoted by \u003cimg src=\"https://render.githubusercontent.com/render/math?math=T%5Cbackslash%20S\"\u003eor \u003cimg src=\"https://render.githubusercontent.com/render/math?math=T-S\"\u003e.\u003c/p\u003e\n\n\u003cp\u003eIn this case, the relative complement of S (or \u003cimg src=\"https://render.githubusercontent.com/render/math?math=T%5Cbackslash%20S\"\u003e) is \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5C%7B2,4,8%5C%7D\"\u003e. The relative complement of T (or \u003cimg src=\"https://render.githubusercontent.com/render/math?math=S%5Cbackslash%20T\"\u003e) is \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5C%7B3,9,12%5C%7D\"\u003e.\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-intro-to-sets/master/images/new_rel_comp.png\" width=\"250\"\u003e\u003c/p\u003e\n\n\u003ch3\u003ed) Absolute Complement\u003c/h3\u003e\n\n\u003cp\u003eThere is another definition of the complement when considering universal sets \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5COmega\"\u003eas well. In this context, we're talking about the \u003cem\u003eabsolute complement\u003c/em\u003e.\u003c/p\u003e\n\n\u003cp\u003eThe absolute complement of \u003cimg src=\"https://render.githubusercontent.com/render/math?math=S\"\u003e, with respect to the Universal set \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5COmega\"\u003e, is the collection of the objects in \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5COmega\"\u003ethat don't belong to \u003cimg src=\"https://render.githubusercontent.com/render/math?math=S\"\u003e.\u003c/p\u003e\n\n\u003cp\u003eNote how the definition of \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5COmega\"\u003eis very important here. Imagine a set \u003cimg src=\"https://render.githubusercontent.com/render/math?math=S=%5C%7B%5Ctext%7Belephant,%20alligator,%20tiger,%20bear%7D%5C%7D\"\u003e. The complement of this set will depend on how the universal set is defined: Is \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5COmega\"\u003eequal to \u003cem\u003ethe animals in the Bronx Zoo\u003c/em\u003e, or \u003cem\u003ethe 20 most deadly animals in the world\u003c/em\u003e?\u003c/p\u003e\n\n\u003cp\u003eMathematically, the absolute complement of \u003cimg src=\"https://render.githubusercontent.com/render/math?math=S\"\u003eis denoted as \u003cimg src=\"https://render.githubusercontent.com/render/math?math=S'\"\u003eor \u003cimg src=\"https://render.githubusercontent.com/render/math?math=S%5Ec\"\u003e.\u003c/p\u003e\n\n\u003cp\u003eLet's reconsider \u003cimg src=\"https://render.githubusercontent.com/render/math?math=S\"\u003eand \u003cimg src=\"https://render.githubusercontent.com/render/math?math=T\"\u003eas defined previously.\u003c/p\u003e\n\n\u003cp\u003eLet's define \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5COmega\"\u003e, the universal set (denoted by the box around the two Venn diagrams), as the set that contains the multiples of both 2 and 3 until 20. Then the elements of \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5COmega\"\u003eare \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5C%7B2,3,4,6,8,9,10,12,14,15,16,18,20%5C%7D\"\u003e. \u003c/p\u003e\n\n\u003cp\u003eThe absolute complement of \u003cimg src=\"https://render.githubusercontent.com/render/math?math=S\"\u003e(so, \u003cimg src=\"https://render.githubusercontent.com/render/math?math=S'\"\u003eor \u003cimg src=\"https://render.githubusercontent.com/render/math?math=S%5Ec\"\u003e) is then given by \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5C%7B2,4,8,10,14,15,16,18,20%5C%7D\"\u003e.\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-intro-to-sets/master/images/new_abs_comp.png\" width=\"250\"\u003e\u003c/p\u003e\n\n\u003ch2\u003eInclusion-Exclusion Principle\u003c/h2\u003e\n\n\u003cp\u003eNote that if you want to know how many elements are in set \u003cimg src=\"https://render.githubusercontent.com/render/math?math=S\"\u003eversus \u003cimg src=\"https://render.githubusercontent.com/render/math?math=T\"\u003e, you can't simply sum up the elements, because they have elements in common.\u003c/p\u003e\n\n\u003cp\u003eIn combinational mathematics, the inclusion-exclusion principle is a counting technique that solves this problem.\u003c/p\u003e\n\n\u003cp\u003eWhen having two sets, the method for counting the number of elements in the union of two finite sets is given by:\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cmid%20S%20%5Ccup%20T%5Cmid%20=%5Cmid%20S%5Cmid%20%2b%5Cmid%20T%5Cmid%20-%5Cmid%20S%20%5Ccap%20T%5Cmid\"\u003e,\u003c/p\u003e\n\n\u003cp\u003ewhere the horizontal lines denote the \u003cem\u003ecardinality\u003c/em\u003e of a set, which is the number of elements in the set, considering a set with a finite number of elements. \u003c/p\u003e\n\n\u003cp\u003eThe formula expresses the fact that the sum of the sizes of the two sets may be too large since some elements may be counted twice. For the double-counted elements, one is subtracted again.\u003c/p\u003e\n\n\u003cp\u003eThis formula can be extended to three sets, four sets, etc. For example, imagine you have a third set \u003cimg src=\"https://render.githubusercontent.com/render/math?math=R\"\u003e. The number of elements in the union of three finite sets is given by:\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cmid%20S%20%5Ccup%20T%20%5Ccup%20R%5Cmid%20=%5Cmid%20S%5Cmid%20%2b%5Cmid%20T%5Cmid%20%2b%5Cmid%20R%5Cmid%20-%5Cmid%20S%20%5Ccap%20T%5Cmid%20%20-%5Cmid%20S%20%5Ccap%20R%5Cmid%20-%5Cmid%20R%20%5Ccap%20T%5Cmid%20%20%2b%5Cmid%20S%20%5Ccap%20T%20%5Ccap%20R%5Cmid\"\u003e\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-intro-to-sets/master/images/new_venn_diagram.png\" width=\"350\"\u003e\u003c/p\u003e\n\n\u003ch2\u003eEmpty Sets\u003c/h2\u003e\n\n\u003cp\u003eWhen there are no elements in a certain set, this set is \u003cstrong\u003eempty\u003c/strong\u003e, denoted by \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cemptyset\"\u003eor simply \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5C%7B%5C%7D\"\u003e\u003c/p\u003e\n\n\u003ch2\u003eSets in Python\u003c/h2\u003e\n\n\u003cp\u003eSome things to bear in mind when working with sets in Python:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eSets are unordered collections of unique elements.\u003c/li\u003e\n\u003cli\u003eSets are iterable.\u003c/li\u003e\n\u003cli\u003eSets are collections of lower level python objects (just like lists or dictionaries).\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003eDocumentation for sets in Python can be found here: \u003ca href=\"https://docs.python.org/3.6/library/stdtypes.html#set-types-set-frozenset\"\u003eSets\u003c/a\u003e\u003c/p\u003e\n\n\u003ch2\u003eSets and Set Operations: A Summative Example\u003c/h2\u003e\n\n\u003cp\u003eTo put this all together, let's consider an example with restaurants:\u003c/p\u003e\n\n\u003cp\u003eThink about a \u003cem\u003eset A\u003c/em\u003e with all the restaurants that serve Italian food.\nNext, there is a \u003cem\u003eset B\u003c/em\u003e with all the restaurants that serve burgers.\u003c/p\u003e\n\n\u003cp\u003eThe \u003cstrong\u003eunion\u003c/strong\u003e of these sets, \u003cem\u003eset C\u003c/em\u003e, contains the set of restaurants that either serve Italian food, burgers or both.\u003c/p\u003e\n\n\u003cp\u003eYou could say that the \u003cstrong\u003euniversal set\u003c/strong\u003e here, \u003cem\u003eset U\u003c/em\u003e, contains all the restaurants in the world (with any type of food). Then \u003cem\u003eset C\u003c/em\u003e is a \u003cstrong\u003esubset\u003c/strong\u003e of \u003cem\u003eset U\u003c/em\u003e. \u003c/p\u003e\n\n\u003cp\u003eThe \u003cstrong\u003eintersection\u003c/strong\u003e of \u003cem\u003eA\u003c/em\u003e and \u003cem\u003eB\u003c/em\u003e contains the restaurants that serve \u003cem\u003eboth\u003c/em\u003e Italian food and burgers.\u003c/p\u003e\n\n\u003cp\u003eThe \u003cstrong\u003erelative complement\u003c/strong\u003e of \u003cem\u003eset A\u003c/em\u003e contains the restaurants that serve burgers but \u003cem\u003enot\u003c/em\u003e Italian food.\u003c/p\u003e\n\n\u003cp\u003eThe \u003cstrong\u003eabsolute complement\u003c/strong\u003e of \u003cem\u003eset A\u003c/em\u003e contains the restaurants that serve \u003cem\u003eany food\u003c/em\u003e but \u003cem\u003eno\u003c/em\u003e Italian food.\u003c/p\u003e\n\n\u003ch2\u003eSummary\u003c/h2\u003e\n\n\u003cp\u003eIn this section, you learned about sets, subsets, and universal sets. Next, you were introduced to some elementary set operations such as unions, intersections, and complements. After that, all this information was tied together through the inclusion-exclusion principle. Next, you saw how sets translate into Python. You'll start exploring this in further detail in the next lab!\u003c/p\u003e\n\u003cfooter class=\"fis-footer\" style=\"visibility: hidden;\"\u003e\u003cdiv class=\"fis-feedback\"\u003e\n\u003ch5\u003eHow do you feel about this lesson?\u003c/h5\u003e\n\u003cimg id=\"thumbs-up\" data-repository=\"dsc-intro-to-sets\" title=\"Thumbs up!\" alt=\"thumbs up\"\u003e\u003cimg id=\"thumbs-down\" data-repository=\"dsc-intro-to-sets\" title=\"Thumbs down!\" alt=\"thumbs down\"\u003e\n\u003c/div\u003e\n\u003ch5\u003eHave specific feedback? \u003ca href=\"https://github.com/learn-co-curriculum/dsc-intro-to-sets/issues/new\"\u003eTell us here!\u003c/a\u003e\n\u003c/h5\u003e\u003c/footer\u003e","frontPage":false},{"exportId":"welchs-t-test","title":"Welch's T-Test","type":"WikiPage","content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-welchs-ttest\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-welchs-ttest\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-welchs-ttest/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eThus far, you've seen the traditional Student's t-test for hypothesis testing between two sample means. Recall that z-tests are also appropriate for statistics, such as the mean, which can be assumed to be normally distributed. However, when sample sizes are low (n_observations \u0026lt; 30), the t-test is more appropriate, as the t-distribution has heavier tails. Even with this modification, remember that there are still several assumptions to the model. Most notably, traditional t-tests assume that sample sizes and sample variances between the two groups are equal. When these assumptions are not met, Welch's t-test is generally a more reliable test.\u003c/p\u003e\n\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\n\u003cp\u003eYou will be able to: \u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eList the conditions needed to require a Welch's t-test \u003c/li\u003e\n\u003cli\u003eCalculate the degrees of freedom for a Welch's t-test \u003c/li\u003e\n\u003cli\u003eCalculate p-values using Welch's  t-test \u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch2\u003eT-test review\u003c/h2\u003e\n\n\u003cp\u003eRecall that t-tests are a useful method for determining whether the mean of two small samples indicate different underlying population parameters. The reasoning behind this begins with the use of z-tests to calculate the likelihood of sampling a particular value from a normal distribution. Furthermore, by the central limit theorem, the mean of a sample is a normally distributed variable centered around the actual underlying population mean. That said, t-tests are more appropriate for small samples (n_observations \u0026lt; 30), due to disproportionate tails. Finally, recall that the t-distribution actually converges to a normal distribution as the degrees of freedom continues to increase.  \u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-welchs-ttest/master/images/new_t_vs_norm_dist.png\"\u003e\u003c/p\u003e\n\n\u003cblockquote\u003e\n\u003cp\u003eA normal distribution vs. t-distributions with varying degrees of freedom. Note how the t-distribution approaches the normal distribution as the degrees of freedom increases. Recall that when performing a two-sample t-test, assuming that sample variances are equal, the degrees of freedom equals the total number of observations in the samples minus two.\u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003ch2\u003eWelch's t-test\u003c/h2\u003e\n\n\u003cp\u003eJust as Student's t-test is a useful adaptation of the normal distribution which can lead to better likelihood estimates under certain conditions, the Welch's t-test is a further adaptation that accounts for additional perturbations in the underlying assumptions of the model. Specifically, the Student's t-test assumes that the samples are of equal size and equal variance. When these assumptions are not met, then Welch's t-test provides a more accurate p-value.\u003c/p\u003e\n\n\u003cp\u003eHere is how you calculate it: \u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5CLarge%20t%20=%20%5Cfrac%7B%5Cbar%7BX_1%7D-%5Cbar%7BX_2%7D%7D%7B%5Csqrt%7B%5Cfrac%7Bs_1%5E2%7D%7BN_1%7D%20%2b%20%5Cfrac%7Bs_2%5E2%7D%7BN_2%7D%7D%7D%20=%20%5Cfrac%7B%5Cbar%7BX_1%7D-%5Cbar%7BX_2%7D%7D%7B%5Csqrt%7Bse_1%5E2%2bse_2%5E2%7D%7D\"\u003e \u003c/p\u003e\n\n\u003cp\u003ewhere\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003e \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cbar%7BX_i%7D\"\u003e - mean of sample i\u003c/li\u003e\n\u003cli\u003e \u003cimg src=\"https://render.githubusercontent.com/render/math?math=s_i%5E2\"\u003e - variance of sample i\u003c/li\u003e\n\u003cli\u003e \u003cimg src=\"https://render.githubusercontent.com/render/math?math=N_i\"\u003e - sample size of sample i\u003cbr\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003eThe modification is related to the \u003cstrong\u003edegrees of freedom\u003c/strong\u003e in the t-test, which tends to increase the test power for samples with unequal variance. When two groups have equal sample sizes and variances, Welch’s t-test tends to give the same result as the Student’s t-test. However, when sample sizes and variances are unequal, Student’s t-test is quite unreliable, whereas Welch’s tends perform better.\u003c/p\u003e\n\n\u003ch2\u003eCalculate the degrees of freedom\u003c/h2\u003e\n\n\u003cp\u003eOnce the t-score has been calculated for the experiment using the above formula, you then must calculate the degrees of freedom for the t-distribution. Under the two-sample Student's t-test, this is simply the total number of observations in the samples size minus two, but given that the sample sizes may vary using the Welch's t-test, the calculation is a bit more complex:  \u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5CLarge%20v%20%5Capprox%20%5Cfrac%7B%5Cleft(%20%5Cfrac%7Bs_1%5E2%7D%7BN_1%7D%20%2b%20%5Cfrac%7Bs_2%5E2%7D%7BN_2%7D%5Cright)%5E2%7D%7B%5Cfrac%7Bs_1%5E4%7D%7BN_1%5E2v_1%7D%20%2b%20%5Cfrac%7Bs_2%5E4%7D%7BN_2%5E2v_2%7D%7D\"\u003e \u003c/p\u003e\n\n\u003ch2\u003eCalculate p-values\u003c/h2\u003e\n\n\u003cp\u003eFinally, as with the Student's t-test (or a z-test for that matter), you convert the calculated score into a p-value in order to confirm or reject the null-hypothesis of your statistical experiment. For example, you might be using a one-sided t-test to determine whether a new drug had a positive effect on patient outcomes. The p-value for the experiment is equivalent to the area under the t-distribution with the degrees of freedom, as calculated above, and the corresponding t-score.\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-welchs-ttest/master/images/new_AUC.png\" width=\"500\"\u003e\u003c/p\u003e\n\n\u003cp\u003eThe easiest method for determining said p-values is to use the \u003ccode\u003e.cdf()\u003c/code\u003e method from \u003ccode\u003escipy.stats\u003c/code\u003e to find the complement and subtracting this from 1.\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-welchs-ttest/master/images/new_CdfAndPdf.png\" width=\"500\"\u003e\u003c/p\u003e\n\n\u003cp\u003eHere's the relevant code snippet:\u003c/p\u003e\n\n\u003cpre\u003e\u003ccode class=\"python\"\u003eimport scipy.stats as stats\n\np = 1 - stats.t.cdf(t, df)\n\u003c/code\u003e\u003c/pre\u003e\n\n\u003ch2\u003eSummary\u003c/h2\u003e\n\n\u003cp\u003eThis lesson briefly introduced you to another statistical test for comparing the means of two samples: Welch's t-test. Remember that when your samples are not of equal size or do not have equal variances, it is a more appropriate statistical test than the Student's t-test!\u003c/p\u003e","frontPage":false},{"exportId":"central-limit-theorem-and-confidence-intervals-introduction","title":"Central Limit Theorem and Confidence Intervals - Introduction","type":"WikiPage","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-inferential-statistics-section-intro\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-inferential-statistics-section-intro/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eIn this section, you'll be introduced to inferential statistics. You'll learn about sampling, the central limit theorem, and the T-distribution.\u003c/p\u003e\n\u003ch2\u003eDistributions and Sampling\u003c/h2\u003e\n\u003cp\u003eIn this section, we're returning to statistics to broaden and deepen our understanding of distributions and sampling.\u003c/p\u003e\n\u003ch3\u003eSampling\u003c/h3\u003e\n\u003cp\u003eWe'll start by providing an introduction to the idea of \u003cstrong\u003e\u003cem\u003eSampling\u003c/em\u003e\u003c/strong\u003e - selecting a subset of a population to survey. We'll then start to introduce some statistics related to sampling by explaining and showing how to calculate the standard error.\u003c/p\u003e\n\u003ch3\u003eThe Central Limit Theorem\u003c/h3\u003e\n\u003cp\u003eOnce we understand a bit about sampling, we'll explore how we can use it by digging deep into one of the coolest and most important concepts in inferential statistics--the \u003cstrong\u003e\u003cem\u003eCentral Limit Theorem\u003c/em\u003e\u003c/strong\u003e! We'll start by learning about how the Central Limit Theorem works, and explore how we can use it in a way that allows us to treat non-normal distributions as normal distributions, and provides a way for us to estimate parameters about a population.\u003c/p\u003e\n\u003ch3\u003eThe T-Distribution\u003c/h3\u003e\n\u003cp\u003eFinally, we'll end this section by learning about how we can use the \u003cstrong\u003e\u003cem\u003eT-Distribution\u003c/em\u003e\u003c/strong\u003e for dealing with samples that are smaller, and that have an unknown standard deviation. We'll explore how the T-Distribution works, learn about \u003cem\u003edegrees of freedom\u003c/em\u003e, and then see how we can calculate confidence intervals using our newfound knowledge of the T-Distribution.\u003c/p\u003e\n\u003ch2\u003eSummary\u003c/h2\u003e\n\u003cp\u003eWhile some of this material may seem a little dry, a deep understanding of and intuition for distributions and sampling will be important in your career as a data scientist. This knowledge will help you avoid making mistakes in your EDA (exploratory data analysis), feature selection, and modeling work, which could lead to faulty predictions from your models.\u003c/p\u003e\n\u003cfooter class=\"fis-footer\" style=\"visibility: hidden;\"\u003e\n\u003cdiv class=\"fis-feedback\"\u003e\n\u003ch5\u003eHow do you feel about this lesson?\u003c/h5\u003e\n\u003cimg id=\"thumbs-up\" title=\"Thumbs up!\" alt=\"thumbs up\" data-repository=\"dsc-inferential-statistics-section-intro\"\u003e\u003cimg id=\"thumbs-down\" title=\"Thumbs down!\" alt=\"thumbs down\" data-repository=\"dsc-inferential-statistics-section-intro\"\u003e\n\u003c/div\u003e\n\u003ch5\u003eHave specific feedback? \u003ca href=\"https://github.com/learn-co-curriculum/dsc-inferential-statistics-section-intro/issues/new\"\u003eTell us here!\u003c/a\u003e\n\u003c/h5\u003e\n\u003c/footer\u003e","frontPage":false},{"exportId":"ab-testing-introduction","title":"AB Testing - Introduction","type":"WikiPage","content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-ab-testing-introduction\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-ab-testing-introduction\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-ab-testing-introduction/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eIn this section, you'll get to develop your skills regarding AB testing. Before diving in, take a minute to note some key points which you should keep in mind when completing the various labs and conducting your own hypothesis tests in practice.\u003c/p\u003e\n\u003ch2\u003eExperimental Design\u003c/h2\u003e\n\u003cp\u003eYou've seen that a lot goes into the proper design of statistical tests. You've learned about Goodheart's law as well as the multiple comparisons problem. Additionally, you've also seen that a p-value by itself is prone to misinterpretation if not presented with other relevant design parameters such as effect size, sample size, and \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Calpha\"\u003e . With that, here are three overarching considerations to keep in mind.\u003c/p\u003e\n\u003ch3\u003eWell Formulated Questions\u003c/h3\u003e\n\u003cp\u003eA well-formulated question is essential to a good statistical experiment. This includes careful thought of unintended consequences, as you saw in the discussion of Goodheart's law. Additionally, the question should also be specific and measurable.\u003c/p\u003e\n\u003ch3\u003eChoosing Appropriate Parameters\u003c/h3\u003e\n\u003cp\u003eIt cannot be stressed enough how important the relationship between \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Calpha\"\u003e , power, sample size and effect size is. While larger sample sizes provide more powerful tests, one should also realize that tiny effects can produce significant p-values with large samples. While this may be interesting, such small practical differences might have little to no applicable value. Furthermore, avoiding pitfalls such as the multiple comparisons problem is also important. Recall that if you perform multiple t-tests, The probability of encountering a type I error will continue to increase with additional tests. (Each test will still have the corresponding alpha value set, but collectively, the chance that a false positive type I error exists in your conclusions increases.)\u003c/p\u003e\n\u003ch3\u003ePreprocessing, Data Anomalies and Outliers\u003c/h3\u003e\n\u003cp\u003eOn the other end of problem formulation is formatting the data to actually answer said question. You'll encounter this most explicitly in the final lab of this section. There, you'll have to transform your data into an appropriate format before conducting the statistical test. Furthermore, it's important to note how idiosyncrasies in your data can impact results. For example, monumental outliers can drastically impact the outcome of statistical tests. Whether or not to remove these data points can be a source of contention and will vary upon the circumstance. Similarly, it should go without saying that erroneous data or faulty data will clearly degrade statistical tests. All in all, it's always important to get familiar with the structure of the data and the context of the question being asked before diving into the statistics themselves.\u003c/p\u003e\n\u003ch2\u003eSummary\u003c/h2\u003e\n\u003cp\u003eTime to have at it! Dive in and start practicing some hypothesis testing!\u003c/p\u003e","frontPage":false},{"exportId":"bayesians-vs-frequentists","title":"Bayesians vs Frequentists","type":"WikiPage","content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-bayesians-vs-frequentists\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-bayesians-vs-frequentists\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-bayesians-vs-frequentists/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eUp until now, all of the statistical theory you have encountered has been through the lens of a Frequentist. This has included discussions of z-tests, t-tests, p-values, and ANOVA; all are from the Frequentist perspective. In this lesson, you'll start to explore an alternative perspective donned by Bayesians.\u003c/p\u003e\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\u003cp\u003eYou will be able to:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eCompare the Bayesian v. Frequentist statistical frameworks\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003ePhilosophical Interpretations\u003c/h2\u003e\n\u003cp\u003eA natural place to start when outlining the differences between Bayesians and Frequentists is to talk of their interpretation of probability itself. For Frequentists, the probability of an event is the limit of the rate of occurrences of the event if the same scenario including context and assumptions were repeated ad infinitum. In contrast, Bayesians interpret probability as the level of confidence, or belief, in a particular event occurring. In many ways, this makes a more natural interpretation for rare events that cannot possibly reoccur in the same context and circumstances.\u003c/p\u003e\n\u003ch2\u003ePractical Implications\u003c/h2\u003e\n\u003cp\u003eThe practical implications of Bayesians versus Frequentists rest upon making assumptions about unknown quantities. In the Bayesian framework, you make assumptions about unknown variables which you are attempting to estimate. For example, you might assume that the number of individuals who will buy a product can be represented by a binomial variable with parameter \u003cimg src=\"https://render.githubusercontent.com/render/math?math=p\"\u003e . In contrast, the Frequentist perspective does not allow embedding of prior beliefs such as this into statistical experiments and analyses.\u003c/p\u003e\n\u003ch2\u003eSummary\u003c/h2\u003e\n\u003cp\u003eIn this lesson, you started to learn about the differences in Bayesian versus Frequentist statistical perspectives. Keep in mind that there are not always rigid lines between these modes of thought. Nonetheless, the discussion is a worthwhile one in considering which approach you wish to take in your research and analysis.\u003c/p\u003e","frontPage":false},{"exportId":"coefficient-of-determination","title":"Coefficient of Determination","type":"WikiPage","content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-coefficient-of-determination\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-coefficient-of-determination\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-coefficient-of-determination/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eFor linear regression analysis, as we saw earlier, the straight line does not \u003cstrong\u003efully\u003c/strong\u003e describe the relationship between variables and there is always some error. In general, you'll want to determine a \"goodness of fit\"-measure of the fitted line. In this lesson, you'll learn about the \"R-Squared\"( \u003cimg src=\"https://render.githubusercontent.com/render/math?math=R%5E2\"\u003e ) measure, also known as the Coefficient of Determination.\u003c/p\u003e\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\u003cp\u003eYou will be able to:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eCalculate the coefficient of determination using self-constructed functions\u003c/li\u003e\n\u003cli\u003eUse the coefficient of determination to determine model performance\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003eR-Squared\u003c/h2\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003eThe \u003cimg src=\"https://render.githubusercontent.com/render/math?math=R%5E2\"\u003e or Coefficient of determination is a statistical measure that is used to assess the goodness of fit of a regression model\u003c/strong\u003e\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eHere is how it works.\u003c/p\u003e\n\u003cp\u003eR-Squared uses a so-called \"baseline\" model which is a very simple, naive model. This baseline model does not make use of any independent variables to predict the value of dependent variable Y. Instead, it uses the \u003cstrong\u003emean\u003c/strong\u003e of the observed responses of the dependent variable \u003cimg src=\"https://render.githubusercontent.com/render/math?math=y\"\u003e and always predicts this mean as the value of \u003cimg src=\"https://render.githubusercontent.com/render/math?math=y\"\u003e for any value of \u003cimg src=\"https://render.githubusercontent.com/render/math?math=x\"\u003e . In the image below, this model is given by the straight orange line.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-coefficient-of-determination/master/images/linreg_rsq.png\" width=\"600\"\u003e\u003c/p\u003e\n\u003cp\u003eYou can see that, in this plot, the baseline model always predicts the mean of \u003cimg src=\"https://render.githubusercontent.com/render/math?math=y\"\u003e \u003cstrong\u003eirrespective\u003c/strong\u003e of the value of the \u003cimg src=\"https://render.githubusercontent.com/render/math?math=x\"\u003e . The gray line, however, is our fitted regression line which makes use of \u003cimg src=\"https://render.githubusercontent.com/render/math?math=x\"\u003e values to predict the values of \u003cimg src=\"https://render.githubusercontent.com/render/math?math=y\"\u003e . Looking at the plot above, R-Squared simply asks the question:\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003eIs our fitted regression line better than our baseline model?\u003c/strong\u003e\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eAny regression model that we fit is compared to this baseline model to understand its \u003cstrong\u003egoodness of fit\u003c/strong\u003e. Simply put, R-Squared just explains how good your model is when compared to the baseline model. That's about it.\u003c/p\u003e\n\u003ch3\u003eGreat, so how do you calculate R-Squared?\u003c/h3\u003e\n\u003cp\u003eThe mathematical formula to calculate R-Squared for a linear regression line is in terms of \u003cstrong\u003esquared errors\u003c/strong\u003e for the fitted model and the baseline model. It's calculated as :\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Clarge%20R%5E2%20=%201-%20%5Cdfrac%7BSS_%7BRES%7D%7D%7BSS_%7BTOT%7D%7D%20=%201%20-%20%5Cdfrac%7B%5Csum_i(y_i%20-%20%5Chat%20y_i)%5E2%7D%7B%5Csum_i(y_i%20-%20%5Coverline%20y_i)%5E2%7D\"\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=SS_%7BRES%7D\"\u003e (also called RSS) is the \u003cstrong\u003eResidual\u003c/strong\u003e sum of squared errors of our regression model, also known as \u003cimg src=\"https://render.githubusercontent.com/render/math?math=SSE\"\u003e (Sum of Squared Errors). \u003cimg src=\"https://render.githubusercontent.com/render/math?math=SS_%7BRES%7D\"\u003e is the squared difference between \u003cimg src=\"https://render.githubusercontent.com/render/math?math=y\"\u003e and \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Chat%20y\"\u003e . For the one highlighted observation in our graph above, the \u003cimg src=\"https://render.githubusercontent.com/render/math?math=SS_%7BRES%7D\"\u003e is denoted by the red arrow. This part of the error is not explained by our model.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=SS_%7BTOT%7D\"\u003e (also called TSS) is the \u003cstrong\u003eTotal\u003c/strong\u003e sum of squared error. \u003cimg src=\"https://render.githubusercontent.com/render/math?math=SS_%7BTOT%7D\"\u003e is the squared difference between \u003cimg src=\"https://render.githubusercontent.com/render/math?math=y\"\u003e and \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Coverline%20y\"\u003e . For the one highlighted observation in our graph above, the \u003cimg src=\"https://render.githubusercontent.com/render/math?math=SS_%7BTOT%7D\"\u003e is denoted by the orange arrow.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eLooking at this, you'll understand that you can interpret R-Squared as \"1 - the proportion of the variance \u003cem\u003enot\u003c/em\u003e explained by the model\", which means as much as \"the variation explained by the model\". As a result, you'll want to maximize the R-Squared.\u003c/p\u003e\n\u003cp\u003eFor completion,\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cimg src=\"https://render.githubusercontent.com/render/math?math=SS_%7BEXP%7D\"\u003e (also called ESS) is the \u003cstrong\u003eExplained\u003c/strong\u003e sum of squared error. \u003cimg src=\"https://render.githubusercontent.com/render/math?math=SS_%7BEXP%7D\"\u003e is the squared difference between \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Chat%20y\"\u003e and \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Coverline%20y\"\u003e . For the one highlighted observation in our graph above, the \u003cimg src=\"https://render.githubusercontent.com/render/math?math=SS_%7BEXP%7D\"\u003e is denoted by the gray arrow.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003eLet's interpret the outcome of \u003cimg src=\"https://render.githubusercontent.com/render/math?math=R%5E2\"\u003e\n\u003c/h3\u003e\n\u003cp\u003eOur worst possible regression model could be the baseline model itself. In that case, the RSS is equal to TSS looking at the graph above. Then your R_squared value is 0. Look at the plot below, where you notice that \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Chat%20y\"\u003e is simply a straight line.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-coefficient-of-determination/master/images/rs5.png\" width=\"500\"\u003e\u003c/p\u003e\n\u003cp\u003eDue to this particular relationship between \u003cimg src=\"https://render.githubusercontent.com/render/math?math=x\"\u003e and \u003cimg src=\"https://render.githubusercontent.com/render/math?math=y\"\u003e , the regression line \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Chat%20y\"\u003e is the same as the mean line for \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Coverline%20y\"\u003e . The R-Squared for this model is 0. It's clear that a straight line is probably not the right fit for this data.\u003c/p\u003e\n\u003cp\u003eOn the other extreme, the best model could also be one that fits all the data points perfectly. Because the unexplained part of the variation is 0, R-Squared is 1–0, so 1 in this case, which indicates a perfect model. Below is an example of this (know that this will rarely happen with real world data).\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-coefficient-of-determination/master/images/rs6.png\" width=\"500\"\u003e\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003eR-Squared can take a value between 0 and 1 where values closer to 0 represent a poor fit and values closer to 1 represent an (almost) perfect fit\u003c/strong\u003e\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch3\u003ePhrasing R-Squared values\u003c/h3\u003e\n\u003cp\u003eAn R-squared value of say 0.85 can be described conceptually as:\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003e\u003cem\u003e85% of the variations in dependent variable \u003cimg src=\"https://render.githubusercontent.com/render/math?math=y\"\u003e are explained by the independent variable in our model.\u003c/em\u003e\u003c/strong\u003e\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch2\u003eSummary\u003c/h2\u003e\n\u003cp\u003eIn this lesson, you looked at the R-Squared, or the Coefficient of Determination to evaluate the goodness of fit for a regression line. You saw how R-Squared is calculated by comparing a given model to a baseline model and learned that it must be a value between 0 and 1. In the next lab, you'll move on to calculating R-Squared in Python.\u003c/p\u003e","frontPage":false},{"exportId":"combinatorics-and-probability-introduction","title":"Combinatorics and Probability - Introduction","type":"WikiPage","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-probability-section-intro\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-probability-section-intro/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eIn this section, you'll learn about the foundation of statistics: probability!\u003c/p\u003e\n\u003ch2\u003eCombinatorics and Probability\u003c/h2\u003e\n\u003cp\u003eIn this section, we'll take a little time to \"get our math on\" with some basic probability. You're going to start with some basic set theory and look at how to operate on related sets using Python.\u003c/p\u003e\n\u003cp\u003eFrom there, we're going to use what we learned about sets to start to learn and apply some of the basic rules of probability.\u003c/p\u003e\n\u003ch3\u003eFactorials and Permutations\u003c/h3\u003e\n\u003cp\u003eNext we're going to dig into factorials, and how they can be used to calculate various permutations.\u003c/p\u003e\n\u003ch3\u003eCombinations\u003c/h3\u003e\n\u003cp\u003eWe're then going to examine the difference between permutations and combinations. We'll get some practice calculating combinations for everything from drawing letters from a bag to creating soccer teams for a tournament!\u003c/p\u003e\n\u003ch3\u003eConditional Probability\u003c/h3\u003e\n\u003cp\u003eWe start the section off with an introduction to conditional probability. We look at the difference between dependent and independent events, look at how to calculate dependent probabilities, and then introduce some key theorems related to conditional probabilities: the product rule, the chain rule, and Bayes theorem.\u003c/p\u003e\n\u003ch3\u003ePartitioning and the Law of Total Probabilities\u003c/h3\u003e\n\u003cp\u003eFrom there, we introduce the concept of partitioning a sample space, explain the law of total probabilities, and then introduce the idea of conditional independence.\u003c/p\u003e\n\u003ch2\u003eSummary\u003c/h2\u003e\n\u003cp\u003eThis is a mathematics heavy section. Some of the discrete problems you'll solve may not seem to be particularly relevant to machine learning, but we deliberately introduce them so that you have the foundations required to make thoughtful choices as we introduce you to a range of new machine learning models in the later sections.\u003c/p\u003e\n\u003cfooter class=\"fis-footer\" style=\"visibility: hidden;\"\u003e\n\u003cdiv class=\"fis-feedback\"\u003e\n\u003ch5\u003eHow do you feel about this lesson?\u003c/h5\u003e\n\u003cimg id=\"thumbs-up\" title=\"Thumbs up!\" alt=\"thumbs up\" data-repository=\"dsc-probability-section-intro\"\u003e\u003cimg id=\"thumbs-down\" title=\"Thumbs down!\" alt=\"thumbs down\" data-repository=\"dsc-probability-section-intro\"\u003e\n\u003c/div\u003e\n\u003ch5\u003eHave specific feedback? \u003ca href=\"https://github.com/learn-co-curriculum/dsc-probability-section-intro/issues/new\"\u003eTell us here!\u003c/a\u003e\n\u003c/h5\u003e\n\u003c/footer\u003e","frontPage":false},{"exportId":"data-science-toolbox-review","title":"Data Science Toolbox Review","type":"WikiPage","content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-data-science-toolbox-review-v2-1\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-data-science-toolbox-review-v2-1\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-data-science-toolbox-review-v2-1/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eIn this lesson, you'll get a chance to review some of the many skills you've learned to date! You've developed a range of skills from general programming skills to databases, data analysis, and statistics! Hopefully, you're excited to pull all of these pieces together and conduct a full Data Science investigation from data extraction to modeling! With that, here's a quick review of some of the most pertinent skills you'll be using along the way!\u003c/p\u003e\n\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\n\u003cp\u003eYou will be able to:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003ePerform SQL select statements, including joins\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch2\u003eSQL\u003c/h2\u003e\n\n\u003cp\u003eYou've seen how SQL, Structured Query Language, is a wonderful method for organizing data. Having data in a database allows multiple users to access, query, and update shared data simultaneously. As such, many companies store their data in databases and SQL is undoubtedly the most popular implementation. You saw that most statements began with the \u003ccode\u003eSELECT\u003c/code\u003e clause, and from there you specify which columns you wish to select, the table from which you wish to select them, and appropriate filters using the \u003ccode\u003eWHERE\u003c/code\u003e clause. You can also aggregate data with the \u003ccode\u003eGROUP BY\u003c/code\u003e clause, and apply further filters to those roll-ups using the \u003ccode\u003eHAVING\u003c/code\u003e clause. Furthermore, you can select data from multiple tables using a \u003ccode\u003eJOIN\u003c/code\u003e, so long as the two tables have a common key(s) which you specify with the \u003ccode\u003eUSING\u003c/code\u003e clause, if the column names are identical, or more verbosely, you can specify how to join the tables with the \u003ccode\u003eON\u003c/code\u003e clause. \u003c/p\u003e\n\n\u003cp\u003eFor example, here's the schema for the mock customer relationship database that you've seen before: \u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-data-science-toolbox-review-v2-1/master/images/Database-Schema.png\" width=\"550\"\u003e\u003c/p\u003e\n\n\u003cp\u003eIf you wanted to select some aggregate statistics regarding employees and customers on a per office basis for cities with at least 2 offices, you could write a query like this:  \u003c/p\u003e\n\n\u003cpre\u003e\u003ccode class=\"SQL\"\u003eSELECT officeCode,\n       city,\n       COUNT(employeeNumber) AS num_employees,\n       COUNT(customerNumber) AS num_customers,\n       sum(amount) AS total_payments_received\n       FROM offices\n       JOIN employees\n       USING(officeCode)\n       JOIN customers\n       ON employeeNumber = salesRepEmployeeNumber\n       JOIN payments\n       USING(customerNumber)\n       GROUP BY 1, 2\n       WHERE city IN (SELECT city FROM offices GROUP BY 1 HAVING COUNT(*)\u0026gt;1);\n\u003c/code\u003e\u003c/pre\u003e\n\n\u003cp\u003eLook at that; you even got to see the use of a subquery once again!\u003c/p\u003e\n\n\u003ch2\u003eData Exploration and Visualization\u003c/h2\u003e\n\n\u003cp\u003eOnce you've loaded in some data from a SQL database or elsewhere, it's generally time to start investigating and cleaning up that data. Remember that Pandas comes with many useful methods for quickly exploring your dataset. For example, you may want to initially check the datatypes of your dataset and how many null values exist for each of the various features. This is incredibly easy using the \u003ccode\u003e.info()\u003c/code\u003e method. Similarly, if you want to generate aggregate statistics, you can simply call the \u003ccode\u003e.describe()\u003c/code\u003e method on the DataFrame. If you want to quickly explore the distribution and pairwise correlations between your features, then using the \u003ccode\u003epd.plotting.scatter_matrix()\u003c/code\u003e function will quickly display a grid of histograms and scatter plots for the various features; the diagonal entries are the distributions for each of the variables, while any other cell [i,j] is a scatter plot of feature i vs feature j.  \u003c/p\u003e\n\n\u003cp\u003eRecall that when initially exploring your data, you are both looking for insights and anomalies. Typically, you want to get familiar with the data, what it represents, and how it is represented. This will often give you further ideas about how to mine the data for structure and answer questions regarding the application.\u003c/p\u003e\n\n\u003ch2\u003eRegression\u003c/h2\u003e\n\n\u003cp\u003eAfter acquiring and exploring your data (including cleaning it up), you'll then go on to model said data using the regression techniques you learned about earlier. With this, recall that there are four main assumptions underlying a linear regression model.\u003c/p\u003e\n\n\u003ch3\u003e1. Linearity\u003c/h3\u003e\n\n\u003cp\u003eWith linear models, the target variable is being modeled as a linear combination of the independent variables. As such, there should be a linear relationship between the target variable and the various features being used. If the rate of change between the target variable and one of the features is non-linear and displays other characteristics such as an exponential acceleration, then prior transformations of the data are necessary before applying a regression model. \u003c/p\u003e\n\n\u003ch3\u003e2. Normality\u003c/h3\u003e\n\n\u003cp\u003eWith linear models, the errors (residuals) from the model are assumed to be normally distributed. A good heuristic to initially check for this is to use a Q-Q plot. \u003c/p\u003e\n\n\u003ch3\u003e3. Homoscedasticity\u003c/h3\u003e\n\n\u003cp\u003eAlong with the assumption of normal distribution, error terms should also not be correlated with the target variable or other features within the model. If errors indeed appear to be random and there are no discernible trends, then the errors are said to be homoscedastic. Looking at a simple plot of residuals against the target variable or other feature is generally sufficient to gauge this.\u003c/p\u003e\n\n\u003ch3\u003e4. Independence\u003c/h3\u003e\n\n\u003cp\u003eFinally, regression models assume that the various independent features feeding into the model are independent. You'll take a further look at this in this section and investigate how to check for multicollinearity. Multicollinearity is when a variable can be predicted with substantial accuracy by a separate set of features. Previously, you've examined multicollinearity in the context of the \"dummy variable trap\" and the two variable case. It's unwise to include two features in a regression model that are highly correlated. Similarly, in a multivariate case, having a set of features that can effectively predict another independent feature can be problematic. Such phenomenon will not reduce the overall accuracy of the model, but will \u003cstrong\u003eseverely impede interpretation as coefficient weights of the model\u003c/strong\u003e become unstable so it is difficult or impossible to determine which features are most influential.\u003c/p\u003e\n\n\u003ch2\u003eSummary\u003c/h2\u003e\n\n\u003cp\u003eIn this lesson, you took a quick review of SQL, data exploration, and regression models. If you are looking for a more substantial review, feel free to turn back to some of the previous material. With that, it's time to take a look at some general frameworks for Data Science workflows!\u003c/p\u003e","frontPage":false},{"exportId":"combinations","title":"Combinations","type":"WikiPage","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-combinations\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-combinations/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eIn the previous section, you learned about how to apply permutations. Permutations come in handy when we want to know how many ways we can order sets. Now, what if order is not important? That's where \u003cem\u003ecombinations\u003c/em\u003e come in.\u003c/p\u003e\n\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\n\u003cp\u003eYou will be able to: \u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eDescribe how combinations are used when order is not important\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch2\u003eWhy combinations?\u003c/h2\u003e\n\n\u003cp\u003eIn some settings, the order of the selection is not important.\u003c/p\u003e\n\n\u003cp\u003eLet's go back to our example of a coverband creating a setlist. Imagine that the band is playing 3 songs out of their 8 song repertoire. How many ways can they select songs, assuming that the \u003cstrong\u003eorder of the chosen songs is not important\u003c/strong\u003e? Here, we just want to know \u003cem\u003ewhich\u003c/em\u003e three songs they play, and not which song goes first, second and last.\u003c/p\u003e\n\n\u003cp\u003eYou can use a backward rationale here. You know that when order \u003cem\u003edid\u003c/em\u003e matter, our answer was  \u003cimg src=\"https://render.githubusercontent.com/render/math?math=8%20*%207%20*%206\"\u003e . When having three elements, there are 6 possible orders (ABC, ACB, CAB, CBA, BAC, BCA), so the answer can be obtained by dividing our previous answer by 6. \u003c/p\u003e\n\n\u003cp\u003eThis type of problem can be solved by using \u003cem\u003ecombinations\u003c/em\u003e.\nIn general, combinations answer the question: \"How many ways can we create a subset  \u003cimg src=\"https://render.githubusercontent.com/render/math?math=k\"\u003e out of  \u003cimg src=\"https://render.githubusercontent.com/render/math?math=n\"\u003e objects?\". The subset is not ordered. \u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cdisplaystyle%5Cbinom%7Bn%7D%7Bk%7D%20=%20%5Cdfrac%7BP_%7Bk%7D%5E%7Bn%7D%7D%7Bk!%7D=%5Cdfrac%7B%20%5Cdfrac%7Bn!%7D%7B(n-k)!%7D%7D%7Bk!%7D%20=%20%5Cdfrac%7Bn!%7D%7B(n-k)!k!%7D\"\u003e \u003c/p\u003e\n\n\u003cp\u003eApplied to our example, this means that there are \u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cdfrac%7B8!%7D%7B(8-3)!3!%7D%20=%20%5Cdfrac%7B8!%7D%7B(8-3)!3!%7D%20=%5Cdfrac%7B%208*7*6%7D%7B6%7D%20=%2056\"\u003e \u003c/p\u003e\n\n\u003cp\u003eso there are 56 ways to choose 3 songs out of an 8 song repertoire.\u003c/p\u003e\n\n\u003ch2\u003eSummary\u003c/h2\u003e\n\n\u003cp\u003eIn this section, you learned what combinations are and how to use them. Let's put this knowledge into practice!\u003c/p\u003e\n\u003cfooter class=\"fis-footer\" style=\"visibility: hidden;\"\u003e\u003cdiv class=\"fis-feedback\"\u003e\n\u003ch5\u003eHow do you feel about this lesson?\u003c/h5\u003e\n\u003cimg id=\"thumbs-up\" data-repository=\"dsc-combinations\" title=\"Thumbs up!\" alt=\"thumbs up\"\u003e\u003cimg id=\"thumbs-down\" data-repository=\"dsc-combinations\" title=\"Thumbs down!\" alt=\"thumbs down\"\u003e\n\u003c/div\u003e\n\u003ch5\u003eHave specific feedback? \u003ca href=\"https://github.com/learn-co-curriculum/dsc-combinations/issues/new\"\u003eTell us here!\u003c/a\u003e\n\u003c/h5\u003e\u003c/footer\u003e","frontPage":false},{"exportId":"what-is-probability","title":"What is Probability?","type":"WikiPage","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-probability-introduction\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-probability-introduction/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eAs an aspiring data scientist, it's important to know the foundations of probability and combinatorics, as these areas form the backbone of many concepts in data science. In the following lessons and labs, you'll get a gentle introduction to several concepts that are related to probability, such as sets, combinations, and permutations.\u003c/p\u003e\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\u003cp\u003eYou will be able to:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eDefine probability\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003eWhat is probability, and how does it relate to data science?\u003c/h2\u003e\n\u003cp\u003eProbability is the chance that a certain event will happen, in other words, how \"likely\" it is that some event will happen.\u003c/p\u003e\n\u003cp\u003eAs data science often uses \u003cem\u003estatistical inference\u003c/em\u003e to analyze or predict certain events or trends, knowing probability and its applications is important because statistical inference uses probability distributions of the data. Although it might take a little more time for you to understand just how important the foundations of probability are for data science, by the end of the first part of the probability section, you'll be able to answer questions like:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eHow likely is it to end up with heads when flipping a coin once? (the answer here is 50% - not very surprising)\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eHow likely is it to end up with exactly 2 x heads and 3 x tails when flipping a coin 5 times?\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eHow likely is it to throw tails first, then heads, then tails, then heads, then tails when flipping a coin 5 times?\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eIf you throw 5 dice, what is the probability of throwing a \u003ca href=\"http://grail.sourceforge.net/demo/yahtzee/rules.html\"\u003e\"full house\"\u003c/a\u003e?\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eWhat is the probability of drawing 2 consecutive aces from a standard deck of cards?\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003eSummary\u003c/h2\u003e\n\u003cp\u003eNow, let's dive deeper into the understanding of sets. Getting these concepts will make calculating your first probabilities much easier!\u003c/p\u003e\n\u003cfooter class=\"fis-footer\" style=\"visibility: hidden;\"\u003e\n\u003cdiv class=\"fis-feedback\"\u003e\n\u003ch5\u003eHow do you feel about this lesson?\u003c/h5\u003e\n\u003cimg id=\"thumbs-up\" title=\"Thumbs up!\" alt=\"thumbs up\" data-repository=\"dsc-probability-introduction\"\u003e\u003cimg id=\"thumbs-down\" title=\"Thumbs down!\" alt=\"thumbs down\" data-repository=\"dsc-probability-introduction\"\u003e\n\u003c/div\u003e\n\u003ch5\u003eHave specific feedback? \u003ca href=\"https://github.com/learn-co-curriculum/dsc-probability-introduction/issues/new\"\u003eTell us here!\u003c/a\u003e\n\u003c/h5\u003e\n\u003c/footer\u003e","frontPage":false},{"exportId":"a-complete-data-science-project-using-multiple-regression-introduction","title":"A Complete Data Science Project Using Multiple Regression - Introduction","type":"WikiPage","content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-full-ds-regression-intro\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-full-ds-regression-intro\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-full-ds-regression-intro/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eIn this section, you'll get a chance to synthesize your skills and work through the entire Data Science workflow. To start, you'll extract appropriate data from a SQL database. From there, you'll continue exploring and cleaning your data, modeling the data, and conducting statistical analyses!\u003c/p\u003e\n\u003ch2\u003eData Science Processes\u003c/h2\u003e\n\u003cp\u003eYou'll take a look at three general frameworks for conducting Data Science processes using the skills you've learned thus far:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cstrong\u003eCR\u003c/strong\u003eoss-\u003cstrong\u003eI\u003c/strong\u003endustry \u003cstrong\u003eS\u003c/strong\u003etandard \u003cstrong\u003eP\u003c/strong\u003erocess for \u003cstrong\u003eD\u003c/strong\u003eata \u003cstrong\u003eM\u003c/strong\u003eining - \u003cstrong\u003eCRISP-DM\u003c/strong\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cstrong\u003eK\u003c/strong\u003enowledge \u003cstrong\u003eD\u003c/strong\u003eiscovery in \u003cstrong\u003eD\u003c/strong\u003eatabases - \u003cstrong\u003eKDD\u003c/strong\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cstrong\u003eO\u003c/strong\u003ebtain \u003cstrong\u003eS\u003c/strong\u003ecrub \u003cstrong\u003eE\u003c/strong\u003explore \u003cstrong\u003eM\u003c/strong\u003eodel i\u003cstrong\u003eN\u003c/strong\u003eterpret - \u003cstrong\u003eOSEMN\u003c/strong\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cblockquote\u003e\n\u003cp\u003eNote: OSEMN is pronounced \"OH-sum\" and rhymes with \"possum\"\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eFrom there, the lessons follow a similar structure:\u003c/p\u003e\n\u003ch2\u003eObtaining Data\u003c/h2\u003e\n\u003cp\u003eYou'll review SQL and practice importing data from a relational database using the ETL (Extract, Transform and Load) process.\u003c/p\u003e\n\u003ch2\u003eScrubbing Data\u003c/h2\u003e\n\u003cp\u003eFrom there, you'll practice cleaning data:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eCasting columns to the appropriate data types\u003c/li\u003e\n\u003cli\u003eIdentifying and dealing with null values appropriately\u003c/li\u003e\n\u003cli\u003eRemoving columns that aren't required for modeling\u003c/li\u003e\n\u003cli\u003eChecking for and dealing with multicollinearity\u003c/li\u003e\n\u003cli\u003eNormalizing the data\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003eExploring Data\u003c/h2\u003e\n\u003cp\u003eOnce you've the cleaned data, you'll then do some further EDA (Exploratory Data Analysis) to check out the distributions of the various columns, examine the descriptive statistics for the dataset, and to create some initial visualizations to better understand the dataset.\u003c/p\u003e\n\u003ch2\u003eModeling Data\u003c/h2\u003e\n\u003cp\u003eFinally, you'll create a definitive model. This will include fitting an initial regression model, and then conducting statistical analyses of the results. You'll take a look at the p-values of the various features and perform some feature selection. You'll test for regression assumptions including normality, heteroscedasticity, and independence. From these tests, you'll then refine and improve the model, not just for performance, but for interpretability as well.\u003c/p\u003e\n\u003ch2\u003eSummary\u003c/h2\u003e\n\u003cp\u003eIn this section, you'll conduct end-to-end review of the Data Science process!\u003c/p\u003e","frontPage":false},{"exportId":"introduction-to-linear-regression-recap","title":"Introduction to Linear Regression - Recap","type":"WikiPage","content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-linear-regression-section-recap-v2-1\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-linear-regression-section-recap-v2-1\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-linear-regression-section-recap-v2-1/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eThis short lesson summarizes the topics we covered in this section and why they'll be important to you as a data scientist.\u003c/p\u003e\n\u003ch2\u003eKey Takeaways\u003c/h2\u003e\n\u003cp\u003eIn this section, the nominal focus was on how to perform a linear regression, but the real value was learning how to think about the application of machine learning models to data sets.\u003c/p\u003e\n\u003cp\u003eKey takeaways include:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eStatistical learning theory deals with the problem of finding a predictive function based on data\u003c/li\u003e\n\u003cli\u003eA loss function calculates how well a given model represents the relationship between data values\u003c/li\u003e\n\u003cli\u003eA linear regression is simply a (straight) line of best fit for predicting a continuous value (y = mx + c)\u003c/li\u003e\n\u003cli\u003eThe Coefficient of Determination (R Squared) can be used to determine how well a given line fits a given data set\u003c/li\u003e\n\u003cli\u003eCertain assumptions must hold true for a least squares linear regression to be useful - linearity, normality and heteroscedasticity\u003c/li\u003e\n\u003cli\u003eQ-Q plots can check for normality in residual errors\u003c/li\u003e\n\u003cli\u003eThe Jarque-Bera test can be used to test for normality - especially when the number of data points is large\u003c/li\u003e\n\u003cli\u003eThe Goldfeld-Quant test can be used to check for homoscedasticity\u003c/li\u003e\n\u003c/ul\u003e","frontPage":false},{"exportId":"bayesian-statistics-recap","title":"Bayesian Statistics - Recap","type":"WikiPage","content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-bayesian-stats-recap\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-bayesian-stats-recap\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-bayesian-stats-recap/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eWell done! You covered a lot of ground in this section. From Bayes' theorem to Maximum Likelihood Estimation (MLE), you now have the foundation to dive into the world of Bayesians!\u003c/p\u003e\n\u003ch2\u003eBayes' Theorem\u003c/h2\u003e\n\u003cp\u003eTo start, you investigated Bayes' theorem and some hypothetical examples.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Clarge%20P(A%7CB)%20=%20%5Cdfrac%7BP(B%7CA)P(A)%7D%7BP(B)%7D\"\u003e\u003c/p\u003e\n\u003ch2\u003eBayesian Statistics\u003c/h2\u003e\n\u003cp\u003eFrom there, you then went on to read more about some of the philosophical differences between Bayesians and Frequentists. Bayesians interpret probability as the level of confidence or belief in an event. In contrast, Frequentists view probability as the limit as the number of trials goes to infinity of successes versus trials.\u003c/p\u003e\n\u003ch2\u003eMLE and MAP\u003c/h2\u003e\n\u003cp\u003eIn outlining the discussion of Bayesian techniques, you got an introduction to Maximum Likelihood Estimation and Maximum A Posteriori Estimation. In both, you saw methods for optimizing one's beliefs given certain information. This was used to estimate parameters for assumed distributions.\u003c/p\u003e\n\u003ch2\u003eSummary\u003c/h2\u003e\n\u003cp\u003eAgain, quite a bit was covered in this section. There are certainly plenty of additional resources available if you wish to further dive into MLE, MAP, or other Bayesian techniques. Bayesian inference can provide a powerful framework for quantifying and reasoning with uncertainty that has continued to gain popularity with additional computing resources.\u003c/p\u003e","frontPage":false},{"exportId":"assumptions-for-linear-regression","title":"Assumptions for Linear Regression","type":"WikiPage","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-regression-assumptions\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-regression-assumptions/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eLeast Squares is one of the most common regression techniques for linear models. As long as our model satisfies the least squares regression assumptions, we can get the best possible estimates. In this lesson, you will learn about these assumptions.\u003c/p\u003e\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\u003cp\u003eYou will be able to:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eList the assumptions of linear regression\u003c/li\u003e\n\u003cli\u003eDetermine if a particular set of data exhibits the assumptions of linear regression\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003eAbout Regression Assumptions\u003c/h2\u003e\n\u003cp\u003eRegression is a powerful analysis technique that is routinely used to answer complex analytical questions. However, if some of the necessary assumptions are not satisfied, you may not be able to get good and trustworthy results!\u003c/p\u003e\n\u003cp\u003eIn this lesson, you'll dig deeper into the topic of ordinary least squares (OLS) regression assumptions. Additionally, you'll learn about their importance as well as some techniques to help us determine whether your model satisfies the assumptions.\u003c/p\u003e\n\u003ch2\u003eRegression is \"Parametric\"\u003c/h2\u003e\n\u003cp\u003eRegression is a parametric technique, which means that it uses parameters learned from the data. Because of that, certain assumptions must be made. These assumptions define the complete scope of regression analysis and it is \u003cstrong\u003emandatory\u003c/strong\u003e that the underlying data fulfills these assumptions. If violated, regression makes biased and unreliable predictions. Luckily, we have measures to check for these assumptions.\u003c/p\u003e\n\u003ch2\u003e1. Linearity\u003c/h2\u003e\n\u003cblockquote\u003e\n\u003cp\u003eThe linearity assumptions requires that there is a \u003cstrong\u003elinear relationship\u003c/strong\u003e between the response variable (Y) and predictor (X). Linear means that the change in Y by 1-unit change in X, is constant.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-regression-assumptions/master/images/lin_2.png\" width=\"800\"\u003e\u003c/p\u003e\n\u003cp\u003eAs shown above, If we try to fit a linear model to a non-linear data set, OLS will fail to capture the trend mathematically, resulting in an inaccurate relationship. This will also result in erroneous predictions on an unseen data set.\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eThe linearity assumption can best be tested with scatter plots\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eFor non-linear relationships, you can use non-linear mathematical functions to fit the data e.g. polynomial and exponential functions. You'll come across these later.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eNote: As an extra measure, it is also important to check for outliers as the presence of outliers in the data can have a major impact on the model.\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-regression-assumptions/master/images/outliers.png\" width=\"600\"\u003e\u003c/p\u003e\n\u003cp\u003eIn the above example, we can see that an outlier prohibits the model to estimate the true relationship between variables by introducing bias.\u003c/p\u003e\n\u003ch2\u003e2. Normality\u003c/h2\u003e\n\u003cblockquote\u003e\n\u003cp\u003eThe normality assumption states that the \u003cstrong\u003emodel residuals\u003c/strong\u003e should follow a normal distribution\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eNote that the normality assumption talks about the \u003cstrong\u003emodel residuals\u003c/strong\u003e and \u003cem\u003enot\u003c/em\u003e about the distributions of the \u003cstrong\u003evariables\u003c/strong\u003e! In general, data scientists will often check the distributions of the variables as well. Keep in mind that the normality assumption is mandatory for the residuals, and it is useful to check normality of your variables to check for weirdness (more on data distributions later), but OLS works fine for non-normal data distributions in the context of prediction.\u003c/p\u003e\n\u003cp\u003eThe easiest way to check for the normality assumption is with histograms or a Q-Q-Plots.\u003c/p\u003e\n\u003ch3\u003eHistograms\u003c/h3\u003e\n\u003cp\u003eWe have already seen quite a few histograms and also know how to build them. You can use histograms to check the errors generated by the model and see if the plot shows a so-called \"normal distribution\" (bell curve shape). As the error term follows a normal distribution, we can develop better confidence in the results and calculate the statistical significance. An example of a regression error histogram is shown below:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-regression-assumptions/master/images/inhouse_histo.png\" width=\"800\"\u003e\u003c/p\u003e\n\u003ch3\u003eQ-Q Plots\u003c/h3\u003e\n\u003cblockquote\u003e\n\u003cp\u003eIn statistics, a Q–Q (quantile-quantile) plot is a probability plot, which is a graphical method for comparing two probability distributions by plotting their quantiles against each other.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eThe Q-Q plot (quantile-quantile plot) is used to help assess if a sample comes from a known distribution such as a normal distribution. For regression, when checking if the data in this sample is normally distributed, we can use a Normal Q-Q plot to test that assumption. Remember that this is just a visual check, so the interpretation remains subjective. However, it is a good first check to see the overall shape of your data against the required distribution. If you can reject normality through Q-Q plots, you have saved yourself from a lot of statistical testing. You have to be careful, however, when deciding that data is totally normal just by looking at a Q-Q plot.\u003c/p\u003e\n\u003cp\u003eBelow, you can find a few examples of comparing histograms and corresponding plots. You can see how the quantiles of normal data appear as a straight line along the diagonal when plotted against a standard normal distribution's quantiles. The skewness and kurtosis of data can also be inspected this way\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-regression-assumptions/master/images/inhouse_qq_plots.png\" width=\"600\"\u003e\u003c/p\u003e\n\u003cp\u003eIn the context of normality of residuals, Q-Q plots can help you validate the assumption of normally distributed residuals. It uses standardized values of residuals to determine the normal distribution of errors. Ideally, this plot should show a straight line. A curved, distorted line suggests residuals have a non-normal distribution.\u003ca href=\"https://data.library.virginia.edu/understanding-q-q-plots/\"\u003eHere is a good article\u003c/a\u003e explaining the interpretation of Q-Q plots in detail.\u003c/p\u003e\n\u003cp\u003eNormality can also be checked with goodness of fit tests such as the Kolmogorov-Smirnov test. When the data is not normally distributed, there are some ways to fix that, such as a non-linear transformation (e.g., log-transformation).\u003c/p\u003e\n\u003ch2\u003e3. Homoscedasticity\u003c/h2\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cem\u003eHeteroscedasticity\u003c/em\u003e (also spelled heteroskedasticity) refers to the circumstance in which the dependent variable is unequal across the range of values of the predictor(s).\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eWhen there is heteroscedasticity in the data, a scatterplot of these variables will often create a cone-like shape. The scatter of the dependent variable widens or narrows as the value of the independent variable increases.\u003c/p\u003e\n\u003cp\u003eThe inverse of heteroscedasticity is \u003cem\u003ehomoscedasticity\u003c/em\u003e, which indicates that a dependent variable's variability is equal across values of the independent variable. \u003cstrong\u003eHomoscedasticity is the third assumption necessary when creating a linear regression model.\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-regression-assumptions/master/images/homo_2.png\" width=\"700\"\u003e\u003c/p\u003e\n\u003cp\u003eA scatter plot is good way to check whether the data are homoscedastic (meaning the residuals are equal across the regression line). The scatter plots shown here are examples of data that are heteroscedastic (except the plot far right). You can also use significance tests like Breusch-Pagan / Cook-Weisberg test or White general test to detect this phenomenon. You will learn about p-values later, but for now, you can remember that, if these tests give you a p-value \u0026lt; 0.05, the null hypothesis can rejected, and you can assume the data is heteroscedastic.\u003c/p\u003e\n\u003ch2\u003eWhat Else?\u003c/h2\u003e\n\u003cp\u003eThere are other assumptions for linear regression that apply to more complicated cases, but for now these three assumptions are sufficient.\u003c/p\u003e\n\u003cp\u003eAs a first check, always looks at plots of the residuals. If you see anything similar to what is shown below, you are violating one or more assumptions and the results will not be reliable.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-regression-assumptions/master/images/prob_2.png\" width=\"700\"\u003e\u003c/p\u003e\n\u003ch2\u003eSummary\u003c/h2\u003e\n\u003cp\u003eIn this lesson, you learned about some assumptions for a simple linear regression that must be held in order to interpret the results reliably. As mentioned earlier, once these assumptions are confirmed, you can run your regression model. Next, you'll be exposed to some examples!\u003c/p\u003e\n\u003cfooter class=\"fis-footer\" style=\"visibility: hidden;\"\u003e\n\u003cdiv class=\"fis-feedback\"\u003e\n\u003ch5\u003eHow do you feel about this lesson?\u003c/h5\u003e\n\u003cimg id=\"thumbs-up\" title=\"Thumbs up!\" alt=\"thumbs up\" data-repository=\"dsc-regression-assumptions\"\u003e\u003cimg id=\"thumbs-down\" title=\"Thumbs down!\" alt=\"thumbs down\" data-repository=\"dsc-regression-assumptions\"\u003e\n\u003c/div\u003e\n\u003ch5\u003eHave specific feedback? \u003ca href=\"https://github.com/learn-co-curriculum/dsc-regression-assumptions/issues/new\"\u003eTell us here!\u003c/a\u003e\n\u003c/h5\u003e\n\u003c/footer\u003e","frontPage":false},{"exportId":"the-uniform-distribution","title":"The Uniform Distribution","type":"WikiPage","content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-uniform-distribution\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-uniform-distribution\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-uniform-distribution/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eNow that you've been introduced to both discrete (Binomial) and continuous (Normal) distributions, and know how to perform a simple test, let's move on with some more distributions. The uniform distribution is a special one, as there is a discrete as well as a continuous version of this distribution!\u003c/p\u003e\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\u003cp\u003eYou will be able to:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eDefine a uniform distribution\u003c/li\u003e\n\u003cli\u003eCalculate mean and variance of uniform distribution\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003eUniform Distribution\u003c/h2\u003e\n\u003cp\u003eThe \u003cstrong\u003eUniform Distribution\u003c/strong\u003e describes an event where every possible outcome is equally likely. No single outcome carries any more or less probability of happening than any other possible outcome. \u003cstrong\u003eThe Uniform Distribution can be discrete or continuous\u003c/strong\u003e.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-uniform-distribution/master/images/new_uniform.png\" width=\"500\"\u003e\u003c/p\u003e\n\u003ch2\u003eThe Discrete Uniform Distribution\u003c/h2\u003e\n\u003cp\u003eYou've seen an example of a Discrete Uniform Distribution before: rolling a 6-sided dice. This idea can be extended to an \u003cimg src=\"https://render.githubusercontent.com/render/math?math=n\"\u003e-sided dice. No matter how many sides the dice has, with a fair dice, you'd be equally likely to roll every side.\u003c/p\u003e\n\u003cp\u003eIf \u003cimg src=\"https://render.githubusercontent.com/render/math?math=a\"\u003e is the smallest number and \u003cimg src=\"https://render.githubusercontent.com/render/math?math=b\"\u003e is the biggest number that can be observed (for a fair dice, \u003cimg src=\"https://render.githubusercontent.com/render/math?math=a=1\"\u003e, \u003cimg src=\"https://render.githubusercontent.com/render/math?math=b=6\"\u003e )\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eProbability Mass Function:\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=p(x)%20=%20%5Cdfrac%7B1%7D%7B(b%20-%20a%2b1)%7D%5Ctext%7B%20when%20%7D%20a%20%5Cleq%20x%20%5Cleq%20b\"\u003e\u003c/p\u003e\n\u003cp\u003eand\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=p(x)%20=%200%5Ctext%7B%20when%20%7Dx%20%5Cnotin%20%5Ba,%20b%5D\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eUniform Distribution Mean:\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=E(X)=%20%5Cfrac%7Ba%20%2b%20b%7D%7B2%7D\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eUniform Distribution Variance\u003c/strong\u003e:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=Var(X)%20=%20%5Csqrt%7B%5Cfrac%7B(b-a)(b-a%2b2)%7D%7B12%7D%7D\"\u003e\u003c/p\u003e\n\u003ch2\u003eThe Continuous Uniform Distribution\u003c/h2\u003e\n\u003cp\u003eAn example of a \u003cstrong\u003eContinuous Uniform\u003c/strong\u003e distributed variable would be the waiting time for an elevator that could be on any floor in the building when you call it and can take between 0 and 40 seconds to arrive at your floor. Since the elevator is equally likely to be at any given floor, you can assume every amount of time between 0 and 40 seconds before the elevator arrives (decimals and fractions allowed, to an infinite amount of precision). The formulas for a continuous uniform distribution diverge slightly!\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eProbability Density Function:\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=f(x)%20=%20%5Cdfrac%7B1%7D%7B(b%20-%20a)%7D%5Ctext%7B%20when%20%7D%20a%20%5Cleq%20x%20\u003c%20b\"\u003e\u003c/p\u003e\n\u003cp\u003eand\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=f(x)%20=%200%5Ctext%7B%20when%20%7Dx%20%5Cnotin%20%5Ba,%20b%5D\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eUniform Distribution Mean:\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=E(X)=%20%5Cfrac%7Ba%20%2b%20b%7D%7B2%7D\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eUniform Distribution Variance\u003c/strong\u003e:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=Var(X)%20=%20%5Cfrac%7B(b%20-%20a)%5E2%7D%7B12%7D\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e\u003cem\u003eNOTE:\u003c/em\u003e\u003c/strong\u003e If you're confused why there is a 12 in the denominator of the formula of the variance for a Uniform Distribution, you're not alone. The short answer is that it involves calculus. As a data scientist, you don't need to understand the derivation of this formula and where this 12 comes from. However, if you're interested, this \u003ca href=\"https://www.quora.com/Why-is-there-a-12-in-the-variance-of-uniform-distribution\"\u003eQuora answer gives an excellent explanation\u003c/a\u003e!\u003c/p\u003e\n\u003ch2\u003eSummary\u003c/h2\u003e\n\u003cp\u003eIn this lesson, you learned about the uniform distribution, its applications, and the equations used to compute the mean and variance of a uniformly distributed random variable.\u003c/p\u003e","frontPage":false},{"exportId":"hypothesis-testing-recap","title":"Hypothesis Testing - Recap","type":"WikiPage","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-hypothesis-testing-section-recap-v2-1\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-hypothesis-testing-section-recap-v2-1/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eYou just learned how to create an experiment and interpret the results! Let's review some of the specific things you have learned.\u003c/p\u003e\n\n\u003ch2\u003eKey Takeaways\u003c/h2\u003e\n\n\u003cp\u003eSome of the key takeaways from this section include:  \u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eIt's important to have a sound approach to experimental design to be able to determine the significance of your findings. \u003c/li\u003e\n\u003cli\u003eStart by examining any existing research to see if it can shed light on the problem you're studying. \u003c/li\u003e\n\u003cli\u003eStart with a clear alternative and null hypothesis for your experiment to \"prove\". \u003c/li\u003e\n\u003cli\u003eIt's important to have a thoughtfully selected control group from the same population for your trial to distinguish effect from variations based on population, time or other factors. \u003c/li\u003e\n\u003cli\u003eYour sample size needs to be selected carefully to ensure your results have a good chance of being statistically significant. \u003c/li\u003e\n\u003cli\u003eYour results should be reproducible by other people and using different samples from the population. \u003c/li\u003e\n\u003cli\u003eThe p-value for an outcome determines how likely it is that the outcome could occur under the null hypothesis. \u003c/li\u003e\n\u003cli\u003e \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Calpha\"\u003e is the marginal threshold at which we're comfortable rejecting the null hypothesis. \u003c/li\u003e\n\u003cli\u003eAn  \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Calpha\"\u003e value of 0.05 is a common choice for many experiments. \u003c/li\u003e\n\u003cli\u003eEffect size measures just the size of the difference between two groups under observation, whereas statistical significance combines effect size with sample size. \u003c/li\u003e\n\u003cli\u003eA one sample t-test is used to determine whether a sample comes from a population with a specific mean\u003cbr\u003e\n\u003c/li\u003e\n\u003cli\u003eA two sample t-test is used to determine if two population means are equal. \u003c/li\u003e\n\u003cli\u003eType 1 errors (false positives) are when we accept an alternative hypothesis which is actually false. \u003c/li\u003e\n\u003cli\u003eThe  \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Calpha\"\u003e that we pick is the likelihood that we will get a type 1 error due to random chance. \u003c/li\u003e\n\u003cli\u003eType 2 errors (false negatives) are when we reject an alternative hypothesis which is actually true. \u003c/li\u003e\n\u003cli\u003eResampling methods allow for improved precision in estimating sample statistics and validating models by using random subsets. \u003c/li\u003e\n\u003cli\u003eCommon resampling techniques include bootstrapping, jackknifing and permutation tests.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cfooter class=\"fis-footer\" style=\"visibility: hidden;\"\u003e\u003cdiv class=\"fis-feedback\"\u003e\n\u003ch5\u003eHow do you feel about this lesson?\u003c/h5\u003e\n\u003cimg id=\"thumbs-up\" data-repository=\"dsc-hypothesis-testing-section-recap-v2-1\" title=\"Thumbs up!\" alt=\"thumbs up\"\u003e\u003cimg id=\"thumbs-down\" data-repository=\"dsc-hypothesis-testing-section-recap-v2-1\" title=\"Thumbs down!\" alt=\"thumbs down\"\u003e\n\u003c/div\u003e\n\u003ch5\u003eHave specific feedback? \u003ca href=\"https://github.com/learn-co-curriculum/dsc-hypothesis-testing-section-recap-v2-1/issues/new\"\u003eTell us here!\u003c/a\u003e\n\u003c/h5\u003e\u003c/footer\u003e","frontPage":false},{"exportId":"statistical-distributions-and-their-use-cases","title":"Statistical Distributions and Their Use Cases","type":"WikiPage","content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-stat-distributions-use-cases\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-stat-distributions-use-cases\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-stat-distributions-use-cases/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eAs a data scientist, you'll often have to work with statistical distributions. This includes selecting which distribution is most representative of a given set of data. A typical use case includes A/B testing, where understanding the process that generated the data is important. You can think of distributions in relation to statistical analysis as to data structures to computer programming.\u003c/p\u003e\n\u003cp\u003eThere are an enormous amount of distributions out there, but you'll see about a handful of distributions that can represent the vast majority of situations you'll come across. In the upcoming series of lessons, you'll look at ways to analyze common distributions you will encounter most frequently.\u003c/p\u003e\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\u003cp\u003eYou will be able to:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eDefine statistical distributions\u003c/li\u003e\n\u003cli\u003eDifferentiate between discrete and continuous distributions\u003c/li\u003e\n\u003cli\u003eList the common distributions and their use cases\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003eWhat is a Statistical Distribution?\u003c/h2\u003e\n\u003cp\u003eA statistical distribution is a representation of the frequencies of potential events or the percentage of time each event occurs.\u003c/p\u003e\n\u003cp\u003eThis may feel pretty vague which is why we'll use two examples to clarify this concept.\u003c/p\u003e\n\u003ch3\u003eRolling a Dice Distribution\u003c/h3\u003e\n\u003cp\u003eLet's think back about our example rolling a dice. You know that when rolling dice once, you will obtain a number between 1 and 6, with each outcome to be as likely, as denoted in this table:\u003c/p\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003eoutcome\u003c/th\u003e\n\u003cth\u003e1\u003c/th\u003e\n\u003cth\u003e2\u003c/th\u003e\n\u003cth\u003e3\u003c/th\u003e\n\u003cth\u003e4\u003c/th\u003e\n\u003cth\u003e5\u003c/th\u003e\n\u003cth\u003e6\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd\u003eprobability\u003c/td\u003e\n\u003ctd\u003e1/6\u003c/td\u003e\n\u003ctd\u003e1/6\u003c/td\u003e\n\u003ctd\u003e1/6\u003c/td\u003e\n\u003ctd\u003e1/6\u003c/td\u003e\n\u003ctd\u003e1/6\u003c/td\u003e\n\u003ctd\u003e1/6\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003cp\u003eYou can also represent this graphically as follows:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-stat-distributions-use-cases/master/images/dice_roll_pmf.png\" width=\"350\"\u003e\u003c/p\u003e\n\u003cp\u003eNote how, with a fair die, the chance of throwing each number is \u003cem\u003eexactly\u003c/em\u003e 1/6 (or 0.1666). The number of outcomes is finite and the outcome is a set of values. In this case, you are dealing with a \u003cstrong\u003ediscrete distribution\u003c/strong\u003e.\u003c/p\u003e\n\u003ch3\u003eWeather Distribution\u003c/h3\u003e\n\u003cp\u003eLet's look at another situation. Imagine we want to think of the distribution of the temperature in New York on June 1st. Thinking about this, you could say that the temperature would generally range between 65 and 95 Degrees (more extreme values would be exceptional), with the average around 80 Degrees Fahrenheit.\u003c/p\u003e\n\u003cp\u003eA potential distribution looks like this:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-stat-distributions-use-cases/master/images/weather_pdf.png\" width=\"400\"\u003e\u003c/p\u003e\n\u003cp\u003eNote that instead of bars, which we had for the dice example, we have \u003cem\u003econtinuous\u003c/em\u003e lines here. Our distribution is a \u003cstrong\u003econtinuous distribution\u003c/strong\u003e because temperature is a continuous value (we can have a temperature of 80 degrees, of 80.5 degrees, of 80.0034 degrees, etc.).\u003c/p\u003e\n\u003ch3\u003eDiscrete vs Continuous Distributions\u003c/h3\u003e\n\u003cp\u003eWhen dealing with \u003cstrong\u003ediscrete\u003c/strong\u003e data you use a \u003cstrong\u003eProbability Mass Function (PMF)\u003c/strong\u003e (as in our dice example). When dealing with \u003cstrong\u003econtinuous\u003c/strong\u003e data, you use a \u003cstrong\u003eProbability Density Function (PDF)\u003c/strong\u003e (see our weather example).\u003c/p\u003e\n\u003cp\u003eBased on the variation of their attributes, data distributions can take many shapes and forms. In the next few lessons, you'll learn how to describe data distributions. Very often, distributions are described using their statistical mean (or \u003cstrong\u003eexpected value\u003c/strong\u003e) and variance of the data, but this is not always the case. You'll see more on this in the next few lessons.\u003c/p\u003e\n\u003ch2\u003eCommon Distributions\u003c/h2\u003e\n\u003cp\u003eIn this image, you can see the general shapes of some common distributions. The horizontal axis in each chart represents the set of possible numeric outcomes. The vertical axis describes the probability of the respective outcomes.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-stat-distributions-use-cases/master/images/dists.png\" width=\"600\"\u003e\u003c/p\u003e\n\u003cp\u003eYou'll get a more in-depth overview of some important distributions in the next few lessons, but to give you an initial idea of some applications, we'll give you a quick overview below. Let's quickly talk about some common distributions and their use cases below.\u003c/p\u003e\n\u003ch2\u003eExamples of Discrete Distributions\u003c/h2\u003e\n\u003ch3\u003eThe Bernoulli Distribution\u003c/h3\u003e\n\u003cp\u003eThe Bernoulli distribution represents the probability of success for a certain experiment (the outcome being \"success or not\", so there are two possible outcomes). A coin toss is a classic example of a Bernoulli experiment with a probability of success 0.5 or 50%, but a Bernoulli experiment can have any probability of success between 0 and 1.\u003c/p\u003e\n\u003ch3\u003eThe Poisson Distribution\u003c/h3\u003e\n\u003cp\u003eThe Poisson distribution represents the probability of \u003cimg src=\"https://render.githubusercontent.com/render/math?math=n\"\u003e events in a given time period when the overall rate of occurrence is constant. A typical example is pieces of mail. If your overall mail received is constant, the number of items received on a single day (or month) follows a Poisson distribution. Other examples might include visitors to a website, or customers arriving at a store, or clients waiting to be served in a queue.\u003c/p\u003e\n\u003ch3\u003eThe Uniform Distribution\u003c/h3\u003e\n\u003cp\u003eThe uniform distribution occurs when all possible outcomes are equally likely. The dice example shown before follows a uniform distribution with equal probabilities for throwing values from 1 to 6. The dice example follows a discrete uniform distribution, but continuous uniform distributions exist as well.\u003c/p\u003e\n\u003ch2\u003eExamples of Continuous Distributions\u003c/h2\u003e\n\u003ch3\u003eThe Normal or Gaussian Distribution\u003c/h3\u003e\n\u003cp\u003eA normal distribution is the single most important distribution, you'll basically come across it very often. The normal distribution follows a bell shape and is a foundational distribution for many models and theories in statistics and data science. A normal distribution turns up very often when dealing with real-world data including heights, weights of different people, errors in some measurement or grades on a test. Our temperature example above follows a normal distribution as well!\u003c/p\u003e\n\u003ch2\u003eSummary\u003c/h2\u003e\n\u003cp\u003eIn this lesson, you learned about the concept of (discrete and continuous) statistical distributions, as well as some common ones. You'll learn more about distributions and their properties in the next few lessons!\u003c/p\u003e","frontPage":false},{"exportId":"poisson-distribution","title":"Poisson Distribution","type":"WikiPage","content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-poisson-distribution\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-poisson-distribution\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-poisson-distribution/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eIn this lesson, you'll learn about the \u003cstrong\u003ePoisson Distribution\u003c/strong\u003e and explore some practical ways you can use it.\u003c/p\u003e\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\u003cp\u003eYou will be able to:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eExplain the parameters of the Poisson distribution and its use cases\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003eWhat is the Poisson Distribution?\u003c/h2\u003e\n\u003cp\u003eThe \u003cstrong\u003ePoisson Distribution\u003c/strong\u003e is yet another statistical distribution you can use to answer questions about the probability of a given number of successes, the probability of success, and a series of independent trials. Specifically, the Poisson Distribution allows you to calculate the probability of a given event happening by examining the mean number of events that happen in a given time period. Given a set time period, we can use the Poisson Distribution to predict how many times a given event will happen over that time period. To help you better understand this, let's examine a few sample questions that we can answer using the Poisson Distribution.\u003c/p\u003e\n\u003ch3\u003eSample Question 1\u003c/h3\u003e\n\u003cp\u003eAn average of 20 customers walk into a store in a given hour. What is the probability that 25 customers walk into a store in the next hour?\u003c/p\u003e\n\u003ch3\u003eSample Question 2\u003c/h3\u003e\n\u003cp\u003eA police officer pulls over an average of 3 people for speeding violations per shift. What is the probability that the officer will pull over two people for speeding violations during their next shift?\u003c/p\u003e\n\u003ch2\u003eUnderstanding the Parameters\u003c/h2\u003e\n\u003cp\u003eIn order to use the Poisson Distribution, we only need to know a few parameters:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cmu\"\u003e : The average number of successes over a given time period. For instance, the average number of customers that walk into a store in a given hour, or the average number of speeding violations a police officer sees in a shift.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=x\"\u003e : Our random variable - the number of successes we want to find the probability mass of given our knowledge of \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cmu\"\u003e .\u003c/p\u003e\n\u003ch3\u003eRelationship to the Binomial Distribution\u003c/h3\u003e\n\u003cp\u003eThe Poisson distribution has a special relation to the binomial distribution. The theoretical underpinnings are as follows. Imagine that we take a time period and break it into \u003cimg src=\"https://render.githubusercontent.com/render/math?math=n\"\u003e subintervals that are so small that at most one successful event could occur. We can then imagine that for any of these subintervals, a binomial distribution could apply where there is some probability of the event occurring, \u003cimg src=\"https://render.githubusercontent.com/render/math?math=p\"\u003e , a probability \u003cimg src=\"https://render.githubusercontent.com/render/math?math=q=1-p\"\u003e that the event does not occur, and a probability of 0 that more than one event occurs. We assume that as we cut time into smaller and smaller intervals, the chance of success should go down. If we take the limit of the binomial distribution as \u003cimg src=\"https://render.githubusercontent.com/render/math?math=n\"\u003e goes to infinity (more and more subintervals that are progressively smaller and smaller), the result is the Poisson distribution.\u003c/p\u003e\n\u003cp\u003eBinomial Probability Distribution: \u003cimg src=\"https://render.githubusercontent.com/render/math?math=p(x)%20=%20%5Cbinom%7Bn%7D%7Bx%7Dp%5Ex(1-p)%5E%7Bn-x%7D\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Clambda%20=%20n*p\"\u003e\u003c/p\u003e\n\u003cp\u003ePoisson Probability Distribution: \u003cimg src=\"https://render.githubusercontent.com/render/math?math=p(x)%20=%20%5Cfrac%7B%5Clambda%5Exe%5E%7B-%5Clambda%7D%7D%7Bx!%7D\"\u003e\u003c/p\u003e\n\u003cp\u003eAlso note that lambda \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Clambda\"\u003e is the now the average number of successes that we anticipate in a given interval: the probability \u003cimg src=\"https://render.githubusercontent.com/render/math?math=p\"\u003e of success, times the number of intervals \u003cimg src=\"https://render.githubusercontent.com/render/math?math=n\"\u003e . This is then exactly how the Poisson is used in practice--if we know the average number of occurrences in a given interval, what is the probability that the actual number of occurrences is slightly more, slightly less, far more or far less?\u003c/p\u003e\n\u003ch3\u003eUnderstanding the Formula\u003c/h3\u003e\n\u003cp\u003eLet's take another look at the formula for the Poisson Probability Distribution:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=p(x)%20=%20%5Cfrac%7B%5Clambda%5Exe%5E%7B-%5Clambda%7D%7D%7Bx!%7D\"\u003e\u003c/p\u003e\n\u003cp\u003eIn the other statistical distributions we've explored, we were explicitly given the probability of success or failure as one of our parameters. In this example, we are not given this probability--however, we know how likely an event is to occur the mean number of times over a given time period, which means that we actually \u003cstrong\u003edo\u003c/strong\u003e know the probability--we just need to do some basic calculations to uncover this probability.\u003c/p\u003e\n\u003cp\u003eFor instance, if we know that 6 customers walk into a store per hour, we also know enough to calculate the probability that a customer walks in during a given minute. We do this by just dividing the mean number of customers by the length of our interval!\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=p%20=%20%5Cfrac%7B6%7D%7B60%7D%20=%200.1\"\u003e\u003c/p\u003e\n\u003cp\u003eThere is no expectation that customers will walk into a store in evenly spaced intervals--a customer may walk in every 10 minutes on the dot--however, we may also see 3 customers walk in during the first 5 minutes, 3 more customers 10 minutes later, and no other customers for the rest of the hour. Remember, these events are independent, and this is also the mean number per hour. This doesn't mean that we have 6 customers every hour - it's possible that we do, but it's also possible that we have 12 customers one hour and no customers the next hour. It's also possible that in a 10-hour day, 60 customers enter the store during the first hour, and then none for the rest of the day. If your intuition is telling you that this is possible, but not \u003cstrong\u003e\u003cem\u003eplausible\u003c/em\u003e\u003c/strong\u003e because it has a very low probability of happening, you're right--and the probability of this happening is exactly what the Poisson Distribution allows us to calculate!\u003c/p\u003e\n\u003cp\u003eIn light of this, it makes sense for us to calculate the probability that a customer will walk in during \u003cstrong\u003e\u003cem\u003eany given minute\u003c/em\u003e\u003c/strong\u003e, which we discovered by just dividing our mean number of customers per hour by the number of minutes in our interval, showing us that the probability of a customer walking in during any given minute is 0.1, or 10%. This number is our \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Clambda\"\u003e parameter.\u003c/p\u003e\n\u003cp\u003eTake a look at the following graph - note the relationship of each line to its given \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Clambda\"\u003e parameter:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-poisson-distribution/master/images/new_poisson.png\" width=\"400\"\u003e\u003c/p\u003e\n\u003ch3\u003eThe Rest of the Formula\u003c/h3\u003e\n\u003cp\u003eDon't let the other terms in that equation scare you - you've seen them before, and even if you haven't they're quite easy to work with:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=e\"\u003e : Euler's Constant, which is \u003cimg src=\"https://render.githubusercontent.com/render/math?math=e%20%5Capprox%202.71828\"\u003e . You may also know this as the base of the natural logarithm. On calculators, this is the \u003ccode\u003eexp\u003c/code\u003e button. In Python, we can access it by using NumPy's \u003ccode\u003enp.exp()\u003c/code\u003e function.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=x!\"\u003e : The factorial of \u003cimg src=\"https://render.githubusercontent.com/render/math?math=x\"\u003e . For example, \u003cimg src=\"https://render.githubusercontent.com/render/math?math=3!%20=%203%20*%202%20*%201%20=%206\"\u003e\u003c/p\u003e\n\u003ch2\u003eSummary\u003c/h2\u003e\n\u003cp\u003eIn this lesson, you learned about the Poisson Distribution, the Poisson Probability Formula, and how you can use this distribution to solve real-world problems!\u003c/p\u003e","frontPage":false},{"exportId":"statistical-distributions-introduction","title":"Statistical Distributions - Introduction","type":"WikiPage","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-distributions-section-intro-v2-1\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-distributions-section-intro-v2-1/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eIn this section, you'll learn about different probability distributions!\u003c/p\u003e\n\u003ch2\u003eIt's all Stats!\u003c/h2\u003e\n\u003cp\u003eYou've already seen the value of descriptive statistics when doing exploratory data analysis. In this section, we're going to dive deeper into a range of statistical concepts. We're going to start by looking at discrete and continuous distributions and how you can use stem and leaf plots for visualizing distributions.\u003c/p\u003e\n\u003cp\u003eWe're then going to look at a range of techniques for representing distributions - the Probability Mass Function, the Cumulative Distribution Function, and the Probability Density Function.\u003c/p\u003e\n\u003cp\u003eWe're then going to dig a little deeper into the Normal/Gaussian distribution and the Standard Normal Distribution, and we'll introduce the use cases for z-tables and p-values for describing statistical significance. We'll discuss the \"one-sample z test\", the most basic type of hypothesis test, before introducing the concepts of skewness and kurtosis that can be used to quantify how \"un-normal\" a given distribution is.\u003c/p\u003e\n\u003cp\u003eIn the Appendix to this Module, we'll introduce some additional distribution functions, like the uniform, Poisson, and exponential distributions, and use them to solve practical problems.\u003c/p\u003e\n\u003ch2\u003eSummary\u003c/h2\u003e\n\u003cp\u003eIn this section, we're going to take a deeper dive into a range of foundational statistical concepts that we'll need as we start to dig into machine learning later in the course.\u003c/p\u003e\n\u003cfooter class=\"fis-footer\" style=\"visibility: hidden;\"\u003e\n\u003cdiv class=\"fis-feedback\"\u003e\n\u003ch5\u003eHow do you feel about this lesson?\u003c/h5\u003e\n\u003cimg id=\"thumbs-up\" title=\"Thumbs up!\" alt=\"thumbs up\" data-repository=\"dsc-distributions-section-intro-v2-1\"\u003e\u003cimg id=\"thumbs-down\" title=\"Thumbs down!\" alt=\"thumbs down\" data-repository=\"dsc-distributions-section-intro-v2-1\"\u003e\n\u003c/div\u003e\n\u003ch5\u003eHave specific feedback? \u003ca href=\"https://github.com/learn-co-curriculum/dsc-distributions-section-intro-v2-1/issues/new\"\u003eTell us here!\u003c/a\u003e\n\u003c/h5\u003e\n\u003c/footer\u003e","frontPage":false},{"exportId":"the-kolmogorov-smirnov-test","title":"The Kolmogorov-Smirnov Test","type":"WikiPage","content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-kolmogorov-smirnov-test\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-kolmogorov-smirnov-test\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-kolmogorov-smirnov-test/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eDuring data analysis, you have to satisfy a number of assumptions for the underlying dataset. One of the most common assumptions that you will come across is the \"Normality Assumption\", i.e., the underlying data roughly follows a normal distribution.\u003c/p\u003e\n\n\u003cp\u003eIf the data is not found to be normally distributed (i.e. data with kurtosis and skew while doing linear regression), you may first answer a question like: “Given my data … if there is a deviation from normality, will there be a material impact on my results?”\u003c/p\u003e\n\n\u003cp\u003eIn this lesson, we'll look at a popular statistical test for satisfying the normality assumption, the Kolmogorov-Smirnov test, or simply, the K-S test.\u003c/p\u003e\n\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\n\u003cp\u003eYou will be able to:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eExplain the role of the normality assumption in statistical tests \u003c/li\u003e\n\u003cli\u003eCalculate a one-and two-sample Kolmogorov-Smirnov test\u003c/li\u003e\n\u003cli\u003eInterpret the results of a one- and two-sample Kolmogorov-Smirnov test\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch2\u003eNormality assumption\u003c/h2\u003e\n\n\u003cp\u003eFormal normality tests always reject the huge sample sizes we work with today. When n (our sample size) gets large, even the smallest deviation from perfect normality will lead to a significant result. And as every dataset has some degree of random noise, no single dataset will be a \u003cstrong\u003eperfectly\u003c/strong\u003e normally distributed sample. \u003c/p\u003e\n\n\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003eIn applied statistics, the question is not whether the data/residuals are perfectly normal, but normal enough for the assumptions to hold\u003c/strong\u003e.\u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003cp\u003eThis question is answered through visualization techniques like qqplots, boxplots, or more advanced statistical tests including:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eThe Shapiro-Wilk test;\u003c/li\u003e\n\u003cli\u003eThe Anderson-Darling test, and;\u003c/li\u003e\n\u003cli\u003eThe Kolmogorov-Smirnov test \u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003eIn this lesson, we'll focus on the Kolmogorov-Smirnov test (K-S test) which will give you a strong foundation to help you understand and implement other tests when needed. \u003c/p\u003e\n\n\u003ch2\u003eKolmogorov-Smirnov Test\u003c/h2\u003e\n\n\u003cp\u003eA K-S test provides a way of comparing distributions, whether two sample distributions or a sample distribution with a theoretical distribution - comparable to what we've already seen when we learned about one sample or two-sample t-tests. The distributions are compared in their cumulative form as \u003cstrong\u003eEmpirical Cumulative Distribution Functions\u003c/strong\u003e. The test statistic in K-S test used to compare distributions is simply the maximum vertical distance between the two functions. Essentially, we are testing the sample data against another sample, to compare their distributions for similarities.\u003c/p\u003e\n\n\u003ch3\u003eThe Empirical Cumulative Distribution Function (ECDF)\u003c/h3\u003e\n\n\u003cblockquote\u003e\n\u003cp\u003eAn empirical cumulative distribution function (CDF) is a non-parametric estimator of the underlying CDF of a random variable. It assigns a probability to each data point, orders the data from smallest to largest in value, and calculates the sum of the assigned probabilities up to and including each data point.\u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003cp\u003eThe most intuitive way to think about the empirical distribution function is that it relates to the cumulative distribution function (CDF) in a similar way to how a histogram relates to a probability density function. Let's look at the following figures to get this idea:\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-kolmogorov-smirnov-test/master/images/rnorm.png\" width=\"800\"\u003e\u003c/p\u003e\n\n\u003cp\u003eThe left figure shows a regular histogram with samples looking like a normal distribution. The right figure shows the same samples except each bin in the histogram contains the cumulative count of samples up to that bin, which approximates the shape of the CDF for this random variable. Now the right figure doesn't exactly represent an empirical distribution function because the Y-axis is not normalized to 1 and the samples are binned instead of just plotted cumulatively. Nonetheless, the idea remains the same. An example of an empirical CDF is given below:\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-kolmogorov-smirnov-test/master/images/cumul_prob.png\" width=\"400\"\u003e\u003c/p\u003e\n\n\u003cp\u003eThis image sums up the intuition for empirical distribution function. The blue line is our empirical CDF whereas the  grey one is our theoretical CDF (i.e. plotted using parameters and fitting a probability function).\u003c/p\u003e\n\n\u003cp\u003eIf X is a random variable with CDF \u003cimg src=\"https://render.githubusercontent.com/render/math?math=F(x)=P(X%E2%89%A4x)\"\u003e, and \u003cimg src=\"https://render.githubusercontent.com/render/math?math=x1,%5Cldots,xn\"\u003e are i.i.d. random variables sampled from X. Then, the empirical distribution function, \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Chat%7BF%7D(x)\"\u003e , is a CDF:\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Chat+F%28x%29+%3D+%5Cdfrac%7B%28%5Ctext%7B%23+of+elements+in+sample%7D%29+%5Cleq+x%7D%7Bn%7D+%3D+%5Cdfrac%7B1%7D%7Bn%7D+%5CSigma_%7Bi%3D1%7D%5En+I%28x_i+%5Cleq+x%29+%5Ctag%7B1%7D\"\u003e\u003c/p\u003e\n\n\u003ch3\u003eOne-Sample K-S test\u003c/h3\u003e\n\n\u003cp\u003eThis is also known as the \u003cstrong\u003eKolmogorov-Smirnov Goodness of Fit test\u003c/strong\u003e. It calculates the similarity between an observed (empirical) distribution and a completely specified theoretical continuous distribution. It is sensitive to all attributes of a distribution including mean, variance, and shape.\u003c/p\u003e\n\n\u003cp\u003eThe key assumption of the one-sample test is that the theoretical distribution is fully defined continuous distribution, in terms of its parameters. This obviously means that its most common use case is that of testing normality. The test statistic,  \u003cimg src=\"https://render.githubusercontent.com/render/math?math=d\"\u003e ,  is simply the largest deviation between the observed cumulative function and the expected theoretical cumulative frequency distribution, i.e. \u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=d=max(abs%5BF_0(X)-F_r(X)%5D)\"\u003e \u003c/p\u003e\n\n\u003cp\u003ewhere\n- \u003cstrong\u003ed\u003c/strong\u003e is the maximum deviation Kolmogorov statistic \n- \u003cstrong\u003eF\u003csub\u003e0\u003c/sub\u003e(X)\u003c/strong\u003e = (No.of observations ≤ X)/(Total no.of observations) i.e. the non parametric empirical distribution\n- \u003cstrong\u003eF\u003csub\u003er\u003c/sub\u003e(X)\u003c/strong\u003e = The theoretical frequency distribution of X - parametric (e.g. based on mean value) \u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-kolmogorov-smirnov-test/master/./images/new_d.png\" width=\"600\"\u003e\u003c/p\u003e\n\n\u003cp\u003e\u003cstrong\u003eNull Hypothesis:\u003c/strong\u003e There is no difference between the distribution of our sample and a normal distribution. \u003c/p\u003e\n\n\u003cp\u003e\u003cstrong\u003eAcceptance Criteria:\u003c/strong\u003e If the calculated value is less than the critical value, accept the null hypothesis.\u003c/p\u003e\n\n\u003cp\u003e\u003cstrong\u003eRejection Criteria:\u003c/strong\u003e If the calculated value is greater than the critical value, reject the null hypothesis.\u003c/p\u003e\n\n\u003ch3\u003eExample\u003c/h3\u003e\n\n\u003ch4\u003eProblem Statement:\u003c/h4\u003e\n\n\u003cp\u003eIn a study done from various modules of a data science course with 60 students, equal number of students are samples from each module. These students are interviewed and their intention to join the advanced machine learning module was noted. Following shows how many students showed a positive intention\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003ePython (5) \u003c/li\u003e\n\u003cli\u003eData Visualizations (9)\u003c/li\u003e\n\u003cli\u003eSQL (11)\u003c/li\u003e\n\u003cli\u003eStatistics (16)\u003c/li\u003e\n\u003cli\u003eNLP (19)\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003eIt was expected that 12 students from each module would join advanced ML. \u003c/p\u003e\n\n\u003cp\u003eLet's use K-S test to find if there is any difference among student classes with regard to their intention of joining the advanced machine learning module.\u003c/p\u003e\n\n\u003cp\u003eFirst, we need to set up our null hypothesis. \u003c/p\u003e\n\n\u003cblockquote\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=H_%7B0%7D\"\u003e : There is no difference among students of different modules with respect to their intention of joining advanced ML. \u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003cpre\u003e\u003ccode\u003eStreams    No. of students interested in joining    FO(X)   Fr(X)   |FO(X)−FT(X)|\n           Observed(O)  Theoretical(T)           \nPython     5            12                          5/60    12/60   7/60\nViz.       9            12                          14/60   24/60   10/60\nSQL        11           12                          25/60   36/60   11/60\nStats      16           12                          41/60   48/60   7/60\nNLP        19           12                          60/40   60/60   60/60\n\nTotal      n=60     \n\u003c/code\u003e\u003c/pre\u003e\n\n\u003cp\u003eAccording to the formula above, \n \u003cimg src=\"https://render.githubusercontent.com/render/math?math=d=max(abs%5BF_0(X)-F_r(X)%5D)\"\u003e \u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=d=11/60%20=%200.183\"\u003e \u003c/p\u003e\n\n\u003cp\u003eHere's the Smirnov d-statistic for reference: \n\u003cimg src=\"images/1samp.png\" alt=\"\"\u003e\u003c/p\u003e\n\n\u003cp\u003eThe table value of d at 5% significance level is given by\n \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%7Bd(0.05)=%5Cfrac%7B1.36%7D%7B%5Csqrt%7Bn%7D%7D%7D%5C%5C%5B7pt%5D%5C,=%5Cfrac%7B1.36%7D%7B%5Csqrt%7B60%7D%7D%5C%5C%5B7pt%5D%5C,=0.175\"\u003e \u003c/p\u003e\n\n\u003cp\u003eSince the calculated d value (0.183) is greater than the critical value (0.175), hence we reject the null hypothesis and conclude that there is a difference among students of different modules in their intention of joining the advanced ML course. \u003c/p\u003e\n\n\u003ch3\u003eTwo-Sample K-S Test\u003c/h3\u003e\n\n\u003cp\u003eThe two-sample K-S test checks if two \u003cstrong\u003eindependent\u003c/strong\u003e samples have been drawn from the same population, or, equivalently, from two identical populations (X = Y).\u003c/p\u003e\n\n\u003cp\u003eAs with the one-sample test, it is moderately sensitive to all parameters of the distribution. The one-tailed version of this test has a specific purpose i.e .to test whether values of one population are larger than values of another population. Similar to one-sample test, cumulative distributions are compared, but here two sample distributions are compared instead of a sample distribution and a theoretical distribution as we saw above. For the two-tailed version of the test, the test statistic (d) is the largest absolute deviation between the two observed cumulative step functions, irrespective of the direction of the difference.\u003c/p\u003e\n\n\u003cblockquote\u003e\n\u003cp\u003eThe null hypothesis states for this test that there is no difference between the two distributions. The d-statistic is calculated in the same manner as we saw above.\u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=d%20=%20max%5Babs%5B%7BF_%7Bn1%7D(X)-F_%7Bn2%7D(X)%7D%5D%5D\"\u003e \u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003e\u003cp\u003en1 = Observations from first sample.\u003c/p\u003e\u003c/li\u003e\n\u003cli\u003e\u003cp\u003en2 = Observations from second sample.\u003c/p\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003eWhen the cumulative distribution shows large maximum deviation d, it is a reflection of the difference between the two sample distributions.\u003c/p\u003e\n\n\u003cp\u003eThe critical value of d for samples where n1=n2 and is ≤ 40, the K-S table for two sample case is used. When n1 and/or n2 \u0026gt; 40 then the K-S table for large samples of two-sample test should be used. The null hypothesis is accepted if the calculated value is less than the table value and vice-versa.\u003c/p\u003e\n\n\u003cp\u003eThus, the use of any of these nonparametric tests helps a researcher to test the significance of his results when the characteristics of the target population are unknown or no assumptions had been made about them.\u003c/p\u003e\n\n\u003ch3\u003eExample\u003c/h3\u003e\n\n\u003cp\u003eGiven two samples, test if their distributions are the same.\u003c/p\u003e\n\n\u003cp\u003eCompute the observed cumulative distribution functions of the two samples and compute their maximum difference.\n\u003ccode\u003e\nX : 1.2, 1.4, 1.9, 3.7, 4.4, 4.8, 9.7, 17.3, 21.1, 28.4\nY : 5.6, 6.5, 6.6, 6.9, 9.2, 10.4, 10.6, 19.3\n\u003c/code\u003e\u003c/p\u003e\n\n\u003cp\u003eWe sort the combined sample, in order to compute the empirical cdfs:\u003c/p\u003e\n\n\u003cp\u003ethe combined sample, in order to compute the\nempirical cdf’s:\n\u003ccode\u003e\n1.2 1.4 1.9 3.7 4.4 4.8 5.6 6.5 6.6 6.9 9.2 9.7 10.4 10.6 17.3 19.3 21.1 28.4\nFx 0.1 0.2 0.3 0.4 0.5 0.6 0.6 0.6 0.6 0.6 0.6 0.7 0.7  0.7  0.8  0.8  0.9  1.0\nFy 0.0 0.0 0.0 0.0 0.0 0.0 0.1 0.2 0.4 0.5 0.6 0.6 0.8  0.9  0.9  1.0  1.0  1.0\n\u003c/code\u003e  \u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-kolmogorov-smirnov-test/master/images/dist_2.png\" width=\"600\"\u003e\u003c/p\u003e\n\n\u003cp\u003eThe Kolmogorov-Smirnov statistic is again the maximum absolute difference of the two observed distribution functions. From the above image, and also by feeding above values in the given formula, we get \u003cstrong\u003ed = 0.6\u003c/strong\u003e.\u003c/p\u003e\n\n\u003cp\u003eFor two samples, the 95% critical value can be approximated by\nthe formula:\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=d(0.05)%20=%201.36%5Csqrt%7B1/n_1%20%2b%201/n_2%7D%20=%200.645\"\u003e \u003c/p\u003e\n\n\u003cp\u003eSince 0.6 \u0026lt; 0.645, we retain the null hypothesis in this case. \u003c/p\u003e\n\n\u003chr\u003e\n\n\u003cp\u003eKolmogorov-Smirnov tests have the advantages that:\n- the distribution of statistic does not depend on cumulative distribution function being tested and\u003cbr\u003e\n- the test is exact  \u003c/p\u003e\n\n\u003cp\u003eThey have the disadvantage that they are more sensitive to deviations near the center of the distribution than at the tails.\u003c/p\u003e\n\n\u003ch2\u003eSummary\u003c/h2\u003e\n\n\u003cp\u003eIn this lesson, we looked at K-S test and how this test can be used to test for normality assumptions. We also looked at a one-sample K-S test and a two-sample K-S test with simple examples. Next, we'll see how to implement these tests in Python. \u003c/p\u003e","frontPage":false},{"exportId":"maximum-a-posteriori-estimation-map-and-multinomial-bayes","title":"Maximum A Posteriori Estimation (MAP) and Multinomial Bayes","type":"WikiPage","content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-map-multinomial-bayes\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-map-multinomial-bayes\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-map-multinomial-bayes/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eMaximum A Posteriori provides a means for estimating a parameter given some prior knowledge about a variable. In it, one assumes a given distribution for the variable and then estimates the parameter itself given additional information. In this lesson, you'll see how Bayes' theorem can be applied in this manner and then extended to multivariate cases.\u003c/p\u003e\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\u003cp\u003eYou will be able to\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eIdentify how Maximum A Posteriori Estimation is related to MLE\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003eMaximum A Posteriori Estimation\u003c/h2\u003e\n\u003cp\u003eMaximum A Posteriori Estimation (MAP) is similar to Maximum Likelihood Estimation but extends this concept by allowing one to also account for prior beliefs regarding the distribution of the variable in question. Recall Bayes' theorem:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Clarge%20P(A%7CB)%20=%20%5Cdfrac%7BP(B%7CA)(A)%7D%7BP(B)%7D\"\u003e\u003c/p\u003e\n\u003cp\u003eThe Bayesian interpretation of this formula is\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Clarge%20%5Ctext%7BPosterior%7D%20=%20%5Cdfrac%7B%5Ctext%7BLikelihood%7D%20%5Ccdot%20%5Ctext%7BPrior%7D%7D%7B%5Ctext%7BEvidence%7D%7D\"\u003e\u003c/p\u003e\n\u003cp\u003eWith MAP, you then attempt to optimize a parameter \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Ctheta\"\u003e for the assumed distribution in order to maximize the posterior probability.\u003c/p\u003e\n\u003ch2\u003eMultinomial Bayes\u003c/h2\u003e\n\u003cp\u003eMultinomial Bayes also extends the notions within Bayes' theorem, allowing one to chain inferences. The primary assumption for this is assuming that your variables are independent of one another. Recall that if you assume two events A and B are independent of one another, then \u003cimg src=\"https://render.githubusercontent.com/render/math?math=P(A%20%20%5Ccap%20B)%20=%20P(A)%5Ccdot%20P(B)\"\u003e . Similarly, if independence is assumed when extending Bayes theorem to a multivariate case, one can multiply the successive probability estimates. Mathematically, this can be summarized as:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Clarge%20P(Y%7CX_1,%20X_2,...,X_n)%20=%20%5Cdfrac%7BP(X_1%7CY)%5Ccdot%20P(X_2%7CY)%20%5Ccdot%20...%20%5Ccdot%20P(X_n%7CY)%7D%7BP(X_1,%20X_2,...,X_n)%7DP(Y)\"\u003e\u003c/p\u003e\n\u003ch2\u003eSummary\u003c/h2\u003e\n\u003cp\u003eThis lesson briefly introduced the concept of Maximum A Posteriori Estimation and extending Bayes' theorem to multivariate cases. In later sections, you'll investigate these ideas in practice, working with practical examples and coding your own implementations to gain a full understanding.\u003c/p\u003e","frontPage":false}],"assignments":[{"exportId":"g28be896744baf3d0b8bbf77023ac9489","title":"AB Testing - Lab","type":"Assignment","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-ab-testing-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-ab-testing-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"g8ed2ab5bdad3f72d51c8431e73364c75","title":"ANOVA","type":"Assignment","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-anova\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-anova/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"gc23e46ac9f4a13cccc155fa867e94bfd","title":"ANOVA - Lab","type":"Assignment","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-anova-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-anova-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"g44807f40116a2dac1709e9858fd2b0e8","title":"Bayes' Theorem - Lab","type":"Assignment","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-bayes-theorem-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-bayes-theorem-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"g6dfc40ce560e87b7e40eb2bbb2120f41","title":"Bias-Variance Trade-Off","type":"Assignment","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-bias-variance-trade-off\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-bias-variance-trade-off/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"ga7e11e9e477ee98e6d60d1d83a3fc387","title":"Bias-Variance Trade-Off - Lab","type":"Assignment","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-bias-variance-trade-off-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-bias-variance-trade-off-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"g772df31476f2ba5011a59cdd20744c3e","title":"Central Limit Theorem - Lab","type":"Assignment","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-central-limit-theorem-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-central-limit-theorem-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"g73ff9c9089ebfa445f0c575d8bc968df","title":"Coefficient of Determination - Lab","type":"Assignment","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-coefficient-of-determination-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-coefficient-of-determination-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"g387b78b9456a87e090681645f7516dd3","title":"Combinations - Lab","type":"Assignment","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-combinations-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-combinations-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"gfe5927eda314970e86914220b484ccb5","title":"Combinatorics: Recursive Functions","type":"Assignment","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-recursive-functions\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-recursive-functions/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"g094337fc83281fdb9db56acd323423d3","title":"Combinatorics: Recursive Functions - Lab","type":"Assignment","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-recursive-functions-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-recursive-functions-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"g47df50e52a73ae6dd373534fff104399","title":"Conditional Probability - Lab","type":"Assignment","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-conditional-probability-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-conditional-probability-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"g3298b4089d96872f286c0b225a498364","title":"Conducting T-Tests","type":"Assignment","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-t-tests\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-t-tests/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"gcbaf0fc2f6b7b95100b633a509ccc15d","title":"Confidence Intervals - Lab","type":"Assignment","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-confidence-intervals-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-confidence-intervals-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"g4adbc7b98a3576855368b2984d52ff29","title":"Confidence Intervals with T Distribution","type":"Assignment","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-intervals-with-t-distribution\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-intervals-with-t-distribution/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"g66374fdc758e947e88c4136dd7ac5783","title":"Confidence Intervals with T Distribution - Lab","type":"Assignment","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-intervals-with-t-distribution-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-intervals-with-t-distribution-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"g874a611b267790693a85b226c8808c22","title":"Dealing with Categorical Variables","type":"Assignment","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-dealing-with-categorical-variables\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-dealing-with-categorical-variables/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"g665e30e6b9f11ba8729090e01e51d009","title":"Dealing with Categorical Variables - Lab","type":"Assignment","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-dealing-with-categorical-variables-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-dealing-with-categorical-variables-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"g21c730d27e96f5b0eb549e0b6c960a93","title":"Effect Sizes","type":"Assignment","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-effect-sizes\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-effect-sizes/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"g781cad4217d3a5a0addc52ebfce79fa8","title":"Effect Sizes, P-Values and Power - Lab","type":"Assignment","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-effect-sizes-pvalues-and-power-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-effect-sizes-pvalues-and-power-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"g80702b58e5a2701f993485d209807a05","title":"Exploring Your Data - Lab","type":"Assignment","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-exploring-your-data-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-exploring-your-data-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"ga35eeb5c62e241ff99a9f842c2ea91e3","title":"Feature Scaling and Normalization","type":"Assignment","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-feature-scaling-and-normalization\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-feature-scaling-and-normalization/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"g32c3a1cee302f611d0acce7ed2939261","title":"Feature Scaling and Normalization - Lab","type":"Assignment","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-feature-scaling-and-normalization-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-feature-scaling-and-normalization-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"ge931231c25dfbfc69c2f3ce1900b6d1f","title":"In-Depth AB Testing - Lab","type":"Assignment","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-in-depth-ab-testing-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-in-depth-ab-testing-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"ge0e6a2f8332e742c665ecf4969e3e726","title":"Interactions","type":"Assignment","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-interaction-terms\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-interaction-terms/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"ge1b7089d3bbe8eecb4409c0c95230685","title":"Interactions - Lab","type":"Assignment","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-interaction-terms-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-interaction-terms-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"ga10f4e8c856a1c594cfff0c208b01c0d","title":"Interpreting Significance and P-value","type":"Assignment","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-significance-p-value\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-significance-p-value/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"g29f11839e9d9e36b6bfa67ca158f1526","title":"Introduction to Cross Validation","type":"Assignment","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-cross-validation\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-cross-validation/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"g84feb128ef3de817727980976b3b8d85","title":"Introduction to Cross Validation - Lab","type":"Assignment","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-cross-validation-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-cross-validation-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"g6afe51ee2d4b8c4884a7c2e6046d3f6e","title":"Introduction to Probability - Lab","type":"Assignment","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-intro-to-probability-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-intro-to-probability-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"gb20db705208d830723544a65c459ce36","title":"Introduction to Sampling","type":"Assignment","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-introduction-to-sampling\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-introduction-to-sampling/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"g3d063dab473e0613a7266642ae03008c","title":"Introduction to Sets - Lab","type":"Assignment","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-intro-to-sets-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-intro-to-sets-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"g4c55235a2e51179b89131552c3d353f2","title":"Log Transformations","type":"Assignment","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-log-transformations\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-log-transformations/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"gaff47b4cc6ce1b5b1ce7c5cc86fbce5f","title":"Maximum Likelihood Estimation (MLE)","type":"Assignment","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-mle\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-mle/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"g7e46b1486aeff9c07785f90aa4d92e16","title":"Model Fit in Linear Regression","type":"Assignment","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-model-fit-linear-regression\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-model-fit-linear-regression/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"g5a1d9ef3651d544423ce6621ba391da1","title":"Model Fit in Linear Regression - Lab","type":"Assignment","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-model-fit-linear-regression-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-model-fit-linear-regression-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"g58055dbb2b7e02e813fa0517692153b1","title":"Modeling Your Data","type":"Assignment","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-modeling-your-data\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-modeling-your-data/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"ga55c6aaa21ffa8bde4ea2fa5aeb37635","title":"Modeling Your Data  - Lab","type":"Assignment","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-modeling-your-data-lab/\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-modeling-your-data-lab//issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"ge40df0c9a4b0b6948941d3698f093313","title":"Multicollinearity of Features","type":"Assignment","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-multicollinearity-of-features\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-multicollinearity-of-features/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"g71a9102709a1463e24984220a70d1669","title":"Multicollinearity of Features - Lab","type":"Assignment","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-multicollinearity-of-features-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-multicollinearity-of-features-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"g2f97b5f6108903e2f1a7fe06a5926237","title":"Multiple Linear Regression","type":"Assignment","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-multiple-linear-regression\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-multiple-linear-regression/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"g8286958c43488707cfd86679298362b1","title":"Multiple Linear Regression in Statsmodels","type":"Assignment","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-multiple-linear-regression-in-statsmodels\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-multiple-linear-regression-in-statsmodels/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"gd4eeb2eb95af559b48aee09488984386","title":"Multiple Linear Regression in Statsmodels - Lab","type":"Assignment","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-multiple-linear-regression-statsmodels-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-multiple-linear-regression-statsmodels-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"gba5530562764eb91a21faad9d3b45296","title":"Obtaining Your Data","type":"Assignment","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-obtaining-your-data\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-obtaining-your-data/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"g555884b43af918f3d3eff9f45854edee","title":"Obtaining Your Data - Lab","type":"Assignment","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-obtaining-your-data-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-obtaining-your-data-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"ge45fca72ae6e9e8c0c5e9a78de3c967e","title":"One Sample T-Test - Lab","type":"Assignment","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-one-sample-t-tests-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-one-sample-t-tests-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"g74555cddbb298dd685adf8df41b13252","title":"One-Sample z-test","type":"Assignment","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-one-sample-z-test\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-one-sample-z-test/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"gc78e74e59178dffd9fe23bcd5aaed136","title":"One-Sample z-test - Lab","type":"Assignment","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-one-sample-z-test-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-one-sample-z-test-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"g3774a4758e51b40c8feea81adcc91186","title":"Ordinary Least Squares in Statsmodels (OLS)","type":"Assignment","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-ols-statsmodels\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-ols-statsmodels/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"g028613cbe5404c155b858f6515fff60f","title":"Ordinary Least Squares in Statsmodels (OLS) - Lab","type":"Assignment","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-ols-statsmodels-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-ols-statsmodels-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"g70e33c5b44b83638374c02ed86c54bab","title":"Partitioning and Law of Total Probability - Lab","type":"Assignment","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-law-of-total-probability-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-law-of-total-probability-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"ga5e233748c4f8ff678a69b36af55a7f3","title":"Permutations and Factorials - Lab","type":"Assignment","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-permutations-and-factorials-lab-v2-1\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-permutations-and-factorials-lab-v2-1/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"g339954613039d123828de5cde75c311b","title":"Phase 2 Blog Post","type":"Assignment","content":"\u003cp\u003e\u003cspan\u003ePlease put the URL to your Phase 2 Blog Post here. \u003c/span\u003e\u003cspan\u003eRefer to the \u003c/span\u003e\u003ca title=\"Blogging Overview\" href=\"pages/blogging-overview\"\u003eBlogging Overview\u003c/a\u003e\u003cspan\u003e to learn about how to write good blog posts that\u003c/span\u003e\u003cspan style=\"font-family: inherit; font-size: 1rem;\"\u003e meet Flatiron School’s requirements.\u003c/span\u003e\u003c/p\u003e","submissionTypes":"a website url","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"g15af1c0230e37550fe879a92a9a300e1","title":"Phase 2 Project","type":"Assignment","content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-phase-2-project\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-phase-2-project\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-phase-2-project/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\u003cp\u003eAnother module down--you're almost half way there!\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-phase-2-project-campus/master/halfway-there.gif\" alt=\"awesome\"\u003e\u003c/p\u003e\n\n\u003cp\u003eAll that remains in Phase 2 is to put our newfound data science skills to use with a large project! This project should take 20 to 30 hours to complete.\u003c/p\u003e\n\n\u003ch2\u003eProject Overview\u003c/h2\u003e\n\n\u003cp\u003eFor this project, you will use regression modeling to analyze house sales in a northwestern county.\u003c/p\u003e\n\n\u003ch3\u003eThe Data\u003c/h3\u003e\n\n\u003cp\u003eThis project uses the King County House Sales dataset, which can be found in  \u003ccode\u003ekc_house_data.csv\u003c/code\u003e in the data folder in this repo. The description of the column names can be found in \u003ccode\u003ecolumn_names.md\u003c/code\u003e in the same folder. As with most real world data sets, the column names are not perfectly described, so you'll have to do some research or use your best judgment if you have questions about what the data means.\u003c/p\u003e\n\n\u003cp\u003eIt is up to you to decide what data from this dataset to use and how to use it. If you are feeling overwhelmed or behind, we recommend you ignore some or all of the following features:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003edate\u003c/li\u003e\n\u003cli\u003eview\u003c/li\u003e\n\u003cli\u003esqft_above\u003c/li\u003e\n\u003cli\u003esqft_basement\u003c/li\u003e\n\u003cli\u003eyr_renovated\u003c/li\u003e\n\u003cli\u003ezipcode\u003c/li\u003e\n\u003cli\u003elat\u003c/li\u003e\n\u003cli\u003elong\u003c/li\u003e\n\u003cli\u003esqft_living15\u003c/li\u003e\n\u003cli\u003esqft_lot15\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch3\u003eBusiness Problem\u003c/h3\u003e\n\n\u003cp\u003eIt is up to you to define a stakeholder and business problem appropriate to this dataset.\u003c/p\u003e\n\n\u003cp\u003eIf you are struggling to define a stakeholder, we recommend you complete a project for a real estate agency that helps homeowners buy and/or sell homes. A business problem you could focus on for this stakeholder is the need to provide advice to homeowners about how home renovations might increase the estimated value of their homes, and by what amount.\u003c/p\u003e\n\n\u003ch2\u003eDeliverables\u003c/h2\u003e\n\n\u003cp\u003eThere are three deliverables for this project:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eA \u003cstrong\u003eGitHub repository\u003c/strong\u003e\n\u003c/li\u003e\n\u003cli\u003eA \u003cstrong\u003eJupyter Notebook\u003c/strong\u003e\n\u003c/li\u003e\n\u003cli\u003eA \u003cstrong\u003enon-technical presentation\u003c/strong\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003eReview the \"Project Submission \u0026amp; Review\" page in the \"Milestones Instructions\" topic for instructions on creating and submitting your deliverables. Refer to the rubric associated with this assignment for specifications describing high-quality deliverables.\u003c/p\u003e\n\n\u003ch3\u003eKey Points\u003c/h3\u003e\n\n\u003cul\u003e\n\u003cli\u003e\u003cp\u003e\u003cstrong\u003eYour deliverables should explicitly address each step of the data science process.\u003c/strong\u003e Refer to \u003ca href=\"https://github.com/learn-co-curriculum/dsc-data-science-processes\"\u003ethe Data Science Process lesson\u003c/a\u003e from Topic 19 for more information about process models you can use.\u003c/p\u003e\u003c/li\u003e\n\u003cli\u003e\u003cp\u003e\u003cstrong\u003eYour Jupyter Notebook should demonstrate an iterative approach to modeling.\u003c/strong\u003e This means that you begin with a basic model, evaluate it, and then provide justification for and proceed to a new model. After you finish refining your models, you should provide 1-3 paragraphs discussing your final model - this should include interpreting at least 3 important parameter estimates or statistics.\u003c/p\u003e\u003c/li\u003e\n\u003cli\u003e\u003cp\u003e\u003cstrong\u003eBased on the results of your models, your notebook and presentation should discuss at least two features that have strong relationships with housing prices.\u003c/strong\u003e\u003c/p\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch2\u003eGetting Started\u003c/h2\u003e\n\n\u003cp\u003eStart on this project by forking and cloning \u003ca href=\"https://github.com/learn-co-curriculum/dsc-phase-2-project\"\u003ethis project repository\u003c/a\u003e to get a local copy of the dataset.\u003c/p\u003e\n\n\u003cp\u003eWe recommend structuring your project repository similar to the structure in \u003ca href=\"https://github.com/learn-co-curriculum/dsc-project-template\"\u003ethe Phase 1 Project Template\u003c/a\u003e. You can do this either by creating a new fork of that repository to work in or by building a new repository from scratch that mimics that structure.\u003c/p\u003e\n\n\u003ch2\u003eProject Submission and Review\u003c/h2\u003e\n\n\u003cp\u003eReview the \"Project Submission \u0026amp; Review\" page in the \"Milestones Instructions\" topic to learn how to submit your project and how it will be reviewed. Your project must pass review for you to progress to the next Phase.\u003c/p\u003e\n\n\u003ch2\u003eSummary\u003c/h2\u003e\n\n\u003cp\u003eThis project will give you a valuable opportunity to develop your data science skills using real-world data. The end-of-phase projects are a critical part of the program because they give you a chance to bring together all the skills you've learned, apply them to realistic projects for a business stakeholder, practice communication skills, and get feedback to help you improve. You've got this!\u003c/p\u003e","submissionTypes":"a file upload","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"ge7423c89c672ad02c9cbc487b4bf235c","title":"Phase 2 Project - GitHub Repository URL","type":"Assignment","content":"\u003cp\u003e\u003cspan\u003ePlease put the URL to your Phase 2 Project GitHub Repository here.\u0026nbsp;\u003c/span\u003e\u003c/p\u003e","submissionTypes":"a website url","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"g0617762981782b8a7e4f723eb234d619","title":"Pickle","type":"Assignment","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-pickle\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-pickle/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"g8db93d4be8ec62a0c5862a645f26d3d3","title":"Polynomial Regression","type":"Assignment","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-polynomial-regression\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-polynomial-regression/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"g8b845b3e7563955e624cdd0ef4e66247","title":"Polynomial Regression - Lab","type":"Assignment","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-polynomial-regression-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-polynomial-regression-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"g61b7bfd28c078fbc8422990ab3741476","title":"Project - Regression Modeling with the Ames Housing Dataset","type":"Assignment","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-regression-boston-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-regression-boston-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"geae254d203d24cfab6b3671af2eabc10","title":"Regression Diagnostics in Statsmodels","type":"Assignment","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-ols-regression-diagnostics\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-ols-regression-diagnostics/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"g9a20f43ba3b9ff0723e1dad01d996ec8","title":"Regression - Lab","type":"Assignment","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-complete-regression-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-complete-regression-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"gcb0d6bd7ca78851d6617cf7e0f9335ed","title":"Regression Model Validation","type":"Assignment","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-regression-model-validation\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-regression-model-validation/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"gf8f65d322983cab458b839481a43381e","title":"Regression Model Validation - Lab","type":"Assignment","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-regression-model-validation-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-regression-model-validation-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"g748e82eafe8c9e20fcc845d0c3d2ca89","title":"Resampling Methods - Lab","type":"Assignment","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-resampling-methods-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-resampling-methods-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"g8c8215de715edad3a378b8c8c9b4cbdb","title":"(Re)sampling: Monte Carlo Simulations","type":"Assignment","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-monte-carlo-simulations\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-monte-carlo-simulations/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"gb0f67c593ab19062e27ee4db01ca929f","title":"(Re)sampling: Monte Carlo Simulations - Lab","type":"Assignment","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-monte-carlo-simulations-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-monte-carlo-simulations-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"gf1cd3018d0cc0e9bf45afe0606c00209","title":"Sampling Statistics - Lab","type":"Assignment","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-sampling-statistics-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-sampling-statistics-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"g1368b3f1261b26d9ea0ff9bd53be9125","title":"Scrubbing and Cleaning Data - Lab","type":"Assignment","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-scrubbing-and-cleaning-data-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-scrubbing-and-cleaning-data-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"gb3589801b82b91898ef6a347827897ed","title":"Simple Linear Regression - Lab","type":"Assignment","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-simple-linear-regression-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-simple-linear-regression-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"gd2e2784c49861970728fbbe6b0ff3e0f","title":"Simulations with Conditional and Total Probability - Lab","type":"Assignment","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-probability-simulations-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-probability-simulations-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"g2f104a36fbe644d161e0a1f64478943c","title":"Skewness and Kurtosis - Lab","type":"Assignment","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-skewness-and-kurtosis-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-skewness-and-kurtosis-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"g1ccea933627c2a879f1ff80b34214092","title":"Statistical Power","type":"Assignment","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-statistical-power\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-statistical-power/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"ga57a09de707fa13679dc18747e1d3659","title":"Statistical Power - Lab","type":"Assignment","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-statistical-power-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-statistical-power-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"gd01d4550bbe8406ccadd2473523f73cc","title":"Statistical Testing with z-score and p-value","type":"Assignment","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-z-score-p-value\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-z-score-p-value/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"gff37f5d2e456c53e4445862ab4bfae30","title":"The Bernoulli and Binomial Distribution","type":"Assignment","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-bernoulli-and-binomial-distribution\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-bernoulli-and-binomial-distribution/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"g472db7170ed5b59b47fccb9c6e72cb77","title":"The Bernoulli and Binomial Distribution - Lab","type":"Assignment","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-bernoulli-and-binomial-distribution-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-bernoulli-and-binomial-distribution-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"ga16ad827e78e8d7ff5dd65bce2bd7079","title":"The Cumulative Distribution Function - Lab","type":"Assignment","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-cumulative-distribution-function-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-cumulative-distribution-function-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"gefa254c77133f56abb94724c82340ff2","title":"The Exponential Distribution","type":"Assignment","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-exponential-distribution\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-exponential-distribution/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"g2e8f25bd5db6c49088c8d3cf1f632b63","title":"The Exponential Distribution - Lab","type":"Assignment","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-exponential-distribution-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-exponential-distribution-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"g7703a422072f9b8d04c9bb2dd5c72fcb","title":"The Kolmogorov-Smirnov Test - Lab","type":"Assignment","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-komogorov-smirnov-test-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-komogorov-smirnov-test-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"g8f9ca8f2dd8d6d35eb5444425cda3263","title":"The Monty Hall Problem - Lab","type":"Assignment","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-monty-hall-problem-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-monty-hall-problem-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"g53fed3bbd7b332c89a39edbfeaff06b2","title":"The Normal Distribution","type":"Assignment","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-normal-distribution\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-normal-distribution/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"g4a9042ed5eaea96c3b4f394e59cba58e","title":"The Normal Distribution - Lab","type":"Assignment","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-normal-distribution-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-normal-distribution-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"g6f09a05dcb3184c71a23b39ff743efd0","title":"The Poisson Distribution - Lab","type":"Assignment","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-poisson-distribution-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-poisson-distribution-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"gbeaed92c2bba3b0fc174fb0883710cc6","title":"The Probability Density Function","type":"Assignment","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-probability-density-function\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-probability-density-function/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"g1c2cb0d718cb1c863428626c9535eb85","title":"The Probability Density Function - Lab","type":"Assignment","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/probability-density-functions-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/probability-density-functions-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"g51a8033327123f706ed2c1cd895fdb6f","title":"The Probability Mass Function","type":"Assignment","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-probability-mass-function\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-probability-mass-function/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"gdc0dbd661ca4318278fdc48bdbf533c2","title":"The Probability Mass Function - Lab","type":"Assignment","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-probability-mass-function-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-probability-mass-function-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"g100cabc871d1dd3cccf782278b282cd0","title":"The Standard Normal Distribution","type":"Assignment","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-standard-normal-distribution\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-standard-normal-distribution/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"g80e4421181c9727c6d13f6eacf53ff6d","title":"The Standard Normal Distribution - Lab","type":"Assignment","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-standard-normal-distribution-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-standard-normal-distribution-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"gaf3b5206bf114d937bf087ec04fb392c","title":"Two Sample T-Test - Lab","type":"Assignment","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-two-sample-t-tests-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-two-sample-t-tests-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"gfc3449d1f87be596feb9b0393c970364","title":"Type 1 and Type 2 Errors","type":"Assignment","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-type-1-and-2-error\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-type-1-and-2-error/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"gd7e932904b8d0a121a5d7fd3e8edaa31","title":"Type 1 and Type 2 Errors - Lab","type":"Assignment","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-type-1-and-2-error-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-type-1-and-2-error-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"g7472e73bf23577eb778e4c03765a1aef","title":"Website AB Testing - Lab","type":"Assignment","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-website-ab-testing-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-website-ab-testing-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"g38a0b69cae8a3d7ca7a04ffaab15eb71","title":"Welch's T-Test - Lab","type":"Assignment","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-welchs-ttest-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-welchs-ttest-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null}],"discussion_topics":[],"quizzes":[],"files":null}